[{"body":"Pre-requisites for installation of the CSI Drivers On Upstream Kubernetes clusters, ensure that to install\n VolumeSnapshot CRDs - Install v1 VolumeSnapshot CRDs External Volume Snapshot Controller  Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v5.0.1   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller.  Installing CSI Driver via Operator Refer PowerScale Driver to install the driver via Operator\n Note: If you are using an OLM based installation, example manifests are available in OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\n Verifying the driver installation Once the driver Custom Resource (CR) is created, you can verify the installation as mentioned below\n Check if ContainerStorageModule CR is created successfully using the command below: $ kubectl get csm/\u003cname-of-custom-resource\u003e -n \u003cdriver-namespace\u003e -o yaml  Check the status of the CR to verify if the driver installation is in the Succeeded state. If the status is not Succeeded, see the Troubleshooting guide for more information.  Update CSI Drivers The CSI Drivers and CSM Modules installed by the Dell CSM Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include:\n Modifying the installation directly via kubectl edit For example - If the name of the installed PowerScale driver is powerscale, then run # Replace driver-namespace with the namespace where the PowerScale driver is installed $ kubectl edit csm/powerscale -n \u003cdriver-namespace\u003e and modify the installation\n Modify the API object in-place via kubectl patch  Supported modifications  Changing environment variable values for driver Updating the image of the driver Upgrading the driver version  NOTES:\n If you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required. driver:configVersion:v2.3.0 Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.  Uninstall CSI Driver The CSI Drivers and CSM Modules can be uninstalled by deleting the Custom Resource.\nFor e.g.\n$ kubectl delete csm/powerscale -n \u003cdriver-namespace\u003e By default, the forceRemoveDriver option is set to true which will uninstall the CSI Driver and CSM Modules when the Custom Resource is deleted. Setting this option to false is not recommended.\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. The only exception to this is modifications requested by the documentation, for example, filling in blank IPs or other such system-specific data. Any modifications not specifically requested by the documentation should be only done after consulting with Dell support.\n","excerpt":"Pre-requisites for installation of the CSI Drivers On Upstream …","ref":"/csm-docs/docs/deployment/csmoperator/drivers/","title":"CSI Drivers"},{"body":"","excerpt":"","ref":"/csm-docs/v1/deployment/csmoperator/drivers/","title":"CSI Drivers"},{"body":"","excerpt":"","ref":"/csm-docs/v2/deployment/csmoperator/drivers/","title":"CSI Drivers"},{"body":" The CSM Installer is currently deprecated and will no longer be supported as of CSM v1.4.0\n   Note: The CSM Installer only supports installation of CSM 1.0 Modules and CSI Drivers in environments that do not have any existing deployments of CSM or CSI Drivers. The CSM Installer does not support the upgrade of existing CSM or CSI Driver deployments.\n  The CSM (Container Storage Modules) Installer simplifies the deployment and management of Dell Container Storage Modules and CSI Drivers to provide persistent storage for your containerized workloads.\nCSM Installer Supported Modules and Dell CSI Drivers    Modules/Drivers CSM 1.0     Authorization 1.0   Observability 1.0   Replication 1.0   Resiliency 1.0   CSI Driver for PowerScale v2.0   CSI Driver for Unity XT v2.0   CSI Driver for PowerStore v2.0   CSI Driver for PowerFlex v2.0   CSI Driver for PowerMax v2.0    The CSM Installer must first be deployed in a Kubernetes environment using Helm. After which, the CSM Installer can be used through the following interfaces:\n CSM CLI REST API  How to Deploy the Container Storage Modules Installer  Add the dell helm repository:  helm repo add dell https://dell.github.io/helm-charts If securing the API service and database, following steps 2 to 4 to generate the certificates, or skip to step 5 to deploy without certificates\nGenerate self-signed certificates using the following commands:  mkdir api-certs openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout api-certs/ca.key \\ -x509 -days 365 -out api-certs/ca.crt -subj '/' openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout api-certs/cert.key \\ -out api-certs/cert.csr -subj '/' openssl x509 -req -days 365 -in api-certs/cert.csr -CA api-certs/ca.crt \\ -CAkey api-certs/ca.key -CAcreateserial -out api-certs/cert.crt If required, download the cockroach binary used to generate certificates for the cockroach-db:  curl https://binaries.cockroachdb.com/cockroach-v21.1.8.linux-amd64.tgz | tar -xz \u0026\u0026 sudo cp -i cockroach-v21.1.8.linux-amd64/cockroach /usr/local/bin/ Generate the certificates required for the cockroach-db service:  mkdir db-certs cockroach cert create-ca --certs-dir=db-certs --ca-key=db-certs/ca.key cockroach cert create-node cockroachdb-0.cockroachdb.csm-installer.svc.cluster.local cockroachdb-public cockroachdb-0.cockroachdb --certs-dir=db-certs/ --ca-key=db-certs/ca.key In case multiple instances of cockroachdb are required add all nodes names while creating nodes on the certificates\ncockroach cert create-node cockroachdb-0.cockroachdb.csm-installer.svc.cluster.local cockroachdb-1.cockroachdb.csm-installer.svc.cluster.local cockroachdb-2.cockroachdb.csm-installer.svc.cluster.local cockroachdb-public cockroachdb-0.cockroachdb cockroachdb-1.cockroachdb cockroachdb-2.cockroachdb --certs-dir=db-certs/ --ca-key=db-certs/ca.key cockroach cert create-client root --certs-dir=db-certs/ --ca-key=db-certs/ca.key cockroach cert list --certs-dir=db-certs/ Create a values.yaml file that contains JWT, Cipher key, and Admin username and password of CSM Installer that are required by the installer during helm installation. See the Configuration section for other values that can be set during helm installation.   Note: jwtKey will be used as a shared secret in HMAC algorithm for generating jwt token, cipherKey will be used as a symmetric key in AES cipher for encryption of storage system credentials. Those parameters are arbitrary, and you can set them to whatever you like. Just ensure that cipherKey is exactly 32 characters long.\n # string of any length jwtKey: # string of exactly 32 characters cipherKey: \"\" # Admin username of CSM Installer adminUserName: # Admin password of CSM Installer adminPassword: Follow step a if certificates are being used or step b if certificates are not being used:  a) Install the helm chart, specifying the certificates generated in the previous steps:\nhelm install -n csm-installer --create-namespace \\ --set-file serviceCertificate=api-certs/cert.crt \\ --set-file servicePrivateKey=api-certs/cert.key \\ --set-file databaseCertificate=db-certs/node.crt \\ --set-file databasePrivateKey=db-certs/node.key \\ --set-file dbClientCertificate=db-certs/client.root.crt \\ --set-file dbClientPrivateKey=db-certs/client.root.key \\ --set-file caCrt=db-certs/ca.crt \\ -f values.yaml \\ csm-installer dell/csm-installer b) If not deploying with certificates, execute the following command:\nhelm install -n csm-installer --create-namespace \\ --set-string scheme=http \\ --set-string dbSSLEnabled=\"false\" \\ -f values.yaml \\ csm-installer dell/csm-installer  Note: In an OpenShift environment, the cockroachdb StatefulSet will run privileged pods so that it can mount the Persistent Volume used for storage. Follow the documentation for your OpenShift version to enable privileged pods.\n Configuration    Parameter Description Default     csmInstallerCount Number of replicas for the CSM Installer Deployment 1   dbInstanceCount Number of replicas for the CSM Database StatefulSet 2   imagePullPolicy Image pull policy for the CSM Installer images Always   host Host or IP that will be used to bind to the CSM Installer API service 0.0.0.0   port Port that will be used to bind to the CSM Installer API service 8080   scheme Scheme used for the CSM Installer API service. Valid values are https and http https   jwtKey Key used to sign the JWT token    cipherKey Key used to encrypt/decrypt user and storage system credentials. Must be 32 characters in length.    logLevel Log level used for the CSM Installer. Valid values are DEBUG, INFO, WARN, ERROR, and FATAL INFO   dbHost Host name of the Cockroach DB instance cockroachdb-public   dbPort Port number to access the Cockroach DB instance 26257   dbSSLEnabled Enable SSL for the Cockroach DB connectiong true   installerImage Location of the CSM Installer Docker Image dellemc/dell-csm-installer:v1.0.0   dataCollectorImage Location of the CSM Data Collector Docker Image dellemc/csm-data-collector:v1.0.0   adminUserName Username to authenticate with the CSM Installer    adminPassword Password to authenticate with the CSM Installer    dbVolumeDirectory Directory on the worker node to use for the Persistent Volume /var/lib/cockroachdb   api_server_ip If using Swagger, set to public IP or host of the CSM Installer API service localhost    How to Upgrade the Container Storage Modules Installer When a new version of the CSM Installer helm chart is available, the following steps can be used to upgrade to the latest version.\n Note: Upgrading the CSM Installer does not upgrade the Dell CSI Drivers or modules that were previously deployed with the installer. The CSM Installer does not support upgrading of the Dell CSI Drivers or modules. The Dell CSI Drivers and modules must be deleted and re-deployed using the latest CSM Installer in order to get the most recent version of the Dell CSI Driver and modules.\n  Update the helm repository.  helm repo update Follow step a if certificates were used during the initial installation of the helm chart or step b if certificates were not used:  a) Upgrade the helm chart, specifying the certificates used during initial installation:\nhelm upgrade -n csm-installer \\ --set-file serviceCertificate=api-certs/cert.crt \\ --set-file servicePrivateKey=api-certs/cert.key \\ --set-file databaseCertificate=db-certs/node.crt \\ --set-file databasePrivateKey=db-certs/node.key \\ --set-file dbClientCertificate=db-certs/client.root.crt \\ --set-file dbClientPrivateKey=db-certs/client.root.key \\ --set-file caCrt=db-certs/ca.crt \\ -f values.yaml \\ csm-installer dell/csm-installer b) If not deploying with certificates, execute the following command:\nhelm upgrade -n csm-installer \\ --set-string scheme=http \\ --set-string dbSSLEnabled=\"false\" \\ -f values.yaml \\ csm-installer dell/csm-installer How to Uninstall the Container Storage Modules Installer  Delete the Helm chart  helm delete -n csm-installer csm-installer ","excerpt":" The CSM Installer is currently deprecated and will no longer be …","ref":"/csm-docs/docs/deployment/csminstaller/","title":"CSM Installer"},{"body":" The CSM Installer is currently deprecated and will no longer be supported as of CSM v1.4.0\n   Note: The CSM Installer only supports installation of CSM 1.0 Modules and CSI Drivers in environments that do not have any existing deployments of CSM or CSI Drivers. The CSM Installer does not support the upgrade of existing CSM or CSI Driver deployments.\n  The CSM (Container Storage Modules) Installer simplifies the deployment and management of Dell Container Storage Modules and CSI Drivers to provide persistent storage for your containerized workloads.\nCSM Installer Supported Modules and Dell CSI Drivers    Modules/Drivers CSM 1.0     Authorization 1.0   Observability 1.0   Replication 1.0   Resiliency 1.0   CSI Driver for PowerScale v2.0   CSI Driver for Unity v2.0   CSI Driver for PowerStore v2.0   CSI Driver for PowerFlex v2.0   CSI Driver for PowerMax v2.0    The CSM Installer must first be deployed in a Kubernetes environment using Helm. After which, the CSM Installer can be used through the following interfaces:\n CSM CLI REST API  How to Deploy the Container Storage Modules Installer  Add the dell helm repository:  helm repo add dell https://dell.github.io/helm-charts If securing the API service and database, following steps 2 to 4 to generate the certificates, or skip to step 5 to deploy without certificates\nGenerate self-signed certificates using the following commands:  mkdir api-certs openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout api-certs/ca.key \\ -x509 -days 365 -out api-certs/ca.crt -subj '/' openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout api-certs/cert.key \\ -out api-certs/cert.csr -subj '/' openssl x509 -req -days 365 -in api-certs/cert.csr -CA api-certs/ca.crt \\ -CAkey api-certs/ca.key -CAcreateserial -out api-certs/cert.crt If required, download the cockroach binary used to generate certificates for the cockroach-db:  curl https://binaries.cockroachdb.com/cockroach-v21.1.8.linux-amd64.tgz | tar -xz \u0026\u0026 sudo cp -i cockroach-v21.1.8.linux-amd64/cockroach /usr/local/bin/ Generate the certificates required for the cockroach-db service:  mkdir db-certs cockroach cert create-ca --certs-dir=db-certs --ca-key=db-certs/ca.key cockroach cert create-node cockroachdb-0.cockroachdb.csm-installer.svc.cluster.local cockroachdb-public cockroachdb-0.cockroachdb --certs-dir=db-certs/ --ca-key=db-certs/ca.key In case multiple instances of cockroachdb are required add all nodes names while creating nodes on the certificates\ncockroach cert create-node cockroachdb-0.cockroachdb.csm-installer.svc.cluster.local cockroachdb-1.cockroachdb.csm-installer.svc.cluster.local cockroachdb-2.cockroachdb.csm-installer.svc.cluster.local cockroachdb-public cockroachdb-0.cockroachdb cockroachdb-1.cockroachdb cockroachdb-2.cockroachdb --certs-dir=db-certs/ --ca-key=db-certs/ca.key cockroach cert create-client root --certs-dir=db-certs/ --ca-key=db-certs/ca.key cockroach cert list --certs-dir=db-certs/ Create a values.yaml file that contains JWT, Cipher key, and Admin username and password of CSM Installer that are required by the installer during helm installation. See the Configuration section for other values that can be set during helm installation.   Note: jwtKey will be used as a shared secret in HMAC algorithm for generating jwt token, cipherKey will be used as a symmetric key in AES cipher for encryption of storage system credentials. Those parameters are arbitrary, and you can set them to whatever you like. Just ensure that cipherKey is exactly 32 characters long.\n # string of any length jwtKey: # string of exactly 32 characters cipherKey: \"\" # Admin username of CSM Installer adminUserName: # Admin password of CSM Installer adminPassword: Follow step a if certificates are being used or step b if certificates are not being used:  a) Install the helm chart, specifying the certificates generated in the previous steps:\nhelm install -n csm-installer --create-namespace \\ --set-file serviceCertificate=api-certs/cert.crt \\ --set-file servicePrivateKey=api-certs/cert.key \\ --set-file databaseCertificate=db-certs/node.crt \\ --set-file databasePrivateKey=db-certs/node.key \\ --set-file dbClientCertificate=db-certs/client.root.crt \\ --set-file dbClientPrivateKey=db-certs/client.root.key \\ --set-file caCrt=db-certs/ca.crt \\ -f values.yaml \\ csm-installer dell/csm-installer b) If not deploying with certificates, execute the following command:\nhelm install -n csm-installer --create-namespace \\ --set-string scheme=http \\ --set-string dbSSLEnabled=\"false\" \\ -f values.yaml \\ csm-installer dell/csm-installer  Note: In an OpenShift environment, the cockroachdb StatefulSet will run privileged pods so that it can mount the Persistent Volume used for storage. Follow the documentation for your OpenShift version to enable privileged pods.\n Configuration    Parameter Description Default     csmInstallerCount Number of replicas for the CSM Installer Deployment 1   dbInstanceCount Number of replicas for the CSM Database StatefulSet 2   imagePullPolicy Image pull policy for the CSM Installer images Always   host Host or IP that will be used to bind to the CSM Installer API service 0.0.0.0   port Port that will be used to bind to the CSM Installer API service 8080   scheme Scheme used for the CSM Installer API service. Valid values are https and http https   jwtKey Key used to sign the JWT token    cipherKey Key used to encrypt/decrypt user and storage system credentials. Must be 32 characters in length.    logLevel Log level used for the CSM Installer. Valid values are DEBUG, INFO, WARN, ERROR, and FATAL INFO   dbHost Host name of the Cockroach DB instance cockroachdb-public   dbPort Port number to access the Cockroach DB instance 26257   dbSSLEnabled Enable SSL for the Cockroach DB connectiong true   installerImage Location of the CSM Installer Docker Image dellemc/dell-csm-installer:v1.0.0   dataCollectorImage Location of the CSM Data Collector Docker Image dellemc/csm-data-collector:v1.0.0   adminUserName Username to authenticate with the CSM Installer    adminPassword Password to authenticate with the CSM Installer    dbVolumeDirectory Directory on the worker node to use for the Persistent Volume /var/lib/cockroachdb   api_server_ip If using Swagger, set to public IP or host of the CSM Installer API service localhost    How to Upgrade the Container Storage Modules Installer When a new version of the CSM Installer helm chart is available, the following steps can be used to upgrade to the latest version.\n Note: Upgrading the CSM Installer does not upgrade the Dell CSI Drivers or modules that were previously deployed with the installer. The CSM Installer does not support upgrading of the Dell CSI Drivers or modules. The Dell CSI Drivers and modules must be deleted and re-deployed using the latest CSM Installer in order to get the most recent version of the Dell CSI Driver and modules.\n  Update the helm repository.  helm repo update Follow step a if certificates were used during the initial installation of the helm chart or step b if certificates were not used:  a) Upgrade the helm chart, specifying the certificates used during initial installation:\nhelm upgrade -n csm-installer \\ --set-file serviceCertificate=api-certs/cert.crt \\ --set-file servicePrivateKey=api-certs/cert.key \\ --set-file databaseCertificate=db-certs/node.crt \\ --set-file databasePrivateKey=db-certs/node.key \\ --set-file dbClientCertificate=db-certs/client.root.crt \\ --set-file dbClientPrivateKey=db-certs/client.root.key \\ --set-file caCrt=db-certs/ca.crt \\ -f values.yaml \\ csm-installer dell/csm-installer b) If not deploying with certificates, execute the following command:\nhelm upgrade -n csm-installer \\ --set-string scheme=http \\ --set-string dbSSLEnabled=\"false\" \\ -f values.yaml \\ csm-installer dell/csm-installer How to Uninstall the Container Storage Modules Installer  Delete the Helm chart  helm delete -n csm-installer csm-installer ","excerpt":" The CSM Installer is currently deprecated and will no longer be …","ref":"/csm-docs/v1/deployment/csminstaller/","title":"CSM Installer"},{"body":" The CSM Installer is currently deprecated and will no longer be supported as of CSM v1.4.0\n   Note: The CSM Installer only supports installation of CSM 1.0 Modules and CSI Drivers in environments that do not have any existing deployments of CSM or CSI Drivers. The CSM Installer does not support the upgrade of existing CSM or CSI Driver deployments.\n  The CSM (Container Storage Modules) Installer simplifies the deployment and management of Dell Container Storage Modules and CSI Drivers to provide persistent storage for your containerized workloads.\nCSM Installer Supported Modules and Dell CSI Drivers    Modules/Drivers CSM 1.0     Authorization 1.0   Observability 1.0   Replication 1.0   Resiliency 1.0   CSI Driver for PowerScale v2.0   CSI Driver for Unity v2.0   CSI Driver for PowerStore v2.0   CSI Driver for PowerFlex v2.0   CSI Driver for PowerMax v2.0    The CSM Installer must first be deployed in a Kubernetes environment using Helm. After which, the CSM Installer can be used through the following interfaces:\n CSM CLI REST API  How to Deploy the Container Storage Modules Installer  Add the dell helm repository:  helm repo add dell https://dell.github.io/helm-charts If securing the API service and database, following steps 2 to 4 to generate the certificates, or skip to step 5 to deploy without certificates\nGenerate self-signed certificates using the following commands:  mkdir api-certs openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout api-certs/ca.key \\ -x509 -days 365 -out api-certs/ca.crt -subj '/' openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout api-certs/cert.key \\ -out api-certs/cert.csr -subj '/' openssl x509 -req -days 365 -in api-certs/cert.csr -CA api-certs/ca.crt \\ -CAkey api-certs/ca.key -CAcreateserial -out api-certs/cert.crt If required, download the cockroach binary used to generate certificates for the cockroach-db:  curl https://binaries.cockroachdb.com/cockroach-v21.1.8.linux-amd64.tgz | tar -xz \u0026\u0026 sudo cp -i cockroach-v21.1.8.linux-amd64/cockroach /usr/local/bin/ Generate the certificates required for the cockroach-db service:  mkdir db-certs cockroach cert create-ca --certs-dir=db-certs --ca-key=db-certs/ca.key cockroach cert create-node cockroachdb-0.cockroachdb.csm-installer.svc.cluster.local cockroachdb-public cockroachdb-0.cockroachdb --certs-dir=db-certs/ --ca-key=db-certs/ca.key In case multiple instances of cockroachdb are required add all nodes names while creating nodes on the certificates\ncockroach cert create-node cockroachdb-0.cockroachdb.csm-installer.svc.cluster.local cockroachdb-1.cockroachdb.csm-installer.svc.cluster.local cockroachdb-2.cockroachdb.csm-installer.svc.cluster.local cockroachdb-public cockroachdb-0.cockroachdb cockroachdb-1.cockroachdb cockroachdb-2.cockroachdb --certs-dir=db-certs/ --ca-key=db-certs/ca.key cockroach cert create-client root --certs-dir=db-certs/ --ca-key=db-certs/ca.key cockroach cert list --certs-dir=db-certs/ Create a values.yaml file that contains JWT, Cipher key, and Admin username and password of CSM Installer that are required by the installer during helm installation. See the Configuration section for other values that can be set during helm installation.   Note: jwtKey will be used as a shared secret in HMAC algorithm for generating jwt token, cipherKey will be used as a symmetric key in AES cipher for encryption of storage system credentials. Those parameters are arbitrary, and you can set them to whatever you like. Just ensure that cipherKey is exactly 32 characters long.\n # string of any length jwtKey: # string of exactly 32 characters cipherKey: \"\" # Admin username of CSM Installer adminUserName: # Admin password of CSM Installer adminPassword: Follow step a if certificates are being used or step b if certificates are not being used:  a) Install the helm chart, specifying the certificates generated in the previous steps:\nhelm install -n csm-installer --create-namespace \\ --set-file serviceCertificate=api-certs/cert.crt \\ --set-file servicePrivateKey=api-certs/cert.key \\ --set-file databaseCertificate=db-certs/node.crt \\ --set-file databasePrivateKey=db-certs/node.key \\ --set-file dbClientCertificate=db-certs/client.root.crt \\ --set-file dbClientPrivateKey=db-certs/client.root.key \\ --set-file caCrt=db-certs/ca.crt \\ -f values.yaml \\ csm-installer dell/csm-installer b) If not deploying with certificates, execute the following command:\nhelm install -n csm-installer --create-namespace \\ --set-string scheme=http \\ --set-string dbSSLEnabled=\"false\" \\ -f values.yaml \\ csm-installer dell/csm-installer  Note: In an OpenShift environment, the cockroachdb StatefulSet will run privileged pods so that it can mount the Persistent Volume used for storage. Follow the documentation for your OpenShift version to enable privileged pods.\n Configuration    Parameter Description Default     csmInstallerCount Number of replicas for the CSM Installer Deployment 1   dbInstanceCount Number of replicas for the CSM Database StatefulSet 2   imagePullPolicy Image pull policy for the CSM Installer images Always   host Host or IP that will be used to bind to the CSM Installer API service 0.0.0.0   port Port that will be used to bind to the CSM Installer API service 8080   scheme Scheme used for the CSM Installer API service. Valid values are https and http https   jwtKey Key used to sign the JWT token    cipherKey Key used to encrypt/decrypt user and storage system credentials. Must be 32 characters in length.    logLevel Log level used for the CSM Installer. Valid values are DEBUG, INFO, WARN, ERROR, and FATAL INFO   dbHost Host name of the Cockroach DB instance cockroachdb-public   dbPort Port number to access the Cockroach DB instance 26257   dbSSLEnabled Enable SSL for the Cockroach DB connectiong true   installerImage Location of the CSM Installer Docker Image dellemc/dell-csm-installer:v1.0.0   dataCollectorImage Location of the CSM Data Collector Docker Image dellemc/csm-data-collector:v1.0.0   adminUserName Username to authenticate with the CSM Installer    adminPassword Password to authenticate with the CSM Installer    dbVolumeDirectory Directory on the worker node to use for the Persistent Volume /var/lib/cockroachdb   api_server_ip If using Swagger, set to public IP or host of the CSM Installer API service localhost    How to Upgrade the Container Storage Modules Installer When a new version of the CSM Installer helm chart is available, the following steps can be used to upgrade to the latest version.\n Note: Upgrading the CSM Installer does not upgrade the Dell CSI Drivers or modules that were previously deployed with the installer. The CSM Installer does not support upgrading of the Dell CSI Drivers or modules. The Dell CSI Drivers and modules must be deleted and re-deployed using the latest CSM Installer in order to get the most recent version of the Dell CSI Driver and modules.\n  Update the helm repository.  helm repo update Follow step a if certificates were used during the initial installation of the helm chart or step b if certificates were not used:  a) Upgrade the helm chart, specifying the certificates used during initial installation:\nhelm upgrade -n csm-installer \\ --set-file serviceCertificate=api-certs/cert.crt \\ --set-file servicePrivateKey=api-certs/cert.key \\ --set-file databaseCertificate=db-certs/node.crt \\ --set-file databasePrivateKey=db-certs/node.key \\ --set-file dbClientCertificate=db-certs/client.root.crt \\ --set-file dbClientPrivateKey=db-certs/client.root.key \\ --set-file caCrt=db-certs/ca.crt \\ -f values.yaml \\ csm-installer dell/csm-installer b) If not deploying with certificates, execute the following command:\nhelm upgrade -n csm-installer \\ --set-string scheme=http \\ --set-string dbSSLEnabled=\"false\" \\ -f values.yaml \\ csm-installer dell/csm-installer How to Uninstall the Container Storage Modules Installer  Delete the Helm chart  helm delete -n csm-installer csm-installer ","excerpt":" The CSM Installer is currently deprecated and will no longer be …","ref":"/csm-docs/v2/deployment/csminstaller/","title":"CSM Installer"},{"body":"The CSM (Container Storage Modules) Installer simplifies the deployment and management of Dell EMC Container Storage Modules and CSI Drivers to provide persistent storage for your containerized workloads.\nCSM Installer Supported Modules and Dell EMC CSI Drivers    Modules/Drivers CSM 1.0     Authorization 1.0   Observability 1.0   Replication 1.0   Resiliency 1.0   CSI Driver for PowerScale v2.0   CSI Driver for Unity v2.0   CSI Driver for PowerStore v2.0   CSI Driver for PowerFlex v2.0   CSI Driver for PowerMax v2.0    Note: The CSM Installer supports installation of CSM 1.0 Modules and CSI Drivers in environments that do not have any existing deployments of CSM or CSI Drivers. The CSM Installer does not support the upgrade of existing CSM or CSI Driver deployments.\nThe CSM Installer must first be deployed in a Kubernetes environment using Helm. After which, the CSM Installer can be used through the following interfaces:\n CSM CLI REST API  How to Deploy the Container Storage Modules Installer  Add the dell helm repository:  helm repo add dell https://dell.github.io/helm-charts If securing the API service and database, following steps 2 to 4 to generate the certificates, or skip to step 5 to deploy without certificates\nGenerate self-signed certificates using the following commands:  mkdir api-certs openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout api-certs/ca.key \\ -x509 -days 365 -out api-certs/ca.crt -subj '/' openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout api-certs/cert.key \\ -out api-certs/cert.csr -subj '/' openssl x509 -req -days 365 -in api-certs/cert.csr -CA api-certs/ca.crt \\ -CAkey api-certs/ca.key -CAcreateserial -out api-certs/cert.crt If required, download the cockroach binary used to generate certificates for the cockroach-db:  curl https://binaries.cockroachdb.com/cockroach-v21.1.8.linux-amd64.tgz | tar -xz \u0026\u0026 sudo cp -i cockroach-v21.1.8.linux-amd64/cockroach /usr/local/bin/ Generate the certificates required for the cockroach-db service:  mkdir db-certs cockroach cert create-ca --certs-dir=db-certs --ca-key=db-certs/ca.key cockroach cert create-node cockroachdb-0.cockroachdb.csm-installer.svc.cluster.local cockroachdb-public cockroachdb-0.cockroachdb --certs-dir=db-certs/ --ca-key=db-certs/ca.key In case multiple instances of cockroachdb are required add all nodes names while creating nodes on the certificates\ncockroach cert create-node cockroachdb-0.cockroachdb.csm-installer.svc.cluster.local cockroachdb-1.cockroachdb.csm-installer.svc.cluster.local cockroachdb-2.cockroachdb.csm-installer.svc.cluster.local cockroachdb-public cockroachdb-0.cockroachdb cockroachdb-1.cockroachdb cockroachdb-2.cockroachdb --certs-dir=db-certs/ --ca-key=db-certs/ca.key cockroach cert create-client root --certs-dir=db-certs/ --ca-key=db-certs/ca.key cockroach cert list --certs-dir=db-certs/ Create a values.yaml file that contains JWT, Cipher key, and Admin username and password of CSM Installer that are required by the installer during helm installation. See the Configuration section for other values that can be set during helm installation.   Note: jwtKey will be used as a shared secret in HMAC algorithm for generating jwt token, cipherKey will be used as a symmetric key in AES cipher for encryption of storage system credentials. Those parameters are arbitrary, and you can set them to whatever you like. Just ensure that cipherKey is exactly 32 characters long.\n # string of any length jwtKey: # string of exactly 32 characters cipherKey: \"\" # Admin username of CSM Installer adminUserName: # Admin password of CSM Installer adminPassword: Follow step a if certificates are being used or step b if certificates are not being used:  a) Install the helm chart, specifying the certificates generated in the previous steps:\nhelm install -n csm-installer --create-namespace \\ --set-file serviceCertificate=api-certs/cert.crt \\ --set-file servicePrivateKey=api-certs/cert.key \\ --set-file databaseCertificate=db-certs/node.crt \\ --set-file databasePrivateKey=db-certs/node.key \\ --set-file dbClientCertificate=db-certs/client.root.crt \\ --set-file dbClientPrivateKey=db-certs/client.root.key \\ --set-file caCrt=db-certs/ca.crt \\ -f values.yaml \\ csm-installer dell/csm-installer b) If not deploying with certificates, execute the following command:\nhelm install -n csm-installer --create-namespace \\ --set-string scheme=http \\ --set-string dbSSLEnabled=\"false\" \\ -f values.yaml \\ csm-installer dell/csm-installer  Note: In an OpenShift environment, the cockroachdb StatefulSet will run privileged pods so that it can mount the Persistent Volume used for storage. Follow the documentation for your OpenShift version to enable privileged pods.\n Configuration    Parameter Description Default     csmInstallerCount Number of replicas for the CSM Installer Deployment 1   dbInstanceCount Number of replicas for the CSM Database StatefulSet 2   imagePullPolicy Image pull policy for the CSM Installer images Always   host Host or IP that will be used to bind to the CSM Installer API service 0.0.0.0   port Port that will be used to bind to the CSM Installer API service 8080   scheme Scheme used for the CSM Installer API service. Valid values are https and http https   jwtKey Key used to sign the JWT token    cipherKey Key used to encrypt/decrypt user and storage system credentials. Must be 32 characters in length.    logLevel Log level used for the CSM Installer. Valid values are DEBUG, INFO, WARN, ERROR, and FATAL INFO   dbHost Host name of the Cockroach DB instance cockroachdb-public   dbPort Port number to access the Cockroach DB instance 26257   dbSSLEnabled Enable SSL for the Cockroach DB connectiong true   installerImage Location of the CSM Installer Docker Image dellemc/dell-csm-installer:v1.0.0   dataCollectorImage Location of the CSM Data Collector Docker Image dellemc/csm-data-collector:v1.0.0   adminUserName Username to authenticate with the CSM Installer    adminPassword Password to authenticate with the CSM Installer    dbVolumeDirectory Directory on the worker node to use for the Persistent Volume /var/lib/cockroachdb   api_server_ip If using Swagger, set to public IP or host of the CSM Installer API service localhost    How to Upgrade the Container Storage Modules Installer When a new version of the CSM Installer helm chart is available, the following steps can be used to upgrade to the latest version.\n Note: Upgrading the CSM Installer does not upgrade the Dell EMC CSI Drivers or modules that were previously deployed with the installer. The CSM Installer does not support upgrading of the Dell EMC CSI Drivers or modules. The Dell EMC CSI Drivers and modules must be deleted and re-deployed using the latest CSM Installer in order to get the most recent version of the Dell EMC CSI Driver and modules.\n  Update the helm repository.  helm repo update Follow step a if certificates were used during the initial installation of the helm chart or step b if certificates were not used:  a) Upgrade the helm chart, specifying the certificates used during initial installation:\nhelm upgrade -n csm-installer \\ --set-file serviceCertificate=api-certs/cert.crt \\ --set-file servicePrivateKey=api-certs/cert.key \\ --set-file databaseCertificate=db-certs/node.crt \\ --set-file databasePrivateKey=db-certs/node.key \\ --set-file dbClientCertificate=db-certs/client.root.crt \\ --set-file dbClientPrivateKey=db-certs/client.root.key \\ --set-file caCrt=db-certs/ca.crt \\ -f values.yaml \\ csm-installer dell/csm-installer b) If not deploying with certificates, execute the following command:\nhelm upgrade -n csm-installer \\ --set-string scheme=http \\ --set-string dbSSLEnabled=\"false\" \\ -f values.yaml \\ csm-installer dell/csm-installer How to Uninstall the Container Storage Modules Installer  Delete the Helm chart  helm delete -n csm-installer csm-installer ","excerpt":"The CSM (Container Storage Modules) Installer simplifies the …","ref":"/csm-docs/v3/deployment/csminstaller/","title":"CSM Installer"},{"body":" The Dell CSM Operator is currently in tech-preview and is not supported in production environments. It can be used in environments where no other Dell CSI Drivers or CSM Modules are installed.\n The Dell CSM Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers and CSM Modules provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. The operator can be installed using OLM (Operator Lifecycle Manager) or manually.\nSupported Platforms Dell CSM Operator has been tested and qualified on Upstream Kubernetes and OpenShift. Supported versions are listed below.\n   Kubernetes Version OpenShift Version     1.22, 1.23, 1.24 4.9, 4.10, 4.10 EUS    Supported CSI Drivers    CSI Driver Version ConfigVersion     CSI PowerScale 2.2.0 + v2.2.0 +    Supported CSM Modules    CSM Modules Version ConfigVersion     CSM Authorization 1.2.0 + v1.2.0 +    Installation Dell CSM Operator can be installed manually or via Operator Hub.\nManual Installation Operator Installation on a cluster without OLM  Clone the Dell CSM Operator repository. cd csm-operator (Optional) If using a local Docker image, edit the deploy/operator.yaml file and set the image name for the CSM Operator Deployment. Run bash scripts/install.sh to install the operator.   NOTE: Dell CSM Operator will be installed in the dell-csm-operator namespace.\n  Run the command kubectl get pods -n dell-csm-operator to validate the installation. If installed successfully, you should be able to see the operator pod in the dell-csm-operator namespace.   Operator Installation on a cluster with OLM  Clone the Dell CSM Operator repository. cd csm-operator Run bash scripts/install_olm.sh to install the operator.   NOTE: Dell CSM Operator will get installed in the test-csm-operator-olm namespace.\n  Once installation completes, run the command kubectl get pods -n test-csm-operator-olm to validate the installation. If installed successfully, you should be able to see the operator pods and CSV in the test-csm-operator-olm namespace. The CSV phase will be in Succeeded state.    NOTE: The recommended version of OLM for upstream Kubernetes is v0.18.2.\n Installation via Operator Hub dell-csm-operator can be installed via Operator Hub on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the operator to:\n Automatic - If you want the operator to be automatically installed or upgraded (once an upgrade is available). Manual - If you want a cluster administrator to manually review and approve the InstallPlan for installation/upgrades.  Uninstall Operator uninstallation on a cluster without OLM To uninstall a CSM operator, run bash scripts/uninstall.sh. This will uninstall the operator in dell-csm-operator namespace.\n Operator uninstallation on a cluster with OLM To uninstall a CSM operator installed with OLM run bash scripts/uninstall_olm.sh. This will uninstall the operator in test-csm-operator-olm namespace.\n To upgrade Dell CSM Operator, perform the following steps. Dell CSM Operator can be upgraded in 2 ways:\n1.Using script (for non-OLM based installation)\n2.Using Operator Lifecycle Manager (OLM)\nUsing Installation Script  Clone the Dell CSM Operator repository. cd csm-operator git checkout -b ‘csm-operator-version’ Execute bash scripts/install.sh --upgrade . This command will install the latest version of the operator.   Note: Dell CSM Operator would install to the ‘dell-csm-operator’ namespace by default.\n Using OLM The upgrade of the Dell CSM Operator is done via Operator Lifecycle Manager.\nThe Update approval (InstallPlan in OLM terms) strategy plays a role while upgrading dell-csm-operator on OpenShift. This option can be set during installation of dell-csm-operator on OpenShift via the console and can be either set to Manual or Automatic.\n If the Update approval is set to Automatic, OpenShift automatically detects whenever the latest version of dell-csm-operator is available in the Operator hub, and upgrades it to the latest available version. If the upgrade policy is set to Manual, OpenShift notifies of an available upgrade. This notification can be viewed by the user in the Installed Operators section of the OpenShift console. Clicking on the hyperlink to Approve the installation would trigger the dell-csm-operator upgrade process.  NOTE: The recommended version of OLM for Upstream Kubernetes is v0.18.3.\nCustom Resource Definitions As part of the Dell CSM Operator installation, a CRD representing configuration for the CSI Driver and CSM Modules is also installed.\ncontainerstoragemodule CRD is installed in API Group storage.dell.com.\nDrivers and modules can be installed by creating a customResource.\nCustom Resource Specification Each CSI Driver and CSM Module installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.Below is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - refer here for appropriate config version.\nreplicas - Number of replicas for controller plugin - must be set to 1 for all drivers.\ndnsPolicy - Determines the dnsPolicy for the node daemonset. Accepted values are Default, ClusterFirst, ClusterFirstWithHostNet, None.\ncommon - This field is mandatory and is used to specify common properties for both controller and the node plugin.\n image - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values  Optional fields controller - List of environment variables and values which are applicable only for controller.\nnode - List of environment variables and values which are applicable only for node.\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver.\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver.\ntolerations - List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet. It should be set separately in the controller and node sections if you want separate set of tolerations for them.\nnodeSelector - Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet.\n Note: The image field should point to the correct image tag for version of the driver you are installing.\n ","excerpt":" The Dell CSM Operator is currently in tech-preview and is not …","ref":"/csm-docs/docs/deployment/csmoperator/","title":"CSM Operator"},{"body":" The Dell CSM Operator is currently in tech-preview and is not supported in production environments. It can be used in environments where no other Dell CSI Drivers or CSM Modules are installed.\n The Dell CSM Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers and CSM Modules provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. The operator can be installed using OLM (Operator Lifecycle Manager) or manually.\nSupported Platforms Dell CSM Operator has been tested and qualified on Upstream Kubernetes and OpenShift. Supported versions are listed below.\n   Kubernetes Version OpenShift Version     1.21, 1.22, 1.23 4.8, 4.9    Supported CSI Drivers    CSI Driver Version ConfigVersion     CSI PowerScale 2.2.0 v2.2.0    Supported CSM Modules    CSM Modules Version ConfigVersion     CSM Authorization 1.2.0 v1.2.0    Installation Dell CSM Operator can be installed manually or via Operator Hub.\nManual Installation Operator Installation on a cluster without OLM  Clone the Dell CSM Operator repository. cd csm-operator (Optional) If using a local Docker image, edit the deploy/operator.yaml file and set the image name for the CSM Operator Deployment. Run bash scripts/install.sh to install the operator.   NOTE: Dell CSM Operator will be installed in the dell-csm-operator namespace.\n  Run the command kubectl get pods -n dell-csm-operator to validate the installation. If installed successfully, you should be able to see the operator pod in the dell-csm-operator namespace.   Operator Installation on a cluster with OLM  Clone the Dell CSM Operator repository. cd csm-operator Run bash scripts/install_olm.sh to install the operator.   NOTE: Dell CSM Operator will get installed in the test-csm-operator-olm namespace.\n  Once installation completes, run the command kubectl get pods -n test-csm-operator-olm to validate the installation. If installed successfully, you should be able to see the operator pods and CSV in the test-csm-operator-olm namespace. The CSV phase will be in Succeeded state.    NOTE: The recommended version of OLM for upstream Kubernetes is v0.18.2.\n Installation via Operator Hub dell-csm-operator can be installed via Operator Hub on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the operator to:\n Automatic - If you want the operator to be automatically installed or upgraded (once an upgrade is available). Manual - If you want a cluster administrator to manually review and approve the InstallPlan for installation/upgrades.  Uninstall Operator uninstallation on a cluster without OLM To uninstall a CSM operator, run bash scripts/uninstall.sh. This will uninstall the operator in dell-csm-operator namespace.\n Operator uninstallation on a cluster with OLM To uninstall a CSM operator installed with OLM run bash scripts/uninstall_olm.sh. This will uninstall the operator in test-csm-operator-olm namespace.\n Custom Resource Definitions As part of the Dell CSM Operator installation, a CRD representing configuration for the CSI Driver and CSM Modules is also installed.\ncontainerstoragemodule CRD is installed in API Group storage.dell.com.\nDrivers and modules can be installed by creating a customResource.\nCustom Resource Specification Each CSI Driver and CSM Module installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.Below is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - refer here for appropriate config version.\nreplicas - Number of replicas for controller plugin - must be set to 1 for all drivers.\ndnsPolicy - Determines the dnsPolicy for the node daemonset. Accepted values are Default, ClusterFirst, ClusterFirstWithHostNet, None.\ncommon - This field is mandatory and is used to specify common properties for both controller and the node plugin.\n image - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values  Optional fields controller - List of environment variables and values which are applicable only for controller.\nnode - List of environment variables and values which are applicable only for node.\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver.\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver.\ntolerations - List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet. It should be set separately in the controller and node sections if you want separate set of tolerations for them.\nnodeSelector - Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet.\n Note: The image field should point to the correct image tag for version of the driver you are installing.\n Pre-requisites for installation of the CSI Drivers On Upstream Kubernetes clusters, make sure to install\n VolumeSnapshot CRDs - Install v1 VolumeSnapshot CRDs External Volume Snapshot Controller  Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v5.0.1   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller.  Installing CSI Driver via Operator Refer PowerScale Driver to install the driver via Operator\n Note: If you are using an OLM based installation, example manifests are available in OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\n Verifying the driver installation Once the driver Custom Resource (CR) is created, you can verify the installation as mentioned below\n Check if ContainerStorageModule CR is created successfully using the command below: $ kubectl get csm/\u003cname-of-custom-resource\u003e -n \u003cdriver-namespace\u003e -o yaml  Check the status of the CR to verify if the driver installation is in the Succeeded state. If the status is not Succeeded, see the Troubleshooting guide for more information.  Update CSI Drivers The CSI Drivers and CSM Modules installed by the Dell CSM Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include:\n Modifying the installation directly via kubectl edit For e.g. - If the name of the installed PowerScale driver is powerscale, then run # Replace driver-namespace with the namespace where the PowerScale driver is installed $ kubectl edit csm/powerscale -n \u003cdriver-namespace\u003e and modify the installation\n Modify the API object in-place via kubectl patch  Supported modifications  Changing environment variable values for driver Updating the image of the driver  Uninstall CSI Driver The CSI Drivers and CSM Modules can be uninstalled by deleting the Custom Resource.\nFor e.g.\n$ kubectl delete csm/powerscale -n \u003cdriver-namespace\u003e By default, the forceRemoveDriver option is set to true which will uninstall the CSI Driver and CSM Modules when the Custom Resource is deleted. Setting this option to false is not recommended.\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. The only exception to this is modifications requested by the documentation, for example, filling in blank IPs or other such system-specific data. Any modifications not specifically requested by the documentation should be only done after consulting with Dell support.\nModules The CSM Operator can optionally enable modules that are supported by the specific Dell CSI driver. By default, the modules are disabled but they can be enabled by setting the enabled flag to true and setting any other configuration options for the given module.\n","excerpt":" The Dell CSM Operator is currently in tech-preview and is not …","ref":"/csm-docs/v1/deployment/csmoperator/","title":"CSM Operator"},{"body":" The Dell CSM Operator is currently in tech-preview and is not supported in production environments. It can be used in environments where no other Dell CSI Drivers or CSM Modules are installed.\n The Dell CSM Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers and CSM Modules provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. The operator can be installed using OLM (Operator Lifecycle Manager) or manually.\nSupported Platforms Dell CSM Operator has been tested and qualified on Upstream Kubernetes and OpenShift. Supported versions are listed below.\n   Kubernetes Version OpenShift Version     1.21, 1.22, 1.23 4.8, 4.9    Supported CSI Drivers    CSI Driver Version ConfigVersion     CSI PowerScale 2.2.0 v2.2.0    Supported CSM Modules    CSM Modules Version ConfigVersion     CSM Authorization 1.2.0 v1.2.0    Installation Dell CSM Operator can be installed manually or via Operator Hub.\nManual Installation Operator Installation on a cluster without OLM  Clone the Dell CSM Operator repository. cd csm-operator (Optional) If using a local Docker image, edit the deploy/operator.yaml file and set the image name for the CSM Operator Deployment. Run bash scripts/install.sh to install the operator.   NOTE: Dell CSM Operator will be installed in the dell-csm-operator namespace.\n  Run the command kubectl get pods -n dell-csm-operator to validate the installation. If installed successfully, you should be able to see the operator pod in the dell-csm-operator namespace.   Operator Installation on a cluster with OLM  Clone the Dell CSM Operator repository. cd csm-operator Run bash scripts/install_olm.sh to install the operator.   NOTE: Dell CSM Operator will get installed in the test-csm-operator-olm namespace.\n  Once installation completes, run the command kubectl get pods -n test-csm-operator-olm to validate the installation. If installed successfully, you should be able to see the operator pods and CSV in the test-csm-operator-olm namespace. The CSV phase will be in Succeeded state.    NOTE: The recommended version of OLM for upstream Kubernetes is v0.18.2.\n Installation via Operator Hub dell-csm-operator can be installed via Operator Hub on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the operator to:\n Automatic - If you want the operator to be automatically installed or upgraded (once an upgrade is available). Manual - If you want a cluster administrator to manually review and approve the InstallPlan for installation/upgrades.  Uninstall Operator uninstallation on a cluster without OLM To uninstall a CSM operator, run bash scripts/uninstall.sh. This will uninstall the operator in dell-csm-operator namespace.\n Operator uninstallation on a cluster with OLM To uninstall a CSM operator installed with OLM run bash scripts/uninstall_olm.sh. This will uninstall the operator in test-csm-operator-olm namespace.\n Custom Resource Definitions As part of the Dell CSM Operator installation, a CRD representing configuration for the CSI Driver and CSM Modules is also installed.\ncontainerstoragemodule CRD is installed in API Group storage.dell.com.\nDrivers and modules can be installed by creating a customResource.\nCustom Resource Specification Each CSI Driver and CSM Module installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.Below is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - refer here for appropriate config version.\nreplicas - Number of replicas for controller plugin - must be set to 1 for all drivers.\ndnsPolicy - Determines the dnsPolicy for the node daemonset. Accepted values are Default, ClusterFirst, ClusterFirstWithHostNet, None.\ncommon - This field is mandatory and is used to specify common properties for both controller and the node plugin.\n image - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values  Optional fields controller - List of environment variables and values which are applicable only for controller.\nnode - List of environment variables and values which are applicable only for node.\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver.\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver.\ntolerations - List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet. It should be set separately in the controller and node sections if you want separate set of tolerations for them.\nnodeSelector - Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet.\n Note: The image field should point to the correct image tag for version of the driver you are installing.\n Pre-requisites for installation of the CSI Drivers On Upstream Kubernetes clusters, make sure to install\n VolumeSnapshot CRDs - Install v1 VolumeSnapshot CRDs External Volume Snapshot Controller  Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v5.0.1   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller.  Installing CSI Driver via Operator Refer PowerScale Driver to install the driver via Operator\n Note: If you are using an OLM based installation, example manifests are available in OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\n Verifying the driver installation Once the driver Custom Resource (CR) is created, you can verify the installation as mentioned below\n Check if ContainerStorageModule CR is created successfully using the command below: $ kubectl get csm/\u003cname-of-custom-resource\u003e -n \u003cdriver-namespace\u003e -o yaml  Check the status of the CR to verify if the driver installation is in the Succeeded state. If the status is not Succeeded, see the Troubleshooting guide for more information.  Update CSI Drivers The CSI Drivers and CSM Modules installed by the Dell CSM Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include:\n Modifying the installation directly via kubectl edit For e.g. - If the name of the installed PowerScale driver is powerscale, then run # Replace driver-namespace with the namespace where the PowerScale driver is installed $ kubectl edit csm/powerscale -n \u003cdriver-namespace\u003e and modify the installation\n Modify the API object in-place via kubectl patch  Supported modifications  Changing environment variable values for driver Updating the image of the driver  Uninstall CSI Driver The CSI Drivers and CSM Modules can be uninstalled by deleting the Custom Resource.\nFor e.g.\n$ kubectl delete csm/powerscale -n \u003cdriver-namespace\u003e By default, the forceRemoveDriver option is set to true which will uninstall the CSI Driver and CSM Modules when the Custom Resource is deleted. Setting this option to false is not recommended.\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. The only exception to this is modifications requested by the documentation, for example, filling in blank IPs or other such system-specific data. Any modifications not specifically requested by the documentation should be only done after consulting with Dell support.\nModules The CSM Operator can optionally enable modules that are supported by the specific Dell CSI driver. By default, the modules are disabled but they can be enabled by setting the enabled flag to true and setting any other configuration options for the given module.\n","excerpt":" The Dell CSM Operator is currently in tech-preview and is not …","ref":"/csm-docs/v2/deployment/csmoperator/","title":"CSM Operator"},{"body":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: \"../swagger.yaml\", dom_id: '#ohpen_swagger_ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIStandalonePreset ] }) window.ui = ui } \t","excerpt":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: …","ref":"/csm-docs/docs/deployment/csminstaller/csmapi/","title":"CSM REST API"},{"body":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: \"../swagger.yaml\", dom_id: '#ohpen_swagger_ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIStandalonePreset ] }) window.ui = ui } \t","excerpt":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: …","ref":"/csm-docs/v1/deployment/csmapi/","title":"CSM REST API"},{"body":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: \"../swagger.yaml\", dom_id: '#ohpen_swagger_ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIStandalonePreset ] }) window.ui = ui } \t","excerpt":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: …","ref":"/csm-docs/v1/deployment/csminstaller/csmapi/","title":"CSM REST API"},{"body":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: \"../swagger.yaml\", dom_id: '#ohpen_swagger_ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIStandalonePreset ] }) window.ui = ui } \t","excerpt":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: …","ref":"/csm-docs/v2/deployment/csmapi/","title":"CSM REST API"},{"body":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: \"../swagger.yaml\", dom_id: '#ohpen_swagger_ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIStandalonePreset ] }) window.ui = ui } \t","excerpt":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: …","ref":"/csm-docs/v2/deployment/csminstaller/csmapi/","title":"CSM REST API"},{"body":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: \"../swagger.yaml\", dom_id: '#ohpen_swagger_ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIStandalonePreset ] }) window.ui = ui } \t","excerpt":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: …","ref":"/csm-docs/v3/deployment/csmapi/","title":"CSM REST API"},{"body":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: \"../swagger.yaml\", dom_id: '#ohpen_swagger_ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIStandalonePreset ] }) window.ui = ui } \t","excerpt":" \t window.onload = function() { const ui = SwaggerUIBundle({ url: …","ref":"/csm-docs/v3/deployment/csminstaller/csmapi/","title":"CSM REST API"},{"body":"To upgrade Dell CSI Operator, perform the following steps. Dell CSI Operator can be upgraded based on the supported platforms in one of the 2 ways:\n Using script (for non-OLM based installation) Using Operator Lifecycle Manager (OLM)  Using Installation Script  Clone and checkout the required dell-csi-operator version using git clone -b  https://github.com/dell/dell-csi-operator.git. cd dell-csi-operator Execute bash scripts/install.sh --upgrade . This command will install the latest version of the operator.   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n Using OLM The upgrade of the Dell CSI Operator is done via Operator Lifecycle Manager.\nThe Update approval (InstallPlan in OLM terms) strategy plays a role while upgrading dell-csi-operator on OpenShift. This option can be set during installation of dell-csi-operator on OpenShift via the console and can be either set to Manual or Automatic.\n If the Update approval is set to Automatic, OpenShift automatically detects whenever the latest version of dell-csi-operator is available in the Operator hub, and upgrades it to the latest available version. If the upgrade policy is set to Manual, OpenShift notifies of an available upgrade. This notification can be viewed by the user in the Installed Operators section of the OpenShift console. Clicking on the hyperlink to Approve the installation would trigger the dell-csi-operator upgrade process.  NOTE: The recommended version of OLM for Upstream Kubernetes is v0.18.3 when upgrading operator to v1.5.0.\n","excerpt":"To upgrade Dell CSI Operator, perform the following steps. Dell CSI …","ref":"/csm-docs/docs/csidriver/upgradation/drivers/operator/","title":"Dell CSI Operator"},{"body":"To upgrade Dell CSI Operator, perform the following steps. Dell CSI Operator can be upgraded based on the supported platforms in one of the 2 ways:\n Using script (for non-OLM based installation) Using Operator Lifecycle Manager (OLM)  Using Installation Script  Clone and checkout the required dell-csi-operator version using git clone -b  https://github.com/dell/dell-csi-operator.git. cd dell-csi-operator Execute bash scripts/install.sh --upgrade . This command will install the latest version of the operator.   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n Using OLM The upgrade of the Dell CSI Operator is done via Operator Lifecycle Manager.\nThe Update approval (InstallPlan in OLM terms) strategy plays a role while upgrading dell-csi-operator on OpenShift. This option can be set during installation of dell-csi-operator on OpenShift via the console and can be either set to Manual or Automatic.\n If the Update approval is set to Automatic, OpenShift automatically detects whenever the latest version of dell-csi-operator is available in the Operator hub, and upgrades it to the latest available version. If the upgrade policy is set to Manual, OpenShift notifies of an available upgrade. This notification can be viewed by the user in the Installed Operators section of the OpenShift console. Clicking on the hyperlink to Approve the installation would trigger the dell-csi-operator upgrade process.  NOTE: The recommended version of OLM for Upstream Kubernetes is v0.18.3 when upgrading operator to v1.5.0.\n","excerpt":"To upgrade Dell CSI Operator, perform the following steps. Dell CSI …","ref":"/csm-docs/v1/csidriver/upgradation/drivers/operator/","title":"Dell CSI Operator"},{"body":"To upgrade Dell CSI Operator, perform the following steps. Dell CSI Operator can be upgraded based on the supported platforms in one of the 2 ways:\n Using script (for non-OLM based installation) Using Operator Lifecycle Manager (OLM)  Using Installation Script  Clone the Dell CSI Operator repository. cd dell-csi-operator git checkout dell-csi-operator-‘your-version’ Execute bash scripts/install.sh --upgrade . This command will install the latest version of the operator.   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n Using OLM The upgrade of the Dell CSI Operator is done via Operator Lifecycle Manager.\nThe Update approval (InstallPlan in OLM terms) strategy plays a role while upgrading dell-csi-operator on OpenShift. This option can be set during installation of dell-csi-operator on OpenShift via the console and can be either set to Manual or Automatic.\n If the Update approval is set to Automatic, OpenShift automatically detects whenever the latest version of dell-csi-operator is available in the Operator hub, and upgrades it to the latest available version. If the upgrade policy is set to Manual, OpenShift notifies of an available upgrade. This notification can be viewed by the user in the Installed Operators section of the OpenShift console. Clicking on the hyperlink to Approve the installation would trigger the dell-csi-operator upgrade process.  NOTE: The recommended version of OLM for Upstream Kubernetes is v0.18.3 when upgrading operator to v1.5.0.\n","excerpt":"To upgrade Dell CSI Operator, perform the following steps. Dell CSI …","ref":"/csm-docs/v2/csidriver/upgradation/drivers/operator/","title":"Dell CSI Operator"},{"body":"To upgrade Dell CSI Operator from v1.2.0/v1.3.0 to v1.4.0/v1.5.0/v1.6.0, perform the following steps.\nUsing Installation Script Run the following command to upgrade the operator\n$ bash scripts/install.sh --upgrade Using OLM The upgrade of the Dell CSI Operator is done via Operator Lifecycle Manager. If the InstallPlan for the Operator subscription is set to Automatic, the operator will be automatically upgraded to the new version. If the InstallPlan is set to Manual, then a Cluster Administrator would need to approve the upgrade.\nNOTE: The recommended version of OLM for Upstream Kubernetes is v0.18.3 when upgrading operator to v1.5.0.\n","excerpt":"To upgrade Dell CSI Operator from v1.2.0/v1.3.0 to …","ref":"/csm-docs/v3/csidriver/upgradation/drivers/operator/","title":"Dell CSI Operator"},{"body":"The Container Storage Modules and the required CSI Drivers can each be deployed following the links below:\n Dell CSI Drivers Installation Dell Container Storage Module for Observability Dell Container Storage Module for Authorization Dell Container Storage Module for Resiliency Dell Container Storage Module for Replication  ","excerpt":"The Container Storage Modules and the required CSI Drivers can each be …","ref":"/csm-docs/docs/deployment/","title":"Deployment"},{"body":"","excerpt":"","ref":"/csm-docs/docs/replication/deployment/","title":"Deployment"},{"body":"The Container Storage Modules and the required CSI Drivers can each be deployed following the links below:\n Dell CSI Drivers Installation Dell Container Storage Module for Observability Dell Container Storage Module for Authorization Dell Container Storage Module for Resiliency Dell Container Storage Module for Replication  ","excerpt":"The Container Storage Modules and the required CSI Drivers can each be …","ref":"/csm-docs/v1/deployment/","title":"Deployment"},{"body":"","excerpt":"","ref":"/csm-docs/v1/replication/deployment/","title":"Deployment"},{"body":"The Container Storage Modules and the required CSI Drivers can each be deployed following the links below:\n Dell CSI Drivers Installation Dell Container Storage Module for Observability Dell Container Storage Module for Authorization Dell Container Storage Module for Resiliency Dell Container Storage Module for Replication  ","excerpt":"The Container Storage Modules and the required CSI Drivers can each be …","ref":"/csm-docs/v2/deployment/","title":"Deployment"},{"body":"","excerpt":"","ref":"/csm-docs/v2/replication/deployment/","title":"Deployment"},{"body":"The Container Storage Modules and the required CSI Drivers can each be deployed following the links below:\n Dell EMC CSI Drivers Installation Dell EMC Container Storage Module for Observability Dell EMC Container Storage Module for Authorization Dell EMC Container Storage Module for Resiliency Dell EMC Container Storage Module for Replication  ","excerpt":"The Container Storage Modules and the required CSI Drivers can each be …","ref":"/csm-docs/v3/deployment/","title":"Deployment"},{"body":"","excerpt":"","ref":"/csm-docs/v3/replication/deployment/","title":"Deployment"},{"body":"The Deprecation policy for Dell Container Storage Modules (CSM) is in place to help users prevent any disruptive incidents from occurring. We aim to provide appropriate notice when CLI elements, APIs, features, or behaviors are slated to be removed.\nDeprecating a CLI Element This captures situations when a flag or command is removed from a CLI.\nCLI elements must function after their announced deprecation for no less than two releases. This includes when the releases become Generally Available (GA), including both major or minor release versions.\nWhen deprecating a CLI command, a warning message must be displayed each time the command is used. This warning message should capture the deprecation details along with the release in which the command that is being deprecated will be removed.\nDeprecating an API, Feature, or Behavior CSM features must function after their announced deprecation for no less than two releases. This includes when the releases become Generally Available (GA), including both major or minor release versions.\nTech Previews Features released as tech preview are not supported and therefore are not intended for production. No deprecation notice will be required before removing any features/behaviors that are released as tech previews.\nRequired Deprecation Notice CSM documentation for the release in which the deprecation is being announced must include deprecation details along with the release in which the item(s) being deprecated will be removed.\nIn addition, the changelog and release notes for the release in which the deprecation is being announced must contain a section titled “Important Deprecation Information”. In this section, the deprecation details must be provided along with the release in which the item(s) being deprecated will be removed.\n","excerpt":"The Deprecation policy for Dell Container Storage Modules (CSM) is in …","ref":"/csm-docs/docs/policies/deprecationpolicy/","title":"Deprecation Policy"},{"body":"The Deprecation policy for Dell Container Storage Modules (CSM) is in place to help users prevent any disruptive incidents from occurring. We aim to provide appropriate notice when CLI elements, APIs, features, or behaviors are slated to be removed.\nDeprecating a CLI Element This captures situations when a flag or command is removed from a CLI.\nCLI elements must function after their announced deprecation for no less than two releases. This includes when the releases become Generally Available (GA), including both major or minor release versions.\nWhen deprecating a CLI command, a warning message must be displayed each time the command is used. This warning message should capture the deprecation details along with the release in which the command that is being deprecated will be removed.\nDeprecating an API, Feature, or Behavior CSM features must function after their announced deprecation for no less than two releases. This includes when the releases become Generally Available (GA), including both major or minor release versions.\nTech Previews Features released as tech preview are not supported and therefore are not intended for production. No deprecation notice will be required before removing any features/behaviors that are released as tech previews.\nRequired Deprecation Notice CSM documentation for the release in which the deprecation is being announced must include deprecation details along with the release in which the item(s) being deprecated will be removed.\nIn addition, the changelog and release notes for the release in which the deprecation is being announced must contain a section titled “Important Deprecation Information”. In this section, the deprecation details must be provided along with the release in which the item(s) being deprecated will be removed.\n","excerpt":"The Deprecation policy for Dell Container Storage Modules (CSM) is in …","ref":"/csm-docs/v1/policies/deprecationpolicy/","title":"Deprecation Policy"},{"body":"The Deprecation policy for Dell Container Storage Modules (CSM) is in place to help users prevent any disruptive incidents from occurring. We aim to provide appropriate notice when CLI elements, APIs, features, or behaviors are slated to be removed.\nDeprecating a CLI Element This captures situations when a flag or command is removed from a CLI.\nCLI elements must function after their announced deprecation for no less than two releases. This includes when the releases become Generally Available (GA), including both major or minor release versions.\nWhen deprecating a CLI command, a warning message must be displayed each time the command is used. This warning message should capture the deprecation details along with the release in which the command that is being deprecated will be removed.\nDeprecating an API, Feature, or Behavior CSM features must function after their announced deprecation for no less than two releases. This includes when the releases become Generally Available (GA), including both major or minor release versions.\nTech Previews Features released as tech preview are not supported and therefore are not intended for production. No deprecation notice will be required before removing any features/behaviors that are released as tech previews.\nRequired Deprecation Notice CSM documentation for the release in which the deprecation is being announced must include deprecation details along with the release in which the item(s) being deprecated will be removed.\nIn addition, the changelog and release notes for the release in which the deprecation is being announced must contain a section titled “Important Deprecation Information”. In this section, the deprecation details must be provided along with the release in which the item(s) being deprecated will be removed.\n","excerpt":"The Deprecation policy for Dell Container Storage Modules (CSM) is in …","ref":"/csm-docs/v2/policies/deprecationpolicy/","title":"Deprecation Policy"},{"body":"Container Storage Modules (CSM) for Authorization is designed as a service mesh solution and consists of many internal components that work together in concert to achieve its overall functionality.\nThis document provides an overview of the major components, including how they fit together and pointers to implementation details.\nIf you are a developer who is new to CSM for Authorization and want to build a mental map of how it works, you’re in the right place.\nTerminology  Service Mesh - An infrastructure layer consisting of proxies that intercept and route requests between existing services. CSI - Acronym for the Container Storage Interface. Proxy (L7) - A gateway between networked services that inspects request traffic. Sidecar Proxy - A service mesh proxy that runs alongside existing services, rather than within them. Pod - A Kubernetes abstraction for a set of related containers that are to be considered as one unit. Tenant - A named persona who owns a Kubernetes cluster and is considered the “client-side” user. Storage Administrator - A named persona who owns a storage array and is considered the admin user.  Bird’s Eye View +-----------------------------------+ | Kubernetes | | | | +---------+ +---------+ | +---------------+ | | CSI | | Sidecar | | | CSM | +---------+ | | Driver |---------\u003e Proxy |---------------\u003e Authorization |--------------\u003e Storage | | +---------+ +---------+ | | Server | | Array | | | +---------------+ +---------+ +-----------------------------------+ ^ | | | +------------+ | karavictl | | CLI | +------------+ NOTE: Arrows indicate request or connection initiation, not necessarily data flow direction.\nThe sections below explain each component in the diagram.\nKubernetes The architecture assumes a Kubernetes cluster that intends to offer external storage to applications hosted therein. The mechanism for managing this storage would utilize a CSI Driver.\nArchitecture Invariant: We assume there may be many Kubernetes clusters, potentially containing multiple CSI Drivers each with their own Sidecar Proxy.\nCSI Driver A CSI Driver supports the Container Service Interface (CSI) specification. Dell provides customers with CSI Drivers for its various storage arrays. CSM for Authorization intends to support a majority, if not all, of these drivers.\nA CSI Driver will typically be configured to communicate directly to its intended storage array and as such will be limited in using only the authentication methods supported by the Storage Array itself, e.g. Basic authentication over TLS.\nArchitecture Invariant: We try to avoid having to make any code changes to the CSI Driver when adding support for it. Any CSI Driver should ideally not be aware that it is communicating to the Sidecar Proxy.\nSidecar Proxy The CSM for Authorization Sidecar Proxy is deployed as a sidecar in the CSI Driver’s Pod. It acts as a proxy and forwards all requests to a CSM Authorization Server.\nThe CSI Driver section noted the limitation of a CSI Driver using Storage Array supported authentication methods only. By nature of being a proxy, the CSM for Authorization Sidecar Proxy is able to override the Authorization HTTP header for outbound requests to use Bearer tokens. Such tokens are managed by CSM for Authorization as will be described later in this document.\nCSM for Authorization Server The CSM for Authorization Server is, at its core, a Layer 7 proxy for intercepting traffic between a CSI Driver and a Storage Array.\nInbound requests are expected to originate from the CSM for Authorization Sidecar Proxy, for the following reasons:\n Processing a set of agreed upon HTTP headers (added by the CSM for Authorization Sidecar Proxy) to assist in routing traffic to the intended Storage Array. Inspection of CSM-specific Authorization Bearer tokens.  CSM for Authorization CLI The karavictl CLI (Command Line Interface) application allows Storage Admins to manage and interact with a running CSM for Authorization Server.\nStorage Array A Storage Array is typically considered to be one of the various Dell storage offerings, e.g. Dell PowerFlex which is supported by CSM for Authorization today. Support for more Storage Arrays will come in the future.\nHow it Works CSM for Authorization intends to override the existing authorization methods between a CSI Driver and its Storage Array. This may be desirable for several reasons, if:\n The CSI Driver requires privileged login credentials (e.g. “root”) in order to function. The Storage Array does not natively support the concept of RBAC and/or multi-tenancy.  This section of of the document describes how CSM for Authorization provides a solution to these problems.\nBearer Tokens CSM for Authorization overrides any existing authorization mechanism between a CSI Driver and its corresponding Storage Array with the use of JSON Web Tokens (JWTs). The CSI Driver and Storage Array will not be aware of this taking place.\nIn the context of RFC-6749 there are two such JWTs that are used:\n Access token: a single token valid for a short period of time. Refresh token: a single token used to obtain access tokens. Typically valid for a longer period of time.  Both tokens are opaque to the client, yet provide meaningful information to the server, specifically:\n The Tenant for whom the token is associated with. The Roles that are bound to the Tenant.  Tokens encode the following set of claims:\n{ \"aud\": \"karavi\", \"exp\": 1915585883, \"iss\": \"com.dell.karavi\", \"sub\": \"karavi-tenant\", \"roles\": \"role-a,role-b,role-c\", \"group\": \"Tenant-1\" } Both tokens are signed using a server-side secret preventing the risk of tampering by any client. For example, a bad-actor is unable to modify a token to give themselves a role that they should not have, at least without knowing the server-side secret.\nThe refresh approach is beneficial for the following reasons:\n Accidental exposure of an access token poses a lesser security concern, given the set expiration time is short (e.g. 30 seconds). The CSM for Authorization Server can fully trust the access token without having to perform a database check on each request (doing so would nullify the benefits of using tokens in the first place). The CSM for Authorization Server can defer Tenant checks at refresh time only, e.g. do not allow refresh if the Tenant’s access has been revoked by a Storage Admin. There may be a short time window in between revocation and enforcement, depending on the access token’s expiration time.  The following diagram shows the access and refresh tokens in play and how a valid access token is required for a request to be proxied to the intended Storage Array.\n +---------+ +---------------+ | | | | | | | | +----------+ | |--(A)------------ Access Token -----------\u003e| |------\u003e| | | | | CSM | | | | |\u003c-(B)---------- Protected Resource --------| Authorization |\u003c------| Storage | | Sidecar | | Server | | Array | | Proxy |--(C)------------ Access Token -----------\u003e| | | | | | | | | | | |\u003c-(D)------ Invalid Token Error -----------| | | | | | | | +----------+ | | | | | |--(E)----------- Refresh Token -----------\u003e| | | | \u0026 Expired Access Token | | | |\u003c-(F)----------- Access Token -------------| | +---------+ +---------------+  A) CSI Driver makes a request to the Storage Array:  request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token valid. The CSM for Authorization Server permits the request to be proxied to the intended Storage Array.   B) Storage Array response is sent back as expected. C) CSI Driver makes a request to the Storage Array:  request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token is invalid; it has since expired.   D) The CSM for Authorization Server responds with HTTP 401 Unauthorized. E) Sidecar Proxy requests a new access token by passing both refresh token and expired token. F) The CSM for Authorization Server processes the request:  is the refresh token valid? is the access token expired? has the Tenant had access revoked? a new access token is sent in response if the checks pass.    Roles So we know a token encodes both the identification of a Tenant and their Roles, but what’s in a Role?\nA role can be defined as follows:\n It has a name, e.g. “role-a”. It can be bound to a Tenant It can be unbound from a Tenant. It determines access to zero or more storage pools and assigns a storage quota for each.  Quota represents the upper-limit of the total aggregation of used storage capacity for a Tenant’s resources in a storage pool.   It prevents ambiguity by identifying each storage pool in the form of system-type:system-id:pool-name.  Below is an example of how roles are represented internally in JSON:\n{ \"Developer\": { \"system_types\": { \"powerflex\": { \"system_ids\": { \"542a2d5f5122210f\": { \"pool_quotas\": { \"bronze\": 99000000 } } } } } } } This role says Allow Tenants with the Developer role access to the bronze pool on PowerFlex system 542a2d5f5122210f, and cap their total capacity usage at 99000000Kb (99Gb).\nPolicy CSM for Authorization leverages the Open Policy Agent to use a policy-as-code approach to policy management. It stores a collection of policy files written in Rego language. Each policy file defines a set of policy rules that form the basis of a policy decision. A policy decision is made by processing the inputs provided. For CSM for Authorization, the inputs are:\n The set of roles defined by the Storage Admin. The claims section of a validated JWT. The JSON payload of the storage request.  Given these inputs, many decisions can be made to answer questions like “Can Tenant X, with these roles provision this volume of size Y?”. The result of the policy decision will determine whether or not the request is proxied.\n +----------------+ | Open Policy | | Agent | | | JWT | +--------+ | Claims ------\\ | | Policy | ----------\u003e Allow/Deny -----\u003e | (Rego) | | Storage -----\u003e +--------+ | Request -----/ +-------^--------+ | | | Role Data Quota \u0026 Volume Ownership Policy decisions based on the current request and set of roles alone are not enough. CSM for Authorization must maintain a cache of volumes approved for creation and deletion in order to know if a Tenant has already consumed their quota on a given storage pool.\nA Redis database is used to store this volume data and their relationship with a Tenant, Storage Array and Pool. The use of composite keys provide fast, constant time look up of volumes, e.g. quota:powerflex:542a2d5f5122210f:bronze:Tenant-1:data is a Redis hash with volume data as its values.\nCross-Cutting Concerns This section documents the pieces of code that are general in nature and shared across multiple packages.\nLogging CSM for Authorization uses the Logrus package when logging messages.\nObservability Both the CSM for Authorization Server and Sidecar Proxy are long-running processes, so it’s important to understand what’s going on inside. We use OpenTelemetry (otel) to help with that.\nThe following otel exporters are used:\n go.opentelemetry.io/otel/exporters/metric/prometheus go.opentelemetry.io/otel/exporters/trace/zipkin go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp  ","excerpt":"Container Storage Modules (CSM) for Authorization is designed as a …","ref":"/csm-docs/docs/authorization/design/","title":"Design"},{"body":"The solution takes the approach that each storage system that Container Storage Modules (CSM) for Observability supports will have their own metrics deployments in the Kubernetes cluster.\n Metrics Deployment: Queries the Kubernetes API to gather information about storage resources and then queries the storage system’s REST API to gather specific metrics. These metrics are then exported to the OTEL collector. Each supported storage system will have their own Deployment for metrics. They will each follow a similar pattern of querying the Kubernetes and StorageSystem APIs to gather information about storage resources (ex: volumes, storage pools, etc) and their metrics. Metrics will be exported directly to the OTEL collector.  A single topology deployment will query the Kubernetes API to gather mapping information between Persistent Volumes and storage resources located on multiple storage systems. This information is queried directly from Grafana and displayed in a custom dashboard.\nRequired Components The following prerequisites must be deployed into the namespace where CSM for Observability is located to support the storage system metrics and topology deployments:\n Prometheus for scraping the metrics from the OTEL collector. Grafana for visualizing the metrics from Prometheus and Topology services using custom dashboards. CSM for Observability will use secrets to get details about the storage systems used by the CSI drivers. These secrets should be copied from the namespaces where the drivers are deployed. CSI Powerflex driver uses the ‘vxflexos-config’ secret and CSI PowerStore uses the ‘powerstore-config’ secret.  Deployment Architectures CSM for Observability can be deployed to either direct storage system requests directly to the storage system or through the CSM for Authorization proxy. The CSI driver must be configured to route storage system requests through the CSM for Authorization proxy in order for CSM for Observability to do the same.\nDefault Deployment of CSM for Observability Deployment of CSM for Observability with CSM for Authorization ","excerpt":"The solution takes the approach that each storage system that …","ref":"/csm-docs/docs/observability/design/","title":"Design"},{"body":"This section covers CSM for Resiliency’s design. The detail is sufficient that you should be able to understand what CSM for Resiliency is designed to do in various situations and how it works. CSM for Resiliency is deployed as a sidecar named podmon with a CSI driver in both the controller pods and node pods. These are referred to as controller-podmon and node-podmon respectively.\nGenerally controller-podmon and the driver controller pods are deployed using a Deployment. The Deployments support one or multiple replicas for High Availability and use a standard K8S leader election protocol so that only one controller is active at a time (as does the driver and all the controller sidecars.) The controller deployment also supports a Node Selector that allows the controllers to be placed on K8S Manager (non Worker) nodes.\nNode-podmon and the driver node pods are deployed in a DaemonSet, with a Pod deployed on every K8S Worker Node.\nController-Podmon Controller-podmon is responsible for:\n  Setting up a Watch for CSM for Resiliency labeled pods, and if a Pod is Initialized but Not Ready and resident on a Node with a NoSchedule or NoExecute taint, calling controllerCleanupPod to cleanup the pod so that a replacement pod can be scheduled.\n  Periodically polling the arrays to see if it has connectivity to the nodes that are hosting CSM for Resiliency labeled pods (if enabled.) If an array has lost connectivity to a node hosting CSM for Resiliency labeled pods using that array, controllerCleanupPod is invoked to cleanup the pods that have lost I/O connectivity.\n  Tainting nodes that have failed so that a) no further pods will get scheduled to them until they are returned to service, and b) podmon-node upon seeing the taint will invoke the cleanup operations to make sure any zombie pods (pods that have been replaced) cannot write to the volumes they were using.\n  If a CSM for Resiliency labeled pod enters a CrashLoopBackOff state, deleting that pod so it can be replaced.\n  ControllerCleanupPod cleans up the pod by taking the following actions:\n The VolumeAttachments (VAs) are loaded, and all VAs belonging to the pod being cleaned up are identified. The PVs for each VolumeAttachment are identified and used to get the Volume Handle (array identifier for the volume.) If enabled, the array is queried if any of the volumes to the pod are still doing I/O. If so, cleanup is aborted. The pod’s volumes are “fenced” from the node the pod resides on to prevent any potential I/O from a zombie pod. This is done by calling the CSI ControllerUnpublishVolume call for each of the volumes. A taint is applied to the node to keep any new pods from being scheduled to the node. If the replacement pod were to get scheduled to the same node as a zombie pod, they might both gain access to the volume concurrently causing corruption. The VolumeAttachments for the pod is deleted. This is necessary so the replacement pod to be created can attach the volumes. The pod is forcibly deleted so that a StatefulSet controller which created the pod is free to create a replacement pod.  Node-Podmon Node-podmon has the following responsibilities:\n Establishing a pod watch which is used to maintain a list of pods executing on this node that may need to be cleaned up. The list includes information about each Mount volume or Block volume used by the pod including the volume handle, volume name, private mount path, and mount path in the pod. Periodically (every 30 seconds) polling to see if controller-podmon has applied a taint to the node. If so, node-podmon calls nodeModeCleanupPod for each pod to clean up any remnants of the pod (which is potentially a zombie pod.) If all pods have been successfully cleaned up, and there are no labeled pods on this node still existing, only then will node-podmon remove the taint placed on the node by controller-podmon.  NodeModeCleanupPod cleans up the pod remnants by taking the following actions for each volume used by the pod:\n Calling NodeUnpublishVolume to unpublish the volume from the pod. Unmounting and deleting the target path for the volume. Calling NodeUnstageVolume to unpublish the volume from the node. Unmounting and deleting the staging path for the volume.  Design Limitations There are some limitations with the current design. Some might be able to be addressed in the future- others are inherent in the approach.\n The design relies on the array’s ability to revoke access to a volume for a particular node for the fencing operation. The granularity of access control for a volume is per node. Consequently, it isn’t possible to revoke access from one pod on a node while retaining access to another pod on the same node if we cannot communicate with the node. The implications of this are that if more than one pod on a node is sharing the same volume(s), they all must be protected by CSM for Resiliency, and they all must be cleaned up by controller-podmon if the node fails. If only some of the pods are cleaned up, the other pods will lose access to the volumes shared with pods that have been cleaned, so those pods should also fail. The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint. If the node failure is short-lived and controller-podmon has not evacuated some of the protected pods on the node, they may try and restart on the same pod. This has been observed to cause such pods to go into CrashLoopBackoff. We are currently considering solutions to this problem.  ","excerpt":"This section covers CSM for Resiliency’s design. The detail is …","ref":"/csm-docs/docs/resiliency/design/","title":"Design"},{"body":"Container Storage Modules (CSM) for Authorization is designed as a service mesh solution and consists of many internal components that work together in concert to achieve its overall functionality.\nThis document provides an overview of the major components, including how they fit together and pointers to implementation details.\nIf you are a developer who is new to CSM for Authorization and want to build a mental map of how it works, you’re in the right place.\nTerminology  Service Mesh - An infrastructure layer consisting of proxies that intercept and route requests between existing services. CSI - Acronym for the Container Storage Interface. Proxy (L7) - A gateway between networked services that inspects request traffic. Sidecar Proxy - A service mesh proxy that runs alongside existing services, rather than within them. Pod - A Kubernetes abstraction for a set of related containers that are to be considered as one unit. Tenant - A named persona who owns a Kubernetes cluster and is considered the “client-side” user. Storage Administrator - A named persona who owns a storage array and is considered the admin user.  Bird’s Eye View +-----------------------------------+ | Kubernetes | | | | +---------+ +---------+ | +---------------+ | | CSI | | Sidecar | | | CSM | +---------+ | | Driver |---------\u003e Proxy |---------------\u003e Authorization |--------------\u003e Storage | | +---------+ +---------+ | | Server | | Array | | | +---------------+ +---------+ +-----------------------------------+ ^ | | | +------------+ | karavictl | | CLI | +------------+ NOTE: Arrows indicate request or connection initiation, not necessarily data flow direction.\nThe sections below explain each component in the diagram.\nKubernetes The architecture assumes a Kubernetes cluster that intends to offer external storage to applications hosted therein. The mechanism for managing this storage would utilize a CSI Driver.\nArchitecture Invariant: We assume there may be many Kubernetes clusters, potentially containing multiple CSI Drivers each with their own Sidecar Proxy.\nCSI Driver A CSI Driver supports the Container Service Interface (CSI) specification. Dell provides customers with CSI Drivers for its various storage arrays. CSM for Authorization intends to support a majority, if not all, of these drivers.\nA CSI Driver will typically be configured to communicate directly to its intended storage array and as such will be limited in using only the authentication methods supported by the Storage Array itself, e.g. Basic authentication over TLS.\nArchitecture Invariant: We try to avoid having to make any code changes to the CSI Driver when adding support for it. Any CSI Driver should ideally not be aware that it is communicating to the Sidecar Proxy.\nSidecar Proxy The CSM for Authorization Sidecar Proxy is deployed as a sidecar in the CSI Driver’s Pod. It acts as a proxy and forwards all requests to a CSM Authorization Server.\nThe CSI Driver section noted the limitation of a CSI Driver using Storage Array supported authentication methods only. By nature of being a proxy, the CSM for Authorization Sidecar Proxy is able to override the Authorization HTTP header for outbound requests to use Bearer tokens. Such tokens are managed by CSM for Authorization as will be described later in this document.\nCSM for Authorization Server The CSM for Authorization Server is, at its core, a Layer 7 proxy for intercepting traffic between a CSI Driver and a Storage Array.\nInbound requests are expected to originate from the CSM for Authorization Sidecar Proxy, for the following reasons:\n Processing a set of agreed upon HTTP headers (added by the CSM for Authorization Sidecar Proxy) to assist in routing traffic to the intended Storage Array. Inspection of CSM-specific Authorization Bearer tokens.  CSM for Authorization CLI The karavictl CLI (Command Line Interface) application allows Storage Admins to manage and interact with a running CSM for Authorization Server.\nStorage Array A Storage Array is typically considered to be one of the various Dell storage offerings, e.g. Dell PowerFlex which is supported by CSM for Authorization today. Support for more Storage Arrays will come in the future.\nHow it Works CSM for Authorization intends to override the existing authorization methods between a CSI Driver and its Storage Array. This may be desirable for several reasons, if:\n The CSI Driver requires privileged login credentials (e.g. “root”) in order to function. The Storage Array does not natively support the concept of RBAC and/or multi-tenancy.  This section of of the document describes how CSM for Authorization provides a solution to these problems.\nBearer Tokens CSM for Authorization overrides any existing authorization mechanism between a CSI Driver and its corresponding Storage Array with the use of JSON Web Tokens (JWTs). The CSI Driver and Storage Array will not be aware of this taking place.\nIn the context of RFC-6749 there are two such JWTs that are used:\n Access token: a single token valid for a short period of time. Refresh token: a single token used to obtain access tokens. Typically valid for a longer period of time.  Both tokens are opaque to the client, yet provide meaningful information to the server, specifically:\n The Tenant for whom the token is associated with. The Roles that are bound to the Tenant.  Tokens encode the following set of claims:\n{ \"aud\": \"karavi\", \"exp\": 1915585883, \"iss\": \"com.dell.karavi\", \"sub\": \"karavi-tenant\", \"roles\": \"role-a,role-b,role-c\", \"group\": \"Tenant-1\" } Both tokens are signed using a server-side secret preventing the risk of tampering by any client. For example, a bad-actor is unable to modify a token to give themselves a role that they should not have, at least without knowing the server-side secret.\nThe refresh approach is beneficial for the following reasons:\n Accidental exposure of an access token poses a lesser security concern, given the set expiration time is short (e.g. 30 seconds). The CSM for Authorization Server can fully trust the access token without having to perform a database check on each request (doing so would nullify the benefits of using tokens in the first place). The CSM for Authorization Server can defer Tenant checks at refresh time only, e.g. do not allow refresh if the Tenant’s access has been revoked by a Storage Admin. There may be a short time window in between revocation and enforcement, depending on the access token’s expiration time.  The following diagram shows the access and refresh tokens in play and how a valid access token is required for a request to be proxied to the intended Storage Array.\n +---------+ +---------------+ | | | | | | | | +----------+ | |--(A)------------ Access Token -----------\u003e| |------\u003e| | | | | CSM | | | | |\u003c-(B)---------- Protected Resource --------| Authorization |\u003c------| Storage | | Sidecar | | Server | | Array | | Proxy |--(C)------------ Access Token -----------\u003e| | | | | | | | | | | |\u003c-(D)------ Invalid Token Error -----------| | | | | | | | +----------+ | | | | | |--(E)----------- Refresh Token -----------\u003e| | | | \u0026 Expired Access Token | | | |\u003c-(F)----------- Access Token -------------| | +---------+ +---------------+  A) CSI Driver makes a request to the Storage Array:  request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token valid. The CSM for Authorization Server permits the request to be proxied to the intended Storage Array.   B) Storage Array response is sent back as expected. C) CSI Driver makes a request to the Storage Array:  request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token is invalid; it has since expired.   D) The CSM for Authorization Server responds with HTTP 401 Unauthorized. E) Sidecar Proxy requests a new access token by passing both refresh token and expired token. F) The CSM for Authorization Server processes the request:  is the refresh token valid? is the access token expired? has the Tenant had access revoked? a new access token is sent in response if the checks pass.    Roles So we know a token encodes both the identification of a Tenant and their Roles, but what’s in a Role?\nA role can be defined as follows:\n It has a name, e.g. “role-a”. It can be bound to a Tenant It can be unbound from a Tenant. It determines access to zero or more storage pools and assigns a storage quota for each.  Quota represents the upper-limit of the total aggregation of used storage capacity for a Tenant’s resources in a storage pool.   It prevents ambiguity by identifying each storage pool in the form of system-type:system-id:pool-name.  Below is an example of how roles are represented internally in JSON:\n{ \"Developer\": { \"system_types\": { \"powerflex\": { \"system_ids\": { \"542a2d5f5122210f\": { \"pool_quotas\": { \"bronze\": 99000000 } } } } } } } This role says Allow Tenants with the Developer role access to the bronze pool on PowerFlex system 542a2d5f5122210f, and cap their total capacity usage at 99000000Kb (99Gb).\nPolicy CSM for Authorization leverages the Open Policy Agent to use a policy-as-code approach to policy management. It stores a collection of policy files written in Rego language. Each policy file defines a set of policy rules that form the basis of a policy decision. A policy decision is made by processing the inputs provided. For CSM for Authorization, the inputs are:\n The set of roles defined by the Storage Admin. The claims section of a validated JWT. The JSON payload of the storage request.  Given these inputs, many decisions can be made to answer questions like “Can Tenant X, with these roles provision this volume of size Y?”. The result of the policy decision will determine whether or not the request is proxied.\n +----------------+ | Open Policy | | Agent | | | JWT | +--------+ | Claims ------\\ | | Policy | ----------\u003e Allow/Deny -----\u003e | (Rego) | | Storage -----\u003e +--------+ | Request -----/ +-------^--------+ | | | Role Data Quota \u0026 Volume Ownership Policy decisions based on the current request and set of roles alone are not enough. CSM for Authorization must maintain a cache of volumes approved for creation and deletion in order to know if a Tenant has already consumed their quota on a given storage pool.\nA Redis database is used to store this volume data and their relationship with a Tenant, Storage Array and Pool. The use of composite keys provide fast, constant time look up of volumes, e.g. quota:powerflex:542a2d5f5122210f:bronze:Tenant-1:data is a Redis hash with volume data as its values.\nCross-Cutting Concerns This section documents the pieces of code that are general in nature and shared across multiple packages.\nLogging CSM for Authorization uses the Logrus package when logging messages.\nObservability Both the CSM for Authorization Server and Sidecar Proxy are long-running processes, so it’s important to understand what’s going on inside. We use OpenTelemetry (otel) to help with that.\nThe following otel exporters are used:\n go.opentelemetry.io/otel/exporters/metric/prometheus go.opentelemetry.io/otel/exporters/trace/zipkin go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp  ","excerpt":"Container Storage Modules (CSM) for Authorization is designed as a …","ref":"/csm-docs/v1/authorization/design/","title":"Design"},{"body":"The solution takes the approach that each storage system that Container Storage Modules (CSM) for Observability supports will have their own metrics deployments in the Kubernetes cluster.\n Metrics Deployment: Queries the Kubernetes API to gather information about storage resources and then queries the storage system’s REST API to gather specific metrics. These metrics are then exported to the OTEL collector. Each supported storage system will have their own Deployment for metrics. They will each follow a similar pattern of querying the Kubernetes and StorageSystem APIs to gather information about storage resources (ex: volumes, storage pools, etc) and their metrics. Metrics will be exported directly to the OTEL collector.  A single topology deployment will query the Kubernetes API to gather mapping information between Persistent Volumes and storage resources located on multiple storage systems. This information is queried directly from Grafana and displayed in a custom dashboard.\nRequired Components The following prerequisites must be deployed into the namespace where CSM for Observability is located to support the storage system metrics and topology deployments:\n Prometheus for scraping the metrics from the OTEL collector. Grafana for visualizing the metrics from Prometheus and Topology services using custom dashboards. CSM for Observability will use secrets to get details about the storage systems used by the CSI drivers. These secrets should be copied from the namespaces where the drivers are deployed. CSI Powerflex driver uses the ‘vxflexos-config’ secret and CSI PowerStore uses the ‘powerstore-config’ secret.  Deployment Architectures CSM for Observability can be deployed to either direct storage system requests directly to the storage system or through the CSM for Authorization proxy. The CSI driver must be configured to route storage system requests through the CSM for Authorization proxy in order for CSM for Observability to do the same.\nDefault Deployment of CSM for Observability Deployment of CSM for Observability with CSM for Authorization ","excerpt":"The solution takes the approach that each storage system that …","ref":"/csm-docs/v1/observability/design/","title":"Design"},{"body":"This section covers CSM for Resiliency’s design. The detail is sufficient that you should be able to understand what CSM for Resiliency is designed to do in various situations and how it works. CSM for Resiliency is deployed as a sidecar named podmon with a CSI driver in both the controller pods and node pods. These are referred to as controller-podmon and node-podmon respectively.\nGenerally controller-podmon and the driver controller pods are deployed using a Deployment. The Deployments support one or multiple replicas for High Availability and use a standard K8S leader election protocol so that only one controller is active at a time (as does the driver and all the controller sidecars.) The controller deployment also supports a Node Selector that allows the controllers to be placed on K8S Manager (non Worker) nodes.\nNode-podmon and the driver node pods are deployed in a DaemonSet, with a Pod deployed on every K8S Worker Node.\nController-Podmon Controller-podmon is responsible for:\n  Setting up a Watch for CSM for Resiliency labeled pods, and if a Pod is Initialized but Not Ready and resident on a Node with a NoSchedule or NoExecute taint, calling controllerCleanupPod to cleanup the pod so that a replacement pod can be scheduled.\n  Periodically polling the arrays to see if it has connectivity to the nodes that are hosting CSM for Resiliency labeled pods (if enabled.) If an array has lost connectivity to a node hosting CSM for Resiliency labeled pods using that array, controllerCleanupPod is invoked to cleanup the pods that have lost I/O connectivity.\n  Tainting nodes that have failed so that a) no further pods will get scheduled to them until they are returned to service, and b) podmon-node upon seeing the taint will invoke the cleanup operations to make sure any zombie pods (pods that have been replaced) cannot write to the volumes they were using.\n  If a CSM for Resiliency labeled pod enters a CrashLoopBackOff state, deleting that pod so it can be replaced.\n  ControllerCleanupPod cleans up the pod by taking the following actions:\n The VolumeAttachments (VAs) are loaded, and all VAs belonging to the pod being cleaned up are identified. The PVs for each VolumeAttachment are identified and used to get the Volume Handle (array identifier for the volume.) If enabled, the array is queried if any of the volumes to the pod are still doing I/O. If so, cleanup is aborted. The pod’s volumes are “fenced” from the node the pod resides on to prevent any potential I/O from a zombie pod. This is done by calling the CSI ControllerUnpublishVolume call for each of the volumes. A taint is applied to the node to keep any new pods from being scheduled to the node. If the replacement pod were to get scheduled to the same node as a zombie pod, they might both gain access to the volume concurrently causing corruption. The VolumeAttachments for the pod is deleted. This is necessary so the replacement pod to be created can attach the volumes. The pod is forcibly deleted so that a StatefulSet controller which created the pod is free to create a replacement pod.  Node-Podmon Node-podmon has the following responsibilities:\n Establishing a pod watch which is used to maintain a list of pods executing on this node that may need to be cleaned up. The list includes information about each Mount volume or Block volume used by the pod including the volume handle, volume name, private mount path, and mount path in the pod. Periodically (every 30 seconds) polling to see if controller-podmon has applied a taint to the node. If so, node-podmon calls nodeModeCleanupPod for each pod to clean up any remnants of the pod (which is potentially a zombie pod.) If all pods have been successfully cleaned up, and there are no labeled pods on this node still existing, only then will node-podmon remove the taint placed on the node by controller-podmon.  NodeModeCleanupPod cleans up the pod remnants by taking the following actions for each volume used by the pod:\n Calling NodeUnpublishVolume to unpublish the volume from the pod. Unmounting and deleting the target path for the volume. Calling NodeUnstageVolume to unpublish the volume from the node. Unmounting and deleting the staging path for the volume.  Design Limitations There are some limitations with the current design. Some might be able to be addressed in the future- others are inherent in the approach.\n The design relies on the array’s ability to revoke access to a volume for a particular node for the fencing operation. The granularity of access control for a volume is per node. Consequently, it isn’t possible to revoke access from one pod on a node while retaining access to another pod on the same node if we cannot communicate with the node. The implications of this are that if more than one pod on a node is sharing the same volume(s), they all must be protected by CSM for Resiliency, and they all must be cleaned up by controller-podmon if the node fails. If only some of the pods are cleaned up, the other pods will lose access to the volumes shared with pods that have been cleaned, so those pods should also fail. The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint. If the node failure is short-lived and controller-podmon has not evacuated some of the protected pods on the node, they may try and restart on the same pod. This has been observed to cause such pods to go into CrashLoopBackoff. We are currently considering solutions to this problem.  ","excerpt":"This section covers CSM for Resiliency’s design. The detail is …","ref":"/csm-docs/v1/resiliency/design/","title":"Design"},{"body":"Container Storage Modules (CSM) for Authorization is designed as a service mesh solution and consists of many internal components that work together in concert to achieve its overall functionality.\nThis document provides an overview of the major components, including how they fit together and pointers to implementation details.\nIf you are a developer who is new to CSM for Authorization and want to build a mental map of how it works, you’re in the right place.\nTerminology  Service Mesh - An infrastructure layer consisting of proxies that intercept and route requests between existing services. CSI - Acronym for the Container Storage Interface. Proxy (L7) - A gateway between networked services that inspects request traffic. Sidecar Proxy - A service mesh proxy that runs alongside existing services, rather than within them. Pod - A Kubernetes abstraction for a set of related containers that are to be considered as one unit. Tenant - A named persona who owns a Kubernetes cluster and is considered the “client-side” user. Storage Administrator - A named persona who owns a storage array and is considered the admin user.  Bird’s Eye View +-----------------------------------+ | Kubernetes | | | | +---------+ +---------+ | +---------------+ | | CSI | | Sidecar | | | CSM | +---------+ | | Driver |---------\u003e Proxy |---------------\u003e Authorization |--------------\u003e Storage | | +---------+ +---------+ | | Server | | Array | | | +---------------+ +---------+ +-----------------------------------+ ^ | | | +------------+ | karavictl | | CLI | +------------+ NOTE: Arrows indicate request or connection initiation, not necessarily data flow direction.\nThe sections below explain each component in the diagram.\nKubernetes The architecture assumes a Kubernetes cluster that intends to offer external storage to applications hosted therein. The mechanism for managing this storage would utilize a CSI Driver.\nArchitecture Invariant: We assume there may be many Kubernetes clusters, potentially containing multiple CSI Drivers each with their own Sidecar Proxy.\nCSI Driver A CSI Driver supports the Container Service Interface (CSI) specification. Dell provides customers with CSI Drivers for its various storage arrays. CSM for Authorization intends to support a majority, if not all, of these drivers.\nA CSI Driver will typically be configured to communicate directly to its intended storage array and as such will be limited in using only the authentication methods supported by the Storage Array itself, e.g. Basic authentication over TLS.\nArchitecture Invariant: We try to avoid having to make any code changes to the CSI Driver when adding support for it. Any CSI Driver should ideally not be aware that it is communicating to the Sidecar Proxy.\nSidecar Proxy The CSM for Authorization Sidecar Proxy is deployed as a sidecar in the CSI Driver’s Pod. It acts as a proxy and forwards all requests to a CSM Authorization Server.\nThe CSI Driver section noted the limitation of a CSI Driver using Storage Array supported authentication methods only. By nature of being a proxy, the CSM for Authorization Sidecar Proxy is able to override the Authorization HTTP header for outbound requests to use Bearer tokens. Such tokens are managed by CSM for Authorization as will be described later in this document.\nCSM for Authorization Server The CSM for Authorization Server is, at its core, a Layer 7 proxy for intercepting traffic between a CSI Driver and a Storage Array.\nInbound requests are expected to originate from the CSM for Authorization Sidecar Proxy, for the following reasons:\n Processing a set of agreed upon HTTP headers (added by the CSM for Authorization Sidecar Proxy) to assist in routing traffic to the intended Storage Array. Inspection of CSM-specific Authorization Bearer tokens.  CSM for Authorization CLI The karavictl CLI (Command Line Interface) application allows Storage Admins to manage and interact with a running CSM for Authorization Server.\nStorage Array A Storage Array is typically considered to be one of the various Dell storage offerings, e.g. Dell PowerFlex which is supported by CSM for Authorization today. Support for more Storage Arrays will come in the future.\nHow it Works CSM for Authorization intends to override the existing authorization methods between a CSI Driver and its Storage Array. This may be desirable for several reasons, if:\n The CSI Driver requires privileged login credentials (e.g. “root”) in order to function. The Storage Array does not natively support the concept of RBAC and/or multi-tenancy.  This section of of the document describes how CSM for Authorization provides a solution to these problems.\nBearer Tokens CSM for Authorization overrides any existing authorization mechanism between a CSI Driver and its corresponding Storage Array with the use of JSON Web Tokens (JWTs). The CSI Driver and Storage Array will not be aware of this taking place.\nIn the context of RFC-6749 there are two such JWTs that are used:\n Access token: a single token valid for a short period of time. Refresh token: a single token used to obtain access tokens. Typically valid for a longer period of time.  Both tokens are opaque to the client, yet provide meaningful information to the server, specifically:\n The Tenant for whom the token is associated with. The Roles that are bound to the Tenant.  Tokens encode the following set of claims:\n{ \"aud\": \"karavi\", \"exp\": 1915585883, \"iss\": \"com.dell.karavi\", \"sub\": \"karavi-tenant\", \"roles\": \"role-a,role-b,role-c\", \"group\": \"Tenant-1\" } Both tokens are signed using a server-side secret preventing the risk of tampering by any client. For example, a bad-actor is unable to modify a token to give themselves a role that they should not have, at least without knowing the server-side secret.\nThe refresh approach is beneficial for the following reasons:\n Accidental exposure of an access token poses a lesser security concern, given the set expiration time is short (e.g. 30 seconds). The CSM for Authorization Server can fully trust the access token without having to perform a database check on each request (doing so would nullify the benefits of using tokens in the first place). The CSM for Authorization Server can defer Tenant checks at refresh time only, e.g. do not allow refresh if the Tenant’s access has been revoked by a Storage Admin. There may be a short time window in between revocation and enforcement, depending on the access token’s expiration time.  The following diagram shows the access and refresh tokens in play and how a valid access token is required for a request to be proxied to the intended Storage Array.\n +---------+ +---------------+ | | | | | | | | +----------+ | |--(A)------------ Access Token -----------\u003e| |------\u003e| | | | | CSM | | | | |\u003c-(B)---------- Protected Resource --------| Authorization |\u003c------| Storage | | Sidecar | | Server | | Array | | Proxy |--(C)------------ Access Token -----------\u003e| | | | | | | | | | | |\u003c-(D)------ Invalid Token Error -----------| | | | | | | | +----------+ | | | | | |--(E)----------- Refresh Token -----------\u003e| | | | \u0026 Expired Access Token | | | |\u003c-(F)----------- Access Token -------------| | +---------+ +---------------+  A) CSI Driver makes a request to the Storage Array:  request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token valid. The CSM for Authorization Server permits the request to be proxied to the intended Storage Array.   B) Storage Array response is sent back as expected. C) CSI Driver makes a request to the Storage Array:  request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token is invalid; it has since expired.   D) The CSM for Authorization Server responds with HTTP 401 Unauthorized. E) Sidecar Proxy requests a new access token by passing both refresh token and expired token. F) The CSM for Authorization Server processes the request:  is the refresh token valid? is the access token expired? has the Tenant had access revoked? a new access token is sent in response if the checks pass.    Roles So we know a token encodes both the identification of a Tenant and their Roles, but what’s in a Role?\nA role can be defined as follows:\n It has a name, e.g. “role-a”. It can be bound to a Tenant It can be unbound from a Tenant. It determines access to zero or more storage pools and assigns a storage quota for each.  Quota represents the upper-limit of the total aggregation of used storage capacity for a Tenant’s resources in a storage pool.   It prevents ambiguity by identifying each storage pool in the form of system-type:system-id:pool-name.  Below is an example of how roles are represented internally in JSON:\n{ \"Developer\": { \"system_types\": { \"powerflex\": { \"system_ids\": { \"542a2d5f5122210f\": { \"pool_quotas\": { \"bronze\": 99000000 } } } } } } } This role says Allow Tenants with the Developer role access to the bronze pool on PowerFlex system 542a2d5f5122210f, and cap their total capacity usage at 99000000Kb (99Gb).\nPolicy CSM for Authorization leverages the Open Policy Agent to use a policy-as-code approach to policy management. It stores a collection of policy files written in Rego language. Each policy file defines a set of policy rules that form the basis of a policy decision. A policy decision is made by processing the inputs provided. For CSM for Authorization, the inputs are:\n The set of roles defined by the Storage Admin. The claims section of a validated JWT. The JSON payload of the storage request.  Given these inputs, many decisions can be made to answer questions like “Can Tenant X, with these roles provision this volume of size Y?”. The result of the policy decision will determine whether or not the request is proxied.\n +----------------+ | Open Policy | | Agent | | | JWT | +--------+ | Claims ------\\ | | Policy | ----------\u003e Allow/Deny -----\u003e | (Rego) | | Storage -----\u003e +--------+ | Request -----/ +-------^--------+ | | | Role Data Quota \u0026 Volume Ownership Policy decisions based on the current request and set of roles alone are not enough. CSM for Authorization must maintain a cache of volumes approved for creation and deletion in order to know if a Tenant has already consumed their quota on a given storage pool.\nA Redis database is used to store this volume data and their relationship with a Tenant, Storage Array and Pool. The use of composite keys provide fast, constant time look up of volumes, e.g. quota:powerflex:542a2d5f5122210f:bronze:Tenant-1:data is a Redis hash with volume data as its values.\nCross-Cutting Concerns This section documents the pieces of code that are general in nature and shared across multiple packages.\nLogging CSM for Authorization uses the Logrus package when logging messages.\nObservability Both the CSM for Authorization Server and Sidecar Proxy are long-running processes, so it’s important to understand what’s going on inside. We use OpenTelemetry (otel) to help with that.\nThe following otel exporters are used:\n go.opentelemetry.io/otel/exporters/metric/prometheus go.opentelemetry.io/otel/exporters/trace/zipkin go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp  ","excerpt":"Container Storage Modules (CSM) for Authorization is designed as a …","ref":"/csm-docs/v2/authorization/design/","title":"Design"},{"body":"The solution takes the approach that each storage system that Container Storage Modules (CSM) for Observability supports will have their own metrics deployments in the Kubernetes cluster.\n Metrics Deployment: Queries the Kubernetes API to gather information about storage resources and then queries the storage system’s REST API to gather specific metrics. These metrics are then exported to the OTEL collector. Each supported storage system will have their own Deployment for metrics. They will each follow a similar pattern of querying the Kubernetes and StorageSystem APIs to gather information about storage resources (ex: volumes, storage pools, etc) and their metrics. Metrics will be exported directly to the OTEL collector.  A single topology deployment will query the Kubernetes API to gather mapping information between Persistent Volumes and storage resources located on multiple storage systems. This information is queried directly from Grafana and displayed in a custom dashboard.\nRequired Components The following prerequisites must be deployed into the namespace where CSM for Observability is located to support the storage system metrics and topology deployments:\n Prometheus for scraping the metrics from the OTEL collector. Grafana for visualizing the metrics from Prometheus and Topology services using custom dashboards. CSM for Observability will use secrets to get details about the storage systems used by the CSI drivers. These secrets should be copied from the namespaces where the drivers are deployed. CSI Powerflex driver uses the ‘vxflexos-config’ secret and CSI PowerStore uses the ‘powerstore-config’ secret.  Deployment Architectures CSM for Observability can be deployed to either direct storage system requests directly to the storage system or through the CSM for Authorization proxy. The CSI driver must be configured to route storage system requests through the CSM for Authorization proxy in order for CSM for Observability to do the same.\nDefault Deployment of CSM for Observability Deployment of CSM for Observability with CSM for Authorization ","excerpt":"The solution takes the approach that each storage system that …","ref":"/csm-docs/v2/observability/design/","title":"Design"},{"body":"This section covers CSM for Resiliency’s design. The detail is sufficient that you should be able to understand what CSM for Resiliency is designed to do in various situations and how it works. CSM for Resiliency is deployed as a sidecar named podmon with a CSI driver in both the controller pods and node pods. These are referred to as controller-podmon and node-podmon respectively.\nGenerally controller-podmon and the driver controller pods are deployed using a Deployment. The Deployments support one or multiple replicas for High Availability and use a standard K8S leader election protocol so that only one controller is active at a time (as does the driver and all the controller sidecars.) The controller deployment also supports a Node Selector that allows the controllers to be placed on K8S Manager (non Worker) nodes.\nNode-podmon and the driver node pods are deployed in a DaemonSet, with a Pod deployed on every K8S Worker Node.\nController-Podmon Controller-podmon is responsible for:\n  Setting up a Watch for CSM for Resiliency labeled pods, and if a Pod is Initialized but Not Ready and resident on a Node with a NoSchedule or NoExecute taint, calling controllerCleanupPod to cleanup the pod so that a replacement pod can be scheduled.\n  Periodically polling the arrays to see if it has connectivity to the nodes that are hosting CSM for Resiliency labeled pods (if enabled.) If an array has lost connectivity to a node hosting CSM for Resiliency labeled pods using that array, controllerCleanupPod is invoked to cleanup the pods that have lost I/O connectivity.\n  Tainting nodes that have failed so that a) no further pods will get scheduled to them until they are returned to service, and b) podmon-node upon seeing the taint will invoke the cleanup operations to make sure any zombie pods (pods that have been replaced) cannot write to the volumes they were using.\n  If a CSM for Resiliency labeled pod enters a CrashLoopBackOff state, deleting that pod so it can be replaced.\n  ControllerCleanupPod cleans up the pod by taking the following actions:\n The VolumeAttachments (VAs) are loaded, and all VAs belonging to the pod being cleaned up are identified. The PVs for each VolumeAttachment are identified and used to get the Volume Handle (array identifier for the volume.) If enabled, the array is queried if any of the volumes to the pod are still doing I/O. If so, cleanup is aborted. The pod’s volumes are “fenced” from the node the pod resides on to prevent any potential I/O from a zombie pod. This is done by calling the CSI ControllerUnpublishVolume call for each of the volumes. A taint is applied to the node to keep any new pods from being scheduled to the node. If the replacement pod were to get scheduled to the same node as a zombie pod, they might both gain access to the volume concurrently causing corruption. The VolumeAttachments for the pod is deleted. This is necessary so the replacement pod to be created can attach the volumes. The pod is forcibly deleted so that a StatefulSet controller which created the pod is free to create a replacement pod.  Node-Podmon Node-podmon has the following responsibilities:\n Establishing a pod watch which is used to maintain a list of pods executing on this node that may need to be cleaned up. The list includes information about each Mount volume or Block volume used by the pod including the volume handle, volume name, private mount path, and mount path in the pod. Periodically (every 30 seconds) polling to see if controller-podmon has applied a taint to the node. If so, node-podmon calls nodeModeCleanupPod for each pod to clean up any remnants of the pod (which is potentially a zombie pod.) If all pods have been successfully cleaned up, and there are no labeled pods on this node still existing, only then will node-podmon remove the taint placed on the node by controller-podmon.  NodeModeCleanupPod cleans up the pod remnants by taking the following actions for each volume used by the pod:\n Calling NodeUnpublishVolume to unpublish the volume from the pod. Unmounting and deleting the target path for the volume. Calling NodeUnstageVolume to unpublish the volume from the node. Unmounting and deleting the staging path for the volume.  Design Limitations There are some limitations with the current design. Some might be able to be addressed in the future- others are inherent in the approach.\n The design relies on the array’s ability to revoke access to a volume for a particular node for the fencing operation. The granularity of access control for a volume is per node. Consequently, it isn’t possible to revoke access from one pod on a node while retaining access to another pod on the same node if we cannot communicate with the node. The implications of this are that if more than one pod on a node is sharing the same volume(s), they all must be protected by CSM for Resiliency, and they all must be cleaned up by controller-podmon if the node fails. If only some of the pods are cleaned up, the other pods will lose access to the volumes shared with pods that have been cleaned, so those pods should also fail. The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint. If the node failure is short-lived and controller-podmon has not evacuated some of the protected pods on the node, they may try and restart on the same pod. This has been observed to cause such pods to go into CrashLoopBackoff. We are currently considering solutions to this problem.  ","excerpt":"This section covers CSM for Resiliency’s design. The detail is …","ref":"/csm-docs/v2/resiliency/design/","title":"Design"},{"body":"Container Storage Modules (CSM) for Authorization is designed as a service mesh solution and consists of many internal components that work together in concert to achieve its overall functionality.\nThis document provides an overview of the major components, including how they fit together and pointers to implementation details.\nIf you are a developer who is new to CSM for Authorization and want to build a mental map of how it works, you’re in the right place.\nTerminology  Service Mesh - An infrastructure layer consisting of proxies that intercept and route requests between existing services. CSI - Acronym for the Container Storage Interface. Proxy (L7) - A gateway between networked services that inspects request traffic. Sidecar Proxy - A service mesh proxy that runs alongside existing services, rather than within them. Pod - A Kubernetes abstraction for a set of related containers that are to be considered as one unit. Tenant - A named persona who owns a Kubernetes cluster and is considered the “client-side” user. Storage Administrator - A named persona who owns a storage array and is considered the admin user.  Bird’s Eye View +-----------------------------------+ | Kubernetes | | | | +---------+ +---------+ | +---------------+ | | CSI | | Sidecar | | | CSM | +---------+ | | Driver |---------\u003e Proxy |---------------\u003e Authorization |--------------\u003e Storage | | +---------+ +---------+ | | Server | | Array | | | +---------------+ +---------+ +-----------------------------------+ ^ | | | +------------+ | karavictl | | CLI | +------------+ NOTE: Arrows indicate request or connection initiation, not necessarily data flow direction.\nThe sections below explain each component in the diagram.\nKubernetes The architecture assumes a Kubernetes cluster that intends to offer external storage to applications hosted therein. The mechanism for managing this storage would utilize a CSI Driver.\nArchitecture Invariant: We assume there may be many Kubernetes clusters, potentially containing multiple CSI Drivers each with their own Sidecar Proxy.\nCSI Driver A CSI Driver supports the Container Service Interface (CSI) specification. Dell EMC provides customers with CSI Drivers for its various storage arrays. CSM for Authorization intends to support a majority, if not all, of these drivers.\nA CSI Driver will typically be configured to communicate directly to its intended storage array and as such will be limited in using only the authentication methods supported by the Storage Array itself, e.g. Basic authentication over TLS.\nArchitecture Invariant: We try to avoid having to make any code changes to the CSI Driver when adding support for it. Any CSI Driver should ideally not be aware that it is communicating to the Sidecar Proxy.\nSidecar Proxy The CSM for Authorization Sidecar Proxy is a sidecar container that gets “injected” into the CSI Driver’s Pod. It acts as a proxy and forwards all requests to a CSM Authorization Server.\nThe CSI Driver section noted the limitation of a CSI Driver using Storage Array supported authentication methods only. By nature of being a proxy, the CSM for Authorization Sidecar Proxy is able to override the Authorization HTTP header for outbound requests to use Bearer tokens. Such tokens are managed by CSM for Authorization as will be described later in this document.\nCSM for Authorization Server The CSM for Authorization Server is, at its core, a Layer 7 proxy for intercepting traffic between a CSI Driver and a Storage Array.\nInbound requests are expected to originate from the CSM for Authorization Sidecar Proxy, for the following reasons:\n Processing a set of agreed upon HTTP headers (added by the CSM for Authorization Sidecar Proxy) to assist in routing traffic to the intended Storage Array. Inspection of CSM-specific Authorization Bearer tokens.  CSM for Authorization CLI The karavictl CLI (Command Line Interface) application allows Storage Admins to manage and interact with a running CSM for Authorization Server.\nAdditionally, karavictl provides functionality for supporting the sidecar proxy injection mechanism mentioned above. Injection is discussed in more detail later on in this document.\nStorage Array A Storage Array is typically considered to be one of the various Dell EMC storage offerings, e.g. Dell EMC PowerFlex which is supported by CSM for Authorization today. Support for more Storage Arrays will come in the future.\nHow it Works CSM for Authorization intends to override the existing authorization methods between a CSI Driver and its Storage Array. This may be desirable for several reasons, if:\n The CSI Driver requires privileged login credentials (e.g. “root”) in order to function. The Storage Array does not natively support the concept of RBAC and/or multi-tenancy.  This section of of the document describes how CSM for Authorization provides a solution to these problems.\nBearer Tokens CSM for Authorization overrides any existing authorization mechanism between a CSI Driver and its corresponding Storage Array with the use of JSON Web Tokens (JWTs). The CSI Driver and Storage Array will not be aware of this taking place.\nIn the context of RFC-6749 there are two such JWTs that are used:\n Access token: a single token valid for a short period of time. Refresh token: a single token used to obtain access tokens. Typically valid for a longer period of time.  Both tokens are opaque to the client, yet provide meaningful information to the server, specifically:\n The Tenant for whom the token is associated with. The Roles that are bound to the Tenant.  Tokens encode the following set of claims:\n{ \"aud\": \"karavi\", \"exp\": 1915585883, \"iss\": \"com.dell.karavi\", \"sub\": \"karavi-tenant\", \"roles\": \"role-a,role-b,role-c\", \"group\": \"Tenant-1\" } Both tokens are signed using a server-side secret preventing the risk of tampering by any client. For example, a bad-actor is unable to modify a token to give themselves a role that they should not have, at least without knowing the server-side secret.\nThe refresh approach is beneficial for the following reasons:\n Accidental exposure of an access token poses a lesser security concern, given the set expiration time is short (e.g. 30 seconds). The CSM for Authorization Server can fully trust the access token without having to perform a database check on each request (doing so would nullify the benefits of using tokens in the first place). The CSM for Authorization Server can defer Tenant checks at refresh time only, e.g. do not allow refresh if the Tenant’s access has been revoked by a Storage Admin. There may be a short time window in between revocation and enforcement, depending on the access token’s expiration time.  The following diagram shows the access and refresh tokens in play and how a valid access token is required for a request to be proxied to the intended Storage Array.\n +---------+ +---------------+ | | | | | | | | +----------+ | |--(A)------------ Access Token -----------\u003e| |------\u003e| | | | | CSM | | | | |\u003c-(B)---------- Protected Resource --------| Authorization |\u003c------| Storage | | Sidecar | | Server | | Array | | Proxy |--(C)------------ Access Token -----------\u003e| | | | | | | | | | | |\u003c-(D)------ Invalid Token Error -----------| | | | | | | | +----------+ | | | | | |--(E)----------- Refresh Token -----------\u003e| | | | \u0026 Expired Access Token | | | |\u003c-(F)----------- Access Token -------------| | +---------+ +---------------+  A) CSI Driver makes a request to the Storage Array:  request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token valid. The CSM for Authorization Server permits the request to be proxied to the intended Storage Array.   B) Storage Array response is sent back as expected. C) CSI Driver makes a request to the Storage Array:  request is intercepted by the Sidecar Proxy to add the access token. The CSM for Authorization Server deems the access token is invalid; it has since expired.   D) The CSM for Authorization Server responds with HTTP 401 Unauthorized. E) Sidecar Proxy requests a new access token by passing both refresh token and expired token. F) The CSM for Authorization Server processes the request:  is the refresh token valid? is the access token expired? has the Tenant had access revoked? a new access token is sent in response if the checks pass.    Roles So we know a token encodes both the identification of a Tenant and their Roles, but what’s in a Role?\nA role can be defined as follows:\n It has a name, e.g. “role-a”. It can be bound to a Tenant It can be unbound from a Tenant. It determines access to zero or more storage pools and assigns a storage quota for each.  Quota represents the upper-limit of the total aggregation of used storage capacity for a Tenant’s resources in a storage pool.   It prevents ambiguity by identifying each storage pool in the form of system-type:system-id:pool-name.  Below is an example of how roles are represented internally in JSON:\n{ \"Developer\": { \"system_types\": { \"powerflex\": { \"system_ids\": { \"542a2d5f5122210f\": { \"pool_quotas\": { \"bronze\": 99000000 } } } } } } } This role says Allow Tenants with the Developer role access to the bronze pool on PowerFlex system 542a2d5f5122210f, and cap their total capacity usage at 99000000Kb (99Gb).\nPolicy CSM for Authorization leverages the Open Policy Agent to use a policy-as-code approach to policy management. It stores a collection of policy files written in Rego language. Each policy file defines a set of policy rules that form the basis of a policy decision. A policy decision is made by processing the inputs provided. For CSM for Authorization, the inputs are:\n The set of roles defined by the Storage Admin. The claims section of a validated JWT. The JSON payload of the storage request.  Given these inputs, many decisions can be made to answer questions like “Can Tenant X, with these roles provision this volume of size Y?”. The result of the policy decision will determine whether or not the request is proxied.\n +----------------+ | Open Policy | | Agent | | | JWT | +--------+ | Claims ------\\ | | Policy | ----------\u003e Allow/Deny -----\u003e | (Rego) | | Storage -----\u003e +--------+ | Request -----/ +-------^--------+ | | | Role Data Quota \u0026 Volume Ownership Policy decisions based on the current request and set of roles alone are not enough. CSM for Authorization must maintain a cache of volumes approved for creation and deletion in order to know if a Tenant has already consumed their quota on a given storage pool.\nA Redis database is used to store this volume data and their relationship with a Tenant, Storage Array and Pool. The use of composite keys provide fast, constant time look up of volumes, e.g. quota:powerflex:542a2d5f5122210f:bronze:Tenant-1:data is a Redis hash with volume data as its values.\nCross-Cutting Concerns This section documents the pieces of code that are general in nature and shared across multiple packages.\nLogging CSM for Authorization uses the Logrus package when logging messages.\nObservability Both the CSM for Authorization Server and Sidecar Proxy are long-running processes, so it’s important to understand what’s going on inside. We use OpenTelemetry (otel) to help with that.\nThe following otel exporters are used:\n go.opentelemetry.io/otel/exporters/metric/prometheus go.opentelemetry.io/otel/exporters/trace/zipkin go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp  ","excerpt":"Container Storage Modules (CSM) for Authorization is designed as a …","ref":"/csm-docs/v3/authorization/design/","title":"Design"},{"body":"The solution takes the approach that each storage system that Container Storage Modules (CSM) for Observability supports will have their own metrics deployments in the Kubernetes cluster.\n Metrics Deployment: Queries the Kubernetes API to gather information about storage resources and then queries the storage system’s REST API to gather specific metrics. These metrics are then exported to the OTEL collector. Each supported storage system will have their own Deployment for metrics. They will each follow a similar pattern of querying the Kubernetes and StorageSystem APIs to gather information about storage resources (ex: volumes, storage pools, etc) and their metrics. Metrics will be exported directly to the OTEL collector.  A single topology deployment will query the Kubernetes API to gather mapping information between Persistent Volumes and storage resources located on multiple storage systems. This information is queried directly from Grafana and displayed in a custom dashboard.\nRequired Components The following prerequisites must be deployed into the namespace where CSM for Observability is located to support the storage system metrics and topology deployments:\n Prometheus for scraping the metrics from the OTEL collector. Grafana for visualizing the metrics from Prometheus and Topology services using custom dashboards. CSM for Observability will use secrets to get details about the storage systems used by the CSI drivers. These secrets should be copied from the namespaces where the drivers are deployed. CSI Powerflex driver uses the ‘vxflexos-config’ secret and CSI PowerStore uses the ‘powerstore-config’ secret.  Deployment Architectures CSM for Observability can be deployed to either direct storage system requests directly to the storage system or through the CSM for Authorization proxy. The CSI driver must be configured to route storage system requests through the CSM for Authorization proxy in order for CSM for Observability to do the same.\nDefault Deployment of CSM for Observability Deployment of CSM for Observability with CSM for Authorization ","excerpt":"The solution takes the approach that each storage system that …","ref":"/csm-docs/v3/observability/design/","title":"Design"},{"body":"This section covers CSM for Resiliency’s design. The detail is sufficient that you should be able to understand what CSM for Resiliency is designed to do in various situations and how it works. CSM for Resiliency is deployed as a sidecar named podmon with a CSI driver in both the controller pods and node pods. These are referred to as controller-podmon and node-podmon respectively.\nGenerally controller-podmon and the driver controller pods are deployed using a Deployment. The Deployments support one or multiple replicas for High Availability and use a standard K8S leader election protocol so that only one controller is active at a time (as does the driver and all the controller sidecars.) The controller deployment also supports a Node Selector that allows the controllers to be placed on K8S Manager (non Worker) nodes.\nNode-podmon and the driver node pods are deployed in a DaemonSet, with a Pod deployed on every K8S Worker Node.\nController-Podmon Controller-podmon is responsible for:\n  Setting up a Watch for CSM for Resiliency labeled pods, and if a Pod is Initialized but Not Ready and resident on a Node with a NoSchedule or NoExecute taint, calling controllerCleanupPod to cleanup the pod so that a replacement pod can be scheduled.\n  Periodically polling the arrays to see if it has connectivity to the nodes that are hosting CSM for Resiliency labeled pods (if enabled.) If an array has lost connectivity to a node hosting CSM for Resiliency labeled pods using that array, controllerCleanupPod is invoked to cleanup the pods that have lost I/O connectivity.\n  Tainting nodes that have failed so that a) no further pods will get scheduled to them until they are returned to service, and b) podmon-node upon seeing the taint will invoke the cleanup operations to make sure any zombie pods (pods that have been replaced) cannot write to the volumes they were using.\n  If a CSM for Resiliency labeled pod enters a CrashLoopBackOff state, deleting that pod so it can be replaced.\n  ControllerCleanupPod cleans up the pod by taking the following actions:\n The VolumeAttachments (VAs) are loaded, and all VAs belonging to the pod being cleaned up are identified. The PVs for each VolumeAttachment are identified and used to get the Volume Handle (array identifier for the volume.) If enabled, the array is queried if any of the volumes to the pod are still doing I/O. If so, cleanup is aborted. The pod’s volumes are “fenced” from the node the pod resides on to prevent any potential I/O from a zombie pod. This is done by calling the CSI ControllerUnpublishVolume call for each of the volumes. A taint is applied to the node to keep any new pods from being scheduled to the node. If the replacement pod were to get scheduled to the same node as a zombie pod, they might both gain access to the volume concurrently causing corruption. The VolumeAttachments for the pod is deleted. This is necessary so the replacement pod to be created can attach the volumes. The pod is forcibly deleted so that a StatefulSet controller which created the pod is free to create a replacement pod.  Node-Podmon Node-podmon has the following responsibilities:\n Establishing a pod watch which is used to maintain a list of pods executing on this node that may need to be cleaned up. The list includes information about each Mount volume or Block volume used by the pod including the volume handle, volume name, private mount path, and mount path in the pod. Periodically (every 30 seconds) polling to see if controller-podmon has applied a taint to the node. If so, node-podmon calls nodeModeCleanupPod for each pod to clean up any remnants of the pod (which is potentially a zombie pod.) If all pods have been successfully cleaned up, and there are no labeled pods on this node still existing, only then will node-podmon remove the taint placed on the node by controller-podmon.  NodeModeCleanupPod cleans up the pod remnants by taking the following actions for each volume used by the pod:\n Calling NodeUnpublishVolume to unpublish the volume from the pod. Unmounting and deleting the target path for the volume. Calling NodeUnstageVolume to unpublish the volume from the node. Unmounting and deleting the staging path for the volume.  Design Limitations There are some limitations with the current design. Some might be able to be addressed in the future- others are inherent in the approach.\n The design relies on the array’s ability to revoke access to a volume for a particular node for the fencing operation. The granularity of access control for a volume is per node. Consequently, it isn’t possible to revoke access from one pod on a node while retaining access to another pod on the same node if we cannot communicate with the node. The implications of this are that if more than one pod on a node is sharing the same volume(s), they all must be protected by CSM for Resiliency, and they all must be cleaned up by controller-podmon if the node fails. If only some of the pods are cleaned up, the other pods will lose access to the volumes shared with pods that have been cleaned, so those pods should also fail. The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint. If the node failure is short-lived and controller-podmon has not evacuated some of the protected pods on the node, they may try and restart on the same pod. This has been observed to cause such pods to go into CrashLoopBackoff. We are currently considering solutions to this problem.  ","excerpt":"This section covers CSM for Resiliency’s design. The detail is …","ref":"/csm-docs/v3/resiliency/design/","title":"Design"},{"body":"What is a container: https://www.docker.com/resources/what-container\nWhat is container orchestration: https://www.infoworld.com/article/3268073/what-is-kubernetes-container-orchestration-explained.html\nKubernetes (k8s): https://kubernetes.io/\nDocker: https://www.docker.com/\nUnderstanding CSI: https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b\nHow to write a CSI plugin: https://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/\n","excerpt":"What is a container: https://www.docker.com/resources/what-container …","ref":"/csm-docs/docs/grasp/start/","title":"Getting Started"},{"body":"What is a container: https://www.docker.com/resources/what-container\nWhat is container orchestration: https://www.infoworld.com/article/3268073/what-is-kubernetes-container-orchestration-explained.html\nKubernetes (k8s): https://kubernetes.io/\nDocker: https://www.docker.com/\nUnderstanding CSI: https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b\nHow to write a CSI plugin: https://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/\n","excerpt":"What is a container: https://www.docker.com/resources/what-container …","ref":"/csm-docs/v1/grasp/start/","title":"Getting Started"},{"body":"What is a container: https://www.docker.com/resources/what-container\nWhat is container orchestration: https://www.infoworld.com/article/3268073/what-is-kubernetes-container-orchestration-explained.html\nKubernetes (k8s): https://kubernetes.io/\nDocker: https://www.docker.com/\nUnderstanding CSI: https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b\nHow to write a CSI plugin: https://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/\n","excerpt":"What is a container: https://www.docker.com/resources/what-container …","ref":"/csm-docs/v2/grasp/start/","title":"Getting Started"},{"body":"What is a container: https://www.docker.com/resources/what-container\nWhat is container orchestration: https://www.infoworld.com/article/3268073/what-is-kubernetes-container-orchestration-explained.html\nKubernetes (k8s): https://kubernetes.io/\nDocker: https://www.docker.com/\nUnderstanding CSI: https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b\nHow to write a CSI plugin: https://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/\n","excerpt":"What is a container: https://www.docker.com/resources/what-container …","ref":"/csm-docs/v3/grasp/start/","title":"Getting Started"},{"body":"Volume Snapshot Feature The CSI PowerFlex driver versions 2.0 and higher support v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  Volume Snapshot Class Installation of PowerFlex driver v1.5 and later does not create VolumeSnapshotClass. You can find a sample of a default v1 VolumeSnapshotClass instance in samples/volumesnapshotclass directory. If needed, you can install the default sample. Following is the default sample for v1:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com # Configure what happens to a VolumeSnapshotContent when the VolumeSnapshot object # it is bound to is to be deleted # Allowed values: # Delete: the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. # Retain: both the underlying snapshot and VolumeSnapshotContent remain. deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Create Consistent Snapshot of Group of Volumes This feature extends CSI specification to add the capability to create crash-consistent snapshots of a group of volumes. This feature is available as a technical preview. To use this feature, users have to deploy the csi-volumegroupsnapshotter side-car as part of the PowerFlex driver. Once the sidecar has been deployed, users can make snapshots by using yaml files, More information can be found here: Volume Group Snapshotter.\nVolume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, which is when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true.\nFollowing is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound  NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the next highest multiple of 8.\n Volume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nCustom File System Format Options The CSI PowerFlex driver version 1.5 and later support additional mkfs format options. A user is able to specify additional format options as needed for the driver. Format options are specified in storageclass yaml under mkfsFormatOption as in the following example:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \u003cSTORAGE_POOL\u003e # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID mkfsFormatOption: \"\u003cmkfs_format_option\u003e\" # Insert file system format option volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e # Insert System ID values: - csi-vxflexos.dellemc.com  WARNING: Before utilizing format options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.  Topology Support The CSI PowerFlex driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\n The PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed.  This Topology support does not include customer-defined topology, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that the pod schedule takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\n NOTE: In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\n Controller HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\n NOTE: If the controller count is greater than the number of available nodes, excess controller pods will be stuck in a pending state.\n If you are using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file at csi-vxflexos/helm/csi-vxflexos/values.yaml:\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\"  NOTE: Tolerations/selectors work the same way for node pods.\n For configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nSDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Kubernetes nodes which run the node portion of the CSI driver. The deployment of the SDC kernel module occurs on these nodes with OS platforms which support automatic SDC deployment: currently only Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\n On Kubernetes nodes which run the node portion of the CSI driver, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the nodes with OS version which supports automatic SDC deployment. If there is an SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On nodes that do not support automatic SDC deployment by SDC init container, manual installation steps must be followed. The SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions. There is no automated uninstallation of the SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from the node.  Multiarray Support The CSI PowerFlex driver version 1.4 added support for managing multiple PowerFlex arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration There is a sample yaml file in the samples folder under the top-level directory called config.yaml with the following content:\n# Username for accessing PowerFlex system.\t- username:\"admin\"# Password for accessing PowerFlex system.\tpassword:\"password\"# System name/ID of PowerFlex system.\tsystemID:\"ID1\"# REST API gateway HTTPS endpoint for PowerFlex system.endpoint:\"https://127.0.0.1\"# Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface.# Allowed values: true or false# Default value: trueskipCertificateValidation:true# indicates if this array is the default array# needed for backwards compatibility# only one array is allowed to have this set to true # Default value: falseisDefault:true# defines the MDM(s) that SDC should register with on start.# Allowed values: a list of IP addresses or hostnames separated by comma.# Default value: none mdm:\"10.0.0.1,10.0.0.2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"Here we specify that we want the CSI driver to manage two arrays: one with an IP 127.0.0.1 and the other with an IP 127.0.0.2.\nTo use this config we need to create a Kubernetes secret from it. To do so, run the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml\nDynamic Array Configuration To update or change any array configuration property, edit the secret. The driver will detect the change automatically and use the new values based on the Kubernetes watcher file change detection time. You can use kubectl command to delete the current secret and create a new secret with changes. For example, refer yaml above and change only the password.\n- username:\"admin\"password:\"Password123\"to\n- username:\"admin\"password:\"Password456\"Below are sample command lines to delete a secret and create modified properties from file secret.yaml.\nkubectl delete secret vxflexos-config -n vxflexos kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=./secret.yaml Dynamic array configuration change detection is only used for properties of an existing array, like username or password. To add a new array to the secret, or to alter an array’s mdm field, you must run csi-install.sh with --upgrade option to update the MDM key in secret and restart the node pods.\ncd \u003cDRIVER-HOME\u003e/dell-csi-helm-installer ./csi-install.sh --upgrade --namespace vxflexos --values ../helm/csi-vxflexos/values.yaml kubectl delete pods --all -n vxflexos Creating storage classes To be able to provision Kubernetes volumes using a specific array, we need to create corresponding storage classes.\nFind the sample yaml files under samples/storageclass. Edit storageclass.yaml if you want ext4 filesystem, and use storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have, and replace \u003cSYSTEM_ID\u003e with the system ID or system name for the array you’d like to use.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl apply -f storageclass.yaml After that, you can use the storage class for the corresponding array.\nEphemeral Inline Volume Starting from version 1.4, CSI PowerFlex driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest (found in csi-vxflexos/test/helm/ephemeral) for creating ephemeral volume in pod manifest with CSI PowerFlex driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumesspec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data0\"name:my-csi-volume- mountPath:\"/data1\"name:my-csi-volume-xfsvolumes:- name:my-csi-volumecsi:driver:csi-vxflexos.dellemc.comfsType:\"ext4\"volumeAttributes:volumeName:\"my-csi-volume\"size:\"8Gi\"storagepool:samplesystemID:sample- name:my-csi-volume-xfscsi:driver:csi-vxflexos.dellemc.comfsType:\"xfs\"volumeAttributes:volumeName:\"my-csi-volume-xfs\"size:\"10Gi\"storagepool:samplesystemID:sampleThis manifest creates a pod and attach two newly created ephemeral inline csi volumes to it, one ext4 and the other xfs.\nTo run the corresponding helm test, go to csi-vxflexos/test/helm/ephemeral and fill in the values for storagepool and systemID in sample.yaml.\nThen run:\n./testEphemeral.sh this test deploys the pod with two ephemeral volumes, and write some data to them before deleting the pod.\nWhen creating ephemeral volumes, it is important to specify the following within the volumeAttributes section: volumeName, size, storagepool, and if you want to use a non-default array, systemID.\nDynamic Logging Configuration The dynamic logging configuration that was introduced in v1.5 of the driver was revamped for v2.0; v1.5 logging configuration is not compatible with v2.0.\nTwo fields in values.yaml (located at helm/csi-vxflexos/values.yaml) are used to configure the dynamic logging: logLevel and logFormat.\n# CSI driver log level # Allowed values: \"error\", \"warn\"/\"warning\", \"info\", \"debug\" # Default value: \"debug\" logLevel: \"debug\" # CSI driver log format # Allowed values: \"TEXT\" or \"JSON\" # Default value: \"TEXT\" logFormat: \"TEXT\" To change the logging fields after the driver is deployed, you can use this command to edit the configmap:\nkubectl edit configmap -n vxflexos vxflexos-config-params\nand then make the necessary adjustments for CSI_LOG_LEVEL and CSI_LOG_FORMAT.\nIf either option is set to a value outside of what is supported, the driver will use the default values of “debug” and “text” .\nVolume Health Monitoring  NOTE: This feature requires the alpha feature gate, CSIVolumeHealth to be set to true. If the feature gate is on, and you want to use this feature, ensure the proper values are enabled in your values file. See the values table in the installation doc for more details.\n Starting in version 2.1, CSI Driver for PowerFlex now supports volume health monitoring. This allows Kubernetes to report on the condition of the underlying volumes via events when a volume condition is abnormal. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nTo accomplish this, the driver utilizes the external-health-monitor sidecar. When driver detects a volume condition is abnormal, the sidecar will report an event to the corresponding PVC. For example, in this event from kubectl describe pvc -n \u003cns\u003e we can see that the underlying volume was deleted from the PowerFlex array:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 32s csi-pv-monitor-controller-csi-vxflexos.dellemc.com Volume is not found at 2021-11-03 20:31:04 Events will also be reported to pods that have abnormal volumes. In these two events from kubectl describe pods -n \u003cns\u003e, we can see that this pod has two abnormal volumes: one volume was unmounted outside of Kubernetes, while another was deleted from PowerFlex array.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 35s (x9 over 12m) kubelet Volume vol4: volPath: /var/.../rhel-705f0dcbf1/mount is not mounted: \u003cnil\u003e Warning VolumeConditionAbnormal 5s kubelet Volume vol2: Volume is not found by node driver at 2021-11-11 02:04:49 ","excerpt":"Volume Snapshot Feature The CSI PowerFlex driver versions 2.0 and …","ref":"/csm-docs/docs/csidriver/features/powerflex/","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v2.2 to v2.3 using Helm Steps\n Run git clone -b v2.3.0 https://github.com/dell/csi-powerflex.git to clone the git repository and get the v2.3.0 driver. You need to create config.yaml with the configuration of your system. Check this section in installation documentation: Install the Driver Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  NOTE:\n If you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver. To update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade. The logging configuration from v1.5 will not work in v2.1, since the log configuration parameters are now set in the values.yaml file located at helm/csi-vxflexos/values.yaml. Please set the logging configuration parameters in the values.yaml file.  Upgrade using Dell CSI Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\n Please upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell PowerFlex using Helm or Dell …","ref":"/csm-docs/docs/csidriver/upgradation/drivers/powerflex/","title":"PowerFlex"},{"body":"Volume Snapshot Feature The CSI PowerFlex driver version 2.0 and higher supports v1 snapshots on Kubernetes 1.21/1.22/1.23.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  Volume Snapshot Class Installation of PowerFlex driver v1.5 and later does not create VolumeSnapshotClass. You can find a sample of a default v1 VolumeSnapshotClass instance in samples/volumesnapshotclass directory. If needed, you can install the default sample. Following is the default sample for v1:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com # Configure what happens to a VolumeSnapshotContent when the VolumeSnapshot object # it is bound to is to be deleted # Allowed values: # Delete: the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. # Retain: both the underlying snapshot and VolumeSnapshotContent remain. deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Create Consistent Snapshot of Group of Volumes This feature extends CSI specification to add the capability to create crash-consistent snapshots of a group of volumes. This feature is available as a technical preview. To use this feature, users have to deploy the csi-volumegroupsnapshotter side-car as part of the PowerFlex driver. Once the sidecar has been deployed, users can make snapshots by using yaml files such as this one:\napiVersion: volumegroup.storage.dell.com/v1 kind: DellCsiVolumeGroupSnapshot metadata: name: \"vg-snaprun1\" namespace: \"helmtest-vxflexos\" spec: # Add fields here driverName: \"csi-vxflexos.dellemc.com\" # defines how to process VolumeSnapshot members when volume group snapshot is deleted # \"Retain\" - keep VolumeSnapshot instances # \"Delete\" - delete VolumeSnapshot instances memberReclaimPolicy: \"Retain\" volumesnapshotclass: \"vxflexos-snapclass\" pvcLabel: \"vgs-snap-label\" # pvcList: # - \"pvcName1\" # - \"pvcName2\" The pvcLabel field specifies a label that must be present in PVCs that are to be snapshotted. Here is a sample of that portion of a .yaml for a PVC:\nmetadata: name: pvol0 namespace: helmtest-vxflexos labels: volume-group: vgs-snap-label More details about the installation and use of the VolumeGroup Snapshotter can be found here: dell-csi-volumegroup-snapshotter.\nVolume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, which is when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true.\nFollowing is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound  NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the next highest multiple of 8.\n Volume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nCustom File System Format Options The CSI PowerFlex driver version 1.5 and later support additional mkfs format options. A user is able to specify additional format options as needed for the driver. Format options are specified in storageclass yaml under mkfsFormatOption as in the following example:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \u003cSTORAGE_POOL\u003e # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID mkfsFormatOption: \"\u003cmkfs_format_option\u003e\" # Insert file system format option volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e # Insert System ID values: - csi-vxflexos.dellemc.com  WARNING: Before utilizing format options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.  Topology Support The CSI PowerFlex driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\n The PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed.  This Topology support does not include customer-defined topology, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that the pod schedule takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\n NOTE: In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\n Controller HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\n NOTE: If the controller count is greater than the number of available nodes, excess controller pods will be stuck in a pending state.\n If you are using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file at csi-vxflexos/helm/csi-vxflexos/values.yaml:\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\"  NOTE: Tolerations/selectors work the same way for node pods.\n For configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nSDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Kubernetes nodes which run the node portion of the CSI driver. The deployment of the SDC kernel module occurs on these nodes with OS platforms which support automatic SDC deployment: currently only Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\n On Kubernetes nodes which run the node portion of the CSI driver, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the nodes with OS version which supports automatic SDC deployment. If there is an SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On nodes that do not support automatic SDC deployment by SDC init container, manual installation steps must be followed. The SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions. There is no automated uninstallation of the SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from the node.  Multiarray Support The CSI PowerFlex driver version 1.4 added support for managing multiple PowerFlex arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration There is a sample yaml file in the samples folder under the top-level directory called config.yaml with the following content:\n# Username for accessing PowerFlex system.\t- username:\"admin\"# Password for accessing PowerFlex system.\tpassword:\"password\"# System name/ID of PowerFlex system.\tsystemID:\"ID1\"# REST API gateway HTTPS endpoint for PowerFlex system.endpoint:\"https://127.0.0.1\"# Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface.# Allowed values: true or false# Default value: trueskipCertificateValidation:true# indicates if this array is the default array# needed for backwards compatibility# only one array is allowed to have this set to true # Default value: falseisDefault:true# defines the MDM(s) that SDC should register with on start.# Allowed values: a list of IP addresses or hostnames separated by comma.# Default value: none mdm:\"10.0.0.1,10.0.0.2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"Here we specify that we want the CSI driver to manage two arrays: one with an IP 127.0.0.1 and the other with an IP 127.0.0.2.\nTo use this config we need to create a Kubernetes secret from it. To do so, run the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml\nDynamic Array Configuration To update or change any array configuration property, edit the secret. The driver will detect the change automatically and use the new values based on the Kubernetes watcher file change detection time. You can use kubectl command to delete the current secret and create a new secret with changes. For example, refer yaml above and change only the password.\n- username:\"admin\"password:\"Password123\"to\n- username:\"admin\"password:\"Password456\"Below are sample command lines to delete a secret and create modified properties from file secret.yaml.\nkubectl delete secret vxflexos-config -n vxflexos kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=./secret.yaml Dynamic array configuration change detection is only used for properties of an existing array, like username or password. To add a new array to the secret, or to alter an array’s mdm field, you must run csi-install.sh with --upgrade option to update the MDM key in secret and restart the node pods.\ncd \u003cDRIVER-HOME\u003e/dell-csi-helm-installer ./csi-install.sh --upgrade --namespace vxflexos --values ../helm/csi-vxflexos/values.yaml kubectl delete pods --all -n vxflexos Creating storage classes To be able to provision Kubernetes volumes using a specific array, we need to create corresponding storage classes.\nFind the sample yaml files under samples/storageclass. Edit storageclass.yaml if you want ext4 filesystem, and use storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have, and replace \u003cSYSTEM_ID\u003e with the system ID or system name for the array you’d like to use.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl apply -f storageclass.yaml After that, you can use the storage class for the corresponding array.\nEphemeral Inline Volume Starting from version 1.4, CSI PowerFlex driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest (found in csi-vxflexos/test/helm/ephemeral) for creating ephemeral volume in pod manifest with CSI PowerFlex driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumesspec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data0\"name:my-csi-volume- mountPath:\"/data1\"name:my-csi-volume-xfsvolumes:- name:my-csi-volumecsi:driver:csi-vxflexos.dellemc.comfsType:\"ext4\"volumeAttributes:volumeName:\"my-csi-volume\"size:\"8Gi\"storagepool:samplesystemID:sample- name:my-csi-volume-xfscsi:driver:csi-vxflexos.dellemc.comfsType:\"xfs\"volumeAttributes:volumeName:\"my-csi-volume-xfs\"size:\"10Gi\"storagepool:samplesystemID:sampleThis manifest creates a pod and attach two newly created ephemeral inline csi volumes to it, one ext4 and the other xfs.\nTo run the corresponding helm test, go to csi-vxflexos/test/helm/ephemeral and fill in the values for storagepool and systemID in sample.yaml.\nThen run:\n./testEphemeral.sh this test deploys the pod with two ephemeral volumes, and write some data to them before deleting the pod.\nWhen creating ephemeral volumes, it is important to specify the following within the volumeAttributes section: volumeName, size, storagepool, and if you want to use a non-default array, systemID.\nDynamic Logging Configuration The dynamic logging configuration that was introduced in v1.5 of the driver was revamped for v2.0; v1.5 logging configuration is not compatible with v2.0.\nTwo fields in values.yaml (located at helm/csi-vxflexos/values.yaml) are used to configure the dynamic logging: logLevel and logFormat.\n# CSI driver log level # Allowed values: \"error\", \"warn\"/\"warning\", \"info\", \"debug\" # Default value: \"debug\" logLevel: \"debug\" # CSI driver log format # Allowed values: \"TEXT\" or \"JSON\" # Default value: \"TEXT\" logFormat: \"TEXT\" To change the logging fields after the driver is deployed, you can use this command to edit the configmap:\nkubectl edit configmap -n vxflexos vxflexos-config-params\nand then make the necessary adjustments for CSI_LOG_LEVEL and CSI_LOG_FORMAT.\nIf either option is set to a value outside of what is supported, the driver will use the default values of “debug” and “text” .\nVolume Health Monitoring  NOTE: This feature requires the alpha feature gate, CSIVolumeHealth to be set to true. If the feature gate is on, and you want to use this feature, ensure the proper values are enabled in your values file. See the values table in the installation doc for more details.\n Starting in version 2.1, CSI Driver for PowerFlex now supports volume health monitoring. This allows Kubernetes to report on the condition of the underlying volumes via events when a volume condition is abnormal. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nTo accomplish this, the driver utilizes the external-health-monitor sidecar. When driver detects a volume condition is abnormal, the sidecar will report an event to the corresponding PVC. For example, in this event from kubectl describe pvc -n \u003cns\u003e we can see that the underlying volume was deleted from the PowerFlex array:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 32s csi-pv-monitor-controller-csi-vxflexos.dellemc.com Volume is not found at 2021-11-03 20:31:04 Events will also be reported to pods that have abnormal volumes. In these two events from kubectl describe pods -n \u003cns\u003e, we can see that this pod has two abnormal volumes: one volume was unmounted outside of Kubernetes, while another was deleted from PowerFlex array.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 35s (x9 over 12m) kubelet Volume vol4: volPath: /var/.../rhel-705f0dcbf1/mount is not mounted: \u003cnil\u003e Warning VolumeConditionAbnormal 5s kubelet Volume vol2: Volume is not found by node driver at 2021-11-11 02:04:49 ","excerpt":"Volume Snapshot Feature The CSI PowerFlex driver version 2.0 and …","ref":"/csm-docs/v1/csidriver/features/powerflex/","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v2.1 to v2.2 using Helm Steps\n Run git clone -b v2.2.0 https://github.com/dell/csi-powerflex.git to clone the git repository and get the v2.2.0 driver. You need to create config.yaml with the configuration of your system. Check this section in installation documentation: Install the Driver You must set the only system managed in v1.5/v2.0/v2.1 driver as default in config.json in v2.2 so that the driver knows the existing volumes belong to that system. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  NOTE:\n If you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver. To update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade. The logging configuration from v1.5 will not work in v2.1, since the log configuration parameters are now set in the values.yaml file located at helm/csi-vxflexos/values.yaml. Please set the logging configuration parameters in the values.yaml file.  Upgrade using Dell CSI Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\n Please upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell PowerFlex using Helm or Dell …","ref":"/csm-docs/v1/csidriver/upgradation/drivers/powerflex/","title":"PowerFlex"},{"body":"Volume Snapshot Feature The CSI PowerFlex driver version 2.0 and higher supports v1 snapshots on Kubernetes 1.21/1.22/1.23.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  Volume Snapshot Class Installation of PowerFlex driver v1.5 and later does not create VolumeSnapshotClass. You can find a sample of a default v1 VolumeSnapshotClass instance in samples/volumesnapshotclass directory. If needed, you can install the default sample. Following is the default sample for v1:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com # Configure what happens to a VolumeSnapshotContent when the VolumeSnapshot object # it is bound to is to be deleted # Allowed values: # Delete: the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. # Retain: both the underlying snapshot and VolumeSnapshotContent remain. deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Create Consistent Snapshot of Group of Volumes This feature extends CSI specification to add the capability to create crash-consistent snapshots of a group of volumes. This feature is available as a technical preview. To use this feature, users have to deploy the csi-volumegroupsnapshotter side-car as part of the PowerFlex driver. Once the sidecar has been deployed, users can make snapshots by using yaml files such as this one:\napiVersion: volumegroup.storage.dell.com/v1 kind: DellCsiVolumeGroupSnapshot metadata: name: \"vg-snaprun1\" namespace: \"helmtest-vxflexos\" spec: # Add fields here driverName: \"csi-vxflexos.dellemc.com\" # defines how to process VolumeSnapshot members when volume group snapshot is deleted # \"Retain\" - keep VolumeSnapshot instances # \"Delete\" - delete VolumeSnapshot instances memberReclaimPolicy: \"Retain\" volumesnapshotclass: \"vxflexos-snapclass\" pvcLabel: \"vgs-snap-label\" # pvcList: # - \"pvcName1\" # - \"pvcName2\" The pvcLabel field specifies a label that must be present in PVCs that are to be snapshotted. Here is a sample of that portion of a .yaml for a PVC:\nmetadata: name: pvol0 namespace: helmtest-vxflexos labels: volume-group: vgs-snap-label More details about the installation and use of the VolumeGroup Snapshotter can be found here: dell-csi-volumegroup-snapshotter.\nVolume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, which is when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true.\nFollowing is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound  NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the next highest multiple of 8.\n Volume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nCustom File System Format Options The CSI PowerFlex driver version 1.5 and later support additional mkfs format options. A user is able to specify additional format options as needed for the driver. Format options are specified in storageclass yaml under mkfsFormatOption as in the following example:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \u003cSTORAGE_POOL\u003e # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID mkfsFormatOption: \"\u003cmkfs_format_option\u003e\" # Insert file system format option volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e # Insert System ID values: - csi-vxflexos.dellemc.com  WARNING: Before utilizing format options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.  Topology Support The CSI PowerFlex driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\n The PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed.  This Topology support does not include customer-defined topology, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that the pod schedule takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\n NOTE: In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\n Controller HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\n NOTE: If the controller count is greater than the number of available nodes, excess controller pods will be stuck in a pending state.\n If you are using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file at csi-vxflexos/helm/csi-vxflexos/values.yaml:\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\"  NOTE: Tolerations/selectors work the same way for node pods.\n For configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nSDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Kubernetes nodes which run the node portion of the CSI driver. The deployment of the SDC kernel module occurs on these nodes with OS platforms which support automatic SDC deployment: currently only Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\n On Kubernetes nodes which run the node portion of the CSI driver, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the nodes with OS version which supports automatic SDC deployment. If there is an SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On nodes that do not support automatic SDC deployment by SDC init container, manual installation steps must be followed. The SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions. There is no automated uninstallation of the SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from the node.  Multiarray Support The CSI PowerFlex driver version 1.4 added support for managing multiple PowerFlex arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration There is a sample yaml file in the samples folder under the top-level directory called config.yaml with the following content:\n# Username for accessing PowerFlex system.\t- username:\"admin\"# Password for accessing PowerFlex system.\tpassword:\"password\"# System name/ID of PowerFlex system.\tsystemID:\"ID1\"# REST API gateway HTTPS endpoint for PowerFlex system.endpoint:\"https://127.0.0.1\"# Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface.# Allowed values: true or false# Default value: trueskipCertificateValidation:true# indicates if this array is the default array# needed for backwards compatibility# only one array is allowed to have this set to true # Default value: falseisDefault:true# defines the MDM(s) that SDC should register with on start.# Allowed values: a list of IP addresses or hostnames separated by comma.# Default value: none mdm:\"10.0.0.1,10.0.0.2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"Here we specify that we want the CSI driver to manage two arrays: one with an IP 127.0.0.1 and the other with an IP 127.0.0.2.\nTo use this config we need to create a Kubernetes secret from it. To do so, run the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml\nDynamic Array Configuration To update or change any array configuration property, edit the secret. The driver will detect the change automatically and use the new values based on the Kubernetes watcher file change detection time. You can use kubectl command to delete the current secret and create a new secret with changes. For example, refer yaml above and change only the password.\n- username:\"admin\"password:\"Password123\"to\n- username:\"admin\"password:\"Password456\"Below are sample command lines to delete a secret and create modified properties from file secret.yaml.\nkubectl delete secret vxflexos-config -n vxflexos kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=./secret.yaml Dynamic array configuration change detection is only used for properties of an existing array, like username or password. To add a new array to the secret, or to alter an array’s mdm field, you must run csi-install.sh with --upgrade option to update the MDM key in secret and restart the node pods.\ncd \u003cDRIVER-HOME\u003e/dell-csi-helm-installer ./csi-install.sh --upgrade --namespace vxflexos --values ../helm/csi-vxflexos/values.yaml kubectl delete pods --all -n vxflexos Creating storage classes To be able to provision Kubernetes volumes using a specific array, we need to create corresponding storage classes.\nFind the sample yaml files under samples/storageclass. Edit storageclass.yaml if you want ext4 filesystem, and use storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have, and replace \u003cSYSTEM_ID\u003e with the system ID or system name for the array you’d like to use.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl apply -f storageclass.yaml After that, you can use the storage class for the corresponding array.\nEphemeral Inline Volume Starting from version 1.4, CSI PowerFlex driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest (found in csi-vxflexos/test/helm/ephemeral) for creating ephemeral volume in pod manifest with CSI PowerFlex driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumesspec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data0\"name:my-csi-volume- mountPath:\"/data1\"name:my-csi-volume-xfsvolumes:- name:my-csi-volumecsi:driver:csi-vxflexos.dellemc.comfsType:\"ext4\"volumeAttributes:volumeName:\"my-csi-volume\"size:\"8Gi\"storagepool:samplesystemID:sample- name:my-csi-volume-xfscsi:driver:csi-vxflexos.dellemc.comfsType:\"xfs\"volumeAttributes:volumeName:\"my-csi-volume-xfs\"size:\"10Gi\"storagepool:samplesystemID:sampleThis manifest creates a pod and attach two newly created ephemeral inline csi volumes to it, one ext4 and the other xfs.\nTo run the corresponding helm test, go to csi-vxflexos/test/helm/ephemeral and fill in the values for storagepool and systemID in sample.yaml.\nThen run:\n./testEphemeral.sh this test deploys the pod with two ephemeral volumes, and write some data to them before deleting the pod.\nWhen creating ephemeral volumes, it is important to specify the following within the volumeAttributes section: volumeName, size, storagepool, and if you want to use a non-default array, systemID.\nDynamic Logging Configuration The dynamic logging configuration that was introduced in v1.5 of the driver was revamped for v2.0; v1.5 logging configuration is not compatible with v2.0.\nTwo fields in values.yaml (located at helm/csi-vxflexos/values.yaml) are used to configure the dynamic logging: logLevel and logFormat.\n# CSI driver log level # Allowed values: \"error\", \"warn\"/\"warning\", \"info\", \"debug\" # Default value: \"debug\" logLevel: \"debug\" # CSI driver log format # Allowed values: \"TEXT\" or \"JSON\" # Default value: \"TEXT\" logFormat: \"TEXT\" To change the logging fields after the driver is deployed, you can use this command to edit the configmap:\nkubectl edit configmap -n vxflexos vxflexos-config-params\nand then make the necessary adjustments for CSI_LOG_LEVEL and CSI_LOG_FORMAT.\nIf either option is set to a value outside of what is supported, the driver will use the default values of “debug” and “text” .\nVolume Health Monitoring  NOTE: This feature requires the alpha feature gate, CSIVolumeHealth to be set to true. If the feature gate is on, and you want to use this feature, ensure the proper values are enabled in your values file. See the values table in the installation doc for more details.\n Starting in version 2.1, CSI Driver for PowerFlex now supports volume health monitoring. This allows Kubernetes to report on the condition of the underlying volumes via events when a volume condition is abnormal. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nTo accomplish this, the driver utilizes the external-health-monitor sidecar. When driver detects a volume condition is abnormal, the sidecar will report an event to the corresponding PVC. For example, in this event from kubectl describe pvc -n \u003cns\u003e we can see that the underlying volume was deleted from the PowerFlex array:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 32s csi-pv-monitor-controller-csi-vxflexos.dellemc.com Volume is not found at 2021-11-03 20:31:04 Events will also be reported to pods that have abnormal volumes. In these two events from kubectl describe pods -n \u003cns\u003e, we can see that this pod has two abnormal volumes: one volume was unmounted outside of Kubernetes, while another was deleted from PowerFlex array.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 35s (x9 over 12m) kubelet Volume vol4: volPath: /var/.../rhel-705f0dcbf1/mount is not mounted: \u003cnil\u003e Warning VolumeConditionAbnormal 5s kubelet Volume vol2: Volume is not found by node driver at 2021-11-11 02:04:49 ","excerpt":"Volume Snapshot Feature The CSI PowerFlex driver version 2.0 and …","ref":"/csm-docs/v2/csidriver/features/powerflex/","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v2.1 to v2.2 using Helm Steps\n Run git clone -b v2.2.0 https://github.com/dell/csi-powerflex.git to clone the git repository and get the v2.2.0 driver. You need to create config.yaml with the configuration of your system. Check this section in installation documentation: Install the Driver You must set the only system managed in v1.5/v2.0/v2.1 driver as default in config.json in v2.2 so that the driver knows the existing volumes belong to that system. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  NOTE:\n If you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver. To update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade. The logging configuration from v1.5 will not work in v2.1, since the log configuration parameters are now set in the values.yaml file located at helm/csi-vxflexos/values.yaml. Please set the logging configuration parameters in the values.yaml file.  Upgrade using Dell CSI Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\n Please upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell PowerFlex using Helm or Dell …","ref":"/csm-docs/v2/csidriver/upgradation/drivers/powerflex/","title":"PowerFlex"},{"body":"Volume Snapshot Feature The CSI PowerFlex driver version 2.0 and higher supports v1 snapshots on Kubernetes 1.20/1.21/1.22.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  Volume Snapshot Class Installation of PowerFlex driver v1.5 and later does not create VolumeSnapshotClass. You can find a sample of a default v1 VolumeSnapshotClass instance in samples/volumesnapshotclass directory. If needed, you can install the default sample. Following is the default sample for v1:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com # Configure what happens to a VolumeSnapshotContent when the VolumeSnapshot object # it is bound to is to be deleted # Allowed values: # Delete: the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. # Retain: both the underlying snapshot and VolumeSnapshotContent remain. deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \"2020-07-16T08:42:12Z\" readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Create Consistent Snapshot of Group of Volumes This feature extends CSI specification to add the capability to create crash-consistent snapshots of a group of volumes. This feature is available as a technical preview. To use this feature, users have to deploy the csi-volumegroupsnapshotter side-car as part of the PowerFlex driver. Once the sidecar has been deployed, users can make snapshots by using yaml files such as this one:\napiVersion: volumegroup.storage.dell.com/v1alpha2 kind: DellCsiVolumeGroupSnapshot metadata: # Name must be 13 characters or less in length name: \"vg-snaprun1\" namespace: \"helmtest-vxflexos\" spec: # Add fields here driverName: \"csi-vxflexos.dellemc.com\" # defines how to process VolumeSnapshot members when volume group snapshot is deleted # \"retain\" - keep VolumeSnapshot instances # \"delete\" - delete VolumeSnapshot instances memberReclaimPolicy: \"retain\" volumesnapshotclass: \"vxflexos-snapclass\" pvcLabel: \"vgs-snap-label\" # pvcList: # - \"pvcName1\" # - \"pvcName2\" In the metadata section, the name is limited to 13 characters because the snapshotter will append a timestamp to it. Additionally, the pvcLabel field specifies a label that must be present in PVCs that are to be snapshotted. Here is a sample of that portion of a .yaml for a PVC:\nmetadata: name: pvol0 namespace: helmtest-vxflexos labels: volume-group: vgs-snap-label More details about the installation and use of the VolumeGroup Snapshotter can be found here: dell-csi-volumegroup-snapshotter.\nVolume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, which is when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true.\nFollowing is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound  NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the next highest multiple of 8.\n Volume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \"/dev/data0\" name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nCustom File System Format Options The CSI PowerFlex driver version 1.5 and later support additional mkfs format options. A user is able to specify additional format options as needed for the driver. Format options are specified in storageclass yaml under mkfsFormatOption as in the following example:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: \u003cSTORAGE_POOL\u003e # Insert Storage pool systemID: \u003cSYSTEM_ID\u003e # Insert System ID mkfsFormatOption: \"\u003cmkfs_format_option\u003e\" # Insert file system format option volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/\u003cSYSTEM_ID\u003e # Insert System ID values: - csi-vxflexos.dellemc.com  WARNING: Before utilizing format options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.  Topology Support The CSI PowerFlex driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\n The PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed.  This Topology support does not include customer-defined topology, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that the pod schedule takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.beta.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2020-05-27T13:24:55Z\" labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \"170198\" selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\n NOTE: In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\n Controller HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\n NOTE: If the controller count is greater than the number of available nodes, excess controller pods will be stuck in a pending state.\n If you are using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file at csi-vxflexos/helm/csi-vxflexos/values.yaml:\nTo assign controller pods to worker nodes only (Default):\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: # - key: \"node-role.kubernetes.io/master\" # operator: \"Exists\" # effect: \"NoSchedule\" To assign controller pods to master and worker nodes:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: # node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" To assign controller pods to master nodes only:\n# \"controller\" allows to configure controller specific parameters controller: #\"controller.nodeSelector\" defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes # Allowed values: map of key-value pairs # Default value: None # Examples: # node-role.kubernetes.io/master: \"\" nodeSelector: node-role.kubernetes.io/master: \"\" # \"controller.tolerations\" defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes # Default value: None tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\"  NOTE: Tolerations/selectors work the same way for node pods.\n For configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nSDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Kubernetes nodes which run the node portion of the CSI driver. The deployment of the SDC kernel module occurs on these nodes with OS platforms which support automatic SDC deployment: currently only Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\n On Kubernetes nodes which run the node portion of the CSI driver, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the nodes with OS version which supports automatic SDC deployment. If there is an SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On nodes that do not support automatic SDC deployment by SDC init container, manual installation steps must be followed. The SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions. There is no automated uninstallation of the SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from the node.  Multiarray Support The CSI PowerFlex driver version 1.4 added support for managing multiple PowerFlex arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration There is a sample yaml file in the samples folder under the top-level directory called config.yaml with the following content:\n# Username for accessing PowerFlex system.\t- username:\"admin\"# Password for accessing PowerFlex system.\tpassword:\"password\"# System name/ID of PowerFlex system.\tsystemID:\"ID1\"# REST API gateway HTTPS endpoint for PowerFlex system.endpoint:\"https://127.0.0.1\"# Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface.# Allowed values: true or false# Default value: trueskipCertificateValidation:true# indicates if this array is the default array# needed for backwards compatibility# only one array is allowed to have this set to true # Default value: falseisDefault:true# defines the MDM(s) that SDC should register with on start.# Allowed values: a list of IP addresses or hostnames separated by comma.# Default value: none mdm:\"10.0.0.1,10.0.0.2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"Here we specify that we want the CSI driver to manage two arrays: one with an IP 127.0.0.1 and the other with an IP 127.0.0.2.\nTo use this config we need to create a Kubernetes secret from it. To do so, run the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml\nDynamic Array Configuration To update or change any array configuration property, edit the secret. The driver will detect the change automatically and use the new values based on the Kubernetes watcher file change detection time. You can use kubectl command to delete the current secret and create a new secret with changes. For example, refer yaml above and change only the password.\n- username:\"admin\"password:\"Password123\"to\n- username:\"admin\"password:\"Password456\"Below are sample command lines to delete a secret and create modified properties from file secret.yaml.\nkubectl delete secret vxflexos-config -n vxflexos kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=./secret.yaml Dynamic array configuration change detection is only used for properties of an existing array, like username or password. To add a new array to the secret, or to alter an array’s mdm field, you must run csi-install.sh with --upgrade option to update the MDM key in secret and restart the node pods.\ncd \u003cDRIVER-HOME\u003e/dell-csi-helm-installer ./csi-install.sh --upgrade --namespace vxflexos --values ../helm/csi-vxflexos/values.yaml kubectl delete pods --all -n vxflexos Creating storage classes To be able to provision Kubernetes volumes using a specific array, we need to create corresponding storage classes.\nFind the sample yaml files under samples/storageclass. Edit storageclass.yaml if you want ext4 filesystem, and use storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have, and replace \u003cSYSTEM_ID\u003e with the system ID or system name for the array you’d like to use.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl apply -f storageclass.yaml After that, you can use the storage class for the corresponding array.\nEphemeral Inline Volume Starting from version 1.4, CSI PowerFlex driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest (found in csi-vxflexos/test/helm/ephemeral) for creating ephemeral volume in pod manifest with CSI PowerFlex driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumesspec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data0\"name:my-csi-volume- mountPath:\"/data1\"name:my-csi-volume-xfsvolumes:- name:my-csi-volumecsi:driver:csi-vxflexos.dellemc.comfsType:\"ext4\"volumeAttributes:volumeName:\"my-csi-volume\"size:\"8Gi\"storagepool:samplesystemID:sample- name:my-csi-volume-xfscsi:driver:csi-vxflexos.dellemc.comfsType:\"xfs\"volumeAttributes:volumeName:\"my-csi-volume-xfs\"size:\"10Gi\"storagepool:samplesystemID:sampleThis manifest creates a pod and attach two newly created ephemeral inline csi volumes to it, one ext4 and the other xfs.\nTo run the corresponding helm test, go to csi-vxflexos/test/helm/ephemeral and fill in the values for storagepool and systemID in sample.yaml.\nThen run:\n./testEphemeral.sh this test deploys the pod with two ephemeral volumes, and write some data to them before deleting the pod.\nWhen creating ephemeral volumes, it is important to specify the following within the volumeAttributes section: volumeName, size, storagepool, and if you want to use a non-default array, systemID.\nDynamic Logging Configuration The dynamic logging configuration that was introduced in v1.5 of the driver was revamped for v2.0; v1.5 logging configuration is not compatible with v2.0.\nTwo fields in values.yaml (located at helm/csi-vxflexos/values.yaml) are used to configure the dynamic logging: logLevel and logFormat.\n# CSI driver log level # Allowed values: \"error\", \"warn\"/\"warning\", \"info\", \"debug\" # Default value: \"debug\" logLevel: \"debug\" # CSI driver log format # Allowed values: \"TEXT\" or \"JSON\" # Default value: \"TEXT\" logFormat: \"TEXT\" To change the logging fields after the driver is deployed, you can use this command to edit the configmap:\nkubectl edit configmap -n vxflexos vxflexos-config-params\nand then make the necessary adjustments for CSI_LOG_LEVEL and CSI_LOG_FORMAT.\nIf either option is set to a value outside of what is supported, the driver will use the default values of “debug” and “text” .\nVolume Health Monitoring  NOTE: This feature requires the alpha feature gate, CSIVolumeHealth to be set to true. If the feature gate is on, and you want to use this feature, ensure the proper values are enabled in your values file. See the values table in the installation doc for more details.\n Starting in version 2.1, CSI Driver for PowerFlex now supports volume health monitoring. This allows Kubernetes to report on the condition of the underlying volumes via events when a volume condition is abnormal. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nTo accomplish this, the driver utilizes the external-health-monitor sidecar. When driver detects a volume condition is abnormal, the sidecar will report an event to the corresponding PVC. For example, in this event from kubectl describe pvc -n \u003cns\u003e we can see that the underlying volume was deleted from the PowerFlex array:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 32s csi-pv-monitor-controller-csi-vxflexos.dellemc.com Volume is not found at 2021-11-03 20:31:04 Events will also be reported to pods that have abnormal volumes. In these two events from kubectl describe pods -n \u003cns\u003e, we can see that this pod has two abnormal volumes: one volume was unmounted outside of Kubernetes, while another was deleted from PowerFlex array.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------ Warning VolumeConditionAbnormal 35s (x9 over 12m) kubelet Volume vol4: volPath: /var/.../rhel-705f0dcbf1/mount is not mounted: \u003cnil\u003e Warning VolumeConditionAbnormal 5s kubelet Volume vol2: Volume is not found by node driver at 2021-11-11 02:04:49 ","excerpt":"Volume Snapshot Feature The CSI PowerFlex driver version 2.0 and …","ref":"/csm-docs/v3/csidriver/features/powerflex/","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v2.0 to v2.1 using Helm Steps\n Run git clone -b v2.1.0 https://github.com/dell/csi-powerflex.git to clone the git repository and get the v2.0 driver. You need to create config.yaml with the configuration of your system. Check this section in installation documentation: Install the Driver You must set the only system managed in v1.5/v2.0 driver as default in config.json in v2.1 so that the driver knows the existing volumes belong to that system. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  NOTE:\n If you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver. To update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade. The logging configuration from v1.5 will not work in v2.1, since the log configuration parameters are now set in the values.yaml file located at helm/csi-vxflexos/values.yaml. Please set the logging configuration parameters in the values.yaml file.  Upgrade using Dell CSI Operator:   Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or …","ref":"/csm-docs/v3/csidriver/upgradation/drivers/powerflex/","title":"PowerFlex"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerFlex. The Grafana reference dashboards for PowerFlex metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the sdc_metrics_enabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerflex_export_node_read_bw_megabytes_per_second The export node read bandwidth (MB/s) within PowerFlex system   powerflex_export_node_write_bw_megabytes_per_second The export node write bandwidth (MB/s)   powerflex_export_node_read_latency_milliseconds The time (in ms) to complete read operations within PowerFlex system by the export node   powerflex_export_node_write_latency_milliseconds The time (in ms) to complete write operations within PowerFlex system by the export host   powerflex_export_node_read_iops_per_second The number of read operations performed by an export node (per second)   powerflex_export_node_write_iops_per_second The number of write operations performed by an export node (per second)   powerflex_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s)   powerflex_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s)   powerflex_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume   powerflex_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume   powerflex_volume_read_iops_per_second The number of read operations performed against a volume (per second)   powerflex_volume_write_iops_per_second The number of write operations performed against a volume (per second)    Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the storage_class_pool_metrics_enabled field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerflex_storage_pool_total_logical_capacity_gigabytes The logical capacity (size) of a storage pool (GB)   powerflex_storage_pool_logical_capacity_available_gigabytes The capacity available for use (GB)   powerflex_storage_pool_logical_capacity_in_use_gigabytes The logical capacity of a storage pool in use (GB)   powerflex_storage_pool_logical_provisioned_gigabytes The total size of volumes (thick and thin) provisioned in a storage pool (GB)    ","excerpt":"This section outlines the metrics collected by the Container Storage …","ref":"/csm-docs/docs/observability/metrics/powerflex/","title":"PowerFlex Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerFlex. The Grafana reference dashboards for PowerFlex metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the sdc_metrics_enabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerflex_export_node_read_bw_megabytes_per_second The export node read bandwidth (MB/s) within PowerFlex system   powerflex_export_node_write_bw_megabytes_per_second The export node write bandwidth (MB/s)   powerflex_export_node_read_latency_milliseconds The time (in ms) to complete read operations within PowerFlex system by the export node   powerflex_export_node_write_latency_milliseconds The time (in ms) to complete write operations within PowerFlex system by the export host   powerflex_export_node_read_iops_per_second The number of read operations performed by an export node (per second)   powerflex_export_node_write_iops_per_second The number of write operations performed by an export node (per second)   powerflex_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s)   powerflex_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s)   powerflex_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume   powerflex_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume   powerflex_volume_read_iops_per_second The number of read operations performed against a volume (per second)   powerflex_volume_write_iops_per_second The number of write operations performed against a volume (per second)    Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the storage_class_pool_metrics_enabled field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerflex_storage_pool_total_logical_capacity_gigabytes The logical capacity (size) of a storage pool (GB)   powerflex_storage_pool_logical_capacity_available_gigabytes The capacity available for use (GB)   powerflex_storage_pool_logical_capacity_in_use_gigabytes The logical capacity of a storage pool in use (GB)   powerflex_storage_pool_logical_provisioned_gigabytes The total size of volumes (thick and thin) provisioned in a storage pool (GB)    ","excerpt":"This section outlines the metrics collected by the Container Storage …","ref":"/csm-docs/v1/observability/metrics/powerflex/","title":"PowerFlex Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerFlex. The Grafana reference dashboards for PowerFlex metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the sdc_metrics_enabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerflex_export_node_read_bw_megabytes_per_second The export node read bandwidth (MB/s) within PowerFlex system   powerflex_export_node_write_bw_megabytes_per_second The export node write bandwidth (MB/s)   powerflex_export_node_read_latency_milliseconds The time (in ms) to complete read operations within PowerFlex system by the export node   powerflex_export_node_write_latency_milliseconds The time (in ms) to complete write operations within PowerFlex system by the export host   powerflex_export_node_read_iops_per_second The number of read operations performed by an export node (per second)   powerflex_export_node_write_iops_per_second The number of write operations performed by an export node (per second)   powerflex_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s)   powerflex_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s)   powerflex_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume   powerflex_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume   powerflex_volume_read_iops_per_second The number of read operations performed against a volume (per second)   powerflex_volume_write_iops_per_second The number of write operations performed against a volume (per second)    Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the storage_class_pool_metrics_enabled field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerflex_storage_pool_total_logical_capacity_gigabytes The logical capacity (size) of a storage pool (GB)   powerflex_storage_pool_logical_capacity_available_gigabytes The capacity available for use (GB)   powerflex_storage_pool_logical_capacity_in_use_gigabytes The logical capacity of a storage pool in use (GB)   powerflex_storage_pool_logical_provisioned_gigabytes The total size of volumes (thick and thin) provisioned in a storage pool (GB)    ","excerpt":"This section outlines the metrics collected by the Container Storage …","ref":"/csm-docs/v2/observability/metrics/powerflex/","title":"PowerFlex Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerFlex. The Grafana reference dashboards for PowerFlex metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the sdc_metrics_enabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerflex_export_node_read_bw_megabytes_per_second The export node read bandwidth (MB/s) within PowerFlex system   powerflex_export_node_write_bw_megabytes_per_second The export node write bandwidth (MB/s)   powerflex_export_node_read_latency_milliseconds The time (in ms) to complete read operations within PowerFlex system by the export node   powerflex_export_node_write_latency_milliseconds The time (in ms) to complete write operations within PowerFlex system by the export host   powerflex_export_node_read_iops_per_second The number of read operations performed by an export node (per second)   powerflex_export_node_write_iops_per_second The number of write operations performed by an export node (per second)   powerflex_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s)   powerflex_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s)   powerflex_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume   powerflex_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume   powerflex_volume_read_iops_per_second The number of read operations performed against a volume (per second)   powerflex_volume_write_iops_per_second The number of write operations performed against a volume (per second)    Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the storage_class_pool_metrics_enabled field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerflex_storage_pool_total_logical_capacity_gigabytes The logical capacity (size) of a storage pool (GB)   powerflex_storage_pool_logical_capacity_available_gigabytes The capacity available for use (GB)   powerflex_storage_pool_logical_capacity_in_use_gigabytes The logical capacity of a storage pool in use (GB)   powerflex_storage_pool_logical_provisioned_gigabytes The total size of volumes (thick and thin) provisioned in a storage pool (GB)    ","excerpt":"This section outlines the metrics collected by the Container Storage …","ref":"/csm-docs/v3/observability/metrics/powerflex/","title":"PowerFlex Metrics"},{"body":"Multi Unisphere Support Starting with v1.7, the CSI PowerMax driver can communicate with multiple Unisphere for PowerMax servers to manage multiple PowerMax arrays. In order to use this feature, you must install CSI PowerMax ReverseProxy in StandAlone mode with the driver. For more details on how to configure the driver and ReverseProxy, see the relevant section here\nVolume Snapshot Feature The CSI PowerMax driver version 1.7 and later supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class  To use this feature, enable it in values.yaml\nsnapshot:enabled:true Note: From v1.7, the CSI PowerMax driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the csi-powermax/samples/volumesnapshotclass folder\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powermax-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-restore-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-snapshot-demokind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiCreating PVCs with PVCs as source This is not supported for replicated volumes.\nThis is a sample manifest for creating a PVC with another PVC as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-clone-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-pvc-demokind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP With version 1.3.0, support has been added for the unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI. To enable CHAP authentication:\n Create secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e\nWhere is the name of the iSCSI initiator of a host IQN, and is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which is stored in the host initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name With version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this feature, set the following values under customDriverName in my-powermax-settings.yaml.\n Value: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template.  If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax, then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers To install multiple CSI Drivers for Dell PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions that should be strictly adhered to:\n Only one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations  To install multiple CSI drivers, follow these steps:\n Create (or use) a new namespace. Ensure that all the pre-requisites are met:  powermax-creds secret is created in this namespace (Optional) powermax-certs secret is created in this namespace   Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver.  Volume expansion Starting in v1.4, the CSI PowerMax driver supports the expansion of Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.\n Note: This feature is not supported for replicated volumes.\n To use this feature, enable in values.yaml\nresizer:enabled:trueTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThis is a sample manifest for a storage class that allows for Volume Expansion.\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-expand-scannotations:storageclass.kubernetes.io/is-default-class:falseprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true#Set this attribute to true if you plan to expand any PVCscreatedusingthisstorageclassparameters:SYMID:\"000000000001\"SRP:\"DEFAULT_SRP\"ServiceLevel:\"Bronze\"To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC.\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pmax-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:10Gi#Updated size from 5Gi to 10GistorageClassName:powermax-expand-scNOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of the volume, it cannot be used to shrink a volume.\nRaw block support Starting in v1.4, the CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind:StatefulSetapiVersion:apps/v1metadata:name:powermaxtestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powermaxresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy To get the maximum performance out of the CSI driver for PowerMax and Unisphere for PowerMax REST APIs, starting with v1.4 of the driver, you can deploy the optional CSI PowerMax Reverse Proxy application.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server that acts as a reverse proxy for the Unisphere for PowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy application helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and the performance of the CSI PowerMax driver.\nOptionally, you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation The CSI PowerMax Reverse Proxy can be installed in two ways:\n It can be installed as a Kubernetes deployment in the same namespace as the driver. It can be installed as a sidecar to the driver’s controller Pod.  It is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is an HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be an X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example showing how to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Using Helm installer In the my-powermax-settings.yaml file, the csireverseproxy section can be used to deploy and configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. If it is enabled (using the enabled parameter in the csireverseproxy section of the my-powermax-settings.yaml file), the install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name “powermax-reverseproxy”. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation for PowerMax.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, a new setting, modifyHostName, can be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1, then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of the controller Pod. At any time, only one controller Pod is active(leader), and the rest are on standby. In case of a failure, one of the standby Pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two-controller Pods are ever scheduled on the same node.\nTo increase or decrease the number of controller Pods, edit the following value in values.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. We recommend not changing this unless it is really necessary. Also, if the controller count is greater than the number of available nodes (where the Pods can be scheduled), some controller Pods will remain in the Pending state\n If you are using dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, see the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment and driver node Daemonset. There are two new sections in the values file - controller and node - where you can specify these values separately for the controller and node Pods.\ncontroller If you want to apply nodeSelectors and tolerations for the controller Pods, edit the controller section in the values file.\nHere are some examples:\n To schedule controller Pods to worker nodes only (Default):  controller:nodeSelector:tolerations: Set the following values for controller Pods to tolerate the taint NoSchedule on master nodes:  controller:nodeSelector:tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\" Set the following values for controller Pods to be scheduled only on nodes labelled master (node-role.kubernetes.io/master):  controller:nodeSelector:node-role.kubernetes.io/master:\"\"tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"node If you want to apply nodeSelectors and tolerations for the node Pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add and remove tolerations to this list\n# \"node\" allows to configure node specific parametersnode:# \"node.nodeSelector\" defines what nodes would be selected for Pods of node daemonset# Leave as blank to use all nodesnodeSelector:# node-role.kubernetes.io/master: \"\"# \"node.tolerations\" defines tolerations that would be applied to node daemonset# Add/Remove tolerations as per requirement# Leave as blank if you wish to not apply any tolerationstolerations:- key:\"node.kubernetes.io/memory-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/disk-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/network-unavailable\"operator:\"Exists\"effect:\"NoExecute\"Topology Support Starting from version 1.5, the CSI PowerMax driver supports topology-aware volume provisioning which helps the Kubernetes scheduler place PVCs on worker nodes that have access to the backend storage. When used with nodeSelectors which can be specified for the driver node Pods, it provides an effective way to provision applications on nodes that have access to the PowerMax array.\nAfter a successful installation of the driver, if a node Pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\n csi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi  The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\nStarting from version 2.3.0, topology keys have been enhanced to filter out arrays, associated transport protocol available to each node and create topology keys based on any such user input.\nTopology Usage To use the Topology feature, the storage classes must be modified as follows:\n volumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section  For example, a PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-fcparameters:SRP:\"SRP_1\"SYMID:\"000000000001\"ServiceLevel:\u003cServiceLevel\u003e#Insert Service Level Nameprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumerallowVolumeExpansion:trueallowedTopologies:- matchLabelExpressions:- key:csi-powermax.dellemc.com/000000000001values:- csi-powermax.dellemc.com- key:csi-powermax.dellemc.com/000000000001.fcvalues:- csi-powermax.dellemc.comIn the above example, if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\n A set of sample storage class definitions to enable topology-aware volume provisioning has been provided in the csi-powermax/samples/storageclass folder\n For additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nCustom Topology keys To use the enhanced topology keys:\n To use this feature, set node.topologyControl.enabled to true. Edit the config file topologyConfig.yaml in csi-powermax/samples/configmap folder and provide values for the following parameters.     Parameter Description     allowedConnections List of node, array and protocol info for user allowed configuration   allowedConnections.nodeName Name of the node on which user wants to apply given rules   allowedConnections.rules List of StorageArrayID:TransportProtocol pair   deniedConnections List of node, array and protocol info for user denied configuration   deniedConnections.nodeName Name of the node on which user wants to apply given rules   deniedConnections.rules List of StorageArrayID:TransportProtocol pair    Sample config file:\n# allowedConnections contains a list of (node, array and protocol) info for user allowed configuration # For any given storage array ID and protocol on a Node, topology keys will be created for just those pair and # every other configuration is ignored # Please refer to the doc website about a detailed explanation of each configuration parameter # and the various possible inputs allowedConnections: # nodeName: Name of the node on which user wants to apply given rules # Allowed values: # nodeName - name of a specific node # * - all the nodes # Examples: \"node1\", \"*\" - nodeName: \"node1\" # rules is a list of 'StorageArrayID:TransportProtocol' pair. ':' is required between both value # Allowed values: # StorageArrayID: # - SymmetrixID : for specific storage array # - \"*\" :- for all the arrays connected to the node # TransportProtocol: # - FC : Fibre Channel protocol # - ISCSI : iSCSI protocol # - \"*\" - for all the possible Transport Protocol # Examples: \"000000000001:FC\", \"000000000002:*\", \"*:FC\", \"*:*\" rules: - \"000000000001:FC\" - \"000000000002:FC\" - nodeName: \"*\" rules: - \"000000000002:FC\" # deniedConnections contains a list of (node, array and protocol) info for denied configurations by user # For any given storage array ID and protocol on a Node, topology keys will be created for every other configuration but # not these input pairs deniedConnections: - nodeName: \"node2\" rules: - \"000000000002:*\" - nodeName: \"node3\" rules: - \"*:*\" Use the below command to create ConfigMap with configmap name as node-topology-config in the namespace powermax,  kubectl create configmap node-topology-config --from-file=topologyConfig.yaml -n powermax\nFor example, let there be 3 nodes and 2 arrays, so based on the sample config file above, topology keys will be created as below:\nNew Topology keys N1: csi-driver/000000000001.FC:csi-driver, csi-driver/000000000002.FC:csi-driver N2 and N3: None\n Note: Name of the configmap should always be node-topology-config.\n Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerMax version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in my-powermax-settings.yaml during driver installation.\nTo change the log level dynamically to a different value, the user can edit the same my-powermax-settings.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade Note: my-powermax-settings.yaml is a values.yaml file which the user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level the user can set this field during driver installation.\nTo update the log level dynamically, the user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Volume Health Monitoring CSI Driver for Dell PowerMax 2.2.0 and above supports volume health monitoring. To enable Volume Health Monitoring from the node side, the alpha feature gate CSIVolumeHealth needs to be enabled. To use this feature, set controller.healthMonitor.enabled and node.healthMonitor.enabled to true. To change the monitor interval, set controller.healthMonitor.interval parameter.\nSingle Pod Access Mode for PersistentVolumes- ReadWriteOncePod (ALPHA FEATURE) Use ReadWriteOncePod(RWOP) access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it. This is only supported for CSI Driver for PowerMax 2.2.0+ and Kubernetes version 1.22+.\nTo use this feature, enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet, by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreating a PersistentVolumeClaim kind:PersistentVolumeClaimapiVersion:v1metadata:name:single-writer-onlyspec:accessModes:- ReadWriteOncePod# the volume can be mounted as read-write by a single pod across the whole clusterresources:requests:storage:1GiWhen this feature is enabled, the existing ReadWriteOnce(RWO) access mode restricts volume access to a single node and allows multiple pods on the same node to read from and write to the same volume.\nTo migrate existing PersistentVolumes to use ReadWriteOncePod, please follow the instruction from here.\n","excerpt":"Multi Unisphere Support Starting with v1.7, the CSI PowerMax driver …","ref":"/csm-docs/docs/csidriver/features/powermax/","title":"PowerMax"},{"body":"You can upgrade CSI Driver for Dell PowerMax using Helm or Dell CSI Operator.\nUpdate Driver from v2.2 to v2.3 using Helm Steps\n Run git clone -b v2.3.0 https://github.com/dell/csi-powermax.git to clone the git repository and get the v2.3 driver. Update the values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade.  NOTE:\n If you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver. To update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade.  Upgrade using Dell CSI Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\n Please upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here.  ","excerpt":"You can upgrade CSI Driver for Dell PowerMax using Helm or Dell CSI …","ref":"/csm-docs/docs/csidriver/upgradation/drivers/powermax/","title":"PowerMax"},{"body":"Multi Unisphere Support Starting with v1.7, the CSI PowerMax driver can communicate with multiple Unisphere for PowerMax servers to manage multiple PowerMax arrays. In order to use this feature, you must install CSI PowerMax ReverseProxy in StandAlone mode with the driver. For more details on how to configure the driver and ReverseProxy, see the relevant section here\nVolume Snapshot Feature The CSI PowerMax driver version 1.7 and later supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class  To use this feature, enable it in values.yaml\nsnapshot:enabled:true Note: From v1.7, the CSI PowerMax driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the csi-powermax/samples/volumesnapshotclass folder\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powermax-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-restore-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-snapshot-demokind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiCreating PVCs with PVCs as source This is not supported for replicated volumes.\nThis is a sample manifest for creating a PVC with another PVC as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-clone-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-pvc-demokind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP With version 1.3.0, support has been added for the unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI. To enable CHAP authentication:\n Create secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e\nWhere is the name of the iSCSI initiator of a host IQN, and is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which is stored in the host initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name With version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this feature, set the following values under customDriverName in my-powermax-settings.yaml.\n Value: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template.  If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax, then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers To install multiple CSI Drivers for Dell PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions that should be strictly adhered to:\n Only one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations  To install multiple CSI drivers, follow these steps:\n Create (or use) a new namespace. Ensure that all the pre-requisites are met:  powermax-creds secret is created in this namespace (Optional) powermax-certs secret is created in this namespace   Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver.  Volume expansion Starting in v1.4, the CSI PowerMax driver supports the expansion of Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.\n Note: This feature is not supported for replicated volumes.\n To use this feature, enable in values.yaml\nresizer:enabled:trueTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThis is a sample manifest for a storage class that allows for Volume Expansion.\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-expand-scannotations:storageclass.kubernetes.io/is-default-class:falseprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true#Set this attribute to true if you plan to expand any PVCscreatedusingthisstorageclassparameters:SYMID:\"000000000001\"SRP:\"DEFAULT_SRP\"ServiceLevel:\"Bronze\"To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC.\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pmax-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:10Gi#Updated size from 5Gi to 10GistorageClassName:powermax-expand-scNOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of the volume, it cannot be used to shrink a volume.\nRaw block support Starting in v1.4, the CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind:StatefulSetapiVersion:apps/v1metadata:name:powermaxtestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powermaxresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy To get the maximum performance out of the CSI driver for PowerMax and Unisphere for PowerMax REST APIs, starting with v1.4 of the driver, you can deploy the optional CSI PowerMax Reverse Proxy application.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server that acts as a reverse proxy for the Unisphere for PowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy application helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and the performance of the CSI PowerMax driver.\nOptionally, you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation The CSI PowerMax Reverse Proxy can be installed in two ways:\n It can be installed as a Kubernetes deployment in the same namespace as the driver. It can be installed as a sidecar to the driver’s controller Pod.  It is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is an HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be an X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example showing how to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Using Helm installer In the my-powermax-settings.yaml file, the csireverseproxy section can be used to deploy and configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. If it is enabled (using the enabled parameter in the csireverseproxy section of the my-powermax-settings.yaml file), the install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name “powermax-reverseproxy”. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation for PowerMax.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, a new setting, modifyHostName, can be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1, then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of the controller Pod. At any time, only one controller Pod is active(leader), and the rest are on standby. In case of a failure, one of the standby Pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two-controller Pods are ever scheduled on the same node.\nTo increase or decrease the number of controller Pods, edit the following value in values.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. We recommend not changing this unless it is really necessary. Also, if the controller count is greater than the number of available nodes (where the Pods can be scheduled), some controller Pods will remain in the Pending state\n If you are using dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, see the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment and driver node Daemonset. There are two new sections in the values file - controller and node - where you can specify these values separately for the controller and node Pods.\ncontroller If you want to apply nodeSelectors and tolerations for the controller Pods, edit the controller section in the values file.\nHere are some examples:\n To schedule controller Pods to worker nodes only (Default):  controller:nodeSelector:tolerations: Set the following values for controller Pods to tolerate the taint NoSchedule on master nodes:  controller:nodeSelector:tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\" Set the following values for controller Pods to be scheduled only on nodes labelled master (node-role.kubernetes.io/master):  controller:nodeSelector:node-role.kubernetes.io/master:\"\"tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"node If you want to apply nodeSelectors and tolerations for the node Pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add and remove tolerations to this list\n# \"node\" allows to configure node specific parametersnode:# \"node.nodeSelector\" defines what nodes would be selected for Pods of node daemonset# Leave as blank to use all nodesnodeSelector:# node-role.kubernetes.io/master: \"\"# \"node.tolerations\" defines tolerations that would be applied to node daemonset# Add/Remove tolerations as per requirement# Leave as blank if you wish to not apply any tolerationstolerations:- key:\"node.kubernetes.io/memory-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/disk-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/network-unavailable\"operator:\"Exists\"effect:\"NoExecute\"Topology Support Starting from version 1.5, the CSI PowerMax driver supports topology-aware volume provisioning which helps the Kubernetes scheduler place PVCs on worker nodes that have access to the backend storage. When used with nodeSelectors which can be specified for the driver node Pods, it provides an effective way to provision applications on nodes that have access to the PowerMax array.\nAfter a successful installation of the driver, if a node Pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\n csi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi  The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\n NOTE: The Topology support does not include any customer-defined topology, that is, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\n Topology Usage To use the Topology feature, the storage classes must be modified as follows:\n volumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section  For example, a PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-fcparameters:SRP:\"SRP_1\"SYMID:\"000000000001\"ServiceLevel:\u003cServiceLevel\u003e#Insert Service Level Nameprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumerallowVolumeExpansion:trueallowedTopologies:- matchLabelExpressions:- key:csi-powermax.dellemc.com/000000000001values:- csi-powermax.dellemc.com- key:csi-powermax.dellemc.com/000000000001.fcvalues:- csi-powermax.dellemc.comIn the above example, if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\n A set of sample storage class definitions to enable topology-aware volume provisioning has been provided in the csi-powermax/samples/storageclass folder\n For additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nDynamic Logging Configuration This feature is introduced in CSI Driver for PowerMax version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in my-powermax-settings.yaml during driver installation.\nTo change the log level dynamically to a different value, the user can edit the same my-powermax-settings.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade Note: my-powermax-settings.yaml is a values.yaml file which the user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level the user can set this field during driver installation.\nTo update the log level dynamically, the user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Volume Health Monitoring CSI Driver for Dell PowerMax 2.2.0 and above supports volume health monitoring. To enable Volume Health Monitoring from the node side, the alpha feature gate CSIVolumeHealth needs to be enabled. To use this feature, set controller.healthMonitor.enabled and node.healthMonitor.enabled to true. To change the monitor interval, set controller.healthMonitor.interval parameter.\nSingle Pod Access Mode for PersistentVolumes- ReadWriteOncePod (ALPHA FEATURE) Use ReadWriteOncePod(RWOP) access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it. This is only supported for CSI Driver for PowerMax 2.2.0+ and Kubernetes version 1.22+.\nTo use this feature, enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet, by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreating a PersistentVolumeClaim kind:PersistentVolumeClaimapiVersion:v1metadata:name:single-writer-onlyspec:accessModes:- ReadWriteOncePod# the volume can be mounted as read-write by a single pod across the whole clusterresources:requests:storage:1GiWhen this feature is enabled, the existing ReadWriteOnce(RWO) access mode restricts volume access to a single node and allows multiple pods on the same node to read from and write to the same volume.\nTo migrate existing PersistentVolumes to use ReadWriteOncePod, please follow the instruction from here.\n","excerpt":"Multi Unisphere Support Starting with v1.7, the CSI PowerMax driver …","ref":"/csm-docs/v1/csidriver/features/powermax/","title":"PowerMax"},{"body":"You can upgrade CSI Driver for Dell PowerMax using Helm or Dell CSI Operator.\nUpdate Driver from v2.1 to v2.2 using Helm Steps\n Run git clone -b v2.2.0 https://github.com/dell/csi-powermax.git to clone the git repository and get the v2.2 driver. Update the values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade.  NOTE:\n If you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver. To update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade.  Upgrade using Dell CSI Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\n Please upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here.  ","excerpt":"You can upgrade CSI Driver for Dell PowerMax using Helm or Dell CSI …","ref":"/csm-docs/v1/csidriver/upgradation/drivers/powermax/","title":"PowerMax"},{"body":"Multi Unisphere Support Starting with v1.7, the CSI PowerMax driver can communicate with multiple Unisphere for PowerMax servers to manage multiple PowerMax arrays. In order to use this feature, you must install CSI PowerMax ReverseProxy in StandAlone mode with the driver. For more details on how to configure the driver and ReverseProxy, see the relevant section here\nVolume Snapshot Feature The CSI PowerMax driver version 1.7 and later supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class  To use this feature, enable it in values.yaml\nsnapshot:enabled:true Note: From v1.7, the CSI PowerMax driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the csi-powermax/samples/volumesnapshotclass folder\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powermax-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-restore-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-snapshot-demokind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiCreating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-clone-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-pvc-demokind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP With version 1.3.0, support has been added for the unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI. To enable CHAP authentication:\n Create secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e\nWhere is the name of the iSCSI initiator of a host IQN, and is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which is stored in the host initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name With version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this feature, set the following values under customDriverName in my-powermax-settings.yaml.\n Value: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template.  If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax, then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers To install multiple CSI Drivers for Dell PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions that should be strictly adhered to:\n Only one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations  To install multiple CSI drivers, follow these steps:\n Create (or use) a new namespace. Ensure that all the pre-requisites are met:  powermax-creds secret is created in this namespace (Optional) powermax-certs secret is created in this namespace   Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver.  Volume expansion Starting in v1.4, the CSI PowerMax driver supports the expansion of Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.\nTo use this feature, enable in values.yaml\nresizer:enabled:trueTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThis is a sample manifest for a storage class that allows for Volume Expansion.\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-expand-scannotations:storageclass.kubernetes.io/is-default-class:falseprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true#Set this attribute to true if you plan to expand any PVCscreatedusingthisstorageclassparameters:SYMID:\"000000000001\"SRP:\"DEFAULT_SRP\"ServiceLevel:\"Bronze\"To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC.\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pmax-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:10Gi#Updated size from 5Gi to 10GistorageClassName:powermax-expand-scNOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of the volume, it cannot be used to shrink a volume.\nRaw block support Starting in v1.4, the CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind:StatefulSetapiVersion:apps/v1metadata:name:powermaxtestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powermaxresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy To get the maximum performance out of the CSI driver for PowerMax and Unisphere for PowerMax REST APIs, starting with v1.4 of the driver, you can deploy the optional CSI PowerMax Reverse Proxy application.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server that acts as a reverse proxy for the Unisphere for PowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy application helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and the performance of the CSI PowerMax driver.\nOptionally, you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation The CSI PowerMax Reverse Proxy can be installed in two ways:\n It can be installed as a Kubernetes deployment in the same namespace as the driver. It can be installed as a sidecar to the driver’s controller Pod.  It is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is an HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be an X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example showing how to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Using Helm installer In the my-powermax-settings.yaml file, the csireverseproxy section can be used to deploy and configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. If it is enabled (using the enabled parameter in the csireverseproxy section of the my-powermax-settings.yaml file), the install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name “powermax-reverseproxy”. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation for PowerMax.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, a new setting, modifyHostName, can be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1, then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of the controller Pod. At any time, only one controller Pod is active(leader), and the rest are on standby. In case of a failure, one of the standby Pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two-controller Pods are ever scheduled on the same node.\nTo increase or decrease the number of controller Pods, edit the following value in values.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. We recommend not changing this unless it is really necessary. Also, if the controller count is greater than the number of available nodes (where the Pods can be scheduled), some controller Pods will remain in the Pending state\n If you are using dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, see the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment and driver node Daemonset. There are two new sections in the values file - controller and node - where you can specify these values separately for the controller and node Pods.\ncontroller If you want to apply nodeSelectors and tolerations for the controller Pods, edit the controller section in the values file.\nHere are some examples:\n To schedule controller Pods to worker nodes only (Default):  controller:nodeSelector:tolerations: Set the following values for controller Pods to tolerate the taint NoSchedule on master nodes:  controller:nodeSelector:tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\" Set the following values for controller Pods to be scheduled only on nodes labelled master (node-role.kubernetes.io/master):  controller:nodeSelector:node-role.kubernetes.io/master:\"\"tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"node If you want to apply nodeSelectors and tolerations for the node Pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add and remove tolerations to this list\n# \"node\" allows to configure node specific parametersnode:# \"node.nodeSelector\" defines what nodes would be selected for Pods of node daemonset# Leave as blank to use all nodesnodeSelector:# node-role.kubernetes.io/master: \"\"# \"node.tolerations\" defines tolerations that would be applied to node daemonset# Add/Remove tolerations as per requirement# Leave as blank if you wish to not apply any tolerationstolerations:- key:\"node.kubernetes.io/memory-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/disk-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/network-unavailable\"operator:\"Exists\"effect:\"NoExecute\"Topology Support Starting from version 1.5, the CSI PowerMax driver supports topology-aware volume provisioning which helps the Kubernetes scheduler place PVCs on worker nodes that have access to the backend storage. When used with nodeSelectors which can be specified for the driver node Pods, it provides an effective way to provision applications on nodes that have access to the PowerMax array.\nAfter a successful installation of the driver, if a node Pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\n csi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi  The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\n NOTE: The Topology support does not include any customer-defined topology, that is, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\n Topology Usage To use the Topology feature, the storage classes must be modified as follows:\n volumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section  For example, a PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-fcparameters:SRP:\"SRP_1\"SYMID:\"000000000001\"ServiceLevel:\u003cServiceLevel\u003e#Insert Service Level Nameprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumerallowVolumeExpansion:trueallowedTopologies:- matchLabelExpressions:- key:csi-powermax.dellemc.com/000000000001values:- csi-powermax.dellemc.com- key:csi-powermax.dellemc.com/000000000001.fcvalues:- csi-powermax.dellemc.comIn the above example, if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\n A set of sample storage class definitions to enable topology-aware volume provisioning has been provided in the csi-powermax/samples/storageclass folder\n For additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nDynamic Logging Configuration This feature is introduced in CSI Driver for PowerMax version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in my-powermax-settings.yaml during driver installation.\nTo change the log level dynamically to a different value, the user can edit the same my-powermax-settings.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade Note: my-powermax-settings.yaml is a values.yaml file which the user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level the user can set this field during driver installation.\nTo update the log level dynamically, the user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Volume Health Monitoring CSI Driver for Dell PowerMax 2.2.0 and above supports volume health monitoring. To enable Volume Health Monitoring from the node side, the alpha feature gate CSIVolumeHealth needs to be enabled. To use this feature, set controller.healthMonitor.enabled and node.healthMonitor.enabled to true. To change the monitor interval, set controller.healthMonitor.interval parameter.\nSingle Pod Access Mode for PersistentVolumes- ReadWriteOncePod (ALPHA FEATURE) Use ReadWriteOncePod(RWOP) access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it. This is only supported for CSI Driver for PowerMax 2.2.0+ and Kubernetes version 1.22+.\nTo use this feature, enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet, by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreating a PersistentVolumeClaim kind:PersistentVolumeClaimapiVersion:v1metadata:name:single-writer-onlyspec:accessModes:- ReadWriteOncePod# the volume can be mounted as read-write by a single pod across the whole clusterresources:requests:storage:1GiWhen this feature is enabled, the existing ReadWriteOnce(RWO) access mode restricts volume access to a single node and allows multiple pods on the same node to read from and write to the same volume.\nTo migrate existing PersistentVolumes to use ReadWriteOncePod, please follow the instruction from here.\n","excerpt":"Multi Unisphere Support Starting with v1.7, the CSI PowerMax driver …","ref":"/csm-docs/v2/csidriver/features/powermax/","title":"PowerMax"},{"body":"You can upgrade CSI Driver for Dell PowerMax using Helm or Dell CSI Operator.\nUpdate Driver from v2.1 to v2.2 using Helm Steps\n Run git clone -b v2.2.0 https://github.com/dell/csi-powermax.git to clone the git repository and get the v2.2 driver. Update the values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade.  NOTE:\n If you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver. To update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade.  Upgrade using Dell CSI Operator: Note: Upgrading the Operator does not upgrade the CSI Driver.\n Please upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here.  ","excerpt":"You can upgrade CSI Driver for Dell PowerMax using Helm or Dell CSI …","ref":"/csm-docs/v2/csidriver/upgradation/drivers/powermax/","title":"PowerMax"},{"body":"Multi Unisphere Support Starting with v1.7, the CSI PowerMax driver can communicate with multiple Unisphere for PowerMax servers to manage multiple PowerMax arrays. In order to use this feature, you must install CSI PowerMax ReverseProxy in StandAlone mode with the driver. For more details on how to configure the driver and ReverseProxy, see the relevant section here\nVolume Snapshot Feature The CSI PowerMax driver version 1.7 and later supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class  To use this feature, enable it in values.yaml\nsnapshot:enabled:true Note: From v1.7, the CSI PowerMax driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the csi-powermax/samples/volumesnapshotclass folder\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powermax-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-restore-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-snapshot-demokind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiCreating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-clone-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-pvc-demokind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP With version 1.3.0, support has been added for the unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI. To enable CHAP authentication:\n Create secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u003csymid\u003e -iscsi \u003chost iqn\u003e set chap -cred \u003chost IQN\u003e -secret \u003cCHAP secret\u003e\nWhere is the name of the iSCSI initiator of a host IQN, and is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which is stored in the host initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name With version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell EMC PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this feature, set the following values under customDriverName in my-powermax-settings.yaml.\n Value: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template.  If enabled, the driver name is in the following format: \u003cnamespace\u003e.\u003cdriver name\u003e.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax, then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers To install multiple CSI Drivers for Dell EMC PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions that should be strictly adhered to:\n Only one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations  To install multiple CSI drivers, follow these steps:\n Create (or use) a new namespace. Ensure that all the pre-requisites are met:  powermax-creds secret is created in this namespace (Optional) powermax-certs secret is created in this namespace   Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver.  Volume expansion Starting in v1.4, the CSI PowerMax driver supports the expansion of Persistent Volumes (PVs). This expansion is done online, which is when the PVC is attached to any node.\nTo use this feature, enable in values.yaml\nresizer:enabled:trueTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThis is a sample manifest for a storage class that allows for Volume Expansion.\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true#Set this attribute to true if you plan to expand any PVCscreatedusingthisstorageclassparameters:SYMID:\"000000000001\"SRP:\"DEFAULT_SRP\"ServiceLevel:\"Bronze\"To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC.\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pmax-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:10Gi#Updated size from 5Gi to 10GistorageClassName:powermax-expand-scNOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of the volume, it cannot be used to shrink a volume.\nRaw block support Starting in v1.4, the CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind:StatefulSetapiVersion:apps/v1metadata:name:powermaxtestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powermaxresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy To get the maximum performance out of the CSI driver for PowerMax and Unisphere for PowerMax REST APIs, starting with v1.4 of the driver, you can deploy the optional CSI PowerMax Reverse Proxy application.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server that acts as a reverse proxy for the Unisphere for PowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy application helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and the performance of the CSI PowerMax driver.\nOptionally, you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation The CSI PowerMax Reverse Proxy can be installed in two ways:\n It can be installed as a Kubernetes deployment in the same namespace as the driver. It can be installed as a sidecar to the driver’s controller Pod.  It is also configured as a Kubernetes “NodePort” service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is an HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be an X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example showing how to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u003cnamespace\u003e tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u003cnamespace\u003e tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Using Helm installer In the my-powermax-settings.yaml file, the csireverseproxy section can be used to deploy and configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. If it is enabled (using the enabled parameter in the csireverseproxy section of the my-powermax-settings.yaml file), the install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name “powermax-reverseproxy”. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation for PowerMax.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two ‘%’ characters. String prefixing first ‘%’ and string suffixing second ‘%’ is used as is before and after every node identifier.\nAlso, a new setting, modifyHostName, can be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u003cClusterPrefix\u003e-\u003cHostName\u003e*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1, then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], ‘-’ and ‘_’, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of the controller Pod. At any time, only one controller Pod is active(leader), and the rest are on standby. In case of a failure, one of the standby Pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two-controller Pods are ever scheduled on the same node.\nTo increase or decrease the number of controller Pods, edit the following value in values.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. We recommend not changing this unless it is really necessary. Also, if the controller count is greater than the number of available nodes (where the Pods can be scheduled), some controller Pods will remain in the Pending state\n If you are using dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, see the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment and driver node Daemonset. There are two new sections in the values file - controller and node - where you can specify these values separately for the controller and node Pods.\ncontroller If you want to apply nodeSelectors and tolerations for the controller Pods, edit the controller section in the values file.\nHere are some examples:\n To schedule controller Pods to worker nodes only (Default):  controller:nodeSelector:tolerations: Set the following values for controller Pods to tolerate the taint NoSchedule on master nodes:  controller:nodeSelector:tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\" Set the following values for controller Pods to be scheduled only on nodes labelled master (node-role.kubernetes.io/master):  controller:nodeSelector:node-role.kubernetes.io/master:\"\"tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"node If you want to apply nodeSelectors and tolerations for the node Pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add and remove tolerations to this list\n# \"node\" allows to configure node specific parametersnode:# \"node.nodeSelector\" defines what nodes would be selected for Pods of node daemonset# Leave as blank to use all nodesnodeSelector:# node-role.kubernetes.io/master: \"\"# \"node.tolerations\" defines tolerations that would be applied to node daemonset# Add/Remove tolerations as per requirement# Leave as blank if you wish to not apply any tolerationstolerations:- key:\"node.kubernetes.io/memory-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/disk-pressure\"operator:\"Exists\"effect:\"NoExecute\"- key:\"node.kubernetes.io/network-unavailable\"operator:\"Exists\"effect:\"NoExecute\"Topology Support Starting from version 1.5, the CSI PowerMax driver supports topology-aware volume provisioning which helps the Kubernetes scheduler place PVCs on worker nodes that have access to the backend storage. When used with nodeSelectors which can be specified for the driver node Pods, it provides an effective way to provision applications on nodes that have access to the PowerMax array.\nAfter a successful installation of the driver, if a node Pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\n csi-powermax.dellemc.com/\\\u003carray-id\\\u003e If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u003carray-id\\\u003e.iscsi  The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\n NOTE: The Topology support does not include any customer-defined topology, that is, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\n Topology Usage To use the Topology feature, the storage classes must be modified as follows:\n volumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section  For example, a PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-fcparameters:SRP:\"SRP_1\"SYMID:\"000000000001\"ServiceLevel:\u003cServiceLevel\u003e#Insert Service Level Nameprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumerallowVolumeExpansion:trueallowedTopologies:- matchLabelExpressions:- key:csi-powermax.dellemc.com/000000000001values:- csi-powermax.dellemc.com- key:csi-powermax.dellemc.com/000000000001.fcvalues:- csi-powermax.dellemc.comIn the above example, if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\n A set of sample storage class definitions to enable topology-aware volume provisioning has been provided in the csi-powermax/samples/storageclass folder\n For additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nDynamic Logging Configuration This feature is introduced in CSI Driver for PowerMax version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in my-powermax-settings.yaml during driver installation.\nTo change the log level dynamically to a different value, the user can edit the same my-powermax-settings.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade Note: my-powermax-settings.yaml is a values.yaml file which the user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level the user can set this field during driver installation.\nTo update the log level dynamically, the user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params ","excerpt":"Multi Unisphere Support Starting with v1.7, the CSI PowerMax driver …","ref":"/csm-docs/v3/csidriver/features/powermax/","title":"PowerMax"},{"body":"You can upgrade CSI Driver for Dell EMC PowerMax using Helm or Dell CSI Operator.\nUpdate Driver from v2.0 to v2.1 using Helm Steps\n Run git clone -b v2.1.0 https://github.com/dell/csi-powermax.git to clone the git repository and get the v2.1 driver. Update the values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade.  NOTE:\n If you are upgrading from a driver version that was installed using Helm v2, ensure that you install Helm3 before installing the driver. To update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade.  Upgrade using Dell CSI Operator:   Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command installs the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and later installs to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, see here.  ","excerpt":"You can upgrade CSI Driver for Dell EMC PowerMax using Helm or Dell …","ref":"/csm-docs/v3/csidriver/upgradation/drivers/powermax/","title":"PowerMax"},{"body":"Multicluster support You can connect a single CSI-PowerScale driver with multiple PowerScale clusters.\nPre-Requisites:\n Creation of secret.yaml with credentials related to one or more Clusters. Creation of (at least) one Storage class for each cluster. Creation of custom-volumesnapshot classes with proper isiPath matching corresponding storage classes. Inclusion of cluster name in volume handle, if you want to provision existing static volumes.  Consuming existing volumes with static provisioning You can use existent volumes from the PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, the PowerScale cluster accessZone is assumed as ‘System’, storage class as ‘isilon’, cluster name as ‘pscale-cluster’ and volume’s internal name as ‘isilonvol’. The volume-handle should be in the format of \u003cvolume_name\u003e=_=_=\u003cexport_id\u003e=_=_==_=_=\u003ccluster_name\u003e  apiVersion:v1kind:PersistentVolumemetadata:name:isilonstaticpvnamespace:defaultspec:capacity:storage:5GiaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:RetainstorageClassName:isiloncsi:driver:csi-isilon.dellemc.comvolumeAttributes:Path:\"/ifs/data/csi/isilonvol\"Name:\"isilonvol\"AzServiceIP:'XX.XX.XX.XX'volumeHandle:isilonvol=_=_=652=_=_=System=_=_=pscale-clusterclaimRef:name:isilonstaticpvcnamespace:defaultCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilonstaticpvcnamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:isilonstaticpvstorageClassName:isilonThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:isilonstaticpvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.  PVC Creation Feature Following yaml content can be used to create a PVC without referring any PV.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:testvolumenamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonVolume Snapshot Feature The CSI PowerScale driver version 2.0 and later supports managing v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller   For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerScale driver version 2.0 and higher, no default Volume Snapshot Class will get created.\nFollowing are the manifests for the Volume Snapshot Class:\n VolumeSnapshotClass  apiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:\"isilon-snapclass\"driver:csi-isilon.dellemc.com#The deletionPolicy of a volume snapshot class can either be Retain or Delete#If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object.#If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remaindeletionPolicy:Deleteparameters:#IsiPath should match with respective storageClass IsiPathIsiPath:\"/ifs/data/csi\"The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs; The following snippet assumes that the persistent volume claim name is testvolume.\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvcsnapnamespace:defaultspec:volumeSnapshotClassName:isilon-snapclasssource:persistentVolumeClaimName:testvolumeOnce the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:createfromsnapnamespace:defaultspec:storageClassName:isilondataSource:name:pvcsnapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteManyresources:requests:storage:5Gi Starting from CSI PowerScale driver version 2.2, it is allowed to create PersistentVolumeClaim from VolumeSnapshot with different isi paths i.e., isi paths of the new volume and the VolumeSnapshot can be different.\n Volume Expansion The CSI PowerScale driver version 1.2 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-expand-scannotations:storageclass.kubernetes.io/is-default-class:\"false\"provisioner:\"csi-isilon.dellemc.com\"reclaimPolicy:Deleteparameters:ClusterName:\u003cclusterNamespecifiedinsecret.yaml\u003e AccessZone: SystemisiPath:\"/ifs/data/csi\"AzServiceIP :'XX.XX.XX.XX'rootClientEnabled:\"true\"allowVolumeExpansion:truevolumeBindingMode:ImmediateTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilon-pvc-expansion-demospec:accessModes:- ReadWriteOnceresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Volume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing PVC:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:existing-pvcspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonThe following is a sample manifest for cloning:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:volume-from-volumenamespace:defaultspec:accessModes:- ReadWriteManyvolumeMode:Filesystemresources:requests:storage:50GistorageClassName:isilondataSource:kind:PersistentVolumeClaimname:existing-pvcapiGroup:\"\"Controller HA The CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two-controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if the controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in a Pending state.\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nEphemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves as use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumespec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data\"name:my-csi-volumevolumes:- name:my-csi-volumecsi:driver:csi-isilon.dellemc.comvolumeAttributes:size:\"2Gi\"ClusterName:\"cluster1\"This manifest creates a pod in a given cluster and attaches a newly created ephemeral inline CSI volume to it.\nTopology Topology Support The CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage, as a result of which the nodes which have access to PowerScale Array are appropriately labeled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, the CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and use them to initialize node pod with custom PowerScale FQDN/IP.\nNote: Only a single cluster can be configured as part of secret.yaml for custom topology.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that the Pod schedule takes advantage of the topology and the selected node has access to provisioned volumes.\nNote: Whenever a new storage cluster is being added in secret, even though it is dynamic, the new storage cluster IP address-related label is not added to worker nodes dynamically. The user has to spin off (bounce) driver-related pods (controller and node pods) in order to apply newly added information to be reflected in worker nodes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options.# PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon# Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server# Provide mount options through \"mountOptions\" attribute # to create PVCs with mount options.apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilonprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:trueparameters:AccessZone:SystemIsiPath:\"/ifs/data/csi\"# AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP.#AzServiceIP : 192.168.2.1# When a PVC is being created, it takes the storage class' value of \"storageclass.rootClientEnabled\", # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \"Root clients\" field (when true) or \"Clients\" field (when false) of the NFS export RootClientEnabled:\"false\"# Name of PowerScale cluster where pv will be provisioned# This name should match with name of one of the cluster configs in isilon-creds secret# If this parameter is not specified, then default cluster config in isilon-creds secret will be considered if available#ClusterName: \"\u003ccluster_name\u003e\"# volumeBindingMode controls when volume binding and dynamic provisioning should occur.# Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created# WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume# until a Pod using the PersistentVolumeClaim is createdvolumeBindingMode:WaitForFirstConsumer# allowedTopologies helps scheduling pod on worker nodes which match all of below expressions# If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologiesallowedTopologies:- matchLabelExpressions:- key:csi-isilon.dellemc.com/\u003cISILON_IP\u003e values:- csi-isilon.dellemc.com# specify additional mount options for when a Persistent Volume is being mounted on a node.# To mount volume with NFSv4, specify mount option vers=4. Make sure NFSv4 is enabled on the Isilon Cluster.mountOptions:[\"\u003cmountOption1\u003e\",\"\u003cmountOption2\u003e\",...,\"\u003cmountOptionN\u003e\"]For additional information, see the Kubernetes Topology documentation.\nSupport custom networks for NFS I/O traffic When allowedNetworks is specified for using custom networks to handle NFS traffic, and a user already has workloads scheduled, there is a possibility that it might lead to backward compatibility issues. For example, ControllerUnPublish might not be able to completely remove clients from the NFS exports of previously created pods.\nAlso, the previous workload will still be using the default network and not custom networks. For previous workloads to use custom networks, the recreation of pods is required.\nWhen csi-powerscale driver creates an NFS export, the traffic flows through the client specified in the export. By default, the client is the network interface for Kubernetes communication (same IP/fqdn as k8s node) by default.\nFor a cluster with multiple network interfaces and if a user wants to segregate k8s traffic from NFS traffic; you can use the allowedNetworks option. allowedNetworks takes CIDR addresses as a parameter to match the IPs to be picked up by the driver to allow and route NFS traffic.\nVolume Limit The CSI Driver for Dell PowerScale allows users to specify the maximum number of PowerScale volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-isilon-volumes-per-node and specifying the volume limit for that node.  kubectl label node \u003cnode_name\u003e max-isilon-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxIsilonVolumesPerNode attribute in values.yaml.\n NOTE: The default value of maxIsilonVolumesPerNode is 0. If maxIsilonVolumesPerNode is set to zero, then CO shall decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxIsilonVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-isilon-volumes-per-node is not set.\n Node selector in helm template Now user can define in which worker node, the CSI node pod daemonset can run (just like any other pod in Kubernetes world.)For more information, refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector\nSimilarly, users can define the tolerations based on various conditions like memory pressure, disk pressure and network availability. Refer to https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#taints-and-tolerations for more information.\nUsage of SmartQuotas to Limit Storage Consumption CSI driver for Dell Isilon handles capacity limiting using SmartQuotas feature.\nTo use the SmartQuotas feature user can specify the boolean value ‘enableQuota’ in myvalues.yaml or my-isilon-settings.yaml.\nLet us assume the user creates a PVC with 3 Gi of storage and ‘SmartQuotas’ have already been enabled in PowerScale Cluster.\n  When ‘enableQuota’ is set to ‘true’\n The driver sets the hard limit of the PVC to 3Gi. The user adds data of 2Gi to the above said PVC (by logging into POD). It works as expected. The user tries to add 2Gi more data. Driver doesn’t allow the user to enter more data as total data to be added is 4Gi and PVC limit is 3Gi. The user can expand the volume from 3Gi to 6Gi. The driver allows it and sets the hard limit of PVC to 6Gi. User retries adding 2Gi more data (which has been errored out previously). The driver accepts the data.    When ‘enableQuota’ is set to ‘false’\n Driver doesn’t set any hard limit against the PVC created. The user adds data of 2Gi to the above said PVC, which is having the size 3Gi (by logging into POD). It works as expected. The user tries to add 2Gi more data. Now the total size of data is 4Gi. Driver allows the user to enter more data irrespective of the initial PVC size (since no quota is set against this PVC) The user can expand the volume from an initial size of 3Gi to 4Gi or more. The driver allows it.    Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerScale version 1.6.0 and updated in version 2.0.0\nHelm based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade Note: here my-isilon-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap isilon-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n isilon isilon-config-params  Note: Prior to CSI Driver for PowerScale version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object.\n NAT Support CSI Driver for Dell PowerScale is supported in the NAT environment.\nConfigurable permissions for volume directory This feature is introduced in CSI Driver for PowerScale version 2.0.0\nHelm based installation The permissions for volume directory can now be configured in 3 ways:\n Through values.yaml Through secrets Through storage class   # isiVolumePathPermissions: The permissions for isi volume directory path # This value acts as a default value for isiVolumePathPermissions, if not specified for a cluster config in secret # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" isiVolumePathPermissions: \"0777\" The permissions present in values.yaml are the default for all cluster config.\nIf the volume permission is not present in storage class then secrets are considered and if it is not present even in secrets then values.yaml is considered.\n Note: For volume creation from source (volume from snapshot/volume from volume) permissions are inherited from source. Create myvalues.yaml/my-isilon-settings.yaml and storage class accordingly.\n Operator based installation In the case of operator-based installation, default permission for powerscale directory is present in the samples file.\nOther ways of configuring powerscale volume permissions remain the same as helm-based installation.\nPV/PVC Metrics CSI Driver for Dell PowerScale 2.1.0 and above supports volume health monitoring. This allows Kubernetes to report on the condition, status and usage of the underlying volumes. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nThis feature can be enabled  For controller plugin, by setting attribute controller.healthMonitor.enabled to true in values.yaml file. Also health monitoring interval can be changed through attribute controller.healthMonitor.interval in values.yaml file. For node plugin, by setting attribute node.healthMonitor.enabled to true in values.yaml file and by enabling the alpha feature gate CSIVolumeHealth.  Single Pod Access Mode for PersistentVolumes- ReadWriteOncePod (ALPHA FEATURE) Use ReadWriteOncePod(RWOP) access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it. This is supported for CSI Driver for PowerScale 2.1.0+ and Kubernetes version 1.22+.\nTo use this feature, enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet, by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreating a PersistentVolumeClaim kind:PersistentVolumeClaimapiVersion:v1metadata:name:single-writer-onlyspec:accessModes:- ReadWriteOncePod# the volume can be mounted as read-write by a single pod across the whole clusterresources:requests:storage:1GiWhen this feature is enabled, the existing ReadWriteOnce(RWO) access mode restricts volume access to a single node and allows multiple pods on the same node to read from and write to the same volume.\nTo migrate existing PersistentVolumes to use ReadWriteOncePod, please follow the instruction from here.\n","excerpt":"Multicluster support You can connect a single CSI-PowerScale driver …","ref":"/csm-docs/docs/csidriver/features/powerscale/","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell PowerScale using Helm or Dell CSI Operator.\nUpgrade Driver from version 2.2.0 to 2.3.0 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\n  Clone the repository using git clone -b v2.3.0 https://github.com/dell/csi-powerscale.git, copy the helm/csi-isilon/values.yaml into a new location with a custom name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\n  Change to directory dell-csi-helm-installer to install the Dell PowerScale cd dell-csi-helm-installer\n  Upgrade the CSI Driver for Dell PowerScale using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade\n  Upgrade using Dell CSI Operator: Notes:\n While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes. Upgrading the Operator does not upgrade the CSI Driver.  To upgrade the driver:\n Please upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell PowerScale using Helm or Dell …","ref":"/csm-docs/docs/csidriver/upgradation/drivers/isilon/","title":"PowerScale"},{"body":"Multicluster support You can connect a single CSI-PowerScale driver with multiple PowerScale clusters.\nPre-Requisites:\n Creation of secret.yaml with credentials related to one or more Clusters. Creation of (at least) one Storage class for each cluster. Creation of custom-volumesnapshot classes with proper isiPath matching corresponding storage classes. Inclusion of cluster name in volume handle, if you want to provision existing static volumes.  Consuming existing volumes with static provisioning You can use existent volumes from the PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, the PowerScale cluster accessZone is assumed as ‘System’, storage class as ‘isilon’, cluster name as ‘pscale-cluster’ and volume’s internal name as ‘isilonvol’. The volume-handle should be in the format of \u003cvolume_name\u003e=_=_=\u003cexport_id\u003e=_=_==_=_=\u003ccluster_name\u003e  apiVersion:v1kind:PersistentVolumemetadata:name:isilonstaticpvnamespace:defaultspec:capacity:storage:5GiaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:RetainstorageClassName:isiloncsi:driver:csi-isilon.dellemc.comvolumeAttributes:Path:\"/ifs/data/csi/isilonvol\"Name:\"isilonvol\"AzServiceIP:'XX.XX.XX.XX'volumeHandle:isilonvol=_=_=652=_=_=System=_=_=pscale-clusterclaimRef:name:isilonstaticpvcnamespace:defaultCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilonstaticpvcnamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:isilonstaticpvstorageClassName:isilonThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:isilonstaticpvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.  PVC Creation Feature Following yaml content can be used to create a PVC without referring any PV.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:testvolumenamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonVolume Snapshot Feature The CSI PowerScale driver version 2.0 and later supports managing v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller   For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerScale driver version 2.0 and higher, no default Volume Snapshot Class will get created.\nFollowing are the manifests for the Volume Snapshot Class:\n VolumeSnapshotClass  apiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:\"isilon-snapclass\"driver:csi-isilon.dellemc.com#The deletionPolicy of a volume snapshot class can either be Retain or Delete#If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object.#If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remaindeletionPolicy:Deleteparameters:#IsiPath should match with respective storageClass IsiPathIsiPath:\"/ifs/data/csi\"The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs; The following snippet assumes that the persistent volume claim name is testvolume.\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvcsnapnamespace:defaultspec:volumeSnapshotClassName:isilon-snapclasssource:persistentVolumeClaimName:testvolumeOnce the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:createfromsnapnamespace:defaultspec:storageClassName:isilondataSource:name:pvcsnapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteManyresources:requests:storage:5Gi Starting from CSI PowerScale driver version 2.2, it is allowed to create PersistentVolumeClaim from VolumeSnapshot with different isi paths i.e., isi paths of the new volume and the VolumeSnapshot can be different.\n Volume Expansion The CSI PowerScale driver version 1.2 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-expand-scannotations:storageclass.kubernetes.io/is-default-class:\"false\"provisioner:\"csi-isilon.dellemc.com\"reclaimPolicy:Deleteparameters:ClusterName:\u003cclusterNamespecifiedinsecret.yaml\u003e AccessZone: SystemisiPath:\"/ifs/data/csi\"AzServiceIP :'XX.XX.XX.XX'rootClientEnabled:\"true\"allowVolumeExpansion:truevolumeBindingMode:ImmediateTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilon-pvc-expansion-demospec:accessModes:- ReadWriteOnceresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Volume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing PVC:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:existing-pvcspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonThe following is a sample manifest for cloning:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:volume-from-volumenamespace:defaultspec:accessModes:- ReadWriteManyvolumeMode:Filesystemresources:requests:storage:50GistorageClassName:isilondataSource:kind:PersistentVolumeClaimname:existing-pvcapiGroup:\"\"Controller HA The CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two-controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if the controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in a Pending state.\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nEphemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves as use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumespec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data\"name:my-csi-volumevolumes:- name:my-csi-volumecsi:driver:csi-isilon.dellemc.comvolumeAttributes:size:\"2Gi\"ClusterName:\"cluster1\"This manifest creates a pod in a given cluster and attaches a newly created ephemeral inline CSI volume to it.\nTopology Topology Support The CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage, as a result of which the nodes which have access to PowerScale Array are appropriately labeled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, the CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and use them to initialize node pod with custom PowerScale FQDN/IP.\nNote: Only a single cluster can be configured as part of secret.yaml for custom topology.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that the Pod schedule takes advantage of the topology and the selected node has access to provisioned volumes.\nNote: Whenever a new storage cluster is being added in secret, even though it is dynamic, the new storage cluster IP address-related label is not added to worker nodes dynamically. The user has to spin off (bounce) driver-related pods (controller and node pods) in order to apply newly added information to be reflected in worker nodes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options.# PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon# Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server# Provide mount options through \"mountOptions\" attribute # to create PVCs with mount options.apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilonprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:trueparameters:AccessZone:SystemIsiPath:\"/ifs/data/csi\"# AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP.#AzServiceIP : 192.168.2.1# When a PVC is being created, it takes the storage class' value of \"storageclass.rootClientEnabled\", # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \"Root clients\" field (when true) or \"Clients\" field (when false) of the NFS export RootClientEnabled:\"false\"# Name of PowerScale cluster where pv will be provisioned# This name should match with name of one of the cluster configs in isilon-creds secret# If this parameter is not specified, then default cluster config in isilon-creds secret will be considered if available#ClusterName: \"\u003ccluster_name\u003e\"# volumeBindingMode controls when volume binding and dynamic provisioning should occur.# Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created# WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume# until a Pod using the PersistentVolumeClaim is createdvolumeBindingMode:WaitForFirstConsumer# allowedTopologies helps scheduling pod on worker nodes which match all of below expressions# If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologiesallowedTopologies:- matchLabelExpressions:- key:csi-isilon.dellemc.com/\u003cISILON_IP\u003e values:- csi-isilon.dellemc.com# specify additional mount options for when a Persistent Volume is being mounted on a node.# To mount volume with NFSv4, specify mount option vers=4. Make sure NFSv4 is enabled on the Isilon Cluster.mountOptions:[\"\u003cmountOption1\u003e\",\"\u003cmountOption2\u003e\",...,\"\u003cmountOptionN\u003e\"]For additional information, see the Kubernetes Topology documentation.\nSupport custom networks for NFS I/O traffic When allowedNetworks is specified for using custom networks to handle NFS traffic, and a user already has workloads scheduled, there is a possibility that it might lead to backward compatibility issues. For example, ControllerUnPublish might not be able to completely remove clients from the NFS exports of previously created pods.\nAlso, the previous workload will still be using the default network and not custom networks. For previous workloads to use custom networks, the recreation of pods is required.\nWhen csi-powerscale driver creates an NFS export, the traffic flows through the client specified in the export. By default, the client is the network interface for Kubernetes communication (same IP/fqdn as k8s node) by default.\nFor a cluster with multiple network interfaces and if a user wants to segregate k8s traffic from NFS traffic; you can use the allowedNetworks option. allowedNetworks takes CIDR addresses as a parameter to match the IPs to be picked up by the driver to allow and route NFS traffic.\nVolume Limit The CSI Driver for Dell PowerScale allows users to specify the maximum number of PowerScale volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-isilon-volumes-per-node and specifying the volume limit for that node.  kubectl label node \u003cnode_name\u003e max-isilon-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxIsilonVolumesPerNode attribute in values.yaml.\n NOTE: The default value of maxIsilonVolumesPerNode is 0. If maxIsilonVolumesPerNode is set to zero, then CO shall decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxIsilonVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-isilon-volumes-per-node is not set.\n Node selector in helm template Now user can define in which worker node, the CSI node pod daemonset can run (just like any other pod in Kubernetes world.)For more information, refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector\nSimilarly, users can define the tolerations based on various conditions like memory pressure, disk pressure and network availability. Refer to https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#taints-and-tolerations for more information.\nUsage of SmartQuotas to Limit Storage Consumption CSI driver for Dell Isilon handles capacity limiting using SmartQuotas feature.\nTo use the SmartQuotas feature user can specify the boolean value ‘enableQuota’ in myvalues.yaml or my-isilon-settings.yaml.\nLet us assume the user creates a PVC with 3 Gi of storage and ‘SmartQuotas’ have already been enabled in PowerScale Cluster.\n  When ‘enableQuota’ is set to ‘true’\n The driver sets the hard limit of the PVC to 3Gi. The user adds data of 2Gi to the above said PVC (by logging into POD). It works as expected. The user tries to add 2Gi more data. Driver doesn’t allow the user to enter more data as total data to be added is 4Gi and PVC limit is 3Gi. The user can expand the volume from 3Gi to 6Gi. The driver allows it and sets the hard limit of PVC to 6Gi. User retries adding 2Gi more data (which has been errored out previously). The driver accepts the data.    When ‘enableQuota’ is set to ‘false’\n Driver doesn’t set any hard limit against the PVC created. The user adds data of 2Gi to the above said PVC, which is having the size 3Gi (by logging into POD). It works as expected. The user tries to add 2Gi more data. Now the total size of data is 4Gi. Driver allows the user to enter more data irrespective of the initial PVC size (since no quota is set against this PVC) The user can expand the volume from an initial size of 3Gi to 4Gi or more. The driver allows it.    Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerScale version 1.6.0 and updated in version 2.0.0\nHelm based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade Note: here my-isilon-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap isilon-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n isilon isilon-config-params  Note: Prior to CSI Driver for PowerScale version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object.\n NAT Support CSI Driver for Dell PowerScale is supported in the NAT environment.\nConfigurable permissions for volume directory This feature is introduced in CSI Driver for PowerScale version 2.0.0\nHelm based installation The permissions for volume directory can now be configured in 3 ways:\n Through values.yaml Through secrets Through storage class   # isiVolumePathPermissions: The permissions for isi volume directory path # This value acts as a default value for isiVolumePathPermissions, if not specified for a cluster config in secret # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" isiVolumePathPermissions: \"0777\" The permissions present in values.yaml are the default for all cluster config.\nIf the volume permission is not present in storage class then secrets are considered and if it is not present even in secrets then values.yaml is considered.\n Note: For volume creation from source (volume from snapshot/volume from volume) permissions are inherited from source. Create myvalues.yaml/my-isilon-settings.yaml and storage class accordingly.\n Operator based installation In the case of operator-based installation, default permission for powerscale directory is present in the samples file.\nOther ways of configuring powerscale volume permissions remain the same as helm-based installation.\nPV/PVC Metrics CSI Driver for Dell PowerScale 2.1.0 and above supports volume health monitoring. This allows Kubernetes to report on the condition, status and usage of the underlying volumes. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nThis feature can be enabled  For controller plugin, by setting attribute controller.healthMonitor.enabled to true in values.yaml file. Also health monitoring interval can be changed through attribute controller.healthMonitor.interval in values.yaml file. For node plugin, by setting attribute node.healthMonitor.enabled to true in values.yaml file and by enabling the alpha feature gate CSIVolumeHealth.  Single Pod Access Mode for PersistentVolumes- ReadWriteOncePod (ALPHA FEATURE) Use ReadWriteOncePod(RWOP) access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it. This is supported for CSI Driver for PowerScale 2.1.0+ and Kubernetes version 1.22+.\nTo use this feature, enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet, by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreating a PersistentVolumeClaim kind:PersistentVolumeClaimapiVersion:v1metadata:name:single-writer-onlyspec:accessModes:- ReadWriteOncePod# the volume can be mounted as read-write by a single pod across the whole clusterresources:requests:storage:1GiWhen this feature is enabled, the existing ReadWriteOnce(RWO) access mode restricts volume access to a single node and allows multiple pods on the same node to read from and write to the same volume.\nTo migrate existing PersistentVolumes to use ReadWriteOncePod, please follow the instruction from here.\n","excerpt":"Multicluster support You can connect a single CSI-PowerScale driver …","ref":"/csm-docs/v1/csidriver/features/powerscale/","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell PowerScale using Helm or Dell CSI Operator.\nUpgrade Driver from version 2.1.0 to 2.2.0 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\n  Clone the repository using git clone -b v2.2.0 https://github.com/dell/csi-powerscale.git, copy the helm/csi-isilon/values.yaml into a new location with a custom name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\n  Change to directory dell-csi-helm-installer to install the Dell PowerScale cd dell-csi-helm-installer\n  Upgrade the CSI Driver for Dell PowerScale using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade\n  Upgrade using Dell CSI Operator: Notes:\n While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes. Upgrading the Operator does not upgrade the CSI Driver.  To upgrade the driver:\n Please upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell PowerScale using Helm or Dell …","ref":"/csm-docs/v1/csidriver/upgradation/drivers/isilon/","title":"PowerScale"},{"body":"Multicluster support You can connect a single CSI-PowerScale driver with multiple PowerScale clusters.\nPre-Requisites:\n Creation of secret.yaml with credentials related to one or more Clusters. Creation of (at least) one Storage class for each cluster. Creation of custom-volumesnapshot classes with proper isiPath matching corresponding storage classes. Inclusion of cluster name in volume handle, if you want to provision existing static volumes.  Consuming existing volumes with static provisioning You can use existent volumes from the PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, the PowerScale cluster accessZone is assumed as ‘System’, storage class as ‘isilon’, cluster name as ‘pscale-cluster’ and volume’s internal name as ‘isilonvol’. The volume-handle should be in the format of \u003cvolume_name\u003e=_=_=\u003cexport_id\u003e=_=_==_=_=\u003ccluster_name\u003e  apiVersion:v1kind:PersistentVolumemetadata:name:isilonstaticpvnamespace:defaultspec:capacity:storage:5GiaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:RetainstorageClassName:isiloncsi:driver:csi-isilon.dellemc.comvolumeAttributes:Path:\"/ifs/data/csi/isilonvol\"Name:\"isilonvol\"AzServiceIP:'XX.XX.XX.XX'volumeHandle:isilonvol=_=_=652=_=_=System=_=_=pscale-clusterclaimRef:name:isilonstaticpvcnamespace:defaultCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilonstaticpvcnamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:isilonstaticpvstorageClassName:isilonThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:isilonstaticpvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.  PVC Creation Feature Following yaml content can be used to create a PVC without referring any PV.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:testvolumenamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonVolume Snapshot Feature The CSI PowerScale driver version 2.0 and later supports managing v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller   For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerScale driver version 2.0 and higher, no default Volume Snapshot Class will get created.\nFollowing are the manifests for the Volume Snapshot Class:\n VolumeSnapshotClass  apiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:\"isilon-snapclass\"driver:csi-isilon.dellemc.com#The deletionPolicy of a volume snapshot class can either be Retain or Delete#If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object.#If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remaindeletionPolicy:Deleteparameters:#IsiPath should match with respective storageClass IsiPathIsiPath:\"/ifs/data/csi\"The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs; The following snippet assumes that the persistent volume claim name is testvolume.\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvcsnapnamespace:defaultspec:volumeSnapshotClassName:isilon-snapclasssource:persistentVolumeClaimName:testvolumeOnce the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:createfromsnapnamespace:defaultspec:storageClassName:isilondataSource:name:pvcsnapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteManyresources:requests:storage:5Gi Starting from CSI PowerScale driver version 2.2, it is allowed to create PersistentVolumeClaim from VolumeSnapshot with different isi paths i.e., isi paths of the new volume and the VolumeSnapshot can be different.\n Volume Expansion The CSI PowerScale driver version 1.2 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-expand-scannotations:storageclass.kubernetes.io/is-default-class:\"false\"provisioner:\"csi-isilon.dellemc.com\"reclaimPolicy:Deleteparameters:ClusterName:\u003cclusterNamespecifiedinsecret.yaml\u003e AccessZone: SystemisiPath:\"/ifs/data/csi\"AzServiceIP :'XX.XX.XX.XX'rootClientEnabled:\"true\"allowVolumeExpansion:truevolumeBindingMode:ImmediateTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilon-pvc-expansion-demospec:accessModes:- ReadWriteOnceresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Volume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing PVC:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:existing-pvcspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonThe following is a sample manifest for cloning:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:volume-from-volumenamespace:defaultspec:accessModes:- ReadWriteManyvolumeMode:Filesystemresources:requests:storage:50GistorageClassName:isilondataSource:kind:PersistentVolumeClaimname:existing-pvcapiGroup:\"\"Controller HA The CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two-controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if the controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in a Pending state.\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nEphemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves as use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumespec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data\"name:my-csi-volumevolumes:- name:my-csi-volumecsi:driver:csi-isilon.dellemc.comvolumeAttributes:size:\"2Gi\"ClusterName:\"cluster1\"This manifest creates a pod in a given cluster and attaches a newly created ephemeral inline CSI volume to it.\nTopology Topology Support The CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage, as a result of which the nodes which have access to PowerScale Array are appropriately labeled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, the CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and use them to initialize node pod with custom PowerScale FQDN/IP.\nNote: Only a single cluster can be configured as part of secret.yaml for custom topology.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that the Pod schedule takes advantage of the topology and the selected node has access to provisioned volumes.\nNote: Whenever a new storage cluster is being added in secret, even though it is dynamic, the new storage cluster IP address-related label is not added to worker nodes dynamically. The user has to spin off (bounce) driver-related pods (controller and node pods) in order to apply newly added information to be reflected in worker nodes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options.# PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon# Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server# Provide mount options through \"mountOptions\" attribute # to create PVCs with mount options.apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilonprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:trueparameters:AccessZone:SystemIsiPath:\"/ifs/data/csi\"# AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP.#AzServiceIP : 192.168.2.1# When a PVC is being created, it takes the storage class' value of \"storageclass.rootClientEnabled\", # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \"Root clients\" field (when true) or \"Clients\" field (when false) of the NFS export RootClientEnabled:\"false\"# Name of PowerScale cluster where pv will be provisioned# This name should match with name of one of the cluster configs in isilon-creds secret# If this parameter is not specified, then default cluster config in isilon-creds secret will be considered if available#ClusterName: \"\u003ccluster_name\u003e\"# volumeBindingMode controls when volume binding and dynamic provisioning should occur.# Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created# WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume# until a Pod using the PersistentVolumeClaim is createdvolumeBindingMode:WaitForFirstConsumer# allowedTopologies helps scheduling pod on worker nodes which match all of below expressions# If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologiesallowedTopologies:- matchLabelExpressions:- key:csi-isilon.dellemc.com/\u003cISILON_IP\u003e values:- csi-isilon.dellemc.com# specify additional mount options for when a Persistent Volume is being mounted on a node.# To mount volume with NFSv4, specify mount option vers=4. Make sure NFSv4 is enabled on the Isilon Cluster.mountOptions:[\"\u003cmountOption1\u003e\",\"\u003cmountOption2\u003e\",...,\"\u003cmountOptionN\u003e\"]For additional information, see the Kubernetes Topology documentation.\nSupport custom networks for NFS I/O traffic When allowedNetworks is specified for using custom networks to handle NFS traffic, and a user already has workloads scheduled, there is a possibility that it might lead to backward compatibility issues. For example, ControllerUnPublish might not be able to completely remove clients from the NFS exports of previously created pods.\nAlso, the previous workload will still be using the default network and not custom networks. For previous workloads to use custom networks, the recreation of pods is required.\nWhen csi-powerscale driver creates an NFS export, the traffic flows through the client specified in the export. By default, the client is the network interface for Kubernetes communication (same IP/fqdn as k8s node) by default.\nFor a cluster with multiple network interfaces and if a user wants to segregate k8s traffic from NFS traffic; you can use the allowedNetworks option. allowedNetworks takes CIDR addresses as a parameter to match the IPs to be picked up by the driver to allow and route NFS traffic.\nVolume Limit The CSI Driver for Dell PowerScale allows users to specify the maximum number of PowerScale volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-isilon-volumes-per-node and specifying the volume limit for that node.  kubectl label node \u003cnode_name\u003e max-isilon-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxIsilonVolumesPerNode attribute in values.yaml.\n NOTE: The default value of maxIsilonVolumesPerNode is 0. If maxIsilonVolumesPerNode is set to zero, then CO shall decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxIsilonVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-isilon-volumes-per-node is not set.\n Node selector in helm template Now user can define in which worker node, the CSI node pod daemonset can run (just like any other pod in Kubernetes world.)For more information, refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector\nSimilarly, users can define the tolerations based on various conditions like memory pressure, disk pressure and network availability. Refer to https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#taints-and-tolerations for more information.\nUsage of SmartQuotas to Limit Storage Consumption CSI driver for Dell Isilon handles capacity limiting using SmartQuotas feature.\nTo use the SmartQuotas feature user can specify the boolean value ‘enableQuota’ in myvalues.yaml or my-isilon-settings.yaml.\nLet us assume the user creates a PVC with 3 Gi of storage and ‘SmartQuotas’ have already been enabled in PowerScale Cluster.\n  When ‘enableQuota’ is set to ‘true’\n The driver sets the hard limit of the PVC to 3Gi. The user adds data of 2Gi to the above said PVC (by logging into POD). It works as expected. The user tries to add 2Gi more data. Driver doesn’t allow the user to enter more data as total data to be added is 4Gi and PVC limit is 3Gi. The user can expand the volume from 3Gi to 6Gi. The driver allows it and sets the hard limit of PVC to 6Gi. User retries adding 2Gi more data (which has been errored out previously). The driver accepts the data.    When ‘enableQuota’ is set to ‘false’\n Driver doesn’t set any hard limit against the PVC created. The user adds data of 2Gi to the above said PVC, which is having the size 3Gi (by logging into POD). It works as expected. The user tries to add 2Gi more data. Now the total size of data is 4Gi. Driver allows the user to enter more data irrespective of the initial PVC size (since no quota is set against this PVC) The user can expand the volume from an initial size of 3Gi to 4Gi or more. The driver allows it.    Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerScale version 1.6.0 and updated in version 2.0.0\nHelm based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade Note: here my-isilon-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap isilon-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n isilon isilon-config-params  Note: Prior to CSI Driver for PowerScale version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object.\n NAT Support CSI Driver for Dell PowerScale is supported in the NAT environment.\nConfigurable permissions for volume directory This feature is introduced in CSI Driver for PowerScale version 2.0.0\nHelm based installation The permissions for volume directory can now be configured in 3 ways:\n Through values.yaml Through secrets Through storage class   # isiVolumePathPermissions: The permissions for isi volume directory path # This value acts as a default value for isiVolumePathPermissions, if not specified for a cluster config in secret # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" isiVolumePathPermissions: \"0777\" The permissions present in values.yaml are the default for all cluster config.\nIf the volume permission is not present in storage class then secrets are considered and if it is not present even in secrets then values.yaml is considered.\n Note: For volume creation from source (volume from snapshot/volume from volume) permissions are inherited from source. Create myvalues.yaml/my-isilon-settings.yaml and storage class accordingly.\n Operator based installation In the case of operator-based installation, default permission for powerscale directory is present in the samples file.\nOther ways of configuring powerscale volume permissions remain the same as helm-based installation.\nPV/PVC Metrics CSI Driver for Dell PowerScale 2.1.0 and above supports volume health monitoring. This allows Kubernetes to report on the condition, status and usage of the underlying volumes. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nThis feature can be enabled  For controller plugin, by setting attribute controller.healthMonitor.enabled to true in values.yaml file. Also health monitoring interval can be changed through attribute controller.healthMonitor.interval in values.yaml file. For node plugin, by setting attribute node.healthMonitor.enabled to true in values.yaml file and by enabling the alpha feature gate CSIVolumeHealth.  Single Pod Access Mode for PersistentVolumes- ReadWriteOncePod (ALPHA FEATURE) Use ReadWriteOncePod(RWOP) access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it. This is supported for CSI Driver for PowerScale 2.1.0+ and Kubernetes version 1.22+.\nTo use this feature, enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet, by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreating a PersistentVolumeClaim kind:PersistentVolumeClaimapiVersion:v1metadata:name:single-writer-onlyspec:accessModes:- ReadWriteOncePod# the volume can be mounted as read-write by a single pod across the whole clusterresources:requests:storage:1GiWhen this feature is enabled, the existing ReadWriteOnce(RWO) access mode restricts volume access to a single node and allows multiple pods on the same node to read from and write to the same volume.\nTo migrate existing PersistentVolumes to use ReadWriteOncePod, please follow the instruction from here.\n","excerpt":"Multicluster support You can connect a single CSI-PowerScale driver …","ref":"/csm-docs/v2/csidriver/features/powerscale/","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell PowerScale using Helm or Dell CSI Operator.\nUpgrade Driver from version 2.1.0 to 2.2.0 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\n  Clone the repository using git clone -b v2.2.0 https://github.com/dell/csi-powerscale.git, copy the helm/csi-isilon/values.yaml into a new location with a custom name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\n  Change to directory dell-csi-helm-installer to install the Dell PowerScale cd dell-csi-helm-installer\n  Upgrade the CSI Driver for Dell PowerScale using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade\n  Upgrade using Dell CSI Operator: Notes:\n While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes. Upgrading the Operator does not upgrade the CSI Driver.  To upgrade the driver:\n Please upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell PowerScale using Helm or Dell …","ref":"/csm-docs/v2/csidriver/upgradation/drivers/isilon/","title":"PowerScale"},{"body":"Multicluster support You can connect a single CSI-PowerScale driver with multiple PowerScale clusters.\nPre-Requisites:\n Creation of secret.yaml with credentials related to one or more Clusters. Creation of (at least) one Storage class for each cluster. Creation of custom-volumesnapshot classes with proper isiPath matching corresponding storage classes. Inclusion of cluster name in volume handle, if you want to provision existing static volumes.  Consuming existing volumes with static provisioning You can use existent volumes from the PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, the PowerScale cluster accessZone is assumed as ‘System’, storage class as ‘isilon’, cluster name as ‘pscale-cluster’ and volume’s internal name as ‘isilonvol’. The volume-handle should be in the format of \u003cvolume_name\u003e=_=_=\u003cexport_id\u003e=_=_==_=_=\u003ccluster_name\u003e  apiVersion:v1kind:PersistentVolumemetadata:name:isilonstaticpvnamespace:defaultspec:capacity:storage:5GiaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:RetainstorageClassName:isiloncsi:driver:csi-isilon.dellemc.comvolumeAttributes:Path:\"/ifs/data/csi/isilonvol\"Name:\"isilonvol\"AzServiceIP:'XX.XX.XX.XX'volumeHandle:isilonvol=_=_=652=_=_=System=_=_=pscale-clusterclaimRef:name:isilonstaticpvcnamespace:defaultCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilonstaticpvcnamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:isilonstaticpvstorageClassName:isilonThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:isilonstaticpvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.  PVC Creation Feature Following yaml content can be used to create a PVC without referring any PV.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:testvolumenamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonVolume Snapshot Feature The CSI PowerScale driver version 2.0 and later supports managing v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller   For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerScale driver version 2.0 and higher, no default Volume Snapshot Class will get created.\nFollowing are the manifests for the Volume Snapshot Class:\n VolumeSnapshotClass  # For kubernetes version 20 and above (v1 snaps)apiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:\"isilon-snapclass\"driver:csi-isilon.dellemc.com#The deletionPolicy of a volume snapshot class can either be Retain or Delete#If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object.#If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remaindeletionPolicy:Deleteparameters:#IsiPath should match with respective storageClass IsiPathIsiPath:\"/ifs/data/csi\"The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs; The following snippet assumes that the persistent volume claim name is testvolume.\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvcsnapnamespace:defaultspec:volumeSnapshotClassName:isilon-snapclasssource:persistentVolumeClaimName:testvolumeOnce the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:createfromsnapnamespace:defaultspec:storageClassName:isilondataSource:name:pvcsnapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteManyresources:requests:storage:5GiVolume Expansion The CSI PowerScale driver version 1.2 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:\"false\"provisioner:\"csi-isilon.dellemc.com\"reclaimPolicy:Deleteparameters:ClusterName:\u003cclusterNamespecifiedinsecret.yaml\u003e AccessZone: SystemisiPath:\"/ifs/data/csi\"AzServiceIP :'XX.XX.XX.XX'rootClientEnabled:\"true\"allowVolumeExpansion:truevolumeBindingMode:ImmediateTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilon-pvc-expansion-demospec:accessModes:- ReadWriteOnceresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Volume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing PVC:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:existing-pvcspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonThe following is a sample manifest for cloning:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:volume-from-volumenamespace:defaultspec:accessModes:- ReadWriteManyvolumeMode:Filesystemresources:requests:storage:50GistorageClassName:isilondataSource:kind:PersistentVolumeClaimname:existing-pvcapiGroup:\"\"Controller HA The CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two-controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if the controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in a Pending state.\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nEphemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves as use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumespec:containers:- name:my-frontendimage:busyboxcommand:[\"sleep\",\"100000\"]volumeMounts:- mountPath:\"/data\"name:my-csi-volumevolumes:- name:my-csi-volumecsi:driver:csi-isilon.dellemc.comvolumeAttributes:size:\"2Gi\"ClusterName:\"cluster1\"This manifest creates a pod in a given cluster and attaches a newly created ephemeral inline CSI volume to it.\nTopology Topology Support The CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage, as a result of which the nodes which have access to PowerScale Array are appropriately labeled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, the CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and use them to initialize node pod with custom PowerScale FQDN/IP.\nNote: Only a single cluster can be configured as part of secret.yaml for custom topology.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that the Pod schedule takes advantage of the topology and the selected node has access to provisioned volumes.\nNote: Whenever a new storage cluster is being added in secret, even though it is dynamic, the new storage cluster IP address-related label is not added to worker nodes dynamically. The user has to spin off (bounce) driver-related pods (controller and node pods) in order to apply newly added information to be reflected in worker nodes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options.# PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon# Change all instances of \u003cISILON_IP\u003e to the IP of the PowerScale OneFS API server# Provide mount options through \"mountOptions\" attribute # to create PVCs with mount options.apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilonprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:trueparameters:AccessZone:SystemIsiPath:\"/ifs/data/csi\"# AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP.#AzServiceIP : 192.168.2.1# When a PVC is being created, it takes the storage class' value of \"storageclass.rootClientEnabled\", # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \"Root clients\" field (when true) or \"Clients\" field (when false) of the NFS export RootClientEnabled:\"false\"# Name of PowerScale cluster where pv will be provisioned# This name should match with name of one of the cluster configs in isilon-creds secret# If this parameter is not specified, then default cluster config in isilon-creds secret will be considered if available#ClusterName: \"\u003ccluster_name\u003e\"# volumeBindingMode controls when volume binding and dynamic provisioning should occur.# Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created# WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume# until a Pod using the PersistentVolumeClaim is createdvolumeBindingMode:WaitForFirstConsumer# allowedTopologies helps scheduling pod on worker nodes which match all of below expressions# If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologiesallowedTopologies:- matchLabelExpressions:- key:csi-isilon.dellemc.com/\u003cISILON_IP\u003e values:- csi-isilon.dellemc.com# specify additional mount options for when a Persistent Volume is being mounted on a node.# To mount volume with NFSv4, specify mount option vers=4. Make sure NFSv4 is enabled on the Isilon Cluster.mountOptions:[\"\u003cmountOption1\u003e\",\"\u003cmountOption2\u003e\",...,\"\u003cmountOptionN\u003e\"]For additional information, see the Kubernetes Topology documentation.\nSupport custom networks for NFS I/O traffic When allowedNetworks is specified for using custom networks to handle NFS traffic, and a user already has workloads scheduled, there is a possibility that it might lead to backward compatibility issues. For example, ControllerUnPublish might not be able to completely remove clients from the NFS exports of previously created pods.\nAlso, the previous workload will still be using the default network and not custom networks. For previous workloads to use custom networks, the recreation of pods is required.\nWhen csi-powerscale driver creates an NFS export, the traffic flows through the client specified in the export. By default, the client is the network interface for Kubernetes communication (same IP/fqdn as k8s node) by default.\nFor a cluster with multiple network interfaces and if a user wants to segregate k8s traffic from NFS traffic; you can use the allowedNetworks option. allowedNetworks takes CIDR addresses as a parameter to match the IPs to be picked up by the driver to allow and route NFS traffic.\nVolume Limit The CSI Driver for Dell EMC PowerScale allows users to specify the maximum number of PowerScale volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-isilon-volumes-per-node and specifying the volume limit for that node.  kubectl label node \u003cnode_name\u003e max-isilon-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxIsilonVolumesPerNode attribute in values.yaml.\n NOTE: The default value of maxIsilonVolumesPerNode is 0. If maxIsilonVolumesPerNode is set to zero, then CO shall decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxIsilonVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-isilon-volumes-per-node is not set.\n Node selector in helm template Now user can define in which worker node, the CSI node pod daemonset can run (just like any other pod in Kubernetes world.)For more information, refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector\nSimilarly, users can define the tolerations based on various conditions like memory pressure, disk pressure and network availability. Refer to https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#taints-and-tolerations for more information.\nUsage of SmartQuotas to Limit Storage Consumption CSI driver for Dell EMC Isilon handles capacity limiting using SmartQuotas feature.\nTo use the SmartQuotas feature user can specify the boolean value ‘enableQuota’ in myvalues.yaml or my-isilon-settings.yaml.\nLet us assume the user creates a PVC with 3 Gi of storage and ‘SmartQuotas’ have already been enabled in PowerScale Cluster.\n  When ‘enableQuota’ is set to ‘true’\n The driver sets the hard limit of the PVC to 3Gi. The user adds data of 2Gi to the above said PVC (by logging into POD). It works as expected. The user tries to add 2Gi more data. Driver doesn’t allow the user to enter more data as total data to be added is 4Gi and PVC limit is 3Gi. The user can expand the volume from 3Gi to 6Gi. The driver allows it and sets the hard limit of PVC to 6Gi. User retries adding 2Gi more data (which has been errored out previously). The driver accepts the data.    When ‘enableQuota’ is set to ‘false’\n Driver doesn’t set any hard limit against the PVC created. The user adds data of 2Gi to the above said PVC, which is having the size 3Gi (by logging into POD). It works as expected. The user tries to add 2Gi more data. Now the total size of data is 4Gi. Driver allows the user to enter more data irrespective of the initial PVC size (since no quota is set against this PVC) The user can expand the volume from an initial size of 3Gi to 4Gi or more. The driver allows it.    Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerScale version 1.6.0 and updated in version 2.0.0\nHelm based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade Note: here my-isilon-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name isilon-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap isilon-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n isilon isilon-config-params  Note: Prior to CSI Driver for PowerScale version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object.\n NAT Support CSI Driver for Dell EMC PowerScale is supported in the NAT environment.\nConfigurable permissions for volume directory This feature is introduced in CSI Driver for PowerScale version 2.0.0\nHelm based installation The permissions for volume directory can now be configured in 3 ways:\n Through values.yaml Through secrets Through storage class   # isiVolumePathPermissions: The permissions for isi volume directory path # This value acts as a default value for isiVolumePathPermissions, if not specified for a cluster config in secret # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" isiVolumePathPermissions: \"0777\" The permissions present in values.yaml are the default for all cluster config.\nIf the volume permission is not present in storage class then secrets are considered and if it is not present even in secrets then values.yaml is considered.\n Note: For volume creation from source (volume from snapshot/volume from volume) permissions are inherited from source. Create myvalues.yaml/my-isilon-settings.yaml and storage class accordingly.\n Operator based installation In the case of operator-based installation, default permission for powerscale directory is present in the samples file.\nOther ways of configuring powerscale volume permissions remain the same as helm-based installation.\nPV/PVC Metrics CSI Driver for Dell EMC PowerScale 2.1.0 and above supports volume health monitoring. This allows Kubernetes to report on the condition, status and usage of the underlying volumes. For example, if a volume were to be deleted from the array, or unmounted outside of Kubernetes, Kubernetes will now report these abnormal conditions as events.\nThis feature can be enabled  For controller plugin, by setting attribute controller.healthMonitor.enabled to true in values.yaml file. Also health monitoring interval can be changed through attribute controller.healthMonitor.interval in values.yaml file. For node plugin, by setting attribute node.healthMonitor.enabled to true in values.yaml file and by enabling the alpha feature gate CSIVolumeHealth.  Single Pod Access Mode for PersistentVolumes- ReadWriteOncePod (ALPHA FEATURE) Use ReadWriteOncePod(RWOP) access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it. This is only supported for CSI Driver for PowerScale 2.1.0 and Kubernetes version 1.22+.\nTo use this feature, enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet, by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\"\nCreating a PersistentVolumeClaim kind:PersistentVolumeClaimapiVersion:v1metadata:name:single-writer-onlyspec:accessModes:- ReadWriteOncePod# the volume can be mounted as read-write by a single pod across the whole clusterresources:requests:storage:1GiWhen this feature is enabled, the existing ReadWriteOnce(RWO) access mode restricts volume access to a single node and allows multiple pods on the same node to read from and write to the same volume.\nTo migrate existing PersistentVolumes to use ReadWriteOncePod, please follow the instruction from here.\n","excerpt":"Multicluster support You can connect a single CSI-PowerScale driver …","ref":"/csm-docs/v3/csidriver/features/powerscale/","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerScale using Helm or Dell CSI Operator.\nUpgrade Driver from version 2.0.0 to 2.1.0 Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\n  Verify that all pre-requisites to install CSI Driver for Dell EMC PowerScale version 2.1.0 are fulfilled. Note that change in secret format should be implemented.\n Delete the existing secret (isilon-creds and isilon-certs-0) Create new secrets (isilon-creds and isilon-certs-0) Refer Installation section here.    Clone the repository using git clone -b v2.1.0 https://github.com/dell/csi-powerscale.git, copy the helm/csi-isilon/values.yaml into a new location with a custom name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\n  Change to directory dell-csi-helm-installer to install the Dell EMC PowerScale cd dell-csi-helm-installer\n  Upgrade the CSI Driver for Dell EMC PowerScale version 2.1.0 using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade\n  Upgrade using Dell CSI Operator: Note: While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\nTo upgrade the driver from version 2.0.0 to 2.1.0:\nNote: It is highly recommended to take Backup of existing storage class definition and volumesnapshot class definition, yaml files before the upgrade.\n  Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade . This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerScale using Helm or …","ref":"/csm-docs/v3/csidriver/upgradation/drivers/isilon/","title":"PowerScale"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command creates a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset are created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore\nThe pod must be Ready and Running\n If Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run, use the command:\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id in volumeHandle in format \u003cvolume-id/globalID/protocol\u003e in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:existingvolspec:accessModes:- ReadWriteOncecapacity:storage:30Gicsi:driver:csi-powerstore.dellemc.comvolumeHandle:0055558c-5ae1-4ed1-b421-6f5a9475c19f/unique/scsipersistentVolumeReclaimPolicy:RetainstorageClassName:powerstorevolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30GistorageClassName:powerstoreThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:quay.io/centos/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:pvolAfter the pod is Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerStore driver version 2.0.0 and higher supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class   Note: From v1.4, the CSI PowerStore driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the samples folder\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powerstore-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueSnapshot feature is optional for the installation CSI PowerStore driver version 1.4 makes the snapshot feature optional for the installation.\nTo enable or disable this feature, change values.snapshot.enable parameter to true or false, specify the following in values.yaml to enable this feature\nsnapshot:enable:trueExternal Snapshotter and its CRDs are not installed even if the Snapshot feature is enabled. These have to be installed manually before the installation.\nDisabling the Snapshot feature will opt out of the snapshotter sidecar from the installation.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP The CSI PowerStore driver Version 1.3.0 and later extends Challenge Handshake Authentication Protocol (CHAP) support by adding automatic credentials generation.\nThis means that you no longer need to provide chapsecret/chapuser credentials, they will be automatically generated by the driver for each host.\nTo enable this feature you need to set connection.enableCHAP to true when installing with helm or set X_CSI_POWERSTORE_ENABLE_CHAP to true in your PowerStore CustomResource when installing using operator.\nThe driver uses the generated chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating a new host on powerstore array driver will populate host chap credentials with generated values. When re-using already existing hosts driver must override existing CHAP credentials with newly generated ones.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-expand-scannotations:storageclass.kubernetes.io/is-default-class:falseprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pstore-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:powerstoretestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powerstoreresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvol0namespace:testpowerstorespec:storageClassName:powerstoreaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clonedpvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI PowerStore driver version 1.2 and later supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind:PodapiVersion:v1metadata:name:powerstore-inline-volumespec:containers:- name:test-containerimage:quay.io/centos/centoscommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"20Gi\"arrayID:\"unique\"This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"nfsAcls:\"0777\"Controller HA The CSI PowerStore driver version 1.2 and later introduces the controller HA feature. Instead of StatefulSet, controller pods are deployed as a Deployment.\nBy default number of replicas is set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in the cluster, each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods must be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As mentioned earlier, you can configure where node driver pods would be assigned in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they must use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology features user must create their own storage classes similar to those that can be found in samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-fcprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/127.0.0.1-fcvalues:- \"true\"This example matches all nodes where the driver has a connection to PowerStore with an IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS, iSCSI and NVMe.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels\n Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work.\n For any additional information about the topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as a Host on the storage array before. It will check if Host initiators and node initiators (FC, iSCSI or NVMe) match. If they do, the driver will not create a new host and will take the existing name of the Host as nodeID.\nMultiarray support The CSI PowerStore driver version 1.3.0 and later support managing multiple PowerStore arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration Create a file called config.yaml and populate it with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIglobalID:\"unique\"# global ID to identify arrayusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIskipCertificateValidation:true# use insecure connection or notdefault:true# treat current array as a default (would be used by storage classes without arrayIP parameter)blockProtocol:\"ISCSI\"# what transport protocol use on node side (FC, ISCSI, NVMeTCP, None, or auto)nasName:\"nas-server\"# what NAS must be used for NFS volumesnfsAcls:\"0777\"# (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory.# NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.- endpoint:\"https://10.0.0.2/api/rest\"globalID:\"unique\"username:\"user\"password:\"password\"skipCertificateValidation:trueblockProtocol:\"FC\"Here we specify that we want to CSI driver to manage two arrays: one with an IP 10.0.0.1 and the other with an IP 10.0.0.2, we want to connect to the first array with iSCSI protocol and with FC to the second array. Also, we want to be able to create NFS-based volume so we provide the name of the NAS to the first array.\nTo use this config we need to create a Kubernetes secret from it, to do so create a file called secret.yaml in the same folder and populate it with the following content:\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLApply the secret by running following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Creating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nCreate file storageclass.yaml and populate it with the following content:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-1provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayID:\"GlobalUniqueID\"FsType:\"ext4\"---apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-2provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayID:\"GlobalUniqueID\"FsType:\"xfs\"Here we specify two storage classes: one of them uses the first array and ext4 filesystem, and the other uses the second array and xfs filesystem.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use powerstore-1 storage class to create volumes on the first array and powerstore-2 storage class to create volumes on the second array.\nDynamic secret change detection CSI PowerStore driver version 1.3.0 and later supports the ability to detect changes to array configuration Kubernetes secret. This essentially means that you can change credentials for your PowerStore arrays in-flight (without restarting the driver).\nTo do so just change your configuration file config.yaml and apply it again using the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - After Kubernetes remounts secret to driver containers (this usually takes around one minute), a driver must detect the change and start using this new configuration information.\nConfiguring custom access to NFS exports CSI PowerStore driver Version 1.3.0 and later supports the ability to configure NFS access to nodes that use dedicated storage networks.\nTo enable this feature you need to specify externalAccess parameter in your helm values.yaml file or X_CSI_POWERSTORE_EXTERNAL_ACCESS variable when creating CustomResource using an operator.\nThe value of that parameter is added as an additional entry to NFS Export host access.\nFor example the following notation:\nexternalAccess:\"10.0.0.0/24\"This means that we allow for NFS Export created by driver to be consumed by address range 10.0.0.0-10.0.0.255.\nArray identification based on GlobalID CSI PowerStore driver version 1.4.0 onwards slightly changes the way arrays are being identified in runtime. In previous versions of the driver, a management IP address was used to identify an array. The address change could lead to an invalid state of PV. From version 1.4.0 a unique GlobalID string is used for an array identification. It has to be specified in config.yaml and in Storage Classes.\nThe change provides backward compatibility with previously created PVs. However, to provision new volumes, make sure to delete old Storage Classes and create new ones with arrayID instead of arrayIP specified.\n NOTE: It is recommended to migrate the PVs to new identifiers before changing management IPs of storage systems. The recommended way to do it is to clone the existing volume and delete the old one. The cloned volume will automatically switch to using globalID instead of management IP.\n Root squashing CSI PowerStore driver version 1.4.0 and later allows users to enable root squashing for NFS volumes provisioned by the driver.\nRoot squashing rule prevents root users on NFS clients from exercising root privileges on the NFS server.\nTo enable this rule, you need to set parameter allowRoot to false in your NFS storage class.\nYour storage class definition must look similar to this:\napiVersion:storage.k8s.io/v1kind:StorageClass...parameters:...allowRoot:\"false\"# enables or disables root squashing The 1.4 version and later of the driver also enables any container user, to have full access to provisioned NFS volume, in earlier versions only root user had access\n Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params NAT Support CSI Driver for Dell Powerstore is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nPV/PVC Metrics CSI Driver for Dell Powerstore 2.1.0 and above supports volume health monitoring. To enable Volume Health Monitoring from the node side, the alpha feature gate CSIVolumeHealth needs to be enabled. To use this feature, set controller.healthMonitor.enabled and node.healthMonitor.enabled to true. To change the monitor interval, set controller.healthMonitor.volumeHealthMonitorInterval parameter.\nSingle Pod Access Mode for PersistentVolumes Starting from version 2.1, CSI Driver for Powerstore now supports a new access mode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Powerstore allows restricting volume access to a single pod in the cluster and within a worker node.\nPrerequisites\n  Enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is supported only for CSI volumes. You can enable the feature by setting command-line argument: --feature-gates=\"...,ReadWriteOncePod=true\"\n  Create a PVC with access mode set to ReadWriteOncePod like shown in the sample below\n  kind:PersistentVolumeClaimapiVersion:v1metadata:name:single-node-single-writerspec:accessModes:- ReadWriteOncePod# Allow only a single pod to access single-node-single-writerresources:requests:storage:5Gi Note: The access mode ReadWriteOnce allows multiple pods to access a single volume within a single worker node and the behavior is consistent across all supported Kubernetes versions.\n POSIX mode bits and NFSv4 ACLs CSI PowerStore driver version 2.2.0 and later allows users to set user-defined permissions on NFS target mount directory using POSIX mode bits or NFSv4 ACLs.\nNFSv4 ACLs are supported for NFSv4 shares on NFSv4 enabled NAS servers only. Please ensure the order when providing the NFSv4 ACLs.\nTo use this feature, provide permissions in nfsAcls parameter in values.yaml, secrets or NFS storage class.\nFor example:\n POSIX mode bits  nfsAcls:\"0755\"NFSv4 ACLs  nfsAcls:\"A::OWNER@:rwatTnNcCy,A::GROUP@:rxtncy,A::EVERYONE@:rxtncy,A::user@domain.com:rxtncy\" Note: If no values are specified, default value of “0777” will be set. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.\n NVMe Support NVMeTCP Support CSI Driver for Dell Powerstore 2.2.0 and above supports NVMe/TCP provisioning. To enable NVMe/TCP provisioning, blockProtocol on secret should be specified as NVMeTCP.\n Note: NVMe/TCP is not supported on RHEL 7.x versions and CoreOS. NVMe/TCP is supported with Powerstore 2.1 and above.\n NVMeFC Support CSI Driver for Dell Powerstore 2.3.0 and above supports NVMe/FC provisioning. To enable NVMe/FC provisioning, blockProtocol on secret should be specified as NVMeFC.\n NVMe/FC is supported with Powerstore 3.0 and above.\n  NVMe-FC feature is supported with Helm.\n  Note: In case blockProtocol is specified as auto, the driver will be able to find the initiators on the host and choose the protocol accordingly. If the host has multiple protocols enabled, then NVMeFC gets the highest priority followed by NVMeTCP, followed by FC and then iSCSI.\n Volume group snapshot Support CSI Driver for Dell Powerstore 2.3.0 and above supports creating volume groups and take snapshot of them by making use of CRD (Custom Resource Definition). More information can be found here: Volume Group Snapshotter.\nConfigurable Volume Attributes (Optional) The CSI PowerStore driver version 2.3.0 and above supports Configurable volume atttributes.\nPowerStore array provides a set of optional volume creation attributes. These attributes can be configured for the volume (block and NFS) at the time of creation through PowerStore CSI driver. These attributes can be specified as labels in PVC yaml file. The following is a sample manifest for creating volume with some of the configurable volume attributes.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvc1namespace:defaultlabels:description:DB-volumeappliance_id:A1volume_group_id:f5f9dbbd-d12f-463e-becb-2e6d0a85405espec:accessModes:- ReadWriteOnceresources:requests:storage:8GistorageClassName:powerstore-ext4 Note: Default description value is pvcName-pvcNamespace.\n The following is the list of all the attribtues supported by PowerStore CSI driver:\n   Block Volume NFS Volume     description appliance_id volume_group_id protection_policy_id performance_policy_id app_type app_type_other  description config_type access_policy locking_policy folder_rename_policy is_async_mtime_enabled protection_policy_id file_events_publishing_mode host_io_size flr_attributes.flr_create.mode flr_attributes.flr_create.default_retention flr_attributes.flr_create.maximum_retention flr_attributes.flr_create.minimum_retention    Note:\n Refer to the PowerStore array specification for the allowed values for each attribute, at https://\u003carray-ip\u003e/swaggerui/. Make sure that the attributes specified are supported by the version of PowerStore array used.\n  Configurable Volume Attributes feature is supported with Helm.\n ","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/csm-docs/docs/csidriver/features/powerstore/","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell PowerStore using Helm or Dell CSI Operator.\nUpdate Driver from v2.2 to v2.3 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\n  Run git clone -b v2.3.0 https://github.com/dell/csi-powerstore.git to clone the git repository and get the driver.\n  Edit helm/config.yaml file and configure connection information for your PowerStore arrays changing the following parameters:\n endpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what transport protocol we should use (FC, ISCSI, NVMeTCP, None, or auto). nasName: defines what NAS should be used for NFS volumes. nfsAcls: (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.  Add more blocks similar to above for each PowerStore array if necessary.\n  (optional) create new storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n Storage classes created by v1.4/v2.0/v2.1 driver will not be deleted, v2.2 driver will use default array to manage volumes provisioned with old storage classes. Thus, if you still have volumes provisioned by v1.4/v2.0/v2.1 in your cluster then be sure to include the same array you have used for the v1.4/v2.0/v2.1 driver and make it default in the config.yaml file.\n   Create the secret by running kubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml\n  Copy the default values.yaml file cp ./helm/csi-powerstore/values.yaml ./dell-csi-helm-installer/my-powerstore-settings.yaml and update parameters as per the requirement.\n  Run the csi-install script with the option --upgrade by running: ./dell-csi-helm-installer/csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade.\n  Upgrade using Dell CSI Operator: Notes:\n  While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\n  Upgrading the Operator does not upgrade the CSI Driver.\n  Please upgrade the Dell CSI Operator by following here.\n  Once the operator is upgraded, to upgrade the driver, refer here.\n  ","excerpt":"You can upgrade the CSI Driver for Dell PowerStore using Helm or Dell …","ref":"/csm-docs/docs/csidriver/upgradation/drivers/powerstore/","title":"PowerStore"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command creates a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset are created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore\nThe pod must be Ready and Running\n If Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run, use the command:\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id in volumeHandle in format \u003cvolume-id/globalID/protocol\u003e in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:existingvolspec:accessModes:- ReadWriteOncecapacity:storage:30Gicsi:driver:csi-powerstore.dellemc.comvolumeHandle:0055558c-5ae1-4ed1-b421-6f5a9475c19f/unique/scsipersistentVolumeReclaimPolicy:RetainstorageClassName:powerstorevolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30GistorageClassName:powerstoreThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:quay.io/centos/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:pvolAfter the pod is Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerStore driver version 2.0.0 and higher supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class   Note: From v1.4, the CSI PowerStore driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the samples folder\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powerstore-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueSnapshot feature is optional for the installation CSI PowerStore driver version 1.4 makes the snapshot feature optional for the installation.\nTo enable or disable this feature, change values.snapshot.enable parameter to true or false, specify the following in values.yaml to enable this feature\nsnapshot:enable:trueExternal Snapshotter and its CRDs are not installed even if the Snapshot feature is enabled. These have to be installed manually before the installation.\nDisabling the Snapshot feature will opt out of the snapshotter sidecar from the installation.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP The CSI PowerStore driver Version 1.3.0 and later extends Challenge Handshake Authentication Protocol (CHAP) support by adding automatic credentials generation.\nThis means that you no longer need to provide chapsecret/chapuser credentials, they will be automatically generated by the driver for each host.\nTo enable this feature you need to set connection.enableCHAP to true when installing with helm or set X_CSI_POWERSTORE_ENABLE_CHAP to true in your PowerStore CustomResource when installing using operator.\nThe driver uses the generated chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating a new host on powerstore array driver will populate host chap credentials with generated values. When re-using already existing hosts driver must override existing CHAP credentials with newly generated ones.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-expand-scannotations:storageclass.kubernetes.io/is-default-class:falseprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pstore-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:powerstoretestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powerstoreresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvol0namespace:testpowerstorespec:storageClassName:powerstoreaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clonedpvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI PowerStore driver version 1.2 and later supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind:PodapiVersion:v1metadata:name:powerstore-inline-volumespec:containers:- name:test-containerimage:quay.io/centos/centoscommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"20Gi\"arrayID:\"unique\"This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"nfsAcls:\"0777\"Controller HA The CSI PowerStore driver version 1.2 and later introduces the controller HA feature. Instead of StatefulSet, controller pods are deployed as a Deployment.\nBy default number of replicas is set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in the cluster, each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods must be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As mentioned earlier, you can configure where node driver pods would be assigned in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they must use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology features user must create their own storage classes similar to those that can be found in samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-fcprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/127.0.0.1-fcvalues:- \"true\"This example matches all nodes where the driver has a connection to PowerStore with an IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS, iSCSI and NVMe.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels\n Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work.\n For any additional information about the topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as a Host on the storage array before. It will check if Host initiators and node initiators (FC, iSCSI or NVMe) match. If they do, the driver will not create a new host and will take the existing name of the Host as nodeID.\nMultiarray support The CSI PowerStore driver version 1.3.0 and later support managing multiple PowerStore arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration Create a file called config.yaml and populate it with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIglobalID:\"unique\"# global ID to identify arrayusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIskipCertificateValidation:true# use insecure connection or notdefault:true# treat current array as a default (would be used by storage classes without arrayIP parameter)blockProtocol:\"ISCSI\"# what transport protocol use on node side (FC, ISCSI, NVMeTCP, None, or auto)nasName:\"nas-server\"# what NAS must be used for NFS volumesnfsAcls:\"0777\"# (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory.# NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.- endpoint:\"https://10.0.0.2/api/rest\"globalID:\"unique\"username:\"user\"password:\"password\"skipCertificateValidation:trueblockProtocol:\"FC\"Here we specify that we want to CSI driver to manage two arrays: one with an IP 10.0.0.1 and the other with an IP 10.0.0.2, we want to connect to the first array with iSCSI protocol and with FC to the second array. Also, we want to be able to create NFS-based volume so we provide the name of the NAS to the first array.\nTo use this config we need to create a Kubernetes secret from it, to do so create a file called secret.yaml in the same folder and populate it with the following content:\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLApply the secret by running following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Creating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nCreate file storageclass.yaml and populate it with the following content:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-1provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayID:\"GlobalUniqueID\"FsType:\"ext4\"---apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-2provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayID:\"GlobalUniqueID\"FsType:\"xfs\"Here we specify two storage classes: one of them uses the first array and ext4 filesystem, and the other uses the second array and xfs filesystem.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use powerstore-1 storage class to create volumes on the first array and powerstore-2 storage class to create volumes on the second array.\nDynamic secret change detection CSI PowerStore driver version 1.3.0 and later supports the ability to detect changes to array configuration Kubernetes secret. This essentially means that you can change credentials for your PowerStore arrays in-flight (without restarting the driver).\nTo do so just change your configuration file config.yaml and apply it again using the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - After Kubernetes remounts secret to driver containers (this usually takes around one minute), a driver must detect the change and start using this new configuration information.\nConfiguring custom access to NFS exports CSI PowerStore driver Version 1.3.0 and later supports the ability to configure NFS access to nodes that use dedicated storage networks.\nTo enable this feature you need to specify externalAccess parameter in your helm values.yaml file or X_CSI_POWERSTORE_EXTERNAL_ACCESS variable when creating CustomResource using an operator.\nThe value of that parameter is added as an additional entry to NFS Export host access.\nFor example the following notation:\nexternalAccess:\"10.0.0.0/24\"This means that we allow for NFS Export created by driver to be consumed by address range 10.0.0.0-10.0.0.255.\nArray identification based on GlobalID CSI PowerStore driver version 1.4.0 onwards slightly changes the way arrays are being identified in runtime. In previous versions of the driver, a management IP address was used to identify an array. The address change could lead to an invalid state of PV. From version 1.4.0 a unique GlobalID string is used for an array identification. It has to be specified in config.yaml and in Storage Classes.\nThe change provides backward compatibility with previously created PVs. However, to provision new volumes, make sure to delete old Storage Classes and create new ones with arrayID instead of arrayIP specified.\n NOTE: It is recommended to migrate the PVs to new identifiers before changing management IPs of storage systems. The recommended way to do it is to clone the existing volume and delete the old one. The cloned volume will automatically switch to using globalID instead of management IP.\n Root squashing CSI PowerStore driver version 1.4.0 and later allows users to enable root squashing for NFS volumes provisioned by the driver.\nRoot squashing rule prevents root users on NFS clients from exercising root privileges on the NFS server.\nTo enable this rule, you need to set parameter allowRoot to false in your NFS storage class.\nYour storage class definition must look similar to this:\napiVersion:storage.k8s.io/v1kind:StorageClass...parameters:...allowRoot:\"false\"# enables or disables root squashing The 1.4 version and later of the driver also enables any container user, to have full access to provisioned NFS volume, in earlier versions only root user had access\n Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params NAT Support CSI Driver for Dell Powerstore is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nPV/PVC Metrics CSI Driver for Dell Powerstore 2.1.0 and above supports volume health monitoring. To enable Volume Health Monitoring from the node side, the alpha feature gate CSIVolumeHealth needs to be enabled. To use this feature, set controller.healthMonitor.enabled and node.healthMonitor.enabled to true. To change the monitor interval, set controller.healthMonitor.volumeHealthMonitorInterval parameter.\nSingle Pod Access Mode for PersistentVolumes Starting from version 2.1, CSI Driver for Powerstore now supports a new access mode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Powerstore allows restricting volume access to a single pod in the cluster and within a worker node.\nPrerequisites\n  Enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is supported only for CSI volumes. You can enable the feature by setting command-line argument: --feature-gates=\"...,ReadWriteOncePod=true\"\n  Create a PVC with access mode set to ReadWriteOncePod like shown in the sample below\n  kind:PersistentVolumeClaimapiVersion:v1metadata:name:single-node-single-writerspec:accessModes:- ReadWriteOncePod# Allow only a single pod to access single-node-single-writerresources:requests:storage:5Gi Note: The access mode ReadWriteOnce allows multiple pods to access a single volume within a single worker node and the behavior is consistent across all supported Kubernetes versions.\n POSIX mode bits and NFSv4 ACLs CSI PowerStore driver version 2.2.0 and later allows users to set user-defined permissions on NFS target mount directory using POSIX mode bits or NFSv4 ACLs.\nNFSv4 ACLs are supported for NFSv4 shares on NFSv4 enabled NAS servers only. Please ensure the order when providing the NFSv4 ACLs.\nTo use this feature, provide permissions in nfsAcls parameter in values.yaml, secrets or NFS storage class.\nFor example:\n POSIX mode bits  nfsAcls:\"0755\"NFSv4 ACLs  nfsAcls:\"A::OWNER@:rwatTnNcCy,A::GROUP@:rxtncy,A::EVERYONE@:rxtncy,A::user@domain.com:rxtncy\" Note: If no values are specified, default value of “0777” will be set. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.\n NVMe/TCP Support CSI Driver for Dell Powerstore 2.2.0 and above supports NVMe/TCP provisioning. To enable NVMe/TCP provisioning, blockProtocol on secret should be specified as NVMeTCP. In case blockProtocol is specified as auto, the driver will be able to find the initiators on the host and choose the protocol accordingly. If the host has multiple protocols enabled, then FC gets the highest priority followed by iSCSI and then NVMeTCP.\n Note: NVMe/TCP is not supported on RHEL 7.x versions and CoreOS. NVMe/TCP is supported with Powerstore 2.1 and above.\n ","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/csm-docs/v1/csidriver/features/powerstore/","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell PowerStore using Helm or Dell CSI Operator.\nUpdate Driver from v2.1 to v2.2 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\n  Run git clone -b v2.2.0 https://github.com/dell/csi-powerstore.git to clone the git repository and get the driver.\n  Edit helm/config.yaml file and configure connection information for your PowerStore arrays changing the following parameters:\n endpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what transport protocol we should use (FC, ISCSI, NVMeTCP, None, or auto). nasName: defines what NAS should be used for NFS volumes. nfsAcls: (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.  Add more blocks similar to above for each PowerStore array if necessary.\n  (optional) create new storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n Storage classes created by v1.4/v2.0/v2.1 driver will not be deleted, v2.2 driver will use default array to manage volumes provisioned with old storage classes. Thus, if you still have volumes provisioned by v1.4/v2.0/v2.1 in your cluster then be sure to include the same array you have used for the v1.4/v2.0/v2.1 driver and make it default in the config.yaml file.\n   Create the secret by running kubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml\n  Copy the default values.yaml file cp ./helm/csi-powerstore/values.yaml ./dell-csi-helm-installer/my-powerstore-settings.yaml and update parameters as per the requirement.\n  Run the csi-install script with the option --upgrade by running: ./dell-csi-helm-installer/csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade.\n  Upgrade using Dell CSI Operator: Notes:\n  While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\n  Upgrading the Operator does not upgrade the CSI Driver.\n  Please upgrade the Dell CSI Operator by following here.\n  Once the operator is upgraded, to upgrade the driver, refer here.\n  ","excerpt":"You can upgrade the CSI Driver for Dell PowerStore using Helm or Dell …","ref":"/csm-docs/v1/csidriver/upgradation/drivers/powerstore/","title":"PowerStore"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command creates a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset are created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore\nThe pod must be Ready and Running\n If Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run, use the command:\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id in volumeHandle in format \u003cvolume-id/globalID/protocol\u003e in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:existingvolspec:accessModes:- ReadWriteOncecapacity:storage:30Gicsi:driver:csi-powerstore.dellemc.comvolumeHandle:0055558c-5ae1-4ed1-b421-6f5a9475c19f/unique/scsipersistentVolumeReclaimPolicy:RetainstorageClassName:powerstorevolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30GistorageClassName:powerstoreThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:quay.io/centos/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:pvolAfter the pod is Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerStore driver version 2.0.0 and higher supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class   Note: From v1.4, the CSI PowerStore driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the samples folder\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powerstore-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueSnapshot feature is optional for the installation CSI PowerStore driver version 1.4 makes the snapshot feature optional for the installation.\nTo enable or disable this feature, change values.snapshot.enable parameter to true or false, specify the following in values.yaml to enable this feature\nsnapshot:enable:trueExternal Snapshotter and its CRDs are not installed even if the Snapshot feature is enabled. These have to be installed manually before the installation.\nDisabling the Snapshot feature will opt out of the snapshotter sidecar from the installation.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP The CSI PowerStore driver Version 1.3.0 and later extends Challenge Handshake Authentication Protocol (CHAP) support by adding automatic credentials generation.\nThis means that you no longer need to provide chapsecret/chapuser credentials, they will be automatically generated by the driver for each host.\nTo enable this feature you need to set connection.enableCHAP to true when installing with helm or set X_CSI_POWERSTORE_ENABLE_CHAP to true in your PowerStore CustomResource when installing using operator.\nThe driver uses the generated chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating a new host on powerstore array driver will populate host chap credentials with generated values. When re-using already existing hosts driver must override existing CHAP credentials with newly generated ones.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-expand-scannotations:storageclass.kubernetes.io/is-default-class:falseprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pstore-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:powerstoretestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powerstoreresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvol0namespace:testpowerstorespec:storageClassName:powerstoreaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clonedpvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI PowerStore driver version 1.2 and later supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind:PodapiVersion:v1metadata:name:powerstore-inline-volumespec:containers:- name:test-containerimage:quay.io/centos/centoscommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"20Gi\"arrayID:\"unique\"This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"nfsAcls:\"0777\"Controller HA The CSI PowerStore driver version 1.2 and later introduces the controller HA feature. Instead of StatefulSet, controller pods are deployed as a Deployment.\nBy default number of replicas is set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in the cluster, each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods must be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As mentioned earlier, you can configure where node driver pods would be assigned in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they must use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology features user must create their own storage classes similar to those that can be found in samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-fcprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/127.0.0.1-fcvalues:- \"true\"This example matches all nodes where the driver has a connection to PowerStore with an IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS, iSCSI and NVMe.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels\n Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work.\n For any additional information about the topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as a Host on the storage array before. It will check if Host initiators and node initiators (FC, iSCSI or NVMe) match. If they do, the driver will not create a new host and will take the existing name of the Host as nodeID.\nMultiarray support The CSI PowerStore driver version 1.3.0 and later support managing multiple PowerStore arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration Create a file called config.yaml and populate it with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIglobalID:\"unique\"# global ID to identify arrayusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIskipCertificateValidation:true# use insecure connection or notdefault:true# treat current array as a default (would be used by storage classes without arrayIP parameter)blockProtocol:\"ISCSI\"# what transport protocol use on node side (FC, ISCSI, NVMeTCP, None, or auto)nasName:\"nas-server\"# what NAS must be used for NFS volumesnfsAcls:\"0777\"# (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory.# NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.- endpoint:\"https://10.0.0.2/api/rest\"globalID:\"unique\"username:\"user\"password:\"password\"skipCertificateValidation:trueblockProtocol:\"FC\"Here we specify that we want to CSI driver to manage two arrays: one with an IP 10.0.0.1 and the other with an IP 10.0.0.2, we want to connect to the first array with iSCSI protocol and with FC to the second array. Also, we want to be able to create NFS-based volume so we provide the name of the NAS to the first array.\nTo use this config we need to create a Kubernetes secret from it, to do so create a file called secret.yaml in the same folder and populate it with the following content:\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLApply the secret by running following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Creating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nCreate file storageclass.yaml and populate it with the following content:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-1provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayID:\"GlobalUniqueID\"FsType:\"ext4\"---apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-2provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayID:\"GlobalUniqueID\"FsType:\"xfs\"Here we specify two storage classes: one of them uses the first array and ext4 filesystem, and the other uses the second array and xfs filesystem.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use powerstore-1 storage class to create volumes on the first array and powerstore-2 storage class to create volumes on the second array.\nDynamic secret change detection CSI PowerStore driver version 1.3.0 and later supports the ability to detect changes to array configuration Kubernetes secret. This essentially means that you can change credentials for your PowerStore arrays in-flight (without restarting the driver).\nTo do so just change your configuration file config.yaml and apply it again using the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - After Kubernetes remounts secret to driver containers (this usually takes around one minute), a driver must detect the change and start using this new configuration information.\nConfiguring custom access to NFS exports CSI PowerStore driver Version 1.3.0 and later supports the ability to configure NFS access to nodes that use dedicated storage networks.\nTo enable this feature you need to specify externalAccess parameter in your helm values.yaml file or X_CSI_POWERSTORE_EXTERNAL_ACCESS variable when creating CustomResource using an operator.\nThe value of that parameter is added as an additional entry to NFS Export host access.\nFor example the following notation:\nexternalAccess:\"10.0.0.0/24\"This means that we allow for NFS Export created by driver to be consumed by address range 10.0.0.0-10.0.0.255.\nArray identification based on GlobalID CSI PowerStore driver version 1.4.0 onwards slightly changes the way arrays are being identified in runtime. In previous versions of the driver, a management IP address was used to identify an array. The address change could lead to an invalid state of PV. From version 1.4.0 a unique GlobalID string is used for an array identification. It has to be specified in config.yaml and in Storage Classes.\nThe change provides backward compatibility with previously created PVs. However, to provision new volumes, make sure to delete old Storage Classes and create new ones with arrayID instead of arrayIP specified.\n NOTE: It is recommended to migrate the PVs to new identifiers before changing management IPs of storage systems. The recommended way to do it is to clone the existing volume and delete the old one. The cloned volume will automatically switch to using globalID instead of management IP.\n Root squashing CSI PowerStore driver version 1.4.0 and later allows users to enable root squashing for NFS volumes provisioned by the driver.\nRoot squashing rule prevents root users on NFS clients from exercising root privileges on the NFS server.\nTo enable this rule, you need to set parameter allowRoot to false in your NFS storage class.\nYour storage class definition must look similar to this:\napiVersion:storage.k8s.io/v1kind:StorageClass...parameters:...allowRoot:\"false\"# enables or disables root squashing The 1.4 version and later of the driver also enables any container user, to have full access to provisioned NFS volume, in earlier versions only root user had access\n Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params NAT Support CSI Driver for Dell Powerstore is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nPV/PVC Metrics CSI Driver for Dell Powerstore 2.1.0 and above supports volume health monitoring. To enable Volume Health Monitoring from the node side, the alpha feature gate CSIVolumeHealth needs to be enabled. To use this feature, set controller.healthMonitor.enabled and node.healthMonitor.enabled to true. To change the monitor interval, set controller.healthMonitor.volumeHealthMonitorInterval parameter.\nSingle Pod Access Mode for PersistentVolumes Starting from version 2.1, CSI Driver for Powerstore now supports a new access mode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Powerstore allows restricting volume access to a single pod in the cluster and within a worker node.\nPrerequisites\n  Enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is supported only for CSI volumes. You can enable the feature by setting command-line argument: --feature-gates=\"...,ReadWriteOncePod=true\"\n  Create a PVC with access mode set to ReadWriteOncePod like shown in the sample below\n  kind:PersistentVolumeClaimapiVersion:v1metadata:name:single-node-single-writerspec:accessModes:- ReadWriteOncePod# Allow only a single pod to access single-node-single-writerresources:requests:storage:5Gi Note: The access mode ReadWriteOnce allows multiple pods to access a single volume within a single worker node and the behavior is consistent across all supported Kubernetes versions.\n POSIX mode bits and NFSv4 ACLs CSI PowerStore driver version 2.2.0 and later allows users to set user-defined permissions on NFS target mount directory using POSIX mode bits or NFSv4 ACLs.\nNFSv4 ACLs are supported for NFSv4 shares on NFSv4 enabled NAS servers only. Please ensure the order when providing the NFSv4 ACLs.\nTo use this feature, provide permissions in nfsAcls parameter in values.yaml, secrets or NFS storage class.\nFor example:\n POSIX mode bits  nfsAcls:\"0755\"NFSv4 ACLs  nfsAcls:\"A::OWNER@:rwatTnNcCy,A::GROUP@:rxtncy,A::EVERYONE@:rxtncy,A::user@domain.com:rxtncy\" Note: If no values are specified, default value of “0777” will be set. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.\n NVMe/TCP Support CSI Driver for Dell Powerstore 2.2.0 and above supports NVMe/TCP provisioning. To enable NVMe/TCP provisioning, blockProtocol on secret should be specified as NVMeTCP. In case blockProtocol is specified as auto, the driver will be able to find the initiators on the host and choose the protocol accordingly. If the host has multiple protocols enabled, then FC gets the highest priority followed by iSCSI and then NVMeTCP.\n Note: NVMe/TCP is not supported on RHEL 7.x versions and CoreOS. NVMe/TCP is supported with Powerstore 2.1 and above.\n ","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/csm-docs/v2/csidriver/features/powerstore/","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell PowerStore using Helm or Dell CSI Operator.\nUpdate Driver from v2.1 to v2.2 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\n  Run git clone -b v2.2.0 https://github.com/dell/csi-powerstore.git to clone the git repository and get the driver.\n  Edit helm/config.yaml file and configure connection information for your PowerStore arrays changing the following parameters:\n endpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what transport protocol we should use (FC, ISCSI, NVMeTCP, None, or auto). nasName: defines what NAS should be used for NFS volumes. nfsAcls: (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.  Add more blocks similar to above for each PowerStore array if necessary.\n  (optional) create new storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n Storage classes created by v1.4/v2.0/v2.1 driver will not be deleted, v2.2 driver will use default array to manage volumes provisioned with old storage classes. Thus, if you still have volumes provisioned by v1.4/v2.0/v2.1 in your cluster then be sure to include the same array you have used for the v1.4/v2.0/v2.1 driver and make it default in the config.yaml file.\n   Create the secret by running kubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml\n  Copy the default values.yaml file cp ./helm/csi-powerstore/values.yaml ./dell-csi-helm-installer/my-powerstore-settings.yaml and update parameters as per the requirement.\n  Run the csi-install script with the option --upgrade by running: ./dell-csi-helm-installer/csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade.\n  Upgrade using Dell CSI Operator: Notes:\n  While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\n  Upgrading the Operator does not upgrade the CSI Driver.\n  Please upgrade the Dell CSI Operator by following here.\n  Once the operator is upgraded, to upgrade the driver, refer here.\n  ","excerpt":"You can upgrade the CSI Driver for Dell PowerStore using Helm or Dell …","ref":"/csm-docs/v2/csidriver/upgradation/drivers/powerstore/","title":"PowerStore"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command creates a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset are created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset’s pods by running kubectl get pods -n testpowerstore\nThe pod must be Ready and Running\n If Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run, use the command:\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u003cpowerstore.api.ip\u003e/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id in volumeHandle in format \u003cvolume-id/globalID/protocol\u003e in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:existingvolspec:accessModes:- ReadWriteOncecapacity:storage:30Gicsi:driver:csi-powerstore.dellemc.comvolumeHandle:0055558c-5ae1-4ed1-b421-6f5a9475c19f/unique/scsipersistentVolumeReclaimPolicy:RetainstorageClassName:powerstorevolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30GistorageClassName:powerstoreThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:quay.io/centos/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:pvolAfter the pod is Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerStore driver version 2.0.0 and higher supports v1 snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class   Note: From v1.4, the CSI PowerStore driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the samples folder\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powerstore-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueSnapshot feature is optional for the installation CSI PowerStore driver version 1.4 makes the snapshot feature optional for the installation.\nTo enable or disable this feature, change values.snapshot.enable parameter to true or false, specify the following in values.yaml to enable this feature\nsnapshot:enable:trueExternal Snapshotter and its CRDs are not installed even if the Snapshot feature is enabled. These have to be installed manually before the installation.\nDisabling the Snapshot feature will opt out of the snapshotter sidecar from the installation.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP The CSI PowerStore driver Version 1.3.0 and later extends Challenge Handshake Authentication Protocol (CHAP) support by adding automatic credentials generation.\nThis means that you no longer need to provide chapsecret/chapuser credentials, they will be automatically generated by the driver for each host.\nTo enable this feature you need to set connection.enableCHAP to true when installing with helm or set X_CSI_POWERSTORE_ENABLE_CHAP to true in your PowerStore CustomResource when installing using operator.\nThe driver uses the generated chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating a new host on powerstore array driver will populate host chap credentials with generated values. When re-using already existing hosts driver must override existing CHAP credentials with newly generated ones.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pstore-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:powerstoretestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\"/dev/data0\"name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powerstoreresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you must first have an existing pvc, for example, pvol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvol0namespace:testpowerstorespec:storageClassName:powerstoreaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clonedpvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI PowerStore driver version 1.2 and later supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind:PodapiVersion:v1metadata:name:powerstore-inline-volumespec:containers:- name:test-containerimage:quay.io/centos/centoscommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"20Gi\"arrayID:\"unique\"This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI PowerStore driver version 1.2 and later introduces the controller HA feature. Instead of StatefulSet, controller pods are deployed as a Deployment.\nBy default number of replicas is set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in the cluster, each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods must be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As mentioned earlier, you can configure where node driver pods would be assigned in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they must use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology features user must create their own storage classes similar to those that can be found in samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-fcprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/127.0.0.1-fcvalues:- \"true\"This example matches all nodes where the driver has a connection to PowerStore with an IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS and iSCSI.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels\n Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work.\n For any additional information about the topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as a Host on the storage array before. It will check if Host initiators and node initiators (FC or iSCSI) match. If they do, the driver will not create a new host and will take the existing name of the Host as nodeID.\nMultiarray support The CSI PowerStore driver version 1.3.0 and later support managing multiple PowerStore arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration Create a file called config.yaml and populate it with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIglobalID:\"unique\"# global ID to identify arrayusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIskipCertificateValidation:true# use insecure connection or notdefault:true# treat current array as a default (would be used by storage classes without arrayIP parameter)blockProtocol:\"ISCSI\"# what SCSI transport protocol use on node side (FC, ISCSI, None, or auto)nasName:\"nas-server\"# what NAS must be used for NFS volumes- endpoint:\"https://10.0.0.2/api/rest\"globalID:\"unique\"username:\"user\"password:\"password\"skipCertificateValidation:trueblockProtocol:\"FC\"Here we specify that we want to CSI driver to manage two arrays: one with an IP 10.0.0.1 and the other with an IP 10.0.0.2, we want to connect to the first array with iSCSI protocol and with FC to the second array. Also, we want to be able to create NFS-based volume so we provide the name of the NAS to the first array.\nTo use this config we need to create a Kubernetes secret from it, to do so create a file called secret.yaml in the same folder and populate it with the following content:\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLApply the secret by running following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - Creating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nCreate file storageclass.yaml and populate it with the following content:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-1provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayID:\"GlobalUniqueID\"FsType:\"ext4\"---apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-2provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayID:\"GlobalUniqueID\"FsType:\"xfs\"Here we specify two storage classes: one of them uses the first array and ext4 filesystem, and the other uses the second array and xfs filesystem.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use powerstore-1 storage class to create volumes on the first array and powerstore-2 storage class to create volumes on the second array.\nDynamic secret change detection CSI PowerStore driver version 1.3.0 and later supports the ability to detect changes to array configuration Kubernetes secret. This essentially means that you can change credentials for your PowerStore arrays in-flight (without restarting the driver).\nTo do so just change your configuration file config.yaml and apply it again using the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f - After Kubernetes remounts secret to driver containers (this usually takes around one minute), a driver must detect the change and start using this new configuration information.\nConfiguring custom access to NFS exports CSI PowerStore driver Version 1.3.0 and later supports the ability to configure NFS access to nodes that use dedicated storage networks.\nTo enable this feature you need to specify externalAccess parameter in your helm values.yaml file or X_CSI_POWERSTORE_EXTERNAL_ACCESS variable when creating CustomResource using an operator.\nThe value of that parameter is added as an additional entry to NFS Export host access.\nFor example the following notation:\nexternalAccess:\"10.0.0.0/24\"This means that we allow for NFS Export created by driver to be consumed by address range 10.0.0.0-10.0.0.255.\nArray identification based on GlobalID CSI PowerStore driver version 1.4.0 onwards slightly changes the way arrays are being identified in runtime. In previous versions of the driver, a management IP address was used to identify an array. The address change could lead to an invalid state of PV. From version 1.4.0 a unique GlobalID string is used for an array identification. It has to be specified in config.yaml and in Storage Classes.\nThe change provides backward compatibility with previously created PVs. However, to provision new volumes, make sure to delete old Storage Classes and create new ones with arrayID instead of arrayIP specified.\n NOTE: It is recommended to migrate the PVs to new identifiers before changing management IPs of storage systems. The recommended way to do it is to clone the existing volume and delete the old one. The cloned volume will automatically switch to using globalID instead of management IP.\n Root squashing CSI PowerStore driver version 1.4.0 and later allows users to enable root squashing for NFS volumes provisioned by the driver.\nRoot squashing rule prevents root users on NFS clients from exercising root privileges on the NFS server.\nTo enable this rule, you need to set parameter allowRoot to false in your NFS storage class.\nYour storage class definition must look similar to this:\napiVersion:storage.k8s.io/v1kind:StorageClass...parameters:...allowRoot:\"false\"# enables or disables root squashing The 1.4 version and later of the driver also enables any container user, to have full access to provisioned NFS volume, in earlier versions only root user had access\n Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params NAT Support CSI Driver for Dell EMC Powerstore is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nPV/PVC Metrics CSI Driver for Dell EMC Powerstore 2.1.0 and above supports volume health monitoring. To enable Volume Health Monitoring from the node side, the alpha feature gate CSIVolumeHealth needs to be enabled. To use this feature, set controller.healthMonitor.enabled and node.healthMonitor.enabled to true. To change the monitor interval, set controller.healthMonitor.volumeHealthMonitorInterval parameter.\nSingle Pod Access Mode for PersistentVolumes Starting from version 2.1, CSI Driver for Powerstore now supports a new access mode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Powerstore allows restricting volume access to a single pod in the cluster and within a worker node.\nPrerequisites\n  Enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is supported only for CSI volumes. You can enable the feature by setting command-line argument: --feature-gates=\"...,ReadWriteOncePod=true\"\n  Create a PVC with access mode set to ReadWriteOncePod like shown in the sample below\n  kind:PersistentVolumeClaimapiVersion:v1metadata:name:single-node-single-writerspec:accessModes:- ReadWriteOncePod# Allow only a single pod to access single-node-single-writerresources:requests:storage:5Gi Note: The access mode ReadWriteOnce allows multiple pods to access a single volume within a single worker node and the behavior is consistent across all supported Kubernetes versions.\n ","excerpt":"Creating volumes and consuming them Create a file simple.yaml using …","ref":"/csm-docs/v3/csidriver/features/powerstore/","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or Dell CSI Operator.\nUpdate Driver from v2.0 to v2.1 using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nSteps\n  Run git clone -b v2.1.0 https://github.com/dell/csi-powerstore.git to clone the git repository and get the driver.\n  Edit helm/config.yaml file and configure connection information for your PowerStore arrays changing the following parameters:\n endpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what SCSI transport protocol we should use (FC, ISCSI, None, or auto). nasName: defines what NAS should be used for NFS volumes.  Add more blocks similar to above for each PowerStore array if necessary.\n  (optional) create new storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n Storage classes created by v1.4/v2.0 driver will not be deleted, v2.1 driver will use default array to manage volumes provisioned with old storage classes. Thus, if you still have volumes provisioned by v1.4/v2.0 in your cluster then be sure to include the same array you have used for the v1.4/v2.0 driver and make it default in the config.yaml file.\n   Create the secret by running kubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml\n  Copy the default values.yaml file cp ./helm/csi-powerstore/values.yaml ./dell-csi-helm-installer/my-powerstore-settings.yaml and update parameters as per the requirement.\n  Run the csi-install script with the option --upgrade by running: ./dell-csi-helm-installer/csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade.\n  Upgrade using Dell CSI Operator: Note: While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\n  Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.5.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or …","ref":"/csm-docs/v3/csidriver/upgradation/drivers/powerstore/","title":"PowerStore"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerStore. The Grafana reference dashboards for PowerStore metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the karaviMetricsPowerstore.volumeMetricsEnabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerstore_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s)   powerstore_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s)   powerstore_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume   powerstore_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume   powerstore_volume_read_iops_per_second The number of read operations performed against a volume (per second)   powerstore_volume_write_iops_per_second The number of write operations performed against a volume (per second)   powerstore_filesystem_read_bw_megabytes_per_second The filesystem read bandwidth MB/s   powerstore_filesystem_write_bw_megabytes_per_second The filesystem write bandwidth (MB/s)   powerstore_filesystem_read_iops_per_second The number of read operations performed against a filesystem (per second)   powerstore_filesystem_write_iops_per_second The number of write operations performed against a filesystem (per second)   powerstore_filesystem_read_latency_milliseconds The time (in ms) to complete read operations to a filesystem   powerstore_filesystem_write_latency_milliseconds The time (in ms) to complete write operations to a filesystem    Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the enable_powerstore_metrics field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerstore_array_logical_provisioned_megabytes Total provisioned logical storage on a given array managed by CSI driver   powerstore_array_logical_used_megabytes Total used logical storage on a given array   powerstore_storage_class_logical_provisioned_megabytes Total provisioned logical storage for a given storage class   powerstore_storage_class_logical_used_megabytes Total used logical storage for a given storage class   powerstore_volume_logical_provisioned_megabytes Logical provisioned storage for a volume   powerstore_volume_logical_used_megabytes Logical used storage for a volume   powerstore_filesystem_logical_provisioned_megabytes Logical provisioned storage for a filesystem   powerstore_filesystem_logical_used_megabytes Logical used storage for a filesystem    ","excerpt":"This section outlines the metrics collected by the Container Storage …","ref":"/csm-docs/docs/observability/metrics/powerstore/","title":"PowerStore Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerStore. The Grafana reference dashboards for PowerStore metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the karaviMetricsPowerstore.volumeMetricsEnabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerstore_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s)   powerstore_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s)   powerstore_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume   powerstore_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume   powerstore_volume_read_iops_per_second The number of read operations performed against a volume (per second)   powerstore_volume_write_iops_per_second The number of write operations performed against a volume (per second)   powerstore_filesystem_read_bw_megabytes_per_second The filesystem read bandwidth MB/s   powerstore_filesystem_write_bw_megabytes_per_second The filesystem write bandwidth (MB/s)   powerstore_filesystem_read_iops_per_second The number of read operations performed against a filesystem (per second)   powerstore_filesystem_write_iops_per_second The number of write operations performed against a filesystem (per second)   powerstore_filesystem_read_latency_milliseconds The time (in ms) to complete read operations to a filesystem   powerstore_filesystem_write_latency_milliseconds The time (in ms) to complete write operations to a filesystem    Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the enable_powerstore_metrics field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerstore_array_logical_provisioned_megabytes Total provisioned logical storage on a given array managed by CSI driver   powerstore_array_logical_used_megabytes Total used logical storage on a given array   powerstore_storage_class_logical_provisioned_megabytes Total provisioned logical storage for a given storage class   powerstore_storage_class_logical_used_megabytes Total used logical storage for a given storage class   powerstore_volume_logical_provisioned_megabytes Logical provisioned storage for a volume   powerstore_volume_logical_used_megabytes Logical used storage for a volume   powerstore_filesystem_logical_provisioned_megabytes Logical provisioned storage for a filesystem   powerstore_filesystem_logical_used_megabytes Logical used storage for a filesystem    ","excerpt":"This section outlines the metrics collected by the Container Storage …","ref":"/csm-docs/v1/observability/metrics/powerstore/","title":"PowerStore Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerStore. The Grafana reference dashboards for PowerStore metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the karaviMetricsPowerstore.volumeMetricsEnabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerstore_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s)   powerstore_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s)   powerstore_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume   powerstore_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume   powerstore_volume_read_iops_per_second The number of read operations performed against a volume (per second)   powerstore_volume_write_iops_per_second The number of write operations performed against a volume (per second)   powerstore_filesystem_read_bw_megabytes_per_second The filesystem read bandwidth MB/s   powerstore_filesystem_write_bw_megabytes_per_second The filesystem write bandwidth (MB/s)   powerstore_filesystem_read_iops_per_second The number of read operations performed against a filesystem (per second)   powerstore_filesystem_write_iops_per_second The number of write operations performed against a filesystem (per second)   powerstore_filesystem_read_latency_milliseconds The time (in ms) to complete read operations to a filesystem   powerstore_filesystem_write_latency_milliseconds The time (in ms) to complete write operations to a filesystem    Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the enable_powerstore_metrics field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerstore_array_logical_provisioned_megabytes Total provisioned logical storage on a given array managed by CSI driver   powerstore_array_logical_used_megabytes Total used logical storage on a given array   powerstore_storage_class_logical_provisioned_megabytes Total provisioned logical storage for a given storage class   powerstore_storage_class_logical_used_megabytes Total used logical storage for a given storage class   powerstore_volume_logical_provisioned_megabytes Logical provisioned storage for a volume   powerstore_volume_logical_used_megabytes Logical used storage for a volume   powerstore_filesystem_logical_provisioned_megabytes Logical provisioned storage for a filesystem   powerstore_filesystem_logical_used_megabytes Logical used storage for a filesystem    ","excerpt":"This section outlines the metrics collected by the Container Storage …","ref":"/csm-docs/v2/observability/metrics/powerstore/","title":"PowerStore Metrics"},{"body":"This section outlines the metrics collected by the Container Storage Modules (CSM) Observability module for PowerStore. The Grafana reference dashboards for PowerStore metrics can be uploaded to your Grafana instance.\nI/O Performance Metrics Storage system I/O performance metrics (IOPS, bandwidth, latency) are available by default and broken down by export node and volume.\nTo disable these metrics, set the karaviMetricsPowerstore.volumeMetricsEnabled field to false in helm/values.yaml.\nThe following I/O performance metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerstore_volume_read_bw_megabytes_per_second The volume read bandwidth (MB/s)   powerstore_volume_write_bw_megabytes_per_second The volume write bandwidth (MB/s)   powerstore_volume_read_latency_milliseconds The time (in ms) to complete read operations to a volume   powerstore_volume_write_latency_milliseconds The time (in ms) to complete write operations to a volume   powerstore_volume_read_iops_per_second The number of read operations performed against a volume (per second)   powerstore_volume_write_iops_per_second The number of write operations performed against a volume (per second)   powerstore_filesystem_read_bw_megabytes_per_second The filesystem read bandwidth MB/s   powerstore_filesystem_write_bw_megabytes_per_second The filesystem write bandwidth (MB/s)   powerstore_filesystem_read_iops_per_second The number of read operations performed against a filesystem (per second)   powerstore_filesystem_write_iops_per_second The number of write operations performed against a filesystem (per second)   powerstore_filesystem_read_latency_milliseconds The time (in ms) to complete read operations to a filesystem   powerstore_filesystem_write_latency_milliseconds The time (in ms) to complete write operations to a filesystem    Storage Capacity Metrics Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.\nTo disable these metrics, set the enable_powerstore_metrics field to false in helm/values.yaml.\nThe following storage capacity metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n   Metric Description     powerstore_array_logical_provisioned_megabytes Total provisioned logical storage on a given array managed by CSI driver   powerstore_array_logical_used_megabytes Total used logical storage on a given array   powerstore_storage_class_logical_provisioned_megabytes Total provisioned logical storage for a given storage class   powerstore_storage_class_logical_used_megabytes Total used logical storage for a given storage class   powerstore_volume_logical_provisioned_megabytes Logical provisioned storage for a volume   powerstore_volume_logical_used_megabytes Logical used storage for a volume   powerstore_filesystem_logical_provisioned_megabytes Logical provisioned storage for a filesystem   powerstore_filesystem_logical_used_megabytes Logical used storage for a filesystem    ","excerpt":"This section outlines the metrics collected by the Container Storage …","ref":"/csm-docs/v3/observability/metrics/powerstore/","title":"PowerStore Metrics"},{"body":"Release Notes - CSM Resiliency 1.2.0 New Features/Changes  Support for node taint when driver pod is unhealthy. Resiliency protection on driver node pods, see CSI node failure protection. Resiliency support for CSI Driver for PowerScale, see CSI Driver for PowerScale.  Fixed Issues  Occasional failure unmounting Unity volume for raw block devices via iSCSI, see unmounting Unity volume.  Known Issues ","excerpt":"Release Notes - CSM Resiliency 1.2.0 New Features/Changes  Support for …","ref":"/csm-docs/docs/resiliency/release/","title":"Release notes"},{"body":"Creating volumes and consuming them Create a file sample.yaml using sample yaml files located at test/sample.yaml\nThe following command creates a statefulset that consumes three volumes of default storage classes:\nkubectl create -f test/sample.yaml After executing this command 3 PVC and statefulset are created in the test-unity namespace. You can check created PVCs by running kubectl get pvc -n test-unity and check statefulset’s pods by running kubectl get pods -n test-unity command. The pod should be Ready and Running.\n If Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f test/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\n Open your volume in Unity Management UI (Unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:static-1annotations:pv.kubernetes.io/provisioned-by:csi-unity.dellemc.comspec:accessModes:- ReadWriteOncecapacity:storage:5Gicsi:driver:csi-unity.dellemc.comvolumeHandle:existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: RetainclaimRef:namespace:defaultname:static-pvc1storageClassName:unityvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:static-pvc1spec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:static-1storageClassName:unityThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:static-pvc1After the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of the CSI Unity 2.0 driver and higher, a Volume Snapshot Class is not created and need to create Volume Snapshot Class.\nFollowing is the manifest to create Volume Snapshot Class :\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:unity-snapclassdriver:csi-unity.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:test-unityspec:volumeSnapshotClassName:unity-snapclasssource:persistentVolumeClaimName:pvolOnce the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueNote : For CSI Driver for Unity version 1.6 and later, dell-csi-helm-installer does not create any Volume Snapshot classes as part of the driver installation. A set of annotated volume snapshot class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:test-unityspec:storageClassName:unity-iscsidataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiVolume Expansion The CSI Unity driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-expand-scannotations:storageclass.kubernetes.io/is-default-class:falseprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:csi.storage.k8s.io/fstype:\"xfs\"To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:unity-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support The CSI Unity driver supports Raw Block Volumes. Raw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. The following is an example configuration:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:rawblockpvcnamespace:defaultspec:accessModes:- ReadWriteOncevolumeMode:Blockresources:requests:storage:5GistorageClassName:unity-iscsiapiVersion:v1kind:Podmetadata:name:rawblockpodnamespace:defaultspec:containers:- name:task-pv-containerimage:nginxports:- containerPort:80name:\"http-server\"volumeDevices:- devicePath:/usr/share/nginx/html/devicename:nov-eleventh-1-pv-storagevolumes:- name:nov-eleventh-1-pv-storagepersistentVolumeClaim:claimName:rawblockpvcAccess modes allowed are ReadWriteOnce and ReadWriteMany. Raw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device.\nRaw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size. Access mode ReadOnlyMany is not supported with raw block since we cannot restrict volumes to be readonly from Unity.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:vol0namespace:test-unityspec:storageClassName:unity-nfsaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:cloned-pvcnamespace:test-unityspec:storageClassName:unity-nfsdataSource:name:vol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI Unity driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity driver.\nkind:PodapiVersion:v1metadata:name:test-unity-ephemeral-volumespec:containers:- name:test-containerimage:busyboxcommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"10Gi\"arrayId:APM************protocol:iSCSIthinProvisioned:\"true\"isDataReductionEnabled:\"false\"tieringPolicy:\"1\"storagePool:pool_2This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-unity.dellemc.comcsi.storage.k8s.io/fstype:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI Unity driver supports controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default, number of replicas is set to 2, you can set the controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator you can change the replicas parameter in the spec.driver section in your Unity Custom Resource.\nWhen multiple replicas of controller pods are in a cluster each sidecar (Attacher, Provisioner, Resizer, and Snapshotter) tries to get a lease so only one instance of each sidecar is active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As said before you can configure where node driver pods would be assigned in a similar way in the node section of myvalues.yaml\nTopology The CSI Unity driver supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage User can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-topology-fcprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/\u003carray_id\u003e-fcvalues:- \"true\"This example matches all nodes where the driver has a connection to the Unity array with array ID mentioned via Fiber Channel. Similarly, by replacing fc with iscsi in the key checks for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\n Note that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work properly.\n For any additional information about the topology, see the Kubernetes Topology documentation.\nVolume Limit The CSI Driver for Dell Unity allows users to specify the maximum number of Unity volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-unity-volumes-per-node and specifying the volume limit for that node.  kubectl label node \u003cnode_name\u003e max-unity-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxUnityVolumesPerNode attribute in values.yaml file.\n NOTE: To reflect the changes after setting the value either via node label or in values.yaml file, user has to bounce the driver controller and node pods using the command kubectl get pods -n unity --no-headers=true | awk '/unity-/{print $1}'| xargs kubectl delete -n unity pod. If the value is set both by node label and values.yaml file then node label value will get the precedence and user has to remove the node label in order to reflect the values.yaml value. The default value of maxUnityVolumesPerNode is 0. If maxUnityVolumesPerNode is set to zero, then Container Orchestration decides how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxUnityVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-unity-volumes-per-node is not set.\n NAT Support CSI Driver for Dell Unity is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nSingle Pod Access Mode for PersistentVolumes CSI Driver for Unity supports a new accessmode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Unity allows to restrict volume access to a single pod in the cluster\nPrerequisites\n Enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as the ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is only supported for CSI volumes. You can enable the feature by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\" Create a PVC with access mode set to ReadWriteOncePod like shown in the sample below  kind:PersistentVolumeClaimapiVersion:v1metadata:name:single-writer-onlyspec:accessModes:- ReadWriteOncePod# Allow only a single pod to access single-writer-only.resources:requests:storage:1GiVolume Health Monitoring CSI Driver for Unity supports volume health monitoring. This is an alpha feature and requires feature gate to be enabled by setting command line arguments --feature-gates=\"...,CSIVolumeHealth=true\".\nThis feature:\n Reports on the condition of the underlying volumes via events when a volume condition is abnormal. We can watch the events on the describe of pvc kubectl describe pvc \u003cpvc name\u003e -n \u003cnamespace\u003e Collects the volume stats. We can see the volume usage in the node logs kubectl logs \u003cnodepod\u003e -n \u003cnamespacename\u003e -c driver By default this is disabled in CSI Driver for Unity. You will have to set the healthMonitor.enable flag for controller, node or for both in values.yaml to get the volume stats and volume condition.  Dynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params  Note: Prior to CSI Driver for unity version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object.\n Tenancy support for Unity NFS The CSI Unity driver version 2.1.0 (and later versions) supports the Tenancy feature of Unity such that the user will be able to associate specific worker nodes (in the cluster) and NFS storage volumes with Tenant.\nPrerequisites (to be manually created in Unity Array) before the driver installation:\n Create Tenants Create Pools Create NAS Servers with Tenant and Pool mapping  The following example describes the usage of Tenant in the NFS pod creation:\nInstall the csi driver using myvalues.yaml with the TenantName as follows: Example myvalues.yaml\nlogLevel:\"info\"certSecretCount:1kubeletConfigDir:/var/lib/kubeletcontroller:controllerCount:2volumeNamePrefix :csivolsnapshot:snapNamePrefix:csi-snaptenantName:\"tenant3\"Create storageclass with NAS-Server and the Storage-Pool associated with TenantName as follows: Example storageclass.yaml\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:annotations:storageclass.kubernetes.io/is-default-class:\"false\"name:unity-nfsparameters:arrayId:\"APM0***XXXXXX\"hostIoSize:\"16384\"isDataReductionEnabled:\"false\"storagePool:pool_7thinProvisioned:\"true\"tieringPolicy:\"0\"protocol:\"NFS\"nasServer:\"nas_5\"provisioner:csi-unity.dellemc.comreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumerallowVolumeExpansion:trueCreate the pod and pvc as follows: Example pvc.yaml\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvcnamenamespace:nginxspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:2GistorageClassName:unity-nfsExample pod.yaml\napiVersion:apps/v1kind:Deploymentmetadata:name:podnamenamespace:nginxspec:replicas:1selector:matchLabels:app:podnametemplate:metadata:labels:app:podnamespec:containers:- args:- \"-c\"- \"while true; do dd if=/dev/urandom of=/data0/foo bs=1M count=1;done\"command:- /bin/bashimage:\"docker.io/centos:latest\"name:testvolumeMounts:- mountPath:/data0name:pvcnamevolumes:- name:pvolx0persistentVolumeClaim:claimName:pvcnameWith the usage shown in the example, the user will be able to create an NFS pod with PVC using the NAS and the Pool associated with the added Tenants specified in SC.\n Note: Current feature supports ONLY single Tenant for all the nodes in the cluster. Users may expect an error if PVC is created from the NAS server whose pool is mapped to the different tenants not associated with this SC.\n For operator based installation, mention the TENANT_NAME in configmap as shown in the following example: Example configmap.yaml\napiVersion:v1kind:ConfigMapmetadata:name:unity-config-paramsnamespace:test-unitydata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"info\"ALLOW_RWO_MULTIPOD_ACCESS:\"false\"MAX_UNITY_VOLUMES_PER_NODE:\"0\"SYNC_NODE_INFO_TIME_INTERVAL:\"15\"TENANT_NAME:\"\" Note: csi-unity supports Tenancy in multi-array setup, provided the TenantName is the same across Unity instances.\n ","excerpt":"Creating volumes and consuming them Create a file sample.yaml using …","ref":"/csm-docs/v1/csidriver/features/unity/","title":"Unity"},{"body":"You can upgrade the CSI Driver for Dell Unity using Helm or Dell CSI Operator.\nNote:\n User has to re-create existing custom-storage classes (if any) according to the latest format. User has to create Volumesnapshotclass after upgrade for taking Snapshots. Secret.yaml files can be updated according to Multiarray normalization parameters only after upgrading the driver.  Using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nPreparing myvalues.yaml is the same as explained in the install section.\nTo upgrade the driver from csi-unity v2.1 to csi-unity 2.2\n Get the latest csi-unity 2.2 code from Github using using git clone -b v2.2.0 https://github.com/dell/csi-unity.git. Create myvalues.yaml. Copy the helm/csi-unity/values.yaml to the new location csi-unity/dell-csi-helm-installer with name say myvalues.yaml, to customize settings for installation edit myvalues.yaml as per the requirements. Navigate to common-helm-installer folder and execute the following command: ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade  Using Operator Notes:\n While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes. Upgrading the Operator does not upgrade the CSI Driver.  To upgrade the driver:\n Please upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell Unity using Helm or Dell CSI …","ref":"/csm-docs/v1/csidriver/upgradation/drivers/unity/","title":"Unity"},{"body":"Creating volumes and consuming them Create a file sample.yaml using sample yaml files located at test/sample.yaml\nThe following command creates a statefulset that consumes three volumes of default storage classes:\nkubectl create -f test/sample.yaml After executing this command 3 PVC and statefulset are created in the test-unity namespace. You can check created PVCs by running kubectl get pvc -n test-unity and check statefulset’s pods by running kubectl get pods -n test-unity command. The pod should be Ready and Running.\n If Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f test/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\n Open your volume in Unity Management UI (Unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:static-1annotations:pv.kubernetes.io/provisioned-by:csi-unity.dellemc.comspec:accessModes:- ReadWriteOncecapacity:storage:5Gicsi:driver:csi-unity.dellemc.comvolumeHandle:existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: RetainclaimRef:namespace:defaultname:static-pvc1storageClassName:unityvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:static-pvc1spec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:static-1storageClassName:unityThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:static-pvc1After the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of the CSI Unity 2.0 driver and higher, a Volume Snapshot Class is not created and need to create Volume Snapshot Class.\nFollowing is the manifest to create Volume Snapshot Class :\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:unity-snapclassdriver:csi-unity.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:test-unityspec:volumeSnapshotClassName:unity-snapclasssource:persistentVolumeClaimName:pvolOnce the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueNote : For CSI Driver for Unity version 1.6 and later, dell-csi-helm-installer does not create any Volume Snapshot classes as part of the driver installation. A set of annotated volume snapshot class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:test-unityspec:storageClassName:unity-iscsidataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiVolume Expansion The CSI Unity driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-expand-scannotations:storageclass.kubernetes.io/is-default-class:falseprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:csi.storage.k8s.io/fstype:\"xfs\"To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:unity-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support The CSI Unity driver supports Raw Block Volumes. Raw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. The following is an example configuration:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:rawblockpvcnamespace:defaultspec:accessModes:- ReadWriteOncevolumeMode:Blockresources:requests:storage:5GistorageClassName:unity-iscsiapiVersion:v1kind:Podmetadata:name:rawblockpodnamespace:defaultspec:containers:- name:task-pv-containerimage:nginxports:- containerPort:80name:\"http-server\"volumeDevices:- devicePath:/usr/share/nginx/html/devicename:nov-eleventh-1-pv-storagevolumes:- name:nov-eleventh-1-pv-storagepersistentVolumeClaim:claimName:rawblockpvcAccess modes allowed are ReadWriteOnce and ReadWriteMany. Raw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device.\nRaw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size. Access mode ReadOnlyMany is not supported with raw block since we cannot restrict volumes to be readonly from Unity.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:vol0namespace:test-unityspec:storageClassName:unity-nfsaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:cloned-pvcnamespace:test-unityspec:storageClassName:unity-nfsdataSource:name:vol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI Unity driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity driver.\nkind:PodapiVersion:v1metadata:name:test-unity-ephemeral-volumespec:containers:- name:test-containerimage:busyboxcommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"10Gi\"arrayId:APM************protocol:iSCSIthinProvisioned:\"true\"isDataReductionEnabled:\"false\"tieringPolicy:\"1\"storagePool:pool_2This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-unity.dellemc.comcsi.storage.k8s.io/fstype:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI Unity driver supports controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default, number of replicas is set to 2, you can set the controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator you can change the replicas parameter in the spec.driver section in your Unity Custom Resource.\nWhen multiple replicas of controller pods are in a cluster each sidecar (Attacher, Provisioner, Resizer, and Snapshotter) tries to get a lease so only one instance of each sidecar is active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As said before you can configure where node driver pods would be assigned in a similar way in the node section of myvalues.yaml\nTopology The CSI Unity driver supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage User can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-topology-fcprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/\u003carray_id\u003e-fcvalues:- \"true\"This example matches all nodes where the driver has a connection to the Unity array with array ID mentioned via Fiber Channel. Similarly, by replacing fc with iscsi in the key checks for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\n Note that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work properly.\n For any additional information about the topology, see the Kubernetes Topology documentation.\nVolume Limit The CSI Driver for Dell Unity allows users to specify the maximum number of Unity volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-unity-volumes-per-node and specifying the volume limit for that node.  kubectl label node \u003cnode_name\u003e max-unity-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxUnityVolumesPerNode attribute in values.yaml file.\n NOTE: To reflect the changes after setting the value either via node label or in values.yaml file, user has to bounce the driver controller and node pods using the command kubectl get pods -n unity --no-headers=true | awk '/unity-/{print $1}'| xargs kubectl delete -n unity pod. If the value is set both by node label and values.yaml file then node label value will get the precedence and user has to remove the node label in order to reflect the values.yaml value. The default value of maxUnityVolumesPerNode is 0. If maxUnityVolumesPerNode is set to zero, then Container Orchestration decides how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxUnityVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-unity-volumes-per-node is not set.\n NAT Support CSI Driver for Dell Unity is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nSingle Pod Access Mode for PersistentVolumes CSI Driver for Unity supports a new accessmode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Unity allows to restrict volume access to a single pod in the cluster\nPrerequisites\n Enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as the ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is only supported for CSI volumes. You can enable the feature by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\" Create a PVC with access mode set to ReadWriteOncePod like shown in the sample below  kind:PersistentVolumeClaimapiVersion:v1metadata:name:single-writer-onlyspec:accessModes:- ReadWriteOncePod# Allow only a single pod to access single-writer-only.resources:requests:storage:1GiVolume Health Monitoring CSI Driver for Unity supports volume health monitoring. This is an alpha feature and requires feature gate to be enabled by setting command line arguments --feature-gates=\"...,CSIVolumeHealth=true\".\nThis feature:\n Reports on the condition of the underlying volumes via events when a volume condition is abnormal. We can watch the events on the describe of pvc kubectl describe pvc \u003cpvc name\u003e -n \u003cnamespace\u003e Collects the volume stats. We can see the volume usage in the node logs kubectl logs \u003cnodepod\u003e -n \u003cnamespacename\u003e -c driver By default this is disabled in CSI Driver for Unity. You will have to set the healthMonitor.enable flag for controller, node or for both in values.yaml to get the volume stats and volume condition.  Dynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params  Note: Prior to CSI Driver for unity version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object.\n Tenancy support for Unity NFS The CSI Unity driver version 2.1.0 (and later versions) supports the Tenancy feature of Unity such that the user will be able to associate specific worker nodes (in the cluster) and NFS storage volumes with Tenant.\nPrerequisites (to be manually created in Unity Array) before the driver installation:\n Create Tenants Create Pools Create NAS Servers with Tenant and Pool mapping  The following example describes the usage of Tenant in the NFS pod creation:\nInstall the csi driver using myvalues.yaml with the TenantName as follows: Example myvalues.yaml\nlogLevel:\"info\"certSecretCount:1kubeletConfigDir:/var/lib/kubeletcontroller:controllerCount:2volumeNamePrefix :csivolsnapshot:snapNamePrefix:csi-snaptenantName:\"tenant3\"Create storageclass with NAS-Server and the Storage-Pool associated with TenantName as follows: Example storageclass.yaml\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:annotations:storageclass.kubernetes.io/is-default-class:\"false\"name:unity-nfsparameters:arrayId:\"APM0***XXXXXX\"hostIoSize:\"16384\"isDataReductionEnabled:\"false\"storagePool:pool_7thinProvisioned:\"true\"tieringPolicy:\"0\"protocol:\"NFS\"nasServer:\"nas_5\"provisioner:csi-unity.dellemc.comreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumerallowVolumeExpansion:trueCreate the pod and pvc as follows: Example pvc.yaml\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvcnamenamespace:nginxspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:2GistorageClassName:unity-nfsExample pod.yaml\napiVersion:apps/v1kind:Deploymentmetadata:name:podnamenamespace:nginxspec:replicas:1selector:matchLabels:app:podnametemplate:metadata:labels:app:podnamespec:containers:- args:- \"-c\"- \"while true; do dd if=/dev/urandom of=/data0/foo bs=1M count=1;done\"command:- /bin/bashimage:\"docker.io/centos:latest\"name:testvolumeMounts:- mountPath:/data0name:pvcnamevolumes:- name:pvolx0persistentVolumeClaim:claimName:pvcnameWith the usage shown in the example, the user will be able to create an NFS pod with PVC using the NAS and the Pool associated with the added Tenants specified in SC.\n Note: Current feature supports ONLY single Tenant for all the nodes in the cluster. Users may expect an error if PVC is created from the NAS server whose pool is mapped to the different tenants not associated with this SC.\n For operator based installation, mention the TENANT_NAME in configmap as shown in the following example: Example configmap.yaml\napiVersion:v1kind:ConfigMapmetadata:name:unity-config-paramsnamespace:test-unitydata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"info\"ALLOW_RWO_MULTIPOD_ACCESS:\"false\"MAX_UNITY_VOLUMES_PER_NODE:\"0\"SYNC_NODE_INFO_TIME_INTERVAL:\"15\"TENANT_NAME:\"\" Note: csi-unity supports Tenancy in multi-array setup, provided the TenantName is the same across Unity instances.\n ","excerpt":"Creating volumes and consuming them Create a file sample.yaml using …","ref":"/csm-docs/v2/csidriver/features/unity/","title":"Unity"},{"body":"You can upgrade the CSI Driver for Dell Unity using Helm or Dell CSI Operator.\nNote:\n User has to re-create existing custom-storage classes (if any) according to the latest format. User has to create Volumesnapshotclass after upgrade for taking Snapshots. Secret.yaml files can be updated according to Multiarray normalization parameters only after upgrading the driver.  Using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nPreparing myvalues.yaml is the same as explained in the install section.\nTo upgrade the driver from csi-unity v2.1 to csi-unity 2.2\n Get the latest csi-unity 2.2 code from Github using using git clone -b v2.2.0 https://github.com/dell/csi-unity.git. Create myvalues.yaml. Copy the helm/csi-unity/values.yaml to the new location csi-unity/dell-csi-helm-installer with name say myvalues.yaml, to customize settings for installation edit myvalues.yaml as per the requirements. Navigate to common-helm-installer folder and execute the following command: ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade  Using Operator Notes:\n While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes. Upgrading the Operator does not upgrade the CSI Driver.  To upgrade the driver:\n Please upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell Unity using Helm or Dell CSI …","ref":"/csm-docs/v2/csidriver/upgradation/drivers/unity/","title":"Unity"},{"body":"Creating volumes and consuming them Create a file sample.yaml using sample yaml files located at test/sample.yaml\nThe following command creates a statefulset that consumes three volumes of default storage classes:\nkubectl create -f test/sample.yaml After executing this command 3 PVC and statefulset are created in the test-unity namespace. You can check created PVCs by running kubectl get pvc -n test-unity and check statefulset’s pods by running kubectl get pods -n test-unity command. The pod should be Ready and Running.\n If Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f test/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\n Open your volume in Unity Management UI (Unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:static-1annotations:pv.kubernetes.io/provisioned-by:csi-unity.dellemc.comspec:accessModes:- ReadWriteOncecapacity:storage:5Gicsi:driver:csi-unity.dellemc.comvolumeHandle:existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: RetainclaimRef:namespace:defaultname:static-pvc1storageClassName:unityvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:static-pvc1spec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:static-1storageClassName:unityThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:static-pvc1After the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of the CSI Unity 2.0 driver and higher, a Volume Snapshot Class is not created and need to create Volume Snapshot Class.\nFollowing is the manifest to create Volume Snapshot Class :\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:unity-snapclassdriver:csi-unity.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:test-unityspec:volumeSnapshotClassName:unity-snapclasssource:persistentVolumeClaimName:pvolOnce the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueNote : For CSI Driver for Unity version 1.6 and later, dell-csi-helm-installer does not create any Volume Snapshot classes as part of the driver installation. A set of annotated volume snapshot class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:test-unityspec:storageClassName:unity-iscsidataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiVolume Expansion The CSI Unity driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:unity-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support The CSI Unity driver version 1.4 and later supports Raw Block Volumes. Raw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. The following is an example configuration:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:rawblockpvcnamespace:defaultspec:accessModes:- ReadWriteOncevolumeMode:Blockresources:requests:storage:5GistorageClassName:unity-iscsiapiVersion:v1kind:Podmetadata:name:rawblockpodnamespace:defaultspec:containers:- name:task-pv-containerimage:nginxports:- containerPort:80name:\"http-server\"volumeDevices:- devicePath:/usr/share/nginx/html/devicename:nov-eleventh-1-pv-storagevolumes:- name:nov-eleventh-1-pv-storagepersistentVolumeClaim:claimName:rawblockpvcAccess modes allowed are ReadWriteOnce and ReadWriteMany. Raw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device.\nRaw Block volumes support online Volume Expansion, but it is up to the application to manage to reconfigure the file system (if any) to the new size. Access mode ReadOnlyMany is not supported with raw block since we cannot restrict volumes to be readonly from Unity.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:vol0namespace:test-unityspec:storageClassName:unity-nfsaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:cloned-pvcnamespace:test-unityspec:storageClassName:unity-nfsdataSource:name:vol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI Unity driver version 1.4 and later supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity driver.\nkind:PodapiVersion:v1metadata:name:test-unity-ephemeral-volumespec:containers:- name:test-containerimage:busyboxcommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"10Gi\"arrayId:APM************protocol:iSCSIthinProvisioned:\"true\"isDataReductionEnabled:\"false\"tieringPolicy:\"1\"storagePool:pool_2This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI Unity driver version 1.4 and later supports the controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default, number of replicas is set to 2, you can set the controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator you can change the replicas parameter in the spec.driver section in your Unity Custom Resource.\nWhen multiple replicas of controller pods are in a cluster each sidecar (Attacher, Provisioner, Resizer, and Snapshotter) tries to get a lease so only one instance of each sidecar is active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As said before you can configure where node driver pods would be assigned in a similar way in the node section of myvalues.yaml\nTopology The CSI Unity driver version 1.4 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage User can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-topology-fcprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/\u003carray_id\u003e-fcvalues:- \"true\"This example matches all nodes where the driver has a connection to the Unity array with array ID mentioned via Fiber Channel. Similarly, by replacing fc with iscsi in the key checks for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\n Note that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work properly.\n For any additional information about the topology, see the Kubernetes Topology documentation.\nSupport for SLES 15 SP2 The CSI Driver for Dell EMC Unity requires the following set of packages installed on all worker nodes that run on SLES 15 SP2.\n open-iscsi open-iscsi is required in order to make use of iSCSI protocol for provisioning nfs-utils nfs-utils is required in order to make use of NFS protocol for provisioning multipath-tools multipath-tools is required in order to make use of FC and iSCSI protocols for provisioning  After installing open-iscsi, ensure “iscsi” and “iscsid” services have been started and /etc/isci/initiatorname.iscsi is created and has the host initiator id. The pre-requisites are mandatory for provisioning with the iSCSI protocol to work.\nVolume Limit The CSI Driver for Dell EMC Unity allows users to specify the maximum number of Unity volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-unity-volumes-per-node and specifying the volume limit for that node.  kubectl label node \u003cnode_name\u003e max-unity-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxUnityVolumesPerNode attribute in values.yaml file.\n NOTE: To reflect the changes after setting the value either via node label or in values.yaml file, user has to bounce the driver controller and node pods using the command kubectl get pods -n unity --no-headers=true | awk '/unity-/{print $1}'| xargs kubectl delete -n unity pod. If the value is set both by node label and values.yaml file then node label value will get the precedence and user has to remove the node label in order to reflect the values.yaml value. The default value of maxUnityVolumesPerNode is 0. If maxUnityVolumesPerNode is set to zero, then CO SHALL decide how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxUnityVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-unity-volumes-per-node is not set.\n NAT Support CSI Driver for Dell EMC Unity is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nDynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nSingle Pod Access Mode for PersistentVolumes CSI Driver for Unity now supports a new accessmode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Unity allows to restrict volume access to a single pod in the cluster\nPrerequisites\n Enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as the ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is only supported for CSI volumes. You can enable the feature by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\" Create a PVC with access mode set to ReadWriteOncePod like shown in the sample below  kind:PersistentVolumeClaimapiVersion:v1metadata:name:single-writer-onlyspec:accessModes:- ReadWriteOncePod# Allow only a single pod to access single-writer-only.resources:requests:storage:1GiVolume Health Monitoring CSI Driver for Unity now supports volume health monitoring. This is an alpha feature and requires feature gate to be enabled by setting command line arguments --feature-gates=\"...,CSIVolumeHealth=true\".\nThis feature:\n Reports on the condition of the underlying volumes via events when a volume condition is abnormal. We can watch the events on the describe of pvc kubectl describe pvc \u003cpvc name\u003e -n \u003cnamespace\u003e Collects the volume stats. We can see the volume usage in the node logs kubectl logs \u003cnodepod\u003e -n \u003cnamespacename\u003e -c driver By default this is disabled in CSI Driver for Unity. You will have to set the volumeHealthMonitor.enable flag for controller, node or for both in values.yaml to get the volume stats and volume condition.  Helm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params  Note: Prior to CSI Driver for unity version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object.\n Tenancy support for Unity NFS The CSI Unity driver version 2.1.0 (and later versions) supports the Tenancy feature of Unity such that the user will be able to associate specific worker nodes (in the cluster) and NFS storage volumes with Tenant.\nPrerequisites (to be manually created in Unity Array) before the driver installation:\n Create Tenants Create Pools Create NAS Servers with Tenant and Pool mapping  The following example describes the usage of Tenant in the NFS pod creation:\nInstall the csi driver using myvalues.yaml with the TenantName as follows: Example myvalues.yaml\nlogLevel:\"info\"certSecretCount:1kubeletConfigDir:/var/lib/kubeletcontroller:controllerCount:2volumeNamePrefix :csivolsnapshot:snapNamePrefix:csi-snaptenantName:\"tenant3\"Create storageclass with NAS-Server and the Storage-Pool associated with TenantName as follows: Example storageclass.yaml\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:annotations:storageclass.beta.kubernetes.io/is-default-class:\"false\"name:unity-nfsparameters:arrayId:\"APM0***XXXXXX\"hostIoSize:\"16384\"isDataReductionEnabled:\"false\"storagePool:pool_7thinProvisioned:\"true\"tieringPolicy:\"0\"protocol:\"NFS\"nasServer:\"nas_5\"provisioner:csi-unity.dellemc.comreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumerallowVolumeExpansion:trueCreate the pod and pvc as follows: Example pvc.yaml\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvcnamenamespace:nginxspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:2GistorageClassName:unity-nfsExample pod.yaml\napiVersion:apps/v1kind:Deploymentmetadata:name:podnamenamespace:nginxspec:replicas:1selector:matchLabels:app:podnametemplate:metadata:labels:app:podnamespec:containers:- args:- \"-c\"- \"while true; do dd if=/dev/urandom of=/data0/foo bs=1M count=1;done\"command:- /bin/bashimage:\"docker.io/centos:latest\"name:testvolumeMounts:- mountPath:/data0name:pvcnamevolumes:- name:pvolx0persistentVolumeClaim:claimName:pvcnameWith the usage shown in the example, the user will be able to create an NFS pod with PVC using the NAS and the Pool associated with the added Tenants specified in SC.\n Note: Current feature supports ONLY single Tenant for all the nodes in the cluster. Users may expect an error if PVC is created from the NAS server whose pool is mapped to the different tenants not associated with this SC.\n For operator based installation, mention the TENANT_NAME in configmap as shown in the following example: Example configmap.yaml\napiVersion:v1kind:ConfigMapmetadata:name:unity-config-paramsnamespace:test-unitydata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"info\"ALLOW_RWO_MULTIPOD_ACCESS:\"false\"MAX_UNITY_VOLUMES_PER_NODE:\"0\"SYNC_NODE_INFO_TIME_INTERVAL:\"0\"TENANT_NAME:\"\" Note: csi-unity supports Tenancy in multi-array setup, provided the TenantName is the same across Unity instances.\n ","excerpt":"Creating volumes and consuming them Create a file sample.yaml using …","ref":"/csm-docs/v3/csidriver/features/unity/","title":"Unity"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUsing Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nPreparing myvalues.yaml is the same as explained in the install section.\nTo upgrade the driver from csi-unity v2.0 to csi-unity 2.1\n Get the latest csi-unity 2.1 code from Github using using git clone -b v2.1.0 https://github.com/dell/csi-unity.git. Create myvalues.yaml. Copy the helm/csi-unity/values.yaml to the new location csi-unity/dell-csi-helm-installer with name say myvalues.yaml, to customize settings for installation edit myvalues.yaml as per the requirements. Navigate to common-helm-installer folder and execute the following command: ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade  Note:\n User has to re-create existing custom-storage classes (if any) according to the latest format. User has to create Volumesnapshotclass after upgrade for taking Snapshots. Secret.yaml files can be updated according to Multiarray Normalization parameters only after upgrading the driver.  Using Operator Note: While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\nTo upgrade the driver from csi-unity v2.0 to csi-unity v2.1 :\n  Clone the Dell CSI Operator repository.\n  Execute bash scripts/install.sh --upgrade This command will install the latest version of the operator.\n   Note: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default.\n To upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell …","ref":"/csm-docs/v3/csidriver/upgradation/drivers/unity/","title":"Unity"},{"body":"Creating volumes and consuming them Create a file sample.yaml using sample yaml files located at test/sample.yaml\nThe following command creates a statefulset that consumes three volumes of default storage classes:\nkubectl create -f test/sample.yaml After executing this command 3 PVC and statefulset are created in the test-unity namespace. You can check created PVCs by running kubectl get pvc -n test-unity and check statefulset’s pods by running kubectl get pods -n test-unity command. The pod should be Ready and Running.\n If Pod is in CrashLoopback or PVCs is in a Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f test/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity XT array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\n Open your volume in Unity XT Management UI (Unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:static-1annotations:pv.kubernetes.io/provisioned-by:csi-unity.dellemc.comspec:accessModes:- ReadWriteOncecapacity:storage:5Gicsi:driver:csi-unity.dellemc.comvolumeHandle:existingvol-\u003cprotocol\u003e-\u003carray_id\u003e-\u003cvolume-id\u003e persistentVolumeReclaimPolicy: RetainclaimRef:namespace:defaultname:static-pvc1storageClassName:unityvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:static-pvc1spec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:static-1storageClassName:unityThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data0\"name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:static-pvc1After the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  Volume Snapshot Class Following is the manifest to create Volume Snapshot Class :\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:unity-snapclassdriver:csi-unity.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:test-unityspec:volumeSnapshotClassName:unity-snapclasssource:persistentVolumeClaimName:pvolOnce the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\"2020-07-16T08:42:12Z\"readyToUse:trueNote : A set of annotated volume snapshot class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:test-unityspec:storageClassName:unity-iscsidataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiVolume Expansion The CSI Unity XT driver supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true.\nThe following is a sample manifest for a storage class that allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-expand-scannotations:storageclass.kubernetes.io/is-default-class:falseprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:csi.storage.k8s.io/fstype:\"xfs\"To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:unity-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support The CSI Unity XT driver supports Raw Block Volumes. Raw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. The following is an example configuration:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:rawblockpvcnamespace:defaultspec:accessModes:- ReadWriteOncevolumeMode:Blockresources:requests:storage:5GistorageClassName:unity-iscsiapiVersion:v1kind:Podmetadata:name:rawblockpodnamespace:defaultspec:containers:- name:task-pv-containerimage:nginxports:- containerPort:80name:\"http-server\"volumeDevices:- devicePath:/usr/share/nginx/html/devicename:nov-eleventh-1-pv-storagevolumes:- name:nov-eleventh-1-pv-storagepersistentVolumeClaim:claimName:rawblockpvcAccess modes allowed are ReadWriteOnce and ReadWriteMany. Raw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node’s file system. The driver does not format or check the format of any file system on the block device.\nRaw Block volumes support online Volume Expansion, but it is up to the application to manage and reconfigure the file system (if any) to the new size. Access mode ReadOnlyMany is not supported with raw block since we cannot restrict volumes to be readonly from Unity XT.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity XT driver supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:vol0namespace:test-unityspec:storageClassName:unity-nfsaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:cloned-pvcnamespace:test-unityspec:storageClassName:unity-nfsdataSource:name:vol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI Unity XT driver supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity XT driver.\nkind:PodapiVersion:v1metadata:name:test-unity-ephemeral-volumespec:containers:- name:test-containerimage:busyboxcommand:[\"sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:volumevolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\"ext4\"volumeAttributes:size:\"10Gi\"arrayId:APM************protocol:iSCSIthinProvisioned:\"true\"isDataReductionEnabled:\"false\"tieringPolicy:\"1\"storagePool:pool_2This manifest creates a pod and attaches a newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that point to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-unity.dellemc.comcsi.storage.k8s.io/fstype:\"nfs\"volumeAttributes:size:\"20Gi\"nasName:\"csi-nas-name\"Controller HA The CSI Unity XT driver supports controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default, the number of replicas is set to 2. You can set the controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator, you can change the replicas parameter in the spec.driver section in your Unity XT Custom Resource.\nWhen multiple replicas of controller pods are in a cluster each sidecar (Attacher, Provisioner, Resizer, and Snapshotter) tries to get a lease so only one instance of each sidecar is active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \"controller\" allows to configure controller specific parameterscontroller:# \"controller.nodeSelector\" defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\"\"# \"controller.tolerations\" defines tolerations that would be applied to controller deploymenttolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"As said before you can configure where node driver pods would be assigned in a similar way in the node section of myvalues.yaml\nTopology The CSI Unity XT driver supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer-defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by the driver and applied automatically by Kubernetes on its nodes.\nTopology Usage User can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-topology-fcprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/\u003carray_id\u003e-fcvalues:- \"true\"This example matches all nodes where the driver has a connection to the Unity XT array with array ID mentioned via Fiber Channel. Similarly, by replacing fc with iscsi in the key checks for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\n Note that volumeBindingMode: is set to WaitForFirstConsumer this is required for the topology feature to work properly.\n For any additional information about the topology, see the Kubernetes Topology documentation.\nVolume Limit The CSI Driver for Dell Unity XT allows users to specify the maximum number of Unity XT volumes that can be used in a node.\nThe user can set the volume limit for a node by creating a node label max-unity-volumes-per-node and specifying the volume limit for that node.  kubectl label node \u003cnode_name\u003e max-unity-volumes-per-node=\u003cvolume_limit\u003e\nThe user can also set the volume limit for all the nodes in the cluster by specifying the same to maxUnityVolumesPerNode attribute in values.yaml file.\n NOTE: To reflect the changes after setting the value either via node label or in values.yaml file, user has to bounce the driver controller and node pods using the command kubectl get pods -n unity --no-headers=true | awk '/unity-/{print $1}'| xargs kubectl delete -n unity pod. If the value is set both by node label and values.yaml file then node label value will get the precedence and user has to remove the node label in order to reflect the values.yaml value. The default value of maxUnityVolumesPerNode is 0. If maxUnityVolumesPerNode is set to zero, then Container Orchestration decides how many volumes of this type can be published by the controller to the node.\nThe volume limit specified to maxUnityVolumesPerNode attribute is applicable to all the nodes in the cluster for which node label max-unity-volumes-per-node is not set.\n NAT Support CSI Driver for Dell Unity XT is supported in the NAT environment for NFS protocol.\nThe user will be able to install the driver and able to create pods.\nSingle Pod Access Mode for PersistentVolumes CSI Driver for Unity XT supports a new accessmode ReadWriteOncePod for PersistentVolumes and PersistentVolumeClaims. With this feature, CSI Driver for Unity XT restricts volume access to a single pod in the cluster\nPrerequisites\n Enable the ReadWriteOncePod feature gate for kube-apiserver, kube-scheduler, and kubelet as the ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is only supported for CSI volumes. You can enable the feature by setting command line arguments: --feature-gates=\"...,ReadWriteOncePod=true\" Create a PVC with access mode set to ReadWriteOncePod like shown in the sample below  kind:PersistentVolumeClaimapiVersion:v1metadata:name:single-writer-onlyspec:accessModes:- ReadWriteOncePod# Allow only a single pod to access single-writer-only.resources:requests:storage:1GiVolume Health Monitoring CSI Driver for Unity XT supports volume health monitoring. This is an alpha feature and requires feature gate to be enabled by setting command line arguments --feature-gates=\"...,CSIVolumeHealth=true\".\nThis feature:\n Reports on the condition of the underlying volumes via events when a volume condition is abnormal. We can watch the events on the describe of pvc kubectl describe pvc \u003cpvc name\u003e -n \u003cnamespace\u003e Collects the volume stats. We can see the volume usage in the node logs kubectl logs \u003cnodepod\u003e -n \u003cnamespacename\u003e -c driver By default this is disabled in CSI Driver for Unity XT. You will have to set the healthMonitor.enable flag for controller, node or for both in values.yaml to get the volume stats and volume condition.  Dynamic Logging Configuration Helm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\nOperator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params Tenancy support for Unity XT NFS The CSI Unity XT driver version v2.1.0 (and later versions) supports the Tenancy feature of Unity XT such that the user will be able to associate specific worker nodes (in the cluster) and NFS storage volumes with Tenant.\nPrerequisites (to be manually created in Unity XT Array) before the driver installation:\n Create Tenants Create Pools Create NAS Servers with Tenant and Pool mapping  The following example describes the usage of Tenant in the NFS pod creation:\nInstall the csi driver using myvalues.yaml with the TenantName as follows: Example myvalues.yaml\nlogLevel:\"info\"certSecretCount:1kubeletConfigDir:/var/lib/kubeletcontroller:controllerCount:2volumeNamePrefix :csivolsnapshot:snapNamePrefix:csi-snaptenantName:\"tenant3\"Create storageclass with NAS-Server and the Storage-Pool associated with TenantName as follows: Example storageclass.yaml\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:annotations:storageclass.kubernetes.io/is-default-class:\"false\"name:unity-nfsparameters:arrayId:\"APM0***XXXXXX\"hostIoSize:\"16384\"isDataReductionEnabled:\"false\"storagePool:pool_7thinProvisioned:\"true\"tieringPolicy:\"0\"protocol:\"NFS\"nasServer:\"nas_5\"provisioner:csi-unity.dellemc.comreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumerallowVolumeExpansion:trueCreate the pod and pvc as follows: Example pvc.yaml\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvcnamenamespace:nginxspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:2GistorageClassName:unity-nfsExample pod.yaml\napiVersion:apps/v1kind:Deploymentmetadata:name:podnamenamespace:nginxspec:replicas:1selector:matchLabels:app:podnametemplate:metadata:labels:app:podnamespec:containers:- args:- \"-c\"- \"while true; do dd if=/dev/urandom of=/data0/foo bs=1M count=1;done\"command:- /bin/bashimage:\"docker.io/centos:latest\"name:testvolumeMounts:- mountPath:/data0name:pvcnamevolumes:- name:pvolx0persistentVolumeClaim:claimName:pvcnameWith the usage shown in the example, the user will be able to create an NFS pod with PVC using the NAS and the Pool associated with the added Tenants specified in SC.\n Note: Current feature supports ONLY single Tenant for all the nodes in the cluster. Users may expect an error if PVC is created from the NAS server whose pool is mapped to the different tenants not associated with this SC.\n For operator based installation, mention the TENANT_NAME in configmap as shown in the following example: Example configmap.yaml\napiVersion:v1kind:ConfigMapmetadata:name:unity-config-paramsnamespace:test-unitydata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"info\"ALLOW_RWO_MULTIPOD_ACCESS:\"false\"MAX_UNITY_VOLUMES_PER_NODE:\"0\"SYNC_NODE_INFO_TIME_INTERVAL:\"15\"TENANT_NAME:\"\" Note: csi-unity supports Tenancy in multi-array setup, provided the TenantName is the same across Unity XT instances.\n ","excerpt":"Creating volumes and consuming them Create a file sample.yaml using …","ref":"/csm-docs/docs/csidriver/features/unity/","title":"Unity XT"},{"body":"You can upgrade the CSI Driver for Dell Unity XT using Helm or Dell CSI Operator.\nNote:\n User has to re-create existing custom-storage classes (if any) according to the latest format. User has to create Volumesnapshotclass after upgrade for taking Snapshots. Secret.yaml files can be updated according to Multiarray normalization parameters only after upgrading the driver.  Using Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nPreparing myvalues.yaml is the same as explained in the install section.\nTo upgrade the driver from csi-unity v2.2.0 to csi-unity v2.3.0\n Get the latest csi-unity v2.3.0 code from Github using using git clone -b v2.3.0 https://github.com/dell/csi-unity.git. Copy the helm/csi-unity/values.yaml to the new location csi-unity/dell-csi-helm-installer and rename it to myvalues.yaml. Customize settings for installation by editing myvalues.yaml as needed. Navigate to csi-unity/dell-csi-hem-installer folder and execute this command: ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade  Using Operator Notes:\n While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes. Upgrading the Operator does not upgrade the CSI Driver.  To upgrade the driver:\n Please upgrade the Dell CSI Operator by following here. Once the operator is upgraded, to upgrade the driver, refer here.  ","excerpt":"You can upgrade the CSI Driver for Dell Unity XT using Helm or Dell …","ref":"/csm-docs/docs/csidriver/upgradation/drivers/unity/","title":"Unity XT"},{"body":"Replication design and architecture Container Storage Modules (CSM) for Replication project consists of the following components:\n DellCSIReplicationGroup - A Kubernetes Custom Resource CSM Replication controller which replicates the resources across(or within) Kubernetes clusters. CSM Replication sidecar container which is part of the CSI driver controller pod repctl - Multi cluster Kubernetes client for managing replication related objects  DellCSIReplicationGroup DellCSIReplicationGroup (RG) is a cluster scoped Custom Resource that represents a protection group on the backend storage array. It is used to group volumes with the same replication related properties together. DellCSIReplicationGroup's spec contains an action field which can be used to perform replication related operations on the backing protection groups on the storage arrays. This includes operations like Failover, Reprotect, Suspend, Synchronize e.t.c. Any replication related operation is always carried out on all the volumes present in the group.\nSpecification kind:DellCSIReplicationGroupapiVersion:replication.storage.dell.com/v1alpha1metadata:name:rg-e6be24c0-145d-4b62-8674-639282ebdd13spec:driverName:driver.dellemc.com# Name of the CSI driver (same as provisioner name in StorageClass)action:\"\"# Name of the replication action to be performed on the protection groupprotectionGroupAttributes:localAttributeKey:valueprotectionGroupId:protection-group-id# Identifier of the backing protection group on the Storage ArrayremoteClusterId:tgtClusterID# A unique identifier for the remote Kubernetes ClusterremoteProtectionGroupAttributes:remoteAttributeKey:valueremoteProtectionGroupId:csi-rep-sg-test-5-ASYNC# Identifier for the protection group on the remote Storage ArrayStatus The status sub resource of DellCSIReplicationGroup contains information about the state of replication \u0026 any actions which have been performed on the object.\n   Field Description     state State of the Custom Resource   replicationLinkState State of the replication on the storage arrays   lastAction Result of the last performed action   conditions List of recent conditions the CR instance has gone through    status:conditions:- condition:ActionREPROTECT_REMOTEsucceededtime:\"2021-08-11T12:22:05Z\"- condition:ReplicationLinkState:IsSourcechangedfrom(true)to(false)time:\"2021-08-11T12:18:50Z\"- condition:ActionFAILOVER_REMOTEsucceededtime:\"2021-08-11T12:18:50Z\"lastAction:condition:ActionREPROTECT_REMOTEsucceededtime:\"2021-08-11T12:22:05Z\"replicationLinkState:isSource:falselastSuccessfulUpdate:\"2021-08-11T17:18:12Z\"state:Synchronizedstate:ReadyHere is a diagram representing how the state of the CustomResource changes based on actions CSM Replication sidecar CSM Replication sidecar is deployed as sidecar container in the CSI driver controller pod. This container is similar to Kubernetes CSI Sidecar containers and runs a Controller Manager which manages the following controllers -\n PersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller  The PV \u0026 PVC controllers watch for PV/PVC creation events and use dell-csi-extensions APIs to communicate with the CSI Driver controller plugin to discover/create replication enabled volumes and protection groups on the backend storage array. The PersistentVolume controller then uses these details to create DellCSIReplicationGroup objects in the cluster. These controllers are also responsible for associating the PV \u0026 PVC objects with DellCSIReplicationGroup objects. This association is established by applying annotations \u0026 labels on the PV \u0026 PVC objects.\nThe RG controller manages DellCSIReplicationGroup instances and processes any change requests. It is primarily responsible for the following:\n Perform actions on the protection groups Monitor status of replication Updates to the status sub resource  CSM Replication Controller CSM Replication Controller is a Kubernetes application deployed independently of CSI drivers and is responsible for the communication between Kubernetes clusters.\nThe details about the clusters it needs to connect to are provided in the form of a ConfigMap with references to secrets containing the details(KubeConfig/ServiceAccount tokens) required to connect to the respective clusters.\nIt consists of Controller Manager which manages the following controllers:\n PersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller  The PV controller is responsible for creating PV objects (representing the replicated volumes on the backend storage array) in the remote Kubernetes cluster. This controller also enables deletion of the remote PV object in case it is desired by propagating the deletion request across clusters.\nSimilarly, the RG controller is responsible for creating RG objects in the remote Kubernetes cluster. These RG objects represent the remote protection groups on the backend storage array. This controller can also propagate the deletion request of RG objects across clusters.\nBoth the PV \u0026 RG objects in the remote cluster have extra metadata associated with them in form of annotations \u0026 labels. This metadata includes information about the respective objects in the source cluster.\nThe PVC objects are never replicated across the clusters. Instead, the remote PV objects have annotations related to the source PVC objects. This information can be easily used to create the PVCs whenever required using repctl or even kubectl\nSupported Cluster Topologies Click here for details for the various types of supported cluster topologies\n","excerpt":"Replication design and architecture Container Storage Modules (CSM) …","ref":"/csm-docs/docs/replication/architecture/","title":"Architecture"},{"body":"Replication design and architecture Container Storage Modules (CSM) for Replication project consists of the following components:\n DellCSIReplicationGroup - A Kubernetes Custom Resource CSM Replication controller which replicates the resources across(or within) Kubernetes clusters. CSM Replication sidecar container which is part of the CSI driver controller pod repctl - Multi cluster Kubernetes client for managing replication related objects  DellCSIReplicationGroup DellCSIReplicationGroup (RG) is a cluster scoped Custom Resource that represents a protection group on the backend storage array. It is used to group volumes with the same replication related properties together. DellCSIReplicationGroup's spec contains an action field which can be used to perform replication related operations on the backing protection groups on the storage arrays. This includes operations like Failover, Reprotect, Suspend, Synchronize e.t.c. Any replication related operation is always carried out on all the volumes present in the group.\nSpecification kind:DellCSIReplicationGroupapiVersion:replication.storage.dell.com/v1alpha1metadata:name:rg-e6be24c0-145d-4b62-8674-639282ebdd13spec:driverName:driver.dellemc.com# Name of the CSI driver (same as provisioner name in StorageClass)action:\"\"# Name of the replication action to be performed on the protection groupprotectionGroupAttributes:localAttributeKey:valueprotectionGroupId:protection-group-id# Identifier of the backing protection group on the Storage ArrayremoteClusterId:tgtClusterID# A unique identifier for the remote Kubernetes ClusterremoteProtectionGroupAttributes:remoteAttributeKey:valueremoteProtectionGroupId:csi-rep-sg-test-5-ASYNC# Identifier for the protection group on the remote Storage ArrayStatus The status sub resource of DellCSIReplicationGroup contains information about the state of replication \u0026 any actions which have been performed on the object.\n   Field Description     state State of the Custom Resource   replicationLinkState State of the replication on the storage arrays   lastAction Result of the last performed action   conditions List of recent conditions the CR instance has gone through    status:conditions:- condition:ActionREPROTECT_REMOTEsucceededtime:\"2021-08-11T12:22:05Z\"- condition:ReplicationLinkState:IsSourcechangedfrom(true)to(false)time:\"2021-08-11T12:18:50Z\"- condition:ActionFAILOVER_REMOTEsucceededtime:\"2021-08-11T12:18:50Z\"lastAction:condition:ActionREPROTECT_REMOTEsucceededtime:\"2021-08-11T12:22:05Z\"replicationLinkState:isSource:falselastSuccessfulUpdate:\"2021-08-11T17:18:12Z\"state:Synchronizedstate:ReadyHere is a diagram representing how the state of the CustomResource changes based on actions CSM Replication sidecar CSM Replication sidecar is deployed as sidecar container in the CSI driver controller pod. This container is similar to Kubernetes CSI Sidecar containers and runs a Controller Manager which manages the following controllers -\n PersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller  The PV \u0026 PVC controllers watch for PV/PVC creation events and use dell-csi-extensions APIs to communicate with the CSI Driver controller plugin to discover/create replication enabled volumes and protection groups on the backend storage array. The PersistentVolume controller then uses these details to create DellCSIReplicationGroup objects in the cluster. These controllers are also responsible for associating the PV \u0026 PVC objects with DellCSIReplicationGroup objects. This association is established by applying annotations \u0026 labels on the PV \u0026 PVC objects.\nThe RG controller manages DellCSIReplicationGroup instances and processes any change requests. It is primarily responsible for the following:\n Perform actions on the protection groups Monitor status of replication Updates to the status sub resource  CSM Replication Controller CSM Replication Controller is a Kubernetes application deployed independently of CSI drivers and is responsible for the communication between Kubernetes clusters.\nThe details about the clusters it needs to connect to are provided in the form of a ConfigMap with references to secrets containing the details(KubeConfig/ServiceAccount tokens) required to connect to the respective clusters.\nIt consists of Controller Manager which manages the following controllers:\n PersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller  The PV controller is responsible for creating PV objects (representing the replicated volumes on the backend storage array) in the remote Kubernetes cluster. This controller also enables deletion of the remote PV object in case it is desired by propagating the deletion request across clusters.\nSimilarly, the RG controller is responsible for creating RG objects in the remote Kubernetes cluster. These RG objects represent the remote protection groups on the backend storage array. This controller can also propagate the deletion request of RG objects across clusters.\nBoth the PV \u0026 RG objects in the remote cluster have extra metadata associated with them in form of annotations \u0026 labels. This metadata includes information about the respective objects in the source cluster.\nThe PVC objects are never replicated across the clusters. Instead, the remote PV objects have annotations related to the source PVC objects. This information can be easily used to create the PVCs whenever required using repctl or even kubectl\nSupported Cluster Topologies Click here for details for the various types of supported cluster topologies\n","excerpt":"Replication design and architecture Container Storage Modules (CSM) …","ref":"/csm-docs/v1/replication/architecture/","title":"Architecture"},{"body":"Replication design and architecture Container Storage Modules (CSM) for Replication project consists of the following components:\n DellCSIReplicationGroup - A Kubernetes Custom Resource CSM Replication controller which replicates the resources across(or within) Kubernetes clusters. CSM Replication sidecar container which is part of the CSI driver controller pod repctl - Multi cluster Kubernetes client for managing replication related objects  DellCSIReplicationGroup DellCSIReplicationGroup (RG) is a cluster scoped Custom Resource that represents a protection group on the backend storage array. It is used to group volumes with the same replication related properties together. DellCSIReplicationGroup's spec contains an action field which can be used to perform replication related operations on the backing protection groups on the storage arrays. This includes operations like Failover, Reprotect, Suspend, Synchronize e.t.c. Any replication related operation is always carried out on all the volumes present in the group.\nSpecification kind:DellCSIReplicationGroupapiVersion:replication.storage.dell.com/v1alpha1metadata:name:rg-e6be24c0-145d-4b62-8674-639282ebdd13spec:driverName:driver.dellemc.com# Name of the CSI driver (same as provisioner name in StorageClass)action:\"\"# Name of the replication action to be performed on the protection groupprotectionGroupAttributes:localAttributeKey:valueprotectionGroupId:protection-group-id# Identifier of the backing protection group on the Storage ArrayremoteClusterId:tgtClusterID# A unique identifier for the remote Kubernetes ClusterremoteProtectionGroupAttributes:remoteAttributeKey:valueremoteProtectionGroupId:csi-rep-sg-test-5-ASYNC# Identifier for the protection group on the remote Storage ArrayStatus The status sub resource of DellCSIReplicationGroup contains information about the state of replication \u0026 any actions which have been performed on the object.\n   Field Description     state State of the Custom Resource   replicationLinkState State of the replication on the storage arrays   lastAction Result of the last performed action   conditions List of recent conditions the CR instance has gone through    status:conditions:- condition:ActionREPROTECT_REMOTEsucceededtime:\"2021-08-11T12:22:05Z\"- condition:ReplicationLinkState:IsSourcechangedfrom(true)to(false)time:\"2021-08-11T12:18:50Z\"- condition:ActionFAILOVER_REMOTEsucceededtime:\"2021-08-11T12:18:50Z\"lastAction:condition:ActionREPROTECT_REMOTEsucceededtime:\"2021-08-11T12:22:05Z\"replicationLinkState:isSource:falselastSuccessfulUpdate:\"2021-08-11T17:18:12Z\"state:Synchronizedstate:ReadyHere is a diagram representing how the state of the CustomResource changes based on actions CSM Replication sidecar CSM Replication sidecar is deployed as sidecar container in the CSI driver controller pod. This container is similar to Kubernetes CSI Sidecar containers and runs a Controller Manager which manages the following controllers -\n PersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller  The PV \u0026 PVC controllers watch for PV/PVC creation events and use dell-csi-extensions APIs to communicate with the CSI Driver controller plugin to discover/create replication enabled volumes and protection groups on the backend storage array. The PersistentVolume controller then uses these details to create DellCSIReplicationGroup objects in the cluster. These controllers are also responsible for associating the PV \u0026 PVC objects with DellCSIReplicationGroup objects. This association is established by applying annotations \u0026 labels on the PV \u0026 PVC objects.\nThe RG controller manages DellCSIReplicationGroup instances and processes any change requests. It is primarily responsible for the following:\n Perform actions on the protection groups Monitor status of replication Updates to the status sub resource  CSM Replication Controller CSM Replication Controller is a Kubernetes application deployed independently of CSI drivers and is responsible for the communication between Kubernetes clusters.\nThe details about the clusters it needs to connect to are provided in the form of a ConfigMap with references to secrets containing the details(KubeConfig/ServiceAccount tokens) required to connect to the respective clusters.\nIt consists of Controller Manager which manages the following controllers:\n PersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller  The PV controller is responsible for creating PV objects (representing the replicated volumes on the backend storage array) in the remote Kubernetes cluster. This controller also enables deletion of the remote PV object in case it is desired by propagating the deletion request across clusters.\nSimilarly, the RG controller is responsible for creating RG objects in the remote Kubernetes cluster. These RG objects represent the remote protection groups on the backend storage array. This controller can also propagate the deletion request of RG objects across clusters.\nBoth the PV \u0026 RG objects in the remote cluster have extra metadata associated with them in form of annotations \u0026 labels. This metadata includes information about the respective objects in the source cluster.\nThe PVC objects are never replicated across the clusters. Instead, the remote PV objects have annotations related to the source PVC objects. This information can be easily used to create the PVCs whenever required using repctl or even kubectl\nSupported Cluster Topologies Click here for details for the various types of supported cluster topologies\n","excerpt":"Replication design and architecture Container Storage Modules (CSM) …","ref":"/csm-docs/v2/replication/architecture/","title":"Architecture"},{"body":"Replication design and architecture Container Storage Modules (CSM) for Replication project consists of the following components:\n DellCSIReplicationGroup - A Kubernetes Custom Resource CSM Replication controller which replicates the resources across(or within) Kubernetes clusters. CSM Replication sidecar container which is part of the CSI driver controller pod repctl - Multi cluster Kubernetes client for managing replication related objects  DellCSIReplicationGroup DellCSIReplicationGroup (RG) is a cluster scoped Custom Resource that represents a protection group on the backend storage array. It is used to group volumes with the same replication related properties together. DellCSIReplicationGroup's spec contains an action field which can be used to perform replication related operations on the backing protection groups on the storage arrays. This includes operations like Failover, Reprotect, Suspend, Synchronize e.t.c. Any replication related operation is always carried out on all the volumes present in the group.\nSpecification kind:DellCSIReplicationGroupapiVersion:replication.storage.dell.com/v1alpha1metadata:name:rg-e6be24c0-145d-4b62-8674-639282ebdd13spec:driverName:driver.dellemc.com# Name of the CSI driver (same as provisioner name in StorageClass)action:\"\"# Name of the replication action to be performed on the protection groupprotectionGroupAttributes:localAttributeKey:valueprotectionGroupId:protection-group-id# Identifier of the backing protection group on the Storage ArrayremoteClusterId:tgtClusterID# A unique identifier for the remote Kubernetes ClusterremoteProtectionGroupAttributes:remoteAttributeKey:valueremoteProtectionGroupId:csi-rep-sg-test-5-ASYNC# Identifier for the protection group on the remote Storage ArrayStatus The status sub resource of DellCSIReplicationGroup contains information about the state of replication \u0026 any actions which have been performed on the object.\n   Field Description     state State of the Custom Resource   replicationLinkState State of the replication on the storage arrays   lastAction Result of the last performed action   conditions List of recent conditions the CR instance has gone through    status:conditions:- condition:ActionREPROTECT_REMOTEsucceededtime:\"2021-08-11T12:22:05Z\"- condition:ReplicationLinkState:IsSourcechangedfrom(true)to(false)time:\"2021-08-11T12:18:50Z\"- condition:ActionFAILOVER_REMOTEsucceededtime:\"2021-08-11T12:18:50Z\"lastAction:condition:ActionREPROTECT_REMOTEsucceededtime:\"2021-08-11T12:22:05Z\"replicationLinkState:isSource:falselastSuccessfulUpdate:\"2021-08-11T17:18:12Z\"state:Synchronizedstate:ReadyHere is a diagram representing how the state of the CustomResource changes based on actions CSM Replication sidecar CSM Replication sidecar is deployed as sidecar container in the CSI driver controller pod. This container is similar to Kubernetes CSI Sidecar containers and runs a Controller Manager which manages the following controllers -\n PersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller  The PV \u0026 PVC controllers watch for PV/PVC creation events and use dell-csi-extensions APIs to communicate with the CSI Driver controller plugin to discover/create replication enabled volumes and protection groups on the backend storage array. The PersistentVolume controller then uses these details to create DellCSIReplicationGroup objects in the cluster. These controllers are also responsible for associating the PV \u0026 PVC objects with DellCSIReplicationGroup objects. This association is established by applying annotations \u0026 labels on the PV \u0026 PVC objects.\nThe RG controller manages DellCSIReplicationGroup instances and processes any change requests. It is primarily responsible for the following:\n Perform actions on the protection groups Monitor status of replication Updates to the status sub resource  CSM Replication Controller CSM Replication Controller is a Kubernetes application deployed independently of CSI drivers and is responsible for the communication between Kubernetes clusters.\nThe details about the clusters it needs to connect to are provided in the form of a ConfigMap with references to secrets containing the details(KubeConfig/ServiceAccount tokens) required to connect to the respective clusters.\nIt consists of Controller Manager which manages the following controllers:\n PersistentVolume(PV) Controller PersistentVolumeClaim(PVC) Controller DellCSIReplicationGroup(RG) Controller  The PV controller is responsible for creating PV objects (representing the replicated volumes on the backend storage array) in the remote Kubernetes cluster. This controller also enables deletion of the remote PV object in case it is desired by propagating the deletion request across clusters.\nSimilarly, the RG controller is responsible for creating RG objects in the remote Kubernetes cluster. These RG objects represent the remote protection groups on the backend storage array. This controller can also propagate the deletion request of RG objects across clusters.\nBoth the PV \u0026 RG objects in the remote cluster have extra metadata associated with them in form of annotations \u0026 labels. This metadata includes information about the respective objects in the source cluster.\nThe PVC objects are never replicated across the clusters. Instead, the remote PV objects have annotations related to the source PVC objects. This information can be easily used to create the PVCs whenever required using repctl or even kubectl\nSupported Cluster Topologies Click here for details for the various types of supported cluster topologies\n","excerpt":"Replication design and architecture Container Storage Modules (CSM) …","ref":"/csm-docs/v3/replication/architecture/","title":"Architecture"},{"body":"csm is a command-line client for installation of Dell Container Storage Modules and CSI Drivers for Kubernetes clusters.\nPre-requisites  Deploy the Container Storage Modules Installer Download/Install the csm binary from Github: https://github.com/dell/csm. Alternatively, you can build the binary by:  cloning the csm repository changing into csm/cmd/csm directory running make build   create a cli_env.sh file that contains the correct values for the below variables. And export the variables by running source ./cli_env.sh  # Change this to CSM API Server IP export API_SERVER_IP=\"127.0.0.1\" # Change this to CSM API Server Port export API_SERVER_PORT=\"31313\" # CSM API Server protocol - allowed values are https \u0026 http export SCHEME=\"https\" # Path to store JWT \u003ctoken\u003e export AUTH_CONFIG_PATH=\"/home/user/installer-token/\" Usage ~$ ./csm -h csm is command line tool for csm application Usage: csm [flags] csm [command] Available Commands: add add cluster, configuration or storage approve-task approve task for application authenticate authenticate user change change - subcommand is password create create application delete delete storage, cluster, configuration or application get get storage, cluster, application, configuration, supported driver, module, storage type help Help about any command reject-task reject task for an application update update storage, configuration or cluster Flags: -h, --help help for csm-cli Use \"csm [command] --help\" for more information about a command. Authenticate the User To begin with, you need to authenticate the user who will be managing the CSM Installer and its components.\n./csm authenticate --username=\u003cadmin-username\u003e --password=\u003cadmin-password\u003e Or more securely, run the above command without --password to be prompted for one\n./csm authenticate --username=\u003cadmin-username\u003e Enter user's password: Change Password To change password follow below command\n./csm change password --username=\u003cadmin-username\u003e View Supported Platforms You can now view the supported DellCSI Drivers\n./csm get supported-drivers You can also view the supported Modules\n./csm get supported-modules And also view the supported Storage Array Types\n./csm get supported-storage-arrays Add a Cluster You can now add a cluster by providing cluster detail name and Kubeconfig path\n./csm add cluster --clustername \u003cdesire-cluster-name\u003e --configfilepath \u003cpath-to-kubeconfig\u003e Upload Configuration Files You can now add a configuration file that can be used for creating application by providing filename and path\n./csm add configuration --filename \u003cname-of-the-desire-file\u003e --filepath \u003cpath-to-the-desired-file\u003e Add a Storage System You can now add storage endpoints, array type and its unique id\n./csm add storage --endpoint \u003cstorage-array-endpoint\u003e --storage-type \u003cstorage-array-type\u003e --unique-id \u003cstorage-array-unique-id\u003e --username \u003cstorage-array-username\u003e The optional --meta-data flag can be used to provide additional meta-data for the storage system that is used when creating Secrets for the CSI Driver. These fields include:\n isDefault: Set to true if this storage system is used as default for multi-array configuration skipCertificateValidation: Set to true to skip certificate validation mdmId: Comma separated list of MDM IPs for PowerFlex nasName: NAS Name for PowerStore blockProtocol: Block Protocol for PowerStore port: Port for PowerScale portGroups: Comma separated list of port group names for PowerMax  Create an Application You may now create an application depending on the specific use case. Below are the common use cases:\n CSI Driver ./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-type\u003e   CSI Driver with CSM Authorization CSM Authorization requires a token.yaml issued by storage Admin from the CSM Authorization Server, a certificate file, and the  of the authorization server. The token.yaml and cert should be added by following the steps in adding configuration file. CSM Authorization does not yet support all CSI Drivers/platforms(See supported platforms documentation or supported platforms via CLI)). Finally, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type authorization:\u003ctag\u003e \\ --module-configuration \"karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSM Observability(Standalone) CSM Observability depends on driver config secret(s) corresponding to the metric(s) you want to enable. Please see CSM Observability for all Supported Metrics. For the sake of demonstration, assuming we want to enable CSM Metrics for PowerFlex, the PowerFlex secret yaml should be added by following the steps in adding configuration file. Once this is done, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type observability:\u003ctag\u003e \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true\"   CSM Observability(Standalone) with CSM Authorization See the individual steps for configuaration file pre-requisites for CSM Observability (Standalone) with CSM Authorization\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type \"observability:\u003ctag\u003e,authorization:\u003ctag\u003e\" \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true,karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSI Driver for Dell PowerMax with reverse proxy module To deploy CSI Driver for Dell PowerMax with reverse proxy module, first upload reverse proxy tls crt and tls key via adding configuration file. Then, use the below command to create application:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powermax:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cpowermax-unique-id\u003e \\ --module-type reverse-proxy:\u003ctag\u003e \\ --module-configuration reverseProxy.tlsSecretKeyFile=\u003crevprotlskey\u003e,reverseProxy.tlsSecretCertFile=\u003crevprotlscert\u003e   CSI Driver with replication module To deploy CSI driver with replication module, first add a target cluster through adding cluster. Then, use the below command(this command is an example to deploy CSI Driver for Dell PowerStore with replication module) to create application::\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerstore:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-configuration target_cluster=\u003ccreated-target-cluster-name\u003e \\ --module-type replication:\u003ctag\u003e   CSI Driver with other module(s) not covered above Assuming you want to deploy a driver with module A and module B. If they have specific configurations of A.image=\"docker:v1\",A.filename=hello, and B.namespace=world.\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type \"module A:\u003ctag\u003e,module B:\u003ctag\u003e\" \\ --module-configuration \"A.image=docker:v1,A.filename=hello,B.namespace=world\"\t  Note:\n  --driver-type and --module-type flags in create application command MUST match the values from the supported CSM platforms Replication module supports only using a pair of clusters at a time (source and a target/or single cluster) from CSM installer, However repctl can be used if needed to add multiple pairs of target clusters. Using replication module with other modules during application creation is not yet supported.  Approve application/task You may now approve the task so that you can continue to work with the application\n./csm approve-task --applicationname \u003ccreated-application-name\u003e Reject application/task You may want to reject a task or application to discontinue the ongoing process\n./csm reject-task --applicationname \u003ccreated-application-name\u003e Delete application/task If you want to delete an application\n./csm delete application --name \u003ccreated-application-name\u003e  Note: When deleting an application, the namespace and Secrets are not deleted. These resources need to be deleted manually. See more in Troubleshooting.\n  Note: All commands and associated syntax can be displayed with -h or –help\n ","excerpt":"csm is a command-line client for installation of Dell Container …","ref":"/csm-docs/docs/deployment/csminstaller/csmcli/","title":"CSM CLI"},{"body":"csm is a command-line client for installation of Dell Container Storage Modules and CSI Drivers for Kubernetes clusters.\nPre-requisites  Deploy the Container Storage Modules Installer Download/Install the csm binary from Github: https://github.com/dell/csm. Alternatively, you can build the binary by:  cloning the csm repository changing into csm/cmd/csm directory running make build   create a cli_env.sh file that contains the correct values for the below variables. And export the variables by running source ./cli_env.sh  # Change this to CSM API Server IP export API_SERVER_IP=\"127.0.0.1\" # Change this to CSM API Server Port export API_SERVER_PORT=\"31313\" # CSM API Server protocol - allowed values are https \u0026 http export SCHEME=\"https\" # Path to store JWT \u003ctoken\u003e export AUTH_CONFIG_PATH=\"/home/user/installer-token/\" Usage ~$ ./csm -h csm is command line tool for csm application Usage: csm [flags] csm [command] Available Commands: add add cluster, configuration or storage approve-task approve task for application authenticate authenticate user change change - subcommand is password create create application delete delete storage, cluster, configuration or application get get storage, cluster, application, configuration, supported driver, module, storage type help Help about any command reject-task reject task for an application update update storage, configuration or cluster Flags: -h, --help help for csm-cli Use \"csm [command] --help\" for more information about a command. Authenticate the User To begin with, you need to authenticate the user who will be managing the CSM Installer and its components.\n./csm authenticate --username=\u003cadmin-username\u003e --password=\u003cadmin-password\u003e Or more securely, run the above command without --password to be prompted for one\n./csm authenticate --username=\u003cadmin-username\u003e Enter user's password: Change Password To change password follow below command\n./csm change password --username=\u003cadmin-username\u003e View Supported Platforms You can now view the supported DellCSI Drivers\n./csm get supported-drivers You can also view the supported Modules\n./csm get supported-modules And also view the supported Storage Array Types\n./csm get supported-storage-arrays Add a Cluster You can now add a cluster by providing cluster detail name and Kubeconfig path\n./csm add cluster --clustername \u003cdesire-cluster-name\u003e --configfilepath \u003cpath-to-kubeconfig\u003e Upload Configuration Files You can now add a configuration file that can be used for creating application by providing filename and path\n./csm add configuration --filename \u003cname-of-the-desire-file\u003e --filepath \u003cpath-to-the-desired-file\u003e Add a Storage System You can now add storage endpoints, array type and its unique id\n./csm add storage --endpoint \u003cstorage-array-endpoint\u003e --storage-type \u003cstorage-array-type\u003e --unique-id \u003cstorage-array-unique-id\u003e --username \u003cstorage-array-username\u003e The optional --meta-data flag can be used to provide additional meta-data for the storage system that is used when creating Secrets for the CSI Driver. These fields include:\n isDefault: Set to true if this storage system is used as default for multi-array configuration skipCertificateValidation: Set to true to skip certificate validation mdmId: Comma separated list of MDM IPs for PowerFlex nasName: NAS Name for PowerStore blockProtocol: Block Protocol for PowerStore port: Port for PowerScale portGroups: Comma separated list of port group names for PowerMax  Create an Application You may now create an application depending on the specific use case. Below are the common use cases:\n CSI Driver ./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-type\u003e   CSI Driver with CSM Authorization CSM Authorization requires a token.yaml issued by storage Admin from the CSM Authorization Server, a certificate file, and the  of the authorization server. The token.yaml and cert should be added by following the steps in adding configuration file. CSM Authorization does not yet support all CSI Drivers/platforms(See supported platforms documentation or supported platforms via CLI)). Finally, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type authorization:\u003ctag\u003e \\ --module-configuration \"karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSM Observability(Standalone) CSM Observability depends on driver config secret(s) corresponding to the metric(s) you want to enable. Please see CSM Observability for all Supported Metrics. For the sake of demonstration, assuming we want to enable CSM Metrics for PowerFlex, the PowerFlex secret yaml should be added by following the steps in adding configuration file. Once this is done, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type observability:\u003ctag\u003e \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true\"   CSM Observability(Standalone) with CSM Authorization See the individual steps for configuaration file pre-requisites for CSM Observability (Standalone) with CSM Authorization\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type \"observability:\u003ctag\u003e,authorization:\u003ctag\u003e\" \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true,karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSI Driver for Dell PowerMax with reverse proxy module To deploy CSI Driver for Dell PowerMax with reverse proxy module, first upload reverse proxy tls crt and tls key via adding configuration file. Then, use the below command to create application:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powermax:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cpowermax-unique-id\u003e \\ --module-type reverse-proxy:\u003ctag\u003e \\ --module-configuration reverseProxy.tlsSecretKeyFile=\u003crevprotlskey\u003e,reverseProxy.tlsSecretCertFile=\u003crevprotlscert\u003e   CSI Driver with replication module To deploy CSI driver with replication module, first add a target cluster through adding cluster. Then, use the below command(this command is an example to deploy CSI Driver for Dell PowerStore with replication module) to create application::\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerstore:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-configuration target_cluster=\u003ccreated-target-cluster-name\u003e \\ --module-type replication:\u003ctag\u003e   CSI Driver with other module(s) not covered above Assuming you want to deploy a driver with module A and module B. If they have specific configurations of A.image=\"docker:v1\",A.filename=hello, and B.namespace=world.\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type \"module A:\u003ctag\u003e,module B:\u003ctag\u003e\" \\ --module-configuration \"A.image=docker:v1,A.filename=hello,B.namespace=world\"\t  Note:\n  --driver-type and --module-type flags in create application command MUST match the values from the supported CSM platforms Replication module supports only using a pair of clusters at a time (source and a target/or single cluster) from CSM installer, However repctl can be used if needed to add multiple pairs of target clusters. Using replication module with other modules during application creation is not yet supported.  Approve application/task You may now approve the task so that you can continue to work with the application\n./csm approve-task --applicationname \u003ccreated-application-name\u003e Reject application/task You may want to reject a task or application to discontinue the ongoing process\n./csm reject-task --applicationname \u003ccreated-application-name\u003e Delete application/task If you want to delete an application\n./csm delete application --name \u003ccreated-application-name\u003e  Note: When deleting an application, the namespace and Secrets are not deleted. These resources need to be deleted manually. See more in Troubleshooting.\n  Note: All commands and associated syntax can be displayed with -h or –help\n ","excerpt":"csm is a command-line client for installation of Dell Container …","ref":"/csm-docs/v1/deployment/csminstaller/csmcli/","title":"CSM CLI"},{"body":"csm is a command-line client for installation of Dell Container Storage Modules and CSI Drivers for Kubernetes clusters.\nPre-requisites  Deploy the Container Storage Modules Installer Download/Install the csm binary from Github: https://github.com/dell/csm. Alternatively, you can build the binary by:  cloning the csm repository changing into csm/cmd/csm directory running make build   create a cli_env.sh file that contains the correct values for the below variables. And export the variables by running source ./cli_env.sh  # Change this to CSM API Server IP export API_SERVER_IP=\"127.0.0.1\" # Change this to CSM API Server Port export API_SERVER_PORT=\"31313\" # CSM API Server protocol - allowed values are https \u0026 http export SCHEME=\"https\" # Path to store JWT \u003ctoken\u003e export AUTH_CONFIG_PATH=\"/home/user/installer-token/\" Usage ~$ ./csm -h csm is command line tool for csm application Usage: csm [flags] csm [command] Available Commands: add add cluster, configuration or storage approve-task approve task for application authenticate authenticate user change change - subcommand is password create create application delete delete storage, cluster, configuration or application get get storage, cluster, application, configuration, supported driver, module, storage type help Help about any command reject-task reject task for an application update update storage, configuration or cluster Flags: -h, --help help for csm-cli Use \"csm [command] --help\" for more information about a command. Authenticate the User To begin with, you need to authenticate the user who will be managing the CSM Installer and its components.\n./csm authenticate --username=\u003cadmin-username\u003e --password=\u003cadmin-password\u003e Or more securely, run the above command without --password to be prompted for one\n./csm authenticate --username=\u003cadmin-username\u003e Enter user's password: Change Password To change password follow below command\n./csm change password --username=\u003cadmin-username\u003e View Supported Platforms You can now view the supported DellCSI Drivers\n./csm get supported-drivers You can also view the supported Modules\n./csm get supported-modules And also view the supported Storage Array Types\n./csm get supported-storage-arrays Add a Cluster You can now add a cluster by providing cluster detail name and Kubeconfig path\n./csm add cluster --clustername \u003cdesire-cluster-name\u003e --configfilepath \u003cpath-to-kubeconfig\u003e Upload Configuration Files You can now add a configuration file that can be used for creating application by providing filename and path\n./csm add configuration --filename \u003cname-of-the-desire-file\u003e --filepath \u003cpath-to-the-desired-file\u003e Add a Storage System You can now add storage endpoints, array type and its unique id\n./csm add storage --endpoint \u003cstorage-array-endpoint\u003e --storage-type \u003cstorage-array-type\u003e --unique-id \u003cstorage-array-unique-id\u003e --username \u003cstorage-array-username\u003e The optional --meta-data flag can be used to provide additional meta-data for the storage system that is used when creating Secrets for the CSI Driver. These fields include:\n isDefault: Set to true if this storage system is used as default for multi-array configuration skipCertificateValidation: Set to true to skip certificate validation mdmId: Comma separated list of MDM IPs for PowerFlex nasName: NAS Name for PowerStore blockProtocol: Block Protocol for PowerStore port: Port for PowerScale portGroups: Comma separated list of port group names for PowerMax  Create an Application You may now create an application depending on the specific use case. Below are the common use cases:\n CSI Driver ./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-type\u003e   CSI Driver with CSM Authorization CSM Authorization requires a token.yaml issued by storage Admin from the CSM Authorization Server, a certificate file, and the  of the authorization server. The token.yaml and cert should be added by following the steps in adding configuration file. CSM Authorization does not yet support all CSI Drivers/platforms(See supported platforms documentation or supported platforms via CLI)). Finally, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type authorization:\u003ctag\u003e \\ --module-configuration \"karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSM Observability(Standalone) CSM Observability depends on driver config secret(s) corresponding to the metric(s) you want to enable. Please see CSM Observability for all Supported Metrics. For the sake of demonstration, assuming we want to enable CSM Metrics for PowerFlex, the PowerFlex secret yaml should be added by following the steps in adding configuration file. Once this is done, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type observability:\u003ctag\u003e \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true\"   CSM Observability(Standalone) with CSM Authorization See the individual steps for configuaration file pre-requisites for CSM Observability (Standalone) with CSM Authorization\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type \"observability:\u003ctag\u003e,authorization:\u003ctag\u003e\" \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true,karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSI Driver for Dell PowerMax with reverse proxy module To deploy CSI Driver for Dell PowerMax with reverse proxy module, first upload reverse proxy tls crt and tls key via adding configuration file. Then, use the below command to create application:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powermax:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cpowermax-unique-id\u003e \\ --module-type reverse-proxy:\u003ctag\u003e \\ --module-configuration reverseProxy.tlsSecretKeyFile=\u003crevprotlskey\u003e,reverseProxy.tlsSecretCertFile=\u003crevprotlscert\u003e   CSI Driver with replication module To deploy CSI driver with replication module, first add a target cluster through adding cluster. Then, use the below command(this command is an example to deploy CSI Driver for Dell PowerStore with replication module) to create application::\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerstore:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-configuration target_cluster=\u003ccreated-target-cluster-name\u003e \\ --module-type replication:\u003ctag\u003e   CSI Driver with other module(s) not covered above Assuming you want to deploy a driver with module A and module B. If they have specific configurations of A.image=\"docker:v1\",A.filename=hello, and B.namespace=world.\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type \"module A:\u003ctag\u003e,module B:\u003ctag\u003e\" \\ --module-configuration \"A.image=docker:v1,A.filename=hello,B.namespace=world\"\t  Note:\n  --driver-type and --module-type flags in create application command MUST match the values from the supported CSM platforms Replication module supports only using a pair of clusters at a time (source and a target/or single cluster) from CSM installer, However repctl can be used if needed to add multiple pairs of target clusters. Using replication module with other modules during application creation is not yet supported.  Approve application/task You may now approve the task so that you can continue to work with the application\n./csm approve-task --applicationname \u003ccreated-application-name\u003e Reject application/task You may want to reject a task or application to discontinue the ongoing process\n./csm reject-task --applicationname \u003ccreated-application-name\u003e Delete application/task If you want to delete an application\n./csm delete application --name \u003ccreated-application-name\u003e  Note: When deleting an application, the namespace and Secrets are not deleted. These resources need to be deleted manually. See more in Troubleshooting.\n  Note: All commands and associated syntax can be displayed with -h or –help\n ","excerpt":"csm is a command-line client for installation of Dell Container …","ref":"/csm-docs/v2/deployment/csminstaller/csmcli/","title":"CSM CLI"},{"body":"csm is a command-line client for installation of Dell EMC Container Storage Modules and CSI Drivers for Kubernetes clusters.\nPre-requisites  Deploy the Container Storage Modules Installer Download/Install the csm binary from Github: https://github.com/dell/csm. Alternatively, you can build the binary by:  cloning the csm repository changing into csm/cmd/csm directory running make build   create a cli_env.sh file that contains the correct values for the below variables. And export the variables by running source ./cli_env.sh  # Change this to CSM API Server IP export API_SERVER_IP=\"127.0.0.1\" # Change this to CSM API Server Port export API_SERVER_PORT=\"31313\" # CSM API Server protocol - allowed values are https \u0026 http export SCHEME=\"https\" # Path to store JWT \u003ctoken\u003e export AUTH_CONFIG_PATH=\"/home/user/installer-token/\" Usage ~$ ./csm -h csm is command line tool for csm application Usage: csm [flags] csm [command] Available Commands: add add cluster, configuration or storage approve-task approve task for application authenticate authenticate user change change - subcommand is password create create application delete delete storage, cluster, configuration or application get get storage, cluster, application, configuration, supported driver, module, storage type help Help about any command reject-task reject task for an application update update storage, configuration or cluster Flags: -h, --help help for csm-cli Use \"csm [command] --help\" for more information about a command. Authenticate the User To begin with, you need to authenticate the user who will be managing the CSM Installer and its components.\n./csm authenticate --username=\u003cadmin-username\u003e --password=\u003cadmin-password\u003e Or more securely, run the above command without --password to be prompted for one\n./csm authenticate --username=\u003cadmin-username\u003e Enter user's password: Change Password To change password follow below command\n./csm change password --username=\u003cadmin-username\u003e View Supported Platforms You can now view the supported Dell emcCSI Drivers\n./csm get supported-drivers You can also view the supported Modules\n./csm get supported-modules And also view the supported Storage Array Types\n./csm get supported-storage-arrays Add a Cluster You can now add a cluster by providing cluster detail name and Kubeconfig path\n./csm add cluster --clustername \u003cdesire-cluster-name\u003e --configfilepath \u003cpath-to-kubeconfig\u003e Upload Configuration Files You can now add a configuration file that can be used for creating application by providing filename and path\n./csm add configuration --filename \u003cname-of-the-desire-file\u003e --filepath \u003cpath-to-the-desired-file\u003e Add a Storage System You can now add storage endpoints, array type and its unique id\n./csm add storage --endpoint \u003cstorage-array-endpoint\u003e --storage-type \u003cstorage-array-type\u003e --unique-id \u003cstorage-array-unique-id\u003e --username \u003cstorage-array-username\u003e The optional --meta-data flag can be used to provide additional meta-data for the storage system that is used when creating Secrets for the CSI Driver. These fields include:\n isDefault: Set to true if this storage system is used as default for multi-array configuration skipCertificateValidation: Set to true to skip certificate validation mdmId: Comma separated list of MDM IPs for PowerFlex nasName: NAS Name for PowerStore blockProtocol: Block Protocol for PowerStore port: Port for PowerScale portGroups: Comma separated list of port group names for PowerMax  Create an Application You may now create an application depending on the specific use case. Below are the common use cases:\n CSI Driver ./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-type\u003e   CSI Driver with CSM Authorization CSM Authorization requires a token.yaml issued by storage Admin from the CSM Authorization Server, a certificate file, and the  of the authorization server. The token.yaml and cert should be added by following the steps in adding configuration file. CSM Authorization does not yet support all CSI Drivers/platforms(See supported platforms documentation or supported platforms via CLI)). Finally, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type authorization:\u003ctag\u003e \\ --module-configuration \"karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSM Observability(Standalone) CSM Observability depends on driver config secret(s) corresponding to the metric(s) you want to enable. Please see CSM Observability for all Supported Metrics. For the sake of demonstration, assuming we want to enable CSM Metrics for PowerFlex, the PowerFlex secret yaml should be added by following the steps in adding configuration file. Once this is done, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type observability:\u003ctag\u003e \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true\"   CSM Observability(Standalone) with CSM Authorization See the individual steps for configuaration file pre-requisites for CSM Observability (Standalone) with CSM Authorization\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type \"observability:\u003ctag\u003e,authorization:\u003ctag\u003e\" \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true,karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSI Driver for Dell EMC PowerMax with reverse proxy module To deploy CSI Driver for Dell EMC PowerMax with reverse proxy module, first upload reverse proxy tls crt and tls key via adding configuration file. Then, use the below command to create application:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powermax:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cpowermax-unique-id\u003e \\ --module-type reverse-proxy:\u003ctag\u003e \\ --module-configuration reverseProxy.tlsSecretKeyFile=\u003crevprotlskey\u003e,reverseProxy.tlsSecretCertFile=\u003crevprotlscert\u003e   CSI Driver with replication module To deploy CSI driver with replication module, first add a target cluster through adding cluster. Then, use the below command(this command is an example to deploy CSI Driver for Dell EMC PowerStore with replication module) to create application::\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerstore:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-configuration target_cluster=\u003ccreated-target-cluster-name\u003e \\ --module-type replication:\u003ctag\u003e   CSI Driver with other module(s) not covered above Assuming you want to deploy a driver with module A and module B. If they have specific configurations of A.image=\"docker:v1\",A.filename=hello, and B.namespace=world.\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type \"module A:\u003ctag\u003e,module B:\u003ctag\u003e\" \\ --module-configuration \"A.image=docker:v1,A.filename=hello,B.namespace=world\"\t  Note:\n  --driver-type and --module-type flags in create application command MUST match the values from the supported CSM platforms Replication module supports only using a pair of clusters at a time (source and a target/or single cluster) from CSM installer, However repctl can be used if needed to add multiple pairs of target clusters. Using replication module with other modules during application creation is not yet supported.  Approve application/task You may now approve the task so that you can continue to work with the application\n./csm approve-task --applicationname \u003ccreated-application-name\u003e Reject application/task You may want to reject a task or application to discontinue the ongoing process\n./csm reject-task --applicationname \u003ccreated-application-name\u003e Delete application/task If you want to delete an application\n./csm delete application --name \u003ccreated-application-name\u003e  Note: When deleting an application, the namespace and Secrets are not deleted. These resources need to be deleted manually. See more in Troubleshooting.\n  Note: All commands and associated syntax can be displayed with -h or –help\n ","excerpt":"csm is a command-line client for installation of Dell EMC Container …","ref":"/csm-docs/v3/deployment/csminstaller/csmcli/","title":"CSM CLI"},{"body":"The CSM Operator can optionally enable modules that are supported by the specific Dell CSI driver. By default, the modules are disabled but they can be enabled by setting any pre-requisite configuration options for the given module and setting the enabled flag to true in the custom resource. The steps include:\n Deploy the Dell CSM Operator (if it is not already deployed). Please follow the instructions available here. Configure any pre-requisite for the desired module(s). See the specific module below for more information Follow the instructions available here) to install the Dell CSI Driver via the CSM Operator. The module section in the ContainerStorageModule CR should be updated to enable the desired module(s). There are sample manifests provided which can be edited to do an easy installation of the driver along with the module.  ","excerpt":"The CSM Operator can optionally enable modules that are supported by …","ref":"/csm-docs/docs/deployment/csmoperator/modules/","title":"CSM Modules"},{"body":"The CSM Operator can optionally enable modules that are supported by the specific Dell CSI driver. By default, the modules are disabled but they can be enabled by setting any pre-requisite configuration options for the given module and setting the enabled flag to true in the custom resource. The steps include:\n Deploy the Dell CSM Operator (if it is not already deployed). Please follow the instructions available here. Configure any pre-requisite for the desired module(s). See the specific module below for more information Follow the instructions available here) to install the Dell CSI Driver via the CSM Operator. The module section in the ContainerStorageModule CR should be updated to enable the desired module(s). There are sample manifests provided which can be edited to do an easy installation of the driver along with the module.  ","excerpt":"The CSM Operator can optionally enable modules that are supported by …","ref":"/csm-docs/v1/deployment/csmoperator/modules/","title":"CSM Modules"},{"body":"","excerpt":"","ref":"/csm-docs/v2/deployment/csmoperator/modules/","title":"CSM Modules"},{"body":"Installation information for CSM Authorization can be found in this section.\n","excerpt":"Installation information for CSM Authorization can be found in this …","ref":"/csm-docs/docs/authorization/deployment/","title":"Deployment"},{"body":"This section outlines the deployment steps for Container Storage Modules (CSM) for Authorization. The deployment of CSM for Authorization is handled in 2 parts:\n Deploying the CSM for Authorization proxy server, to be controlled by storage administrators Configuring one to many supported Dell EMC CSI drivers with CSM for Authorization  Prerequisites The CSM for Authorization proxy server requires a Linux host with the following minimum resource allocations:\n 32 GB of memory 4 CPU 200 GB local storage  Deploying the CSM Authorization Proxy Server The first part deploying CSM for Authorization is installing the proxy server. This activity and the administration of the proxy server will be owned by the storage administrator.\nThe CSM for Authorization proxy server is installed using a single binary installer.\nSingle Binary Installer The easiest way to obtain the single binary installer RPM is directly from the GitHub repository’s releases section.\nThe single binary installer can also be built from source by cloning the GitHub repository and using the following Makefile targets to build the installer:\nmake dist build-installer rpm The build-installer step creates a binary at bin/deploy and embeds all components required for installation. The rpm step generates an RPM package and stores it at deploy/rpm/x86_64/. This allows CSM for Authorization to be installed in network-restricted environments.\nA Storage Administrator can execute the installer or rpm package as a root user or via sudo.\nInstalling the RPM   Before installing the rpm, some network and security configuration inputs need to be provided in json format. The json file should be created in the location $HOME/.karavi/config.json having the following contents:\n{ \"web\": { \"sidecarproxyaddr\": \"docker_registry/sidecar-proxy:latest\", \"jwtsigningsecret\": \"secret\" }, \"proxy\": { \"host\": \":8080\" }, \"zipkin\": { \"collectoruri\": \"http://DNS_host_name:9411/api/v2/spans\", \"probability\": 1 }, \"certificate\": { \"keyFile\": \"path_to_private_key_file\", \"crtFile\": \"path_to_host_cert_file\", \"rootCertificate\": \"path_to_root_CA_file\" }, \"hostName\": \"DNS_host_name\" } In the above template, DNS_host_name refers to the hostname of the system in which the CSM for Authorization server will be installed. This hostname can be found by running the below command on the system:\nnslookup \u003cIP_address\u003e   In order to configure secure grpc connectivity, an additional subdomain in the format grpc.DNS_host_name is also required. All traffic from grpc.DNS_host_name needs to be routed to DNS_host_name address, this can be configured by adding a new DNS entry for grpc.DNS_host_name or providing a temporary path in the /etc/hosts file.\nNOTE: The certificate provided in crtFile should be valid for both the DNS_host_name and the grpc.DNS_host_name address.\nFor example, create the certificate config file with alternate names (to include example.com and grpc.example.com) and then create the .crt file:\n ``` CN = example.com subjectAltName = @alt_names [alt_names] DNS.1 = grpc.example.com openssl x509 -req -in cert_request_file.csr -CA root_CA.pem -CAkey private_key_File.key -CAcreateserial -out example.com.crt -days 365 -sha256 ```    To install the rpm package on the system, run the below command:\nrpm -ivh \u003crpm_file_name\u003e   After installation, application data will be stored on the system under /var/lib/rancher/k3s/storage/.\n  Configuring the CSM for Authorization Proxy Server The storage administrator must first configure the proxy server with the following:\n Storage systems Tenants Roles Bind roles to tenants  Run the following commands on the Authorization proxy server:\n# Specify any desired name export RoleName=\"\" export RoleQuota=\"\" export TenantName=\"\" # Specify info about Array1 export Array1Type=\"\" export Array1SystemID=\"\" export Array1User=\"\" export Array1Password=\"\" export Array1Pool=\"\" export Array1Endpoint=\"\" # Specify info about Array2 export Array2Type=\"\" export Array2SystemID=\"\" export Array2User=\"\" export Array2Password=\"\" export Array2Pool=\"\" export Array2Endpoint=\"\" # Specify IPs export DriverHostVMIP=\"\" export DriverHostVMPassword=\"\" export DriverHostVMUser=\"\" # Specify Authorization host address. NOTE: this is not the same as IP export AuthorizationHost=\"\" echo === Creating Storage(s) === # Add array1 to authorization karavictl storage create \\ --type ${Array1Type} \\ --endpoint ${Array1Endpoint} \\ --system-id ${Array1SystemID} \\ --user ${Array1User} \\ --password ${Array1Password} \\ --insecure # Add array2 to authorization karavictl storage create \\ --type ${Array2Type} \\ --endpoint ${Array2Endpoint} \\ --system-id ${Array2SystemID} \\ --user ${Array2User} \\ --password ${Array2Password} \\ --insecure echo === Creating Tenant === karavictl tenant create -n $TenantName --insecure --addr \"grpc.${AuthorizationHost}\" echo === Creating Role === karavictl role create \\ --role=${RoleName}=${Array1Type}=${Array1SystemID}=${Array1Pool}=${RoleQuota} \\ --role=${RoleName}=${Array2Type}=${Array2SystemID}=${Array2Pool}=${RoleQuota} echo === === Binding Role === karavictl rolebinding create --tenant $TenantName --role $RoleName --insecure --addr \"grpc.${AuthorizationHost}\" Generate a Token After creating the role bindings, the next logical step is to generate the access token. The storage admin is responsible for generating and sending the token to the Kubernetes tenant admin.\necho === Generating token === karavictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationHost}\" | jq -r '.Token' \u003e token.yaml echo === Copy token to Driver Host === sshpass -p $DriverHostPassword scp token.yaml ${DriverHostVMUser}@{DriverHostVMIP}:/tmp/token.yaml Note: The sample above copies the token directly to the Kubernetes cluster master node. The requirement here is that the token must be copied and/or stored in any location accessible to the Kubernetes tenant admin.\nCopy the karavictl Binary to the Kubernetes Master Node The karavictl binary is available from the CSM for Authorization proxy server. This needs to be copied to the Kubernetes master node where Kubernetes tenant admins so they configure the Dell EMC CSI driver with CSM for Authorization.\nsshpass -p dangerous scp bin/karavictl root@10.247.96.174:/tmp/karavictl Note: The storage admin is responsible for copying the binary to a location accessible by the Kubernetes tenant admin.\nConfiguring a Dell EMC CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported. This is controlled by the Kubernetes tenant admin.\nThere are currently 2 ways of doing this:\n Using the CSM Installer (Recommended installation method) Manually by following the steps below  Configuring a Dell EMC CSI Driver Given a setup where Kubernetes, a storage system, CSI driver(s), and CSM for Authorization are deployed, follow the steps below to configure the CSI Drivers to work with the Authorization sidecar:\nRun the following commands on the CSI Driver host\n# Specify Authorization host address. NOTE: this is not the same as IP export AuthorizationHost=\"\" echo === Applying token token === # It is assumed that array type powermax has the namespace \"powermax\" and powerflex has the namepace \"vxflexos\" kubectl apply -f /tmp/token.yaml -n powermax kubectl apply -f /tmp/token.yaml -n vxflexos echo === injecting sidecar in all CSI driver hosts that token has been applied to === sudo curl -k https://${AuthorizationHost}/install | sh # NOTE: you can also query parameters(\"namespace\" and \"proxy-port\") with the curl url if you desire a specific behavior. # 1) For instance, if you want to inject into just powermax, you can run # sudo curl -k https://${AuthorizationHost}/install?namespace=powermax | sh # 2) If you want to specify the proxy-port for powermax to be 900001, you can run # sudo curl -k https://${AuthorizationHost}/install?proxy-port=powermax:900001 | sh # 3) You can mix behaviors # sudo curl -k https://${AuthorizationHost}/install?namespace=powermax\u0026proxy-port=powermax:900001\u0026namespace=vxflexos | sh Updating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\n   Parameter Type Default Description     certificate.crtFile String \"” Path to the host certificate file   certificate.keyFile String \"” Path to the host private key file   certificate.rootCertificate String \"” Path to the root CA file   web.sidecarproxyaddr String “127.0.0.1:5000/sidecar-proxy:latest” Docker registry address of the CSM for Authorization sidecar-proxy   web.jwtsigningsecret String “secret” The secret used to sign JWT tokens    Updating configuration parameters can be done by editing the karavi-config-secret on the CSM for the Authorization Server. The secret can be queried using k3s and kubectl like so:\nk3s kubectl -n karavi get secret/karavi-config-secret\nTo update or add parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nk3s kubectl -n karavi get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d\nSave the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64\nCopy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nk3s kubectl -n karavi edit secret/karavi-config-secret\nReplace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM for Authorization will read the changed secret.\nNote: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command like so:\nkaravictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationHost}\" | jq -r '.Token' \u003e kubectl -n $namespace apply -f -\nCSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM for Authorization logging settings during runtime, run the below command on the K3s cluster, make your changes, and save the updated configmap data.\nk3s kubectl -n karavi edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","excerpt":"This section outlines the deployment steps for Container Storage …","ref":"/csm-docs/v1/authorization/deployment/","title":"Deployment"},{"body":"This section outlines the deployment steps for Container Storage Modules (CSM) for Authorization. The deployment of CSM for Authorization is handled in 2 parts:\n Deploying the CSM for Authorization proxy server, to be controlled by storage administrators Configuring one to many supported Dell CSI drivers with CSM for Authorization  Prerequisites The CSM for Authorization proxy server requires a Linux host with the following minimum resource allocations:\n 32 GB of memory 4 CPU 200 GB local storage  Deploying the CSM Authorization Proxy Server The first part of deploying CSM for Authorization is installing the proxy server. This activity and the administration of the proxy server will be owned by the storage administrator.\nThe CSM for Authorization proxy server is installed using a single binary installer.\nSingle Binary Installer The easiest way to obtain the single binary installer RPM is directly from the GitHub repository’s releases section.\nAlternatively, the single binary installer can be built from source by cloning the GitHub repository and using the following Makefile targets to build the installer:\nmake dist build-installer rpm The build-installer step creates a binary at karavi-authorization/bin/deploy and embeds all components required for installation. The rpm step generates an RPM package and stores it at karavi-authorization/deploy/rpm/x86_64/. This allows CSM for Authorization to be installed in network-restricted environments.\nA Storage Administrator can execute the installer or rpm package as a root user or via sudo.\nInstalling the RPM   Before installing the rpm, some network and security configuration inputs need to be provided in json format. The json file should be created in the location $HOME/.karavi/config.json having the following contents:\n{ \"web\": { \"jwtsigningsecret\": \"secret\" }, \"proxy\": { \"host\": \":8080\" }, \"zipkin\": { \"collectoruri\": \"http://DNS-hostname:9411/api/v2/spans\", \"probability\": 1 }, \"certificate\": { \"keyFile\": \"path_to_private_key_file\", \"crtFile\": \"path_to_host_cert_file\", \"rootCertificate\": \"path_to_root_CA_file\" }, \"hostname\": \"DNS-hostname\" } In an instance where a secure deployment is not required, an insecure deployment is possible. Please note that self-signed certificates will be created for you using cert-manager to allow TLS encryption for communication on the CSM for Authorization proxy server. However, this is not recommended for production environments. For an insecure deployment, the json file in the location $HOME/.karavi/config.json only requires the following contents:\n{ \"hostname\": \"DNS-hostname\" }    Note:\n DNS-hostname refers to the hostname of the system in which the CSM for Authorization server will be installed. This hostname can be found by running nslookup \u003cIP_address\u003e There are a number of ways to create certificates. In a production environment, certificates are usually created and managed by an IT administrator. Otherwise, certificates can be created using OpenSSL.   In order to configure secure grpc connectivity, an additional subdomain in the format grpc.DNS-hostname is also required. All traffic from grpc.DNS-hostname needs to be routed to DNS-hostname address, this can be configured by adding a new DNS entry for grpc.DNS-hostname or providing a temporary path in the systems /etc/hosts file.   Note: The certificate provided in crtFile should be valid for both the DNS-hostname and the grpc.DNS-hostname address.\n For example, create the certificate config file with alternate names (to include DNS-hostname and grpc.DNS-hostname) and then create the .crt file: ``` CN = DNS-hostname subjectAltName = @alt_names [alt_names] DNS.1 = grpc.DNS-hostname.com $ openssl x509 -req -in cert_request_file.csr -CA root_CA.pem -CAkey private_key_File.key -CAcreateserial -out DNS-hostname.com.crt -days 365 -sha256 ```   To install the rpm package on the system, run the below command:\nrpm -ivh \u003crpm_file_name\u003e   After installation, application data will be stored on the system under /var/lib/rancher/k3s/storage/.\n  Configuring the CSM for Authorization Proxy Server The storage administrator must first configure the proxy server with the following:\n Storage systems Tenants Roles Bind roles to tenants  Run the following commands on the Authorization proxy server:\n Note: The --insecure flag is only necessary if certificates were not provided in $HOME/.karavi/config.json.\n # Specify any desired name export RoleName=\"\" export RoleQuota=\"\" export TenantName=\"\" # Specify info about Array1 export Array1Type=\"\" export Array1SystemID=\"\" export Array1User=\"\" export Array1Password=\"\" export Array1Pool=\"\" export Array1Endpoint=\"\" # Specify info about Array2 export Array2Type=\"\" export Array2SystemID=\"\" export Array2User=\"\" export Array2Password=\"\" export Array2Pool=\"\" export Array2Endpoint=\"\" # Specify IPs export DriverHostVMIP=\"\" export DriverHostVMPassword=\"\" export DriverHostVMUser=\"\" # Specify Authorization proxy host address. NOTE: this is not the same as IP export AuthorizationProxyHost=\"\" echo === Creating Storage(s) === # Add array1 to authorization karavictl storage create \\ --type ${Array1Type} \\ --endpoint ${Array1Endpoint} \\ --system-id ${Array1SystemID} \\ --user ${Array1User} \\ --password ${Array1Password} \\ --insecure # Add array2 to authorization karavictl storage create \\ --type ${Array2Type} \\ --endpoint ${Array2Endpoint} \\ --system-id ${Array2SystemID} \\ --user ${Array2User} \\ --password ${Array2Password} \\ --insecure echo === Creating Tenant === karavictl tenant create -n $TenantName --insecure --addr \"grpc.${AuthorizationProxyHost}\" echo === Creating Role === karavictl role create \\ --role=${RoleName}=${Array1Type}=${Array1SystemID}=${Array1Pool}=${RoleQuota} \\ --role=${RoleName}=${Array2Type}=${Array2SystemID}=${Array2Pool}=${RoleQuota} echo === === Binding Role === karavictl rolebinding create --tenant $TenantName --role $RoleName --insecure --addr \"grpc.${AuthorizationProxyHost}\" Generate a Token After creating the role bindings, the next logical step is to generate the access token. The storage admin is responsible for generating and sending the token to the Kubernetes tenant admin.\n Note:\n The --insecure flag is only necessary if certificates were not provided in $HOME/.karavi/config.json. This sample copies the token directly to the Kubernetes cluster master node. The requirement here is that the token must be copied and/or stored in any location accessible to the Kubernetes tenant admin.   echo === Generating token === karavictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationProxyHost}\" | jq -r '.Token' \u003e token.yaml echo === Copy token to Driver Host === sshpass -p $DriverHostPassword scp token.yaml ${DriverHostVMUser}@{DriverHostVMIP}:/tmp/token.yaml Copy the karavictl Binary to the Kubernetes Master Node The karavictl binary is available from the CSM for Authorization proxy server. This needs to be copied to the Kubernetes master node for Kubernetes tenant admins so the Kubernetes tenant admins can configure the Dell CSI driver with CSM for Authorization.\nsshpass -p dangerous scp bin/karavictl root@10.247.96.174:/tmp/karavictl  Note: The storage admin is responsible for copying the binary to a location accessible by the Kubernetes tenant admin.\n Configuring a Dell CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported CSI drivers. This is controlled by the Kubernetes tenant admin.\nConfiguring a Dell CSI Driver Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow the steps below to configure the CSI Drivers to work with the Authorization sidecar:\n  Create the secret token in the namespace of the driver.\n# It is assumed that array type powermax has the namespace \"powermax\", powerflex has the namepace \"vxflexos\", and powerscale has the namespace \"isilon\". kubectl apply -f /tmp/token.yaml -n powermax kubectl apply -f /tmp/token.yaml -n vxflexos kubectl apply -f /tmp/token.yaml -n isilon   Edit the following parameters in samples/secret/karavi-authorization-config.json file in CSI PowerFlex, CSI PowerMax, or CSI PowerScale driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\n     Parameter Description Required Default     username Username for connecting to the backend storage array. This parameter is ignored. No -   password Password for connecting to to the backend storage array. This parameter is ignored. No -   intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes -   endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400   systemID System ID of the backend storage array. Yes \" \"   insecure A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true   isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml    Create the karavi-authorization-config secret using the following command:\nkubectl -n [CSI_DRIVER_NAMESPACE] create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f -\n Note:\n Create the driver secret as you would normally except update/add the connection information for communicating with the sidecar instead of the backend storage array and scrub the username and password For PowerScale, the systemID will be the clusterName of the array.  The isilon-creds secret has a mountEndpoint parameter which must be set to the hostname or IP address of the PowerScale OneFS API server, for example, 10.0.0.1.      Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n [CSI_DRIVER_NAMESPACE] create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f -\nOtherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n [CSI_DRIVER_NAMESPACE] create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f -\n   Note: Follow the steps below for additional configurations to one or more of the supported CSI drivers.\n PowerFlex Please refer to step 5 in the installation steps for PowerFlex to edit the parameters in samples/config.yaml file to communicate with the sidecar.\n  Update endpoint to match the endpoint set in samples/secret/karavi-authorization-config.json\n  Create vxflexos-config secret using the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml -o yaml --dry-run=client | kubectl apply -f -\n  Please refer to step 9 in the installation steps for PowerFlex to edit the parameters in myvalues.yaml file to communicate with the sidecar.\n Enable CSM for Authorization and provide proxyHost address\n  Install the CSI PowerFlex driver\n  PowerMax Please refer to step 7 in the installation steps for PowerMax to edit the parameters in my-powermax-settings.yaml to communicate with the sidecar.\n  Update endpoint to match the endpoint set in samples/secret/karavi-authorization-config.json\n  Enable CSM for Authorization and provide proxyHost address\n  Install the CSI PowerMax driver\n  PowerScale Please refer to step 5 in the installation steps for PowerScale to edit the parameters in my-isilon-settings.yaml to communicate with the sidecar.\n Update endpointPort to match the endpoint port number set in samples/secret/karavi-authorization-config.json  Notes:\n  In my-isilon-settings.yaml, endpointPort acts as a default value. If endpointPort is not specified in my-isilon-settings.yaml, then it should be specified in the endpoint parameter of samples/secret/secret.yaml. The isilon-creds secret has a mountEndpoint parameter which must be set to the hostname or IP address of the PowerScale OneFS API server, for example, 10.0.0.1.   Enable CSM for Authorization and provide proxyHost address  Please refer to step 6 in the installation steps for PowerScale to edit the parameters in samples/secret/secret.yaml file to communicate with the sidecar.\nUpdate endpoint to match the endpoint set in samples/secret/karavi-authorization-config.json   Note: Only add the endpoint port if it has not been set in my-isilon-settings.yaml.\n  Create the isilon-creds secret using the following command:\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\n  Install the CSI PowerScale driver\n  Updating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\n   Parameter Type Default Description     certificate.crtFile String \"” Path to the host certificate file   certificate.keyFile String \"” Path to the host private key file   certificate.rootCertificate String \"” Path to the root CA file   web.jwtsigningsecret String “secret” The secret used to sign JWT tokens    Updating configuration parameters can be done by editing the karavi-config-secret on the CSM for the Authorization Server. The secret can be queried using k3s and kubectl like so:\nk3s kubectl -n karavi get secret/karavi-config-secret\nTo update or add parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nk3s kubectl -n karavi get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d\nSave the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64\nCopy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nk3s kubectl -n karavi edit secret/karavi-config-secret\nReplace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM for Authorization will read the changed secret.\n Note: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command like so. The --insecure flag is only necessary if certificates were not provided in $HOME/.karavi/config.json\n karavictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationProxyHost}\" | jq -r '.Token' \u003e kubectl -n $namespace apply -f -\nCSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM for Authorization logging settings during runtime, run the below command on the K3s cluster, make your changes, and save the updated configmap data.\nk3s kubectl -n karavi edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","excerpt":"This section outlines the deployment steps for Container Storage …","ref":"/csm-docs/v1/authorization/deployment/","title":"Deployment"},{"body":"This section outlines the deployment steps for Container Storage Modules (CSM) for Authorization. The deployment of CSM for Authorization is handled in 2 parts:\n Deploying the CSM for Authorization proxy server, to be controlled by storage administrators Configuring one to many supported Dell EMC CSI drivers with CSM for Authorization  Prerequisites The CSM for Authorization proxy server requires a Linux host with the following minimum resource allocations:\n 32 GB of memory 4 CPU 200 GB local storage  Deploying the CSM Authorization Proxy Server The first part deploying CSM for Authorization is installing the proxy server. This activity and the administration of the proxy server will be owned by the storage administrator.\nThe CSM for Authorization proxy server is installed using a single binary installer.\nSingle Binary Installer The easiest way to obtain the single binary installer RPM is directly from the GitHub repository’s releases section.\nThe single binary installer can also be built from source by cloning the GitHub repository and using the following Makefile targets to build the installer:\nmake dist build-installer rpm The build-installer step creates a binary at bin/deploy and embeds all components required for installation. The rpm step generates an RPM package and stores it at deploy/rpm/x86_64/. This allows CSM for Authorization to be installed in network-restricted environments.\nA Storage Administrator can execute the installer or rpm package as a root user or via sudo.\nInstalling the RPM   Before installing the rpm, some network and security configuration inputs need to be provided in json format. The json file should be created in the location $HOME/.karavi/config.json having the following contents:\n{ \"web\": { \"sidecarproxyaddr\": \"docker_registry/sidecar-proxy:latest\", \"jwtsigningsecret\": \"secret\" }, \"proxy\": { \"host\": \":8080\" }, \"zipkin\": { \"collectoruri\": \"http://DNS_host_name:9411/api/v2/spans\", \"probability\": 1 }, \"certificate\": { \"keyFile\": \"path_to_private_key_file\", \"crtFile\": \"path_to_host_cert_file\", \"rootCertificate\": \"path_to_root_CA_file\" }, \"hostName\": \"DNS_host_name\" } In the above template, DNS_host_name refers to the hostname of the system in which the CSM for Authorization server will be installed. This hostname can be found by running the below command on the system:\nnslookup \u003cIP_address\u003e   In order to configure secure grpc connectivity, an additional subdomain in the format grpc.DNS_host_name is also required. All traffic from grpc.DNS_host_name needs to be routed to DNS_host_name address, this can be configured by adding a new DNS entry for grpc.DNS_host_name or providing a temporary path in the /etc/hosts file.\nNOTE: The certificate provided in crtFile should be valid for both the DNS_host_name and the grpc.DNS_host_name address.\nFor example, create the certificate config file with alternate names (to include example.com and grpc.example.com) and then create the .crt file:\n ``` CN = example.com subjectAltName = @alt_names [alt_names] DNS.1 = grpc.example.com openssl x509 -req -in cert_request_file.csr -CA root_CA.pem -CAkey private_key_File.key -CAcreateserial -out example.com.crt -days 365 -sha256 ```    To install the rpm package on the system, run the below command:\nrpm -ivh \u003crpm_file_name\u003e   After installation, application data will be stored on the system under /var/lib/rancher/k3s/storage/.\n  Configuring the CSM for Authorization Proxy Server The storage administrator must first configure the proxy server with the following:\n Storage systems Tenants Roles Bind roles to tenants  Run the following commands on the Authorization proxy server:\n# Specify any desired name export RoleName=\"\" export RoleQuota=\"\" export TenantName=\"\" # Specify info about Array1 export Array1Type=\"\" export Array1SystemID=\"\" export Array1User=\"\" export Array1Password=\"\" export Array1Pool=\"\" export Array1Endpoint=\"\" # Specify info about Array2 export Array2Type=\"\" export Array2SystemID=\"\" export Array2User=\"\" export Array2Password=\"\" export Array2Pool=\"\" export Array2Endpoint=\"\" # Specify IPs export DriverHostVMIP=\"\" export DriverHostVMPassword=\"\" export DriverHostVMUser=\"\" # Specify Authorization host address. NOTE: this is not the same as IP export AuthorizationHost=\"\" echo === Creating Storage(s) === # Add array1 to authorization karavictl storage create \\ --type ${Array1Type} \\ --endpoint ${Array1Endpoint} \\ --system-id ${Array1SystemID} \\ --user ${Array1User} \\ --password ${Array1Password} \\ --insecure # Add array2 to authorization karavictl storage create \\ --type ${Array2Type} \\ --endpoint ${Array2Endpoint} \\ --system-id ${Array2SystemID} \\ --user ${Array2User} \\ --password ${Array2Password} \\ --insecure echo === Creating Tenant === karavictl tenant create -n $TenantName --insecure --addr \"grpc.${AuthorizationHost}\" echo === Creating Role === karavictl role create \\ --role=${RoleName}=${Array1Type}=${Array1SystemID}=${Array1Pool}=${RoleQuota} \\ --role=${RoleName}=${Array2Type}=${Array2SystemID}=${Array2Pool}=${RoleQuota} echo === === Binding Role === karavictl rolebinding create --tenant $TenantName --role $RoleName --insecure --addr \"grpc.${AuthorizationHost}\" Generate a Token After creating the role bindings, the next logical step is to generate the access token. The storage admin is responsible for generating and sending the token to the Kubernetes tenant admin.\necho === Generating token === karavictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationHost}\" | jq -r '.Token' \u003e token.yaml echo === Copy token to Driver Host === sshpass -p $DriverHostPassword scp token.yaml ${DriverHostVMUser}@{DriverHostVMIP}:/tmp/token.yaml Note: The sample above copies the token directly to the Kubernetes cluster master node. The requirement here is that the token must be copied and/or stored in any location accessible to the Kubernetes tenant admin.\nCopy the karavictl Binary to the Kubernetes Master Node The karavictl binary is available from the CSM for Authorization proxy server. This needs to be copied to the Kubernetes master node where Kubernetes tenant admins so they configure the Dell EMC CSI driver with CSM for Authorization.\nsshpass -p dangerous scp bin/karavictl root@10.247.96.174:/tmp/karavictl Note: The storage admin is responsible for copying the binary to a location accessible by the Kubernetes tenant admin.\nConfiguring a Dell EMC CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported. This is controlled by the Kubernetes tenant admin.\nThere are currently 2 ways of doing this:\n Using the CSM Installer (Recommended installation method) Manually by following the steps below  Configuring a Dell EMC CSI Driver Given a setup where Kubernetes, a storage system, CSI driver(s), and CSM for Authorization are deployed, follow the steps below to configure the CSI Drivers to work with the Authorization sidecar:\nRun the following commands on the CSI Driver host\n# Specify Authorization host address. NOTE: this is not the same as IP export AuthorizationHost=\"\" echo === Applying token token === # It is assumed that array type powermax has the namespace \"powermax\" and powerflex has the namepace \"vxflexos\" kubectl apply -f /tmp/token.yaml -n powermax kubectl apply -f /tmp/token.yaml -n vxflexos echo === injecting sidecar in all CSI driver hosts that token has been applied to === sudo curl -k https://${AuthorizationHost}/install | sh # NOTE: you can also query parameters(\"namespace\" and \"proxy-port\") with the curl url if you desire a specific behavior. # 1) For instance, if you want to inject into just powermax, you can run # sudo curl -k https://${AuthorizationHost}/install?namespace=powermax | sh # 2) If you want to specify the proxy-port for powermax to be 900001, you can run # sudo curl -k https://${AuthorizationHost}/install?proxy-port=powermax:900001 | sh # 3) You can mix behaviors # sudo curl -k https://${AuthorizationHost}/install?namespace=powermax\u0026proxy-port=powermax:900001\u0026namespace=vxflexos | sh Updating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\n   Parameter Type Default Description     certificate.crtFile String \"” Path to the host certificate file   certificate.keyFile String \"” Path to the host private key file   certificate.rootCertificate String \"” Path to the root CA file   web.sidecarproxyaddr String “127.0.0.1:5000/sidecar-proxy:latest” Docker registry address of the CSM for Authorization sidecar-proxy   web.jwtsigningsecret String “secret” The secret used to sign JWT tokens    Updating configuration parameters can be done by editing the karavi-config-secret on the CSM for the Authorization Server. The secret can be queried using k3s and kubectl like so:\nk3s kubectl -n karavi get secret/karavi-config-secret\nTo update or add parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nk3s kubectl -n karavi get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d\nSave the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64\nCopy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nk3s kubectl -n karavi edit secret/karavi-config-secret\nReplace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM for Authorization will read the changed secret.\nNote: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command like so:\nkaravictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationHost}\" | jq -r '.Token' \u003e kubectl -n $namespace apply -f -\nCSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM for Authorization logging settings during runtime, run the below command on the K3s cluster, make your changes, and save the updated configmap data.\nk3s kubectl -n karavi edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","excerpt":"This section outlines the deployment steps for Container Storage …","ref":"/csm-docs/v2/authorization/deployment/","title":"Deployment"},{"body":"This section outlines the deployment steps for Container Storage Modules (CSM) for Authorization. The deployment of CSM for Authorization is handled in 2 parts:\n Deploying the CSM for Authorization proxy server, to be controlled by storage administrators Configuring one to many supported Dell CSI drivers with CSM for Authorization  Prerequisites The CSM for Authorization proxy server requires a Linux host with the following minimum resource allocations:\n 32 GB of memory 4 CPU 200 GB local storage  Deploying the CSM Authorization Proxy Server The first part of deploying CSM for Authorization is installing the proxy server. This activity and the administration of the proxy server will be owned by the storage administrator.\nThe CSM for Authorization proxy server is installed using a single binary installer.\nSingle Binary Installer The easiest way to obtain the single binary installer RPM is directly from the GitHub repository’s releases section.\nAlternatively, the single binary installer can be built from source by cloning the GitHub repository and using the following Makefile targets to build the installer:\nmake dist build-installer rpm The build-installer step creates a binary at karavi-authorization/bin/deploy and embeds all components required for installation. The rpm step generates an RPM package and stores it at karavi-authorization/deploy/rpm/x86_64/. This allows CSM for Authorization to be installed in network-restricted environments.\nA Storage Administrator can execute the installer or rpm package as a root user or via sudo.\nInstalling the RPM   Before installing the rpm, some network and security configuration inputs need to be provided in json format. The json file should be created in the location $HOME/.karavi/config.json having the following contents:\n{ \"web\": { \"jwtsigningsecret\": \"secret\" }, \"proxy\": { \"host\": \":8080\" }, \"zipkin\": { \"collectoruri\": \"http://DNS-hostname:9411/api/v2/spans\", \"probability\": 1 }, \"certificate\": { \"keyFile\": \"path_to_private_key_file\", \"crtFile\": \"path_to_host_cert_file\", \"rootCertificate\": \"path_to_root_CA_file\" }, \"hostname\": \"DNS-hostname\" } In an instance where a secure deployment is not required, an insecure deployment is possible. Please note that self-signed certificates will be created for you using cert-manager to allow TLS encryption for communication on the CSM for Authorization proxy server. However, this is not recommended for production environments. For an insecure deployment, the json file in the location $HOME/.karavi/config.json only requires the following contents:\n{ \"hostname\": \"DNS-hostname\" }    Note:\n DNS-hostname refers to the hostname of the system in which the CSM for Authorization server will be installed. This hostname can be found by running nslookup \u003cIP_address\u003e There are a number of ways to create certificates. In a production environment, certificates are usually created and managed by an IT administrator. Otherwise, certificates can be created using OpenSSL.   In order to configure secure grpc connectivity, an additional subdomain in the format grpc.DNS-hostname is also required. All traffic from grpc.DNS-hostname needs to be routed to DNS-hostname address, this can be configured by adding a new DNS entry for grpc.DNS-hostname or providing a temporary path in the systems /etc/hosts file.   Note: The certificate provided in crtFile should be valid for both the DNS-hostname and the grpc.DNS-hostname address.\n For example, create the certificate config file with alternate names (to include DNS-hostname and grpc.DNS-hostname) and then create the .crt file: ``` CN = DNS-hostname subjectAltName = @alt_names [alt_names] DNS.1 = grpc.DNS-hostname.com $ openssl x509 -req -in cert_request_file.csr -CA root_CA.pem -CAkey private_key_File.key -CAcreateserial -out DNS-hostname.com.crt -days 365 -sha256 ```   To install the rpm package on the system, run the below command:\nrpm -ivh \u003crpm_file_name\u003e   After installation, application data will be stored on the system under /var/lib/rancher/k3s/storage/.\n  Configuring the CSM for Authorization Proxy Server The storage administrator must first configure the proxy server with the following:\n Storage systems Tenants Roles Bind roles to tenants  Run the following commands on the Authorization proxy server:\n Note: The --insecure flag is only necessary if certificates were not provided in $HOME/.karavi/config.json.\n # Specify any desired name export RoleName=\"\" export RoleQuota=\"\" export TenantName=\"\" # Specify info about Array1 export Array1Type=\"\" export Array1SystemID=\"\" export Array1User=\"\" export Array1Password=\"\" export Array1Pool=\"\" export Array1Endpoint=\"\" # Specify info about Array2 export Array2Type=\"\" export Array2SystemID=\"\" export Array2User=\"\" export Array2Password=\"\" export Array2Pool=\"\" export Array2Endpoint=\"\" # Specify IPs export DriverHostVMIP=\"\" export DriverHostVMPassword=\"\" export DriverHostVMUser=\"\" # Specify Authorization proxy host address. NOTE: this is not the same as IP export AuthorizationProxyHost=\"\" echo === Creating Storage(s) === # Add array1 to authorization karavictl storage create \\ --type ${Array1Type} \\ --endpoint ${Array1Endpoint} \\ --system-id ${Array1SystemID} \\ --user ${Array1User} \\ --password ${Array1Password} \\ --insecure # Add array2 to authorization karavictl storage create \\ --type ${Array2Type} \\ --endpoint ${Array2Endpoint} \\ --system-id ${Array2SystemID} \\ --user ${Array2User} \\ --password ${Array2Password} \\ --insecure echo === Creating Tenant === karavictl tenant create -n $TenantName --insecure --addr \"grpc.${AuthorizationProxyHost}\" echo === Creating Role === karavictl role create \\ --role=${RoleName}=${Array1Type}=${Array1SystemID}=${Array1Pool}=${RoleQuota} \\ --role=${RoleName}=${Array2Type}=${Array2SystemID}=${Array2Pool}=${RoleQuota} echo === === Binding Role === karavictl rolebinding create --tenant $TenantName --role $RoleName --insecure --addr \"grpc.${AuthorizationProxyHost}\" Generate a Token After creating the role bindings, the next logical step is to generate the access token. The storage admin is responsible for generating and sending the token to the Kubernetes tenant admin.\n Note:\n The --insecure flag is only necessary if certificates were not provided in $HOME/.karavi/config.json. This sample copies the token directly to the Kubernetes cluster master node. The requirement here is that the token must be copied and/or stored in any location accessible to the Kubernetes tenant admin.   echo === Generating token === karavictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationProxyHost}\" | jq -r '.Token' \u003e token.yaml echo === Copy token to Driver Host === sshpass -p $DriverHostPassword scp token.yaml ${DriverHostVMUser}@{DriverHostVMIP}:/tmp/token.yaml Copy the karavictl Binary to the Kubernetes Master Node The karavictl binary is available from the CSM for Authorization proxy server. This needs to be copied to the Kubernetes master node for Kubernetes tenant admins so the Kubernetes tenant admins can configure the Dell CSI driver with CSM for Authorization.\nsshpass -p dangerous scp bin/karavictl root@10.247.96.174:/tmp/karavictl  Note: The storage admin is responsible for copying the binary to a location accessible by the Kubernetes tenant admin.\n Configuring a Dell CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported CSI drivers. This is controlled by the Kubernetes tenant admin.\nConfiguring a Dell CSI Driver Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow the steps below to configure the CSI Drivers to work with the Authorization sidecar:\n  Create the secret token in the namespace of the driver.\n# It is assumed that array type powermax has the namespace \"powermax\", powerflex has the namepace \"vxflexos\", and powerscale has the namespace \"isilon\". kubectl apply -f /tmp/token.yaml -n powermax kubectl apply -f /tmp/token.yaml -n vxflexos kubectl apply -f /tmp/token.yaml -n isilon   Edit the following parameters in samples/secret/karavi-authorization-config.json file in CSI PowerFlex, CSI PowerMax, or CSI PowerScale driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\n     Parameter Description Required Default     username Username for connecting to the backend storage array. This parameter is ignored. No -   password Password for connecting to to the backend storage array. This parameter is ignored. No -   intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes -   endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400   systemID System ID of the backend storage array. Yes \" \"   insecure A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true   isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml    Create the karavi-authorization-config secret using the following command:\nkubectl -n [CSI_DRIVER_NAMESPACE] create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f -\n Note:\n Create the driver secret as you would normally except update/add the connection information for communicating with the sidecar instead of the backend storage array and scrub the username and password For PowerScale, the systemID will be the clusterName of the array.  The isilon-creds secret has a mountEndpoint parameter which must be set to the hostname or IP address of the PowerScale OneFS API server, for example, 10.0.0.1.      Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n [CSI_DRIVER_NAMESPACE] create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f -\nOtherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n [CSI_DRIVER_NAMESPACE] create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f -\n   Note: Follow the steps below for additional configurations to one or more of the supported CSI drivers.\n PowerFlex Please refer to step 5 in the installation steps for PowerFlex to edit the parameters in samples/config.yaml file to communicate with the sidecar.\n  Update endpoint to match the endpoint set in samples/secret/karavi-authorization-config.json\n  Create vxflexos-config secret using the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml -o yaml --dry-run=client | kubectl apply -f -\n  Please refer to step 9 in the installation steps for PowerFlex to edit the parameters in myvalues.yaml file to communicate with the sidecar.\n Enable CSM for Authorization and provide proxyHost address\n  Install the CSI PowerFlex driver\n  PowerMax Please refer to step 7 in the installation steps for PowerMax to edit the parameters in my-powermax-settings.yaml to communicate with the sidecar.\n  Update endpoint to match the endpoint set in samples/secret/karavi-authorization-config.json\n  Enable CSM for Authorization and provide proxyHost address\n  Install the CSI PowerMax driver\n  PowerScale Please refer to step 5 in the installation steps for PowerScale to edit the parameters in my-isilon-settings.yaml to communicate with the sidecar.\n Update endpointPort to match the endpoint port number set in samples/secret/karavi-authorization-config.json  Notes:\n  In my-isilon-settings.yaml, endpointPort acts as a default value. If endpointPort is not specified in my-isilon-settings.yaml, then it should be specified in the endpoint parameter of samples/secret/secret.yaml. The isilon-creds secret has a mountEndpoint parameter which must be set to the hostname or IP address of the PowerScale OneFS API server, for example, 10.0.0.1.   Enable CSM for Authorization and provide proxyHost address  Please refer to step 6 in the installation steps for PowerScale to edit the parameters in samples/secret/secret.yaml file to communicate with the sidecar.\nUpdate endpoint to match the endpoint set in samples/secret/karavi-authorization-config.json   Note: Only add the endpoint port if it has not been set in my-isilon-settings.yaml.\n  Create the isilon-creds secret using the following command:\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\n  Install the CSI PowerScale driver\n  Updating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\n   Parameter Type Default Description     certificate.crtFile String \"” Path to the host certificate file   certificate.keyFile String \"” Path to the host private key file   certificate.rootCertificate String \"” Path to the root CA file   web.jwtsigningsecret String “secret” The secret used to sign JWT tokens    Updating configuration parameters can be done by editing the karavi-config-secret on the CSM for the Authorization Server. The secret can be queried using k3s and kubectl like so:\nk3s kubectl -n karavi get secret/karavi-config-secret\nTo update or add parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nk3s kubectl -n karavi get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d\nSave the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64\nCopy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nk3s kubectl -n karavi edit secret/karavi-config-secret\nReplace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM for Authorization will read the changed secret.\n Note: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command like so. The --insecure flag is only necessary if certificates were not provided in $HOME/.karavi/config.json\n karavictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationProxyHost}\" | jq -r '.Token' \u003e kubectl -n $namespace apply -f -\nCSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM for Authorization logging settings during runtime, run the below command on the K3s cluster, make your changes, and save the updated configmap data.\nk3s kubectl -n karavi edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","excerpt":"This section outlines the deployment steps for Container Storage …","ref":"/csm-docs/v2/authorization/deployment/","title":"Deployment"},{"body":"This section outlines the deployment steps for Container Storage Modules (CSM) for Authorization. The deployment of CSM for Authorization is handled in 2 parts:\n Deploying the CSM for Authorization proxy server, to be controlled by storage administrators Configuring one to many supported Dell EMC CSI drivers with CSM for Authorization  Prerequisites The CSM for Authorization proxy server requires a Linux host with the following minimum resource allocations:\n 32 GB of memory 4 CPU 200 GB local storage  Deploying the CSM Authorization Proxy Server The first part deploying CSM for Authorization is installing the proxy server. This activity and the administration of the proxy server will be owned by the storage administrator.\nThe CSM for Authorization proxy server is installed using a single binary installer.\nSingle Binary Installer The easiest way to obtain the single binary installer RPM is directly from the GitHub repository’s releases section.\nThe single binary installer can also be built from source by cloning the GitHub repository and using the following Makefile targets to build the installer:\nmake dist build-installer rpm The build-installer step creates a binary at bin/deploy and embeds all components required for installation. The rpm step generates an RPM package and stores it at deploy/rpm/x86_64/. This allows CSM for Authorization to be installed in network-restricted environments.\nA Storage Administrator can execute the installer or rpm package as a root user or via sudo.\nInstalling the RPM   Before installing the rpm, some network and security configuration inputs need to be provided in json format. The json file should be created in the location $HOME/.karavi/config.json having the following contents:\n{ \"web\": { \"sidecarproxyaddr\": \"docker_registry/sidecar-proxy:latest\", \"jwtsigningsecret\": \"secret\" }, \"proxy\": { \"host\": \":8080\" }, \"zipkin\": { \"collectoruri\": \"http://DNS_host_name:9411/api/v2/spans\", \"probability\": 1 }, \"certificate\": { \"keyFile\": \"path_to_private_key_file\", \"crtFile\": \"path_to_host_cert_file\", \"rootCertificate\": \"path_to_root_CA_file\" }, \"hostName\": \"DNS_host_name\" } In the above template, DNS_host_name refers to the hostname of the system in which the CSM for Authorization server will be installed. This hostname can be found by running the below command on the system:\nnslookup \u003cIP_address\u003e   In order to configure secure grpc connectivity, an additional subdomain in the format grpc.DNS_host_name is also required. All traffic from grpc.DNS_host_name needs to be routed to DNS_host_name address, this can be configured by adding a new DNS entry for grpc.DNS_host_name or providing a temporary path in the /etc/hosts file.\nNOTE: The certificate provided in crtFile should be valid for both the DNS_host_name and the grpc.DNS_host_name address.\nFor example, create the certificate config file with alternate names (to include example.com and grpc.example.com) and then create the .crt file:\n ``` CN = example.com subjectAltName = @alt_names [alt_names] DNS.1 = grpc.example.com openssl x509 -req -in cert_request_file.csr -CA root_CA.pem -CAkey private_key_File.key -CAcreateserial -out example.com.crt -days 365 -sha256 ```    To install the rpm package on the system, run the below command:\nrpm -ivh \u003crpm_file_name\u003e   After installation, application data will be stored on the system under /var/lib/rancher/k3s/storage/.\n  Configuring the CSM for Authorization Proxy Server The storage administrator must first configure the proxy server with the following:\n Storage systems Tenants Roles Bind roles to tenants  Run the following commands on the Authorization proxy server:\n# Specify any desired name export RoleName=\"\" export RoleQuota=\"\" export TenantName=\"\" # Specify info about Array1 export Array1Type=\"\" export Array1SystemID=\"\" export Array1User=\"\" export Array1Password=\"\" export Array1Pool=\"\" export Array1Endpoint=\"\" # Specify info about Array2 export Array2Type=\"\" export Array2SystemID=\"\" export Array2User=\"\" export Array2Password=\"\" export Array2Pool=\"\" export Array2Endpoint=\"\" # Specify IPs export DriverHostVMIP=\"\" export DriverHostVMPassword=\"\" export DriverHostVMUser=\"\" # Specify Authorization host address. NOTE: this is not the same as IP export AuthorizationHost=\"\" echo === Creating Storage(s) === # Add array1 to authorization karavictl storage create \\ --type ${Array1Type} \\ --endpoint ${Array1Endpoint} \\ --system-id ${Array1SystemID} \\ --user ${Array1User} \\ --password ${Array1Password} \\ --insecure # Add array2 to authorization karavictl storage create \\ --type ${Array2Type} \\ --endpoint ${Array2Endpoint} \\ --system-id ${Array2SystemID} \\ --user ${Array2User} \\ --password ${Array2Password} \\ --insecure echo === Creating Tenant === karavictl tenant create -n $TenantName --insecure --addr \"grpc.${AuthorizationHost}\" echo === Creating Role === karavictl role create \\ --role=${RoleName}=${Array1Type}=${Array1SystemID}=${Array1Pool}=${RoleQuota} \\ --role=${RoleName}=${Array2Type}=${Array2SystemID}=${Array2Pool}=${RoleQuota} echo === === Binding Role === karavictl rolebinding create --tenant $TenantName --role $RoleName --insecure --addr \"grpc.${AuthorizationHost}\" Generate a Token After creating the role bindings, the next logical step is to generate the access token. The storage admin is responsible for generating and sending the token to the Kubernetes tenant admin.\necho === Generating token === karavictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationHost}\" | jq -r '.Token' \u003e token.yaml echo === Copy token to Driver Host === sshpass -p $DriverHostPassword scp token.yaml ${DriverHostVMUser}@{DriverHostVMIP}:/tmp/token.yaml Note: The sample above copies the token directly to the Kubernetes cluster master node. The requirement here is that the token must be copied and/or stored in any location accessible to the Kubernetes tenant admin.\nCopy the karavictl Binary to the Kubernetes Master Node The karavictl binary is available from the CSM for Authorization proxy server. This needs to be copied to the Kubernetes master node where Kubernetes tenant admins so they configure the Dell EMC CSI driver with CSM for Authorization.\nsshpass -p dangerous scp bin/karavictl root@10.247.96.174:/tmp/karavictl Note: The storage admin is responsible for copying the binary to a location accessible by the Kubernetes tenant admin.\nConfiguring a Dell EMC CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported. This is controlled by the Kubernetes tenant admin.\nThere are currently 2 ways of doing this:\n Using the CSM Installer (Recommended installation method) Manually by following the steps below  Configuring a Dell EMC CSI Driver Given a setup where Kubernetes, a storage system, CSI driver(s), and CSM for Authorization are deployed, follow the steps below to configure the CSI Drivers to work with the Authorization sidecar:\nRun the following commands on the CSI Driver host\n# Specify Authorization host address. NOTE: this is not the same as IP export AuthorizationHost=\"\" echo === Applying token token === # It is assumed that array type powermax has the namespace \"powermax\" and powerflex has the namepace \"vxflexos\" kubectl apply -f /tmp/token.yaml -n powermax kubectl apply -f /tmp/token.yaml -n vxflexos echo === injecting sidecar in all CSI driver hosts that token has been applied to === sudo curl -k https://${AuthorizationHost}/install | sh # NOTE: you can also query parameters(\"namespace\" and \"proxy-port\") with the curl url if you desire a specific behavior. # 1) For instance, if you want to inject into just powermax, you can run # sudo curl -k https://${AuthorizationHost}/install?namespace=powermax | sh # 2) If you want to specify the proxy-port for powermax to be 900001, you can run # sudo curl -k https://${AuthorizationHost}/install?proxy-port=powermax:900001 | sh # 3) You can mix behaviors # sudo curl -k https://${AuthorizationHost}/install?namespace=powermax\u0026proxy-port=powermax:900001\u0026namespace=vxflexos | sh Updating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\n   Parameter Type Default Description     certificate.crtFile String \"” Path to the host certificate file   certificate.keyFile String \"” Path to the host private key file   certificate.rootCertificate String \"” Path to the root CA file   web.sidecarproxyaddr String “127.0.0.1:5000/sidecar-proxy:latest” Docker registry address of the CSM for Authorization sidecar-proxy   web.jwtsigningsecret String “secret” The secret used to sign JWT tokens    Updating configuration parameters can be done by editing the karavi-config-secret on the CSM for the Authorization Server. The secret can be queried using k3s and kubectl like so:\nk3s kubectl -n karavi get secret/karavi-config-secret\nTo update or add parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nk3s kubectl -n karavi get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d\nSave the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64\nCopy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nk3s kubectl -n karavi edit secret/karavi-config-secret\nReplace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM for Authorization will read the changed secret.\nNote: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command like so:\nkaravictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationHost}\" | jq -r '.Token' \u003e kubectl -n $namespace apply -f -\nCSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM for Authorization logging settings during runtime, run the below command on the K3s cluster, make your changes, and save the updated configmap data.\nk3s kubectl -n karavi edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","excerpt":"This section outlines the deployment steps for Container Storage …","ref":"/csm-docs/v3/authorization/deployment/","title":"Deployment"},{"body":"This section outlines the deployment steps for Container Storage Modules (CSM) for Authorization. The deployment of CSM for Authorization is handled in 2 parts:\n Deploying the CSM for Authorization proxy server, to be controlled by storage administrators Configuring one to many supported Dell EMC CSI drivers with CSM for Authorization  Prerequisites The CSM for Authorization proxy server requires a Linux host with the following minimum resource allocations:\n 32 GB of memory 4 CPU 200 GB local storage  Deploying the CSM Authorization Proxy Server The first part of deploying CSM for Authorization is installing the proxy server. This activity and the administration of the proxy server will be owned by the storage administrator.\nThe CSM for Authorization proxy server is installed using a single binary installer.\nSingle Binary Installer The easiest way to obtain the single binary installer RPM is directly from the GitHub repository’s releases section.\nThe single binary installer can also be built from source by cloning the GitHub repository and using the following Makefile targets to build the installer:\nmake dist build-installer rpm The build-installer step creates a binary at bin/deploy and embeds all components required for installation. The rpm step generates an RPM package and stores it at deploy/rpm/x86_64/. This allows CSM for Authorization to be installed in network-restricted environments.\nA Storage Administrator can execute the installer or rpm package as a root user or via sudo.\nInstalling the RPM   Before installing the rpm, some network and security configuration inputs need to be provided in json format. The json file should be created in the location $HOME/.karavi/config.json having the following contents:\n{ \"web\": { \"sidecarproxyaddr\": \"docker_registry/sidecar-proxy:latest\", \"jwtsigningsecret\": \"secret\" }, \"proxy\": { \"host\": \":8080\" }, \"zipkin\": { \"collectoruri\": \"http://DNS_host_name:9411/api/v2/spans\", \"probability\": 1 }, \"certificate\": { \"keyFile\": \"path_to_private_key_file\", \"crtFile\": \"path_to_host_cert_file\", \"rootCertificate\": \"path_to_root_CA_file\" }, \"hostName\": \"DNS_host_name\" } In the above template, DNS_host_name refers to the hostname of the system in which the CSM for Authorization server will be installed. This hostname can be found by running the below command on the system:\nnslookup \u003cIP_address\u003e   In order to configure secure grpc connectivity, an additional subdomain in the format grpc.DNS_host_name is also required. All traffic from grpc.DNS_host_name needs to be routed to DNS_host_name address, this can be configured by adding a new DNS entry for grpc.DNS_host_name or providing a temporary path in the /etc/hosts file.\n   Note: The certificate provided in crtFile should be valid for both the DNS_host_name and the grpc.DNS_host_name address.\n For example, create the certificate config file with alternate names (to include example.com and grpc.example.com) and then create the .crt file: ``` CN = example.com subjectAltName = @alt_names [alt_names] DNS.1 = grpc.example.com openssl x509 -req -in cert_request_file.csr -CA root_CA.pem -CAkey private_key_File.key -CAcreateserial -out example.com.crt -days 365 -sha256 ```   To install the rpm package on the system, run the below command:\nrpm -ivh \u003crpm_file_name\u003e   After installation, application data will be stored on the system under /var/lib/rancher/k3s/storage/.\n  Configuring the CSM for Authorization Proxy Server The storage administrator must first configure the proxy server with the following:\n Storage systems Tenants Roles Bind roles to tenants  Run the following commands on the Authorization proxy server:\n# Specify any desired name export RoleName=\"\" export RoleQuota=\"\" export TenantName=\"\" # Specify info about Array1 export Array1Type=\"\" export Array1SystemID=\"\" export Array1User=\"\" export Array1Password=\"\" export Array1Pool=\"\" export Array1Endpoint=\"\" # Specify info about Array2 export Array2Type=\"\" export Array2SystemID=\"\" export Array2User=\"\" export Array2Password=\"\" export Array2Pool=\"\" export Array2Endpoint=\"\" # Specify IPs export DriverHostVMIP=\"\" export DriverHostVMPassword=\"\" export DriverHostVMUser=\"\" # Specify Authorization proxy host address. NOTE: this is not the same as IP export AuthorizationProxyHost=\"\" echo === Creating Storage(s) === # Add array1 to authorization karavictl storage create \\ --type ${Array1Type} \\ --endpoint ${Array1Endpoint} \\ --system-id ${Array1SystemID} \\ --user ${Array1User} \\ --password ${Array1Password} \\ --insecure # Add array2 to authorization karavictl storage create \\ --type ${Array2Type} \\ --endpoint ${Array2Endpoint} \\ --system-id ${Array2SystemID} \\ --user ${Array2User} \\ --password ${Array2Password} \\ --insecure echo === Creating Tenant === karavictl tenant create -n $TenantName --insecure --addr \"grpc.${AuthorizationProxyHost}\" echo === Creating Role === karavictl role create \\ --role=${RoleName}=${Array1Type}=${Array1SystemID}=${Array1Pool}=${RoleQuota} \\ --role=${RoleName}=${Array2Type}=${Array2SystemID}=${Array2Pool}=${RoleQuota} echo === === Binding Role === karavictl rolebinding create --tenant $TenantName --role $RoleName --insecure --addr \"grpc.${AuthorizationProxyHost}\" Generate a Token After creating the role bindings, the next logical step is to generate the access token. The storage admin is responsible for generating and sending the token to the Kubernetes tenant admin.\necho === Generating token === karavictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationProxyHost}\" | jq -r '.Token' \u003e token.yaml echo === Copy token to Driver Host === sshpass -p $DriverHostPassword scp token.yaml ${DriverHostVMUser}@{DriverHostVMIP}:/tmp/token.yaml  Note: The sample above copies the token directly to the Kubernetes cluster master node. The requirement here is that the token must be copied and/or stored in any location accessible to the Kubernetes tenant admin.\n Copy the karavictl Binary to the Kubernetes Master Node The karavictl binary is available from the CSM for Authorization proxy server. This needs to be copied to the Kubernetes master node for Kubernetes tenant admins so the Kubernetes tenant admins can configure the Dell EMC CSI driver with CSM for Authorization.\nsshpass -p dangerous scp bin/karavictl root@10.247.96.174:/tmp/karavictl  Note: The storage admin is responsible for copying the binary to a location accessible by the Kubernetes tenant admin.\n Configuring a Dell EMC CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported CSI drivers. This is controlled by the Kubernetes tenant admin.\nConfiguring a Dell EMC CSI Driver Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow the steps below to configure the CSI Drivers to work with the Authorization sidecar:\n  Create the secret token in the namespace of the driver.\n# It is assumed that array type powermax has the namespace \"powermax\", powerflex has the namepace \"vxflexos\", and powerscale has the namespace \"isilon\". kubectl apply -f /tmp/token.yaml -n powermax kubectl apply -f /tmp/token.yaml -n vxflexos kubectl apply -f /tmp/token.yaml -n isilon   Edit the following parameters in samples/secret/karavi-authorization-config.json file in CSI PowerFlex, CSI PowerMax, or CSI PowerScale driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\n     Parameter Description Required Default     username Username for connecting to the backend storage array. This parameter is ignored. No -   password Password for connecting to to the backend storage array. This parameter is ignored. No -   intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes -   endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400   systemID System ID of the backend storage array. Yes \" \"   insecure A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true   isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml    Create the karavi-authorization-config secret using the following command:\nkubectl -n [CSI_DRIVER_NAMESPACE] create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f -\n Note:\n Create the driver secret as you would normally except update/add the connection information for communicating with the sidecar instead of the backend storage array and scrub the username and password For PowerScale, the systemID will be the clusterName of the array.  The isilon-creds secret has a mountEndpoint parameter which should not be updated by the user. This parameter is updated and used when the driver has been injected with CSM-Authorization.      Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n [CSI_DRIVER_NAMESPACE] create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f -\nOtherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n [CSI_DRIVER_NAMESPACE] create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f -\n   Note: Follow the steps below for additional configurations to one or more of the supported CSI drivers.\n PowerFlex Please refer to step 5 in the installation steps for PowerFlex to edit the parameters in samples/config.yaml file to communicate with the sidecar.\n  Update endpoint to match the endpoint set in samples/secret/karavi-authorization-config.json\n  Create vxflexos-config secret using the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml -o yaml --dry-run=client | kubectl apply -f -\n  Please refer to step 9 in the installation steps for PowerFlex to edit the parameters in myvalues.yaml file to communicate with the sidecar.\n Enable CSM for Authorization and provide proxyHost address\n  Install the CSI PowerFlex driver\n  PowerMax Please refer to step 7 in the installation steps for PowerMax to edit the parameters in my-powermax-settings.yaml to communicate with the sidecar.\n  Update endpoint to match the endpoint set in samples/secret/karavi-authorization-config.json\n  Enable CSM for Authorization and provide proxyHost address\n  Install the CSI PowerMax driver\n  PowerScale Please refer to step 5 in the installation steps for PowerScale to edit the parameters in my-isilon-settings.yaml to communicate with the sidecar.\n Update endpointPort to match the endpoint port number set in samples/secret/karavi-authorization-config.json   Note: In my-isilon-settings.yaml, endpointPort acts as a default value. If endpointPort is not specified in my-isilon-settings.yaml, then it should be specified in the endpoint parameter of samples/secret/secret.yaml.\n Enable CSM for Authorization and provide proxyHost address  Please refer to step 6 in the installation steps for PowerScale to edit the parameters in samples/secret/secret.yaml file to communicate with the sidecar.\nUpdate endpoint to match the endpoint set in samples/secret/karavi-authorization-config.json   Note: Only add the endpoint port if it has not been set in my-isilon-settings.yaml.\n  Create the isilon-creds secret using the following command:\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\n  Install the CSI PowerScale driver\n  Updating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\n   Parameter Type Default Description     certificate.crtFile String \"” Path to the host certificate file   certificate.keyFile String \"” Path to the host private key file   certificate.rootCertificate String \"” Path to the root CA file   web.sidecarproxyaddr String “127.0.0.1:5000/sidecar-proxy:latest” Docker registry address of the CSM for Authorization sidecar-proxy   web.jwtsigningsecret String “secret” The secret used to sign JWT tokens    Updating configuration parameters can be done by editing the karavi-config-secret on the CSM for the Authorization Server. The secret can be queried using k3s and kubectl like so:\nk3s kubectl -n karavi get secret/karavi-config-secret\nTo update or add parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nk3s kubectl -n karavi get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d\nSave the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64\nCopy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nk3s kubectl -n karavi edit secret/karavi-config-secret\nReplace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM for Authorization will read the changed secret.\n Note: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command like so:\n karavictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationProxyHost}\" | jq -r '.Token' \u003e kubectl -n $namespace apply -f -\nCSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM for Authorization logging settings during runtime, run the below command on the K3s cluster, make your changes, and save the updated configmap data.\nk3s kubectl -n karavi edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","excerpt":"This section outlines the deployment steps for Container Storage …","ref":"/csm-docs/v3/authorization/deployment/","title":"Deployment"},{"body":" What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Where do I start with Dell Container Storage Modules (CSM)? What are the prerequisites for deploying Container Storage Modules? How do I uninstall or disable a module? How do I troubleshoot Container Storage Modules? Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? Should I install the module in the same namespace as the driver or another? Which Kubernetes distributions are supported? How do I get a list of Container Storage Modules deployed in my cluster with their versions? Do all Container Storage Modules need to be the same version, or can I mix and match? Can I run Container Storage Modules in a production environment? Is Dell Container Storage Modules (CSM) supported by Dell Technologies? Can I modify a module or contribute to the project? What is coming next?  What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Dell Container Storage Modules are a set of modules that aim to extend features beyond what is available in the CSI specification.\nThe main goal with CSM modules is to expose storage array enterprise features directly within Kubernetes so developers are empowered to leverage them for their deployment in a seamless way.\nWhere do I start with Dell Container Storage Modules (CSM)? The umbrella repository for every Dell Container Storage Module is: https://github.com/dell/csm.\nWhat are the prerequisites for deploying Container Storage Modules? Prerequisites can be found on the respective module deployment pages:\n Dell Container Storage Module for Observability Deployment Dell Container Storage Module for Authorization Deployment Dell Container Storage Module for Resiliency Deployment Dell Container Storage Module for Replication Deployment  Prerequisites for deploying the Dell CSI drivers can be found here:\n Dell CSI Drivers Deployment  How do I uninstall or disable a module?  Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Resiliency  How do I troubleshoot Container Storage Modules?  Dell CSI Drivers Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency  Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? No, all the modules have been designed to work inside Kubernetes with Dell CSI drivers.\nShould I install the module in the same namespace as the driver or another? It is recommended to install CSM for Observability in a namespace separate from the Dell CSI drivers because it works across multiple drivers. All other modules either run as standalone or with the Dell CSI driver as a sidecar.\nWhich Kubernetes distributions are supported? The supported Kubernetes distributions for Container Storage Modules are documented:\n Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency  The supported distros for the Dell CSI Drivers are located here.\nHow do I get a list of Container Storage Modules deployed in my cluster with their versions? The easiest way to find the module version is to check the image tag for the module. For all the namespaces you can execute the following:\nkubectl get pods -A -o jsonpath=\"{..image}\" | tr -s '[[:space:]]' '\\n' | grep 'csm\\|karavi' | sort | uniq -c Or if you know the namespace:\nkubectl get deployment,daemonset -o wide -n {{namespace}} Do all Container Storage Modules need to be the same version, or can I mix and match? It is advised to comply with the support matrices (links below) and not deviate from it with mixed versions.\n Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency Dell CSI Drivers.  Can I run Container Storage Modules in a production environment? As of CSM 1.0, the Container Storage Modules are GA and ready for production systems.\nIs Dell Container Storage Modules (CSM) supported by Dell Technologies? Yes!\nIf you find an issue, please follow our support process\nCan I modify a module or contribute to the project? Yes!\nAll Container Storage Modules are released as open-source projects under Apache-2.0 License. You are free to contribute directly following the contribution guidelines, fork the projects, modify them, and of course share feedback or open tickets ;-)\nWhat is coming next? This is just the beginning of the journey for Dell Container Storage Modules, and there is a full roadmap with more to come, which you can check under the GitHub Milestones page.\n","excerpt":" What are Dell Container Storage Modules (CSM)? How different is it …","ref":"/csm-docs/docs/faq/","title":"CSM FAQ"},{"body":" What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Where do I start with Dell Container Storage Modules (CSM)? What are the prerequisites for deploying Container Storage Modules? How do I uninstall or disable a module? How do I troubleshoot Container Storage Modules? Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? Should I install the module in the same namespace as the driver or another? Which Kubernetes distributions are supported? How do I get a list of Container Storage Modules deployed in my cluster with their versions? Do all Container Storage Modules need to be the same version, or can I mix and match? Can I run Container Storage Modules in a production environment? Is Dell Container Storage Modules (CSM) supported by Dell Technologies? Can I modify a module or contribute to the project? What is coming next?  What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Dell Container Storage Modules are a set of modules that aim to extend features beyond what is available in the CSI specification.\nThe main goal with CSM modules is to expose storage array enterprise features directly within Kubernetes so developers are empowered to leverage them for their deployment in a seamless way.\nWhere do I start with Dell Container Storage Modules (CSM)? The umbrella repository for every Dell Container Storage Module is: https://github.com/dell/csm.\nWhat are the prerequisites for deploying Container Storage Modules? Prerequisites can be found on the respective module deployment pages:\n Dell Container Storage Module for Observability Deployment Dell Container Storage Module for Authorization Deployment Dell Container Storage Module for Resiliency Deployment Dell Container Storage Module for Replication Deployment  Prerequisites for deploying the Dell CSI drivers can be found here:\n Dell CSI Drivers Deployment  How do I uninstall or disable a module?  Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Resiliency  How do I troubleshoot Container Storage Modules?  Dell CSI Drivers Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency  Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? No, all the modules have been designed to work inside Kubernetes with Dell CSI drivers.\nShould I install the module in the same namespace as the driver or another? It is recommended to install CSM for Observability in a namespace separate from the Dell CSI drivers because it works across multiple drivers. All other modules either run as standalone or with the Dell CSI driver as a sidecar.\nWhich Kubernetes distributions are supported? The supported Kubernetes distributions for Container Storage Modules are documented:\n Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency  The supported distros for the Dell CSI Drivers are located here.\nHow do I get a list of Container Storage Modules deployed in my cluster with their versions? The easiest way to find the module version is to check the image tag for the module. For all the namespaces you can execute the following:\nkubectl get pods -A -o jsonpath=\"{..image}\" | tr -s '[[:space:]]' '\\n' | grep 'csm\\|karavi' | sort | uniq -c Or if you know the namespace:\nkubectl get deployment,daemonset -o wide -n {{namespace}} Do all Container Storage Modules need to be the same version, or can I mix and match? It is advised to comply with the support matrices (links below) and not deviate from it with mixed versions.\n Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency Dell CSI Drivers.  Can I run Container Storage Modules in a production environment? As of CSM 1.0, the Container Storage Modules are GA and ready for production systems.\nIs Dell Container Storage Modules (CSM) supported by Dell Technologies? Yes!\nIf you find an issue, please follow our support process\nCan I modify a module or contribute to the project? Yes!\nAll Container Storage Modules are released as open-source projects under Apache-2.0 License. You are free to contribute directly following the contribution guidelines, fork the projects, modify them, and of course share feedback or open tickets ;-)\nWhat is coming next? This is just the beginning of the journey for Dell Container Storage Modules, and there is a full roadmap with more to come, which you can check under the GitHub Milestones page.\n","excerpt":" What are Dell Container Storage Modules (CSM)? How different is it …","ref":"/csm-docs/v1/faq/","title":"CSM FAQ"},{"body":" What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Where do I start with Dell Container Storage Modules (CSM)? What are the prerequisites for deploying Container Storage Modules? How do I uninstall or disable a module? How do I troubleshoot Container Storage Modules? Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? Should I install the module in the same namespace as the driver or another? Which Kubernetes distributions are supported? How do I get a list of Container Storage Modules deployed in my cluster with their versions? Do all Container Storage Modules need to be the same version, or can I mix and match? Can I run Container Storage Modules in a production environment? Is Dell Container Storage Modules (CSM) supported by Dell Technologies? Can I modify a module or contribute to the project? What is coming next?  What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Dell Container Storage Modules are a set of modules that aim to extend features beyond what is available in the CSI specification.\nThe main goal with CSM modules is to expose storage array enterprise features directly within Kubernetes so developers are empowered to leverage them for their deployment in a seamless way.\nWhere do I start with Dell Container Storage Modules (CSM)? The umbrella repository for every Dell Container Storage Module is: https://github.com/dell/csm.\nWhat are the prerequisites for deploying Container Storage Modules? Prerequisites can be found on the respective module deployment pages:\n Dell Container Storage Module for Observability Deployment Dell Container Storage Module for Authorization Deployment Dell Container Storage Module for Resiliency Deployment Dell Container Storage Module for Replication Deployment  Prerequisites for deploying the Dell CSI drivers can be found here:\n Dell CSI Drivers Deployment  How do I uninstall or disable a module?  Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Resiliency  How do I troubleshoot Container Storage Modules?  Dell CSI Drivers Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency  Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? No, all the modules have been designed to work inside Kubernetes with Dell CSI drivers.\nShould I install the module in the same namespace as the driver or another? It is recommended to install CSM for Observability in a namespace separate from the Dell CSI drivers because it works across multiple drivers. All other modules either run as standalone or with the Dell CSI driver as a sidecar.\nWhich Kubernetes distributions are supported? The supported Kubernetes distributions for Container Storage Modules are documented:\n Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency  The supported distros for the Dell CSI Drivers are located here.\nHow do I get a list of Container Storage Modules deployed in my cluster with their versions? The easiest way to find the module version is to check the image tag for the module. For all the namespaces you can execute the following:\nkubectl get pods -A -o jsonpath=\"{..image}\" | tr -s '[[:space:]]' '\\n' | grep 'csm\\|karavi' | sort | uniq -c Or if you know the namespace:\nkubectl get deployment,daemonset -o wide -n {{namespace}} Do all Container Storage Modules need to be the same version, or can I mix and match? It is advised to comply with the support matrices (links below) and not deviate from it with mixed versions.\n Dell Container Storage Module for Authorization Dell Container Storage Module for Observability Dell Container Storage Module for Replication Dell Container Storage Module for Resiliency Dell CSI Drivers.  Can I run Container Storage Modules in a production environment? As of CSM 1.0, the Container Storage Modules are GA and ready for production systems.\nIs Dell Container Storage Modules (CSM) supported by Dell Technologies? Yes!\nIf you find an issue, please follow our support process\nCan I modify a module or contribute to the project? Yes!\nAll Container Storage Modules are released as open-source projects under Apache-2.0 License. You are free to contribute directly following the contribution guidelines, fork the projects, modify them, and of course share feedback or open tickets ;-)\nWhat is coming next? This is just the beginning of the journey for Dell Container Storage Modules, and there is a full roadmap with more to come, which you can check under the GitHub Milestones page.\n","excerpt":" What are Dell Container Storage Modules (CSM)? How different is it …","ref":"/csm-docs/v2/faq/","title":"CSM FAQ"},{"body":" What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Where do I start with Dell Container Storage Modules (CSM)? Is the Container Storage Module XYZ available for my array? What are the prerequisites for deploying Container Storage Modules? How do I uninstall or disable a Container Storage Module? How do I troubleshoot Container Storage Modules? Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? Should I install the module in the same namespace as the driver or another? Which Kubernetes distributions are supported? How do I get a list of Container Storage Modules deployed in my cluster with their versions? Does the CSM Installer provide full Container Storage Modules functionality for all products? Do all Container Storage Modules need to be the same version, or can I mix and match? Can I run Container Storage Modules in a production environment? Is Dell Container Storage Modules (CSM) supported by Dell Technologies? Can I modify a module or contribute to the project? What is coming next?  What are Dell Container Storage Modules (CSM)? How different is it from a CSI driver? Dell Container Storage Modules are a set of modules that aim to extend features beyond what is available in the CSI specification.\nThe main goal with CSM modules is to expose storage array enterprise features directly within Kubernetes so developers are empowered to leverage them for their deployment in a seamless way.\nWhere do I start with Dell Container Storage Modules (CSM)? The umbrella repository for every Dell Container Storage Module is: https://github.com/dell/csm.\nIs the Container Storage Module XYZ available for my array? Please see module and the respectice CSI driver version available for each array:\n   CSM Module CSI PowerFlex v2.1 CSI PowerScale v2.1 CSI PowerStore v2.1 CSI PowerMax v2.1 CSI Unity XT v2.1     Authorization v1.1 ✔️ ✔️ ❌ ✔️ ❌   Observability v1.0 ✔️ ❌ ✔️ ❌ ❌   Replication v1.1 ❌ ❌ ✔️ ✔️ ❌   Resilency v1.0 ✔️ ❌ ❌ ❌ ✔️   CSM Installer v1.0 ✔️ ✔️ ✔️ ✔️ ✔️    What are the prerequisites for deploying Container Storage Modules? Prerequisites can be found on the respective module deployment pages:\n Dell EMC Container Storage Module for Observability Deployment Dell EMC Container Storage Module for Authorization Deployment Dell EMC Container Storage Module for Resiliency Deployment Dell EMC Container Storage Module for Replication Deployment  Prerequisites for deploying the Dell EMC CSI drivers can be found here:\n Dell EMC CSI Drivers Deployment  How do I uninstall or a disable a module?  Dell EMC Container Storage Module for Authorization Dell EMC Container Storage Module for Observability Dell EMC Container Storage Module for Resiliency  How do I troubleshoot Container Storage Modules?  Dell EMC CSI Drivers Dell EMC Container Storage Module for Authorization Dell EMC Container Storage Module for Observability Dell EMC Container Storage Module for Replication Dell EMC Container Storage Module for Resiliency  Can I use the CSM functionality like Prometheus collection or Authorization quotas for my non-Kubernetes storage clients? No, all the modules have been designed to work inside Kubernetes with Dell EMC CSI drivers.\nShould I install the module in the same namespace as the driver or another? It is recommended to install CSM for Observability in a namespace separate from the Dell EMC CSI drivers because it works across multiple drivers. All other modules either run as standalone or are injected into the Dell EMC CSI driver as a sidecar.\nWhich Kubernetes distributions are supported? The supported Kubernetes distributions for Container Storage Modules are documented:\n Dell EMC Container Storage Module for Authorization Dell EMC Container Storage Module for Observability Dell EMC Container Storage Module for Replication Dell EMC Container Storage Module for Resiliency  The supported distros for the Dell EMC CSI Drivers are located here.\nHow do I get a list of Container Storage Modules deployed in my cluster with their versions? The easiest way to find the module version is to check the image tag for the module. For all the namespaces you can execute the following:\nkubectl get pods -A -o jsonpath=\"{..image}\" | tr -s '[[:space:]]' '\\n' | grep 'csm\\|karavi' | sort | uniq -c Or if you know the namespace:\nkubectl get deployment,daemonset -o wide -n {{namespace}} Does the CSM Installer provide full Container Storage Modules functionality for all products? The CSM Installer supports the installation of all the Container Storage Modules and Dell EMC CSI drivers.\nDo all Container Storage Modules need to be the same version, or can I mix and match? It is advised to comply with the support matrices (links below) and not deviate from it with mixed versions.\n Dell EMC Container Storage Module for Authorization Dell EMC Container Storage Module for Observability Dell EMC Container Storage Module for Replication Dell EMC Container Storage Module for Resiliency Dell EMC CSI Drivers.  The CSM installer module will help to stay aligned with compatible versions during the first install and future upgrades.\nCan I run Container Storage Modules in a production environment? As of CSM 1.0, the Container Storage Modules are GA and ready for production systems.\nIs Dell Container Storage Modules (CSM) supported by Dell Technologies? Yes!\nIf you find an issue, please follow our support process\nCan I modify a module or contribute to the project? Yes!\nAll Container Storage Modules are released as open-source projects under Apache-2.0 License. You are free to contribute directly following the contribution guidelines, fork the projects, modify them, and of course share feedback or open tickets ;-)\nWhat is coming next? This is just the beginning of the journey for Dell Container Storage Modules, and there is a full roadmap with more to come, which you can check under the GithHub Milestones page.\n","excerpt":" What are Dell Container Storage Modules (CSM)? How different is it …","ref":"/csm-docs/v3/faq/","title":"CSM FAQ"},{"body":"Installation information for all the drivers/modules can be found on the individual driver’s page in this section\n","excerpt":"Installation information for all the drivers/modules can be found on …","ref":"/csm-docs/docs/csidriver/installation/","title":"Installation"},{"body":"The installation process consists of three steps:\n Install repctl Install Container Storage Modules (CSM) for Replication Controller Install CSI driver after enabling replication  Before you begin Please read this document before proceeding with the installation. It provides detailed steps on how to set up communication between multiple clusters which will be required during or after the installation.\nInstall repctl You can download pre-built repctl binary from our Releases page. Alternately, if you want to build the binary yourself, you can follow these steps:\ngit clone github.com/dell/csm-replication cd csm-replication/repctl make build Installing CSM Replication Controller You can use one of the following methods to install CSM Replication Controller\n Using repctl Installation script (Helm chart)  We recommend using repctl for the installation as it simplifies the installation workflow. This process also helps configure repctl for future use during management operations.\nUsing repctl Please follow the steps here to install \u0026 configure Dell Replication Controller\nUsing the installation script Repeat the following steps on all clusters where you want to configure replication\ngit clone github.com/dell/csm-replication cd csm-replication kubectl create ns dell-replication-controller # Copy and modify values.yaml file if you wish to customize your deployment in any way cp ../helm/csm-replication/values.yaml ./myvalues.yaml bash scripts/install.sh --values ./myvalues.yaml  Note: Current installation method allows you to specify custom \u003cFQDN\u003e:\u003cIP\u003e entries to be appended to controller’s /etc/hosts file. It can be useful if controller is being deployed in private environment where DNS is not set up properly, but kubernetes clusters use FQDN as API server’s address. The feature can be enabled by modifying values.yaml.\n- ip: \"10.10.10.10\" hostnames: - \"foo.bar\" - ip: \"10.10.10.11\" hostnames: - \"foo.baz\"  This script will do the following:\n Install DellCSIReplicationGroup CRD in your cluster Install dell-replication-controller  After the installation ConfigMap will consist of only the logLevel field, to add the rest configuration to the cluster do the following:\n Update the configuration in deploy/config.yaml after going through the guide here Run the following commands to update and complete the installation  cd csm-replication kubectl create configmap dell-replication-controller-config --namespace dell-replication-controller --from-file deploy/config.yaml -o yaml --dry-run | kubectl apply -f - Install CSI driver The following CSI drivers support replication:\n CSI driver for PowerMax CSI driver for PowerStore CSI driver for PowerScale  Please follow the steps outlined in PowerMax, PowerStore or PowerScale pages during the driver installation.\n Note: Please ensure that replication CRDs are installed in the clusters where you are installing the CSI drivers. These CRDs are generally installed as part of the CSM Replication controller installation process.\n Dynamic Log Level Change CSM Replication Controller can dynamically change its logs’ verbosity level. To set log level in runtime you need to edit the controllers ConfigMap:\nkubectl edit cm dell-replication-controller-config -n dell-replication-controller And set the CSI_LOG_LEVEL field to the level of your choosing. CSM Replication controller supports following log levels:\n “PANIC” “FATAL” “ERROR” “WARN” “INFO” “DEBUG” “TRACE”   Note: CSI-Replicator sidecar utilizes the same log level as CSI driver. To change the sidecars log level refer to corresponding csi drivers documentation.\n ","excerpt":"The installation process consists of three steps:\n Install repctl …","ref":"/csm-docs/docs/replication/deployment/installation/","title":"Installation"},{"body":"Installation information for all the drivers/modules can be found on the individual driver’s page in this section\n","excerpt":"Installation information for all the drivers/modules can be found on …","ref":"/csm-docs/v1/csidriver/installation/","title":"Installation"},{"body":"The installation process consists of three steps:\n Install repctl Install Container Storage Modules (CSM) for Replication Controller Install CSI driver after enabling replication  Before you begin Please read this document before proceeding with the installation. It provides detailed steps on how to set up communication between multiple clusters which will be required during or after the installation.\nInstall repctl You can download pre-built repctl binary from our Releases page. Alternately, if you want to build the binary yourself, you can follow these steps:\ngit clone github.com/dell/csm-replication cd csm-replication/repctl make build Installing CSM Replication Controller You can use one of the following methods to install CSM Replication Controller\n Using repctl Installation script (Helm chart)  We recommend using repctl for the installation as it simplifies the installation workflow. This process also helps configure repctl for future use during management operations.\nUsing repctl Please follow the steps here to install \u0026 configure Dell Replication Controller\nUsing the installation script Repeat the following steps on all clusters where you want to configure replication\ngit clone github.com/dell/csm-replication cd csm-replication kubectl create ns dell-replication-controller # Copy and modify values.yaml file if you wish to customize your deployment in any way cp ../helm/csm-replication/values.yaml ./myvalues.yaml bash scripts/install.sh --values ./myvalues.yaml  Note: Current installation method allows you to specify custom \u003cFQDN\u003e:\u003cIP\u003e entry to be appended to controller’s /etc/hosts file. It can be useful if controller is being deployed in private environment where DNS is not set up properly, but kubernetes clusters use FQDN as API server’s address. The feature can be enabled by modifying values.yaml.\nenableHostAliases: true hostName: \"foo.bar\" ip: \"10.10.10.10\"  This script will do the following:\n Install DellCSIReplicationGroup CRD in your cluster Install dell-replication-controller  After the installation ConfigMap will consist of only the logLevel field, to add the rest configuration to the cluster do the following:\n Update the configuration in deploy/config.yaml after going through the guide here Run the following commands to update and complete the installation  cd csm-replication kubectl create configmap dell-replication-controller-config --namespace dell-replication-controller --from-file deploy/config.yaml -o yaml --dry-run | kubectl apply -f - Install CSI driver The following CSI drivers support replication:\n CSI driver for PowerMax CSI driver for PowerStore CSI driver for PowerScale  Please follow the steps outlined in PowerMax, PowerStore or PowerScale pages during the driver installation.\n Note: Please ensure that replication CRDs are installed in the clusters where you are installing the CSI drivers. These CRDs are generally installed as part of the CSM Replication controller installation process.\n Dynamic Log Level Change CSM Replication Controller can dynamically change its logs’ verbosity level. To set log level in runtime you need to edit the controllers ConfigMap:\nkubectl edit cm dell-replication-controller-config -n dell-replication-controller And set the CSI_LOG_LEVEL field to the level of your choosing. CSM Replication controller supports following log levels:\n “PANIC” “FATAL” “ERROR” “WARN” “INFO” “DEBUG” “TRACE”   Note: CSI-Replicator sidecar utilizes the same log level as CSI driver. To change the sidecars log level refer to corresponding csi drivers documentation.\n ","excerpt":"The installation process consists of three steps:\n Install repctl …","ref":"/csm-docs/v1/replication/deployment/installation/","title":"Installation"},{"body":"Installation information for all the drivers/modules can be found on the individual driver’s page in this section\n","excerpt":"Installation information for all the drivers/modules can be found on …","ref":"/csm-docs/v2/csidriver/installation/","title":"Installation"},{"body":"The installation process consists of three steps:\n Install repctl Install Container Storage Modules (CSM) for Replication Controller Install CSI driver after enabling replication  Before you begin Please read this document before proceeding with the installation. It provides detailed steps on how to set up communication between multiple clusters which will be required during or after the installation.\nInstall repctl You can download pre-built repctl binary from our Releases page. Alternately, if you want to build the binary yourself, you can follow these steps:\ngit clone github.com/dell/csm-replication cd csm-replication/repctl make build Installing CSM Replication Controller You can use one of the following methods to install CSM Replication Controller\n Using repctl Installation script (Helm chart)  We recommend using repctl for the installation as it simplifies the installation workflow. This process also helps configure repctl for future use during management operations.\nUsing repctl Please follow the steps here to install \u0026 configure Dell Replication Controller\nUsing the installation script Repeat the following steps on all clusters where you want to configure replication\ngit clone github.com/dell/csm-replication cd csm-replication kubectl create ns dell-replication-controller # Copy and modify values.yaml file if you wish to customize your deployment in any way cp ../helm/csm-replication/values.yaml ./myvalues.yaml bash scripts/install.sh --values ./myvalues.yaml  Note: Current installation method allows you to specify custom \u003cFQDN\u003e:\u003cIP\u003e entry to be appended to controller’s /etc/hosts file. It can be useful if controller is being deployed in private environment where DNS is not set up properly, but kubernetes clusters use FQDN as API server’s address. The feature can be enabled by modifying values.yaml.\nenableHostAliases: true hostName: \"foo.bar\" ip: \"10.10.10.10\"  This script will do the following:\n Install DellCSIReplicationGroup CRD in your cluster Install dell-replication-controller  After the installation ConfigMap will consist of only the logLevel field, to add the rest configuration to the cluster do the following:\n Update the configuration in deploy/config.yaml after going through the guide here Run the following commands to update and complete the installation  cd csm-replication kubectl create configmap dell-replication-controller-config --namespace dell-replication-controller --from-file deploy/config.yaml -o yaml --dry-run | kubectl apply -f - Install CSI driver The following CSI drivers support replication:\n CSI driver for PowerMax CSI driver for PowerStore CSI driver for PowerScale  Please follow the steps outlined in PowerMax, PowerStore or PowerScale pages during the driver installation.\n Note: Please ensure that replication CRDs are installed in the clusters where you are installing the CSI drivers. These CRDs are generally installed as part of the CSM Replication controller installation process.\n Dynamic Log Level Change CSM Replication Controller can dynamically change its logs’ verbosity level. To set log level in runtime you need to edit the controllers ConfigMap:\nkubectl edit cm dell-replication-controller-config -n dell-replication-controller And set the CSI_LOG_LEVEL field to the level of your choosing. CSM Replication controller supports following log levels:\n “PANIC” “FATAL” “ERROR” “WARN” “INFO” “DEBUG” “TRACE”   Note: CSI-Replicator sidecar utilizes the same log level as CSI driver. To change the sidecars log level refer to corresponding csi drivers documentation.\n ","excerpt":"The installation process consists of three steps:\n Install repctl …","ref":"/csm-docs/v2/replication/deployment/installation/","title":"Installation"},{"body":"Installation information for all the drivers/modules can be found on the individual driver’s page in this section\n","excerpt":"Installation information for all the drivers/modules can be found on …","ref":"/csm-docs/v3/csidriver/installation/","title":"Installation"},{"body":"The installation process consists of three steps:\n Install repctl Install Container Storage Modules (CSM) for Replication Controller Install CSI driver after enabling replication  Before you begin Please read this document before proceeding with the installation. It provides detailed steps on how to set up communication between multiple clusters which will be required during or after the installation.\nInstall repctl You can download pre-built repctl binary from our Releases page. Alternately, if you want to build the binary yourself, you can follow these steps:\ngit clone github.com/dell/csm-replication cd csm-replication/repctl make build Installing CSM Replication Controller You can use one of the following methods to install CSM Replication Controller\n Using repctl Installation script (Helm chart)  We recommend using repctl for the installation as it simplifies the installation workflow. This process also helps configure repctl for future use during management operations.\nUsing repctl Please follow the steps here to install \u0026 configure Dell Replication Controller\nUsing the installation script Repeat the following steps on all clusters where you want to configure replication\ngit clone github.com/dell/csm-replication cd csm-replication kubectl create ns dell-replication-controller # Copy and modify values.yaml file if you wish to customize your deployment in any way cp ../helm/csm-replication/values.yaml ./myvalues.yaml bash scripts/install.sh --values ./myvalues.yaml This script will do the following:\n Install DellCSIReplicationGroup CRD in your cluster Install dell-replication-controller  After the installation ConfigMap will consist of only the logLevel field, to add the rest configuration to the cluster do the following:\n Update the configuration in deploy/config.yaml after going through the guide here Run the following commands to update and complete the installation  cd csm-replication kubectl create configmap dell-replication-controller-config --namespace dell-replication-controller --from-file deploy/config.yaml -o yaml --dry-run | kubectl apply -f - Install CSI driver The following CSI drivers support replication:\n CSI driver for PowerMax CSI driver for PowerStore  Please follow the steps outlined here for enabling replication for PowerMax \u0026 here for PowerStore during the driver installation.\n Note: Please ensure that replication CRDs are installed in the clusters where you are installing the CSI drivers. These CRDs are generally installed as part of the CSM Replication controller installation process.\n Dynamic Log Level Change CSM Replication Controller can dynamically change its logs’ verbosity level. To set log level in runtime you need to edit the controllers ConfigMap:\nkubectl edit cm dell-replication-controller-config -n dell-replication-controller And set the CSI_LOG_LEVEL field to the level of your choosing. CSM Replication controller supports following log levels:\n “PANIC” “FATAL” “ERROR” “WARN” “INFO” “DEBUG” “TRACE”   Note: CSI-Replicator sidecar utilizes the same log level as CSI driver. To change the sidecars log level refer to corresponding csi drivers documentation.\n ","excerpt":"The installation process consists of three steps:\n Install repctl …","ref":"/csm-docs/v3/replication/deployment/installation/","title":"Installation"},{"body":"This section outlines the metrics collected by Container Storage Modules (CSM) for Observability in the areas of I/O Performance and Storage Capacity. All metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n","excerpt":"This section outlines the metrics collected by Container Storage …","ref":"/csm-docs/docs/observability/metrics/","title":"Metrics"},{"body":"This section outlines the metrics collected by Container Storage Modules (CSM) for Observability in the areas of I/O Performance and Storage Capacity. All metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n","excerpt":"This section outlines the metrics collected by Container Storage …","ref":"/csm-docs/v1/observability/metrics/","title":"Metrics"},{"body":"This section outlines the metrics collected by Container Storage Modules (CSM) for Observability in the areas of I/O Performance and Storage Capacity. All metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n","excerpt":"This section outlines the metrics collected by Container Storage …","ref":"/csm-docs/v2/observability/metrics/","title":"Metrics"},{"body":"This section outlines the metrics collected by Container Storage Modules (CSM) for Observability in the areas of I/O Performance and Storage Capacity. All metrics are available from the OpenTelemetry collector endpoint. Please see the CSM for Observability for more information on deploying and configuring the OpenTelemetry collector.\n","excerpt":"This section outlines the metrics collected by Container Storage …","ref":"/csm-docs/v3/observability/metrics/","title":"Metrics"},{"body":"This section outlines the deployment steps for Container Storage Modules (CSM) for Authorization. The deployment of CSM for Authorization is handled in 2 parts:\n Deploying the CSM for Authorization proxy server, to be controlled by storage administrators Configuring one to many supported Dell CSI drivers with CSM for Authorization  Prerequisites The CSM for Authorization proxy server requires a Linux host with the following minimum resource allocations:\n 32 GB of memory 4 CPU 200 GB local storage  These packages need to be installed on the Linux host:\n container-selinux https://rpm.rancher.io/k3s/stable/common/centos/7/noarch/k3s-selinux-0.4-1.el7.noarch.rpm  Deploying the CSM Authorization Proxy Server The first part of deploying CSM for Authorization is installing the proxy server. This activity and the administration of the proxy server will be owned by the storage administrator.\nThe CSM for Authorization proxy server is installed using a single binary installer.\nIf CSM for Authorization is being installed on a system where SELinux is enabled, you must ensure the proper SELinux policies have been installed.\nSingle Binary Installer The easiest way to obtain the single binary installer RPM is directly from the GitHub repository’s releases section.\nAlternatively, the single binary installer can be built from source by cloning the GitHub repository and using the following Makefile targets to build the installer:\nmake dist build-installer rpm The build-installer step creates a binary at karavi-authorization/bin/deploy and embeds all components required for installation. The rpm step generates an RPM package and stores it at karavi-authorization/deploy/rpm/x86_64/. This allows CSM for Authorization to be installed in network-restricted environments.\nA Storage Administrator can execute the installer or rpm package as a root user or via sudo.\nInstalling the RPM   Before installing the rpm, some network and security configuration inputs need to be provided in json format. The json file should be created in the location $HOME/.karavi/config.json having the following contents:\n{ \"web\": { \"jwtsigningsecret\": \"secret\" }, \"proxy\": { \"host\": \":8080\" }, \"zipkin\": { \"collectoruri\": \"http://DNS-hostname:9411/api/v2/spans\", \"probability\": 1 }, \"certificate\": { \"keyFile\": \"path_to_private_key_file\", \"crtFile\": \"path_to_host_cert_file\", \"rootCertificate\": \"path_to_root_CA_file\" }, \"hostname\": \"DNS-hostname\" } In an instance where a secure deployment is not required, an insecure deployment is possible. Please note that self-signed certificates will be created for you using cert-manager to allow TLS encryption for communication on the CSM for Authorization proxy server. However, this is not recommended for production environments. For an insecure deployment, the json file in the location $HOME/.karavi/config.json only requires the following contents:\n{ \"hostname\": \"DNS-hostname\" }    Note:\n DNS-hostname refers to the hostname of the system in which the CSM for Authorization server will be installed. This hostname can be found by running nslookup \u003cIP_address\u003e There are a number of ways to create certificates. In a production environment, certificates are usually created and managed by an IT administrator. Otherwise, certificates can be created using OpenSSL.   In order to configure secure grpc connectivity, an additional subdomain in the format grpc.DNS-hostname is also required. All traffic from grpc.DNS-hostname needs to be routed to DNS-hostname address, this can be configured by adding a new DNS entry for grpc.DNS-hostname or providing a temporary path in the systems /etc/hosts file.   Note: The certificate provided in crtFile should be valid for both the DNS-hostname and the grpc.DNS-hostname address.\n For example, create the certificate config file with alternate names (to include DNS-hostname and grpc.DNS-hostname) and then create the .crt file: ``` CN = DNS-hostname subjectAltName = @alt_names [alt_names] DNS.1 = grpc.DNS-hostname.com $ openssl x509 -req -in cert_request_file.csr -CA root_CA.pem -CAkey private_key_File.key -CAcreateserial -out DNS-hostname.com.crt -days 365 -sha256 ```   To install the rpm package on the system, run the below command:\nrpm -ivh \u003crpm_file_name\u003e   After installation, application data will be stored on the system under /var/lib/rancher/k3s/storage/.\n  If errors occur during installation, review the Troubleshooting section.\nConfiguring the CSM for Authorization Proxy Server The storage administrator must first configure the proxy server with the following:\n Storage systems Tenants Roles Bind roles to tenants  Run the following commands on the Authorization proxy server:\n Note: The --insecure flag is only necessary if certificates were not provided in $HOME/.karavi/config.json.\n # Specify any desired name export RoleName=\"\" export RoleQuota=\"\" export TenantName=\"\" # Specify info about Array1 export Array1Type=\"\" export Array1SystemID=\"\" export Array1User=\"\" export Array1Password=\"\" export Array1Pool=\"\" export Array1Endpoint=\"\" # Specify info about Array2 export Array2Type=\"\" export Array2SystemID=\"\" export Array2User=\"\" export Array2Password=\"\" export Array2Pool=\"\" export Array2Endpoint=\"\" # Specify IPs export DriverHostVMIP=\"\" export DriverHostVMPassword=\"\" export DriverHostVMUser=\"\" # Specify Authorization proxy host address. NOTE: this is not the same as IP export AuthorizationProxyHost=\"\" echo === Creating Storage(s) === # Add array1 to authorization karavictl storage create \\ --type ${Array1Type} \\ --endpoint ${Array1Endpoint} \\ --system-id ${Array1SystemID} \\ --user ${Array1User} \\ --password ${Array1Password} \\ --array-insecure # Add array2 to authorization karavictl storage create \\ --type ${Array2Type} \\ --endpoint ${Array2Endpoint} \\ --system-id ${Array2SystemID} \\ --user ${Array2User} \\ --password ${Array2Password} \\ --array-insecure echo === Creating Tenant === karavictl tenant create -n $TenantName --insecure --addr \"grpc.${AuthorizationProxyHost}\" echo === Creating Role === karavictl role create \\ --role=${RoleName}=${Array1Type}=${Array1SystemID}=${Array1Pool}=${RoleQuota} \\ --role=${RoleName}=${Array2Type}=${Array2SystemID}=${Array2Pool}=${RoleQuota} echo === === Binding Role === karavictl rolebinding create --tenant $TenantName --role $RoleName --insecure --addr \"grpc.${AuthorizationProxyHost}\" Generate a Token After creating the role bindings, the next logical step is to generate the access token. The storage admin is responsible for generating and sending the token to the Kubernetes tenant admin.\n Note:\n The --insecure flag is only necessary if certificates were not provided in $HOME/.karavi/config.json. This sample copies the token directly to the Kubernetes cluster master node. The requirement here is that the token must be copied and/or stored in any location accessible to the Kubernetes tenant admin.   echo === Generating token === karavictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationProxyHost}\" | jq -r '.Token' \u003e token.yaml echo === Copy token to Driver Host === sshpass -p $DriverHostPassword scp token.yaml ${DriverHostVMUser}@{DriverHostVMIP}:/tmp/token.yaml Copy the karavictl Binary to the Kubernetes Master Node The karavictl binary is available from the CSM for Authorization proxy server. This needs to be copied to the Kubernetes master node for Kubernetes tenant admins so the Kubernetes tenant admins can configure the Dell CSI driver with CSM for Authorization.\nsshpass -p dangerous scp bin/karavictl root@10.247.96.174:/tmp/karavictl  Note: The storage admin is responsible for copying the binary to a location accessible by the Kubernetes tenant admin.\n Configuring a Dell CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported CSI drivers. This is controlled by the Kubernetes tenant admin.\nConfiguring a Dell CSI Driver Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow the steps below to configure the CSI Drivers to work with the Authorization sidecar:\n  Create the secret token in the namespace of the driver.\n# It is assumed that array type powermax has the namespace \"powermax\", powerflex has the namepace \"vxflexos\", and powerscale has the namespace \"isilon\". kubectl apply -f /tmp/token.yaml -n powermax kubectl apply -f /tmp/token.yaml -n vxflexos kubectl apply -f /tmp/token.yaml -n isilon   Edit the following parameters in samples/secret/karavi-authorization-config.json file in CSI PowerFlex, CSI PowerMax, or CSI PowerScale driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\n     Parameter Description Required Default     username Username for connecting to the backend storage array. This parameter is ignored. No -   password Password for connecting to to the backend storage array. This parameter is ignored. No -   intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes -   endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400   systemID System ID of the backend storage array. Yes \" \"   insecure A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true   isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml    Create the karavi-authorization-config secret using the following command:\nkubectl -n [CSI_DRIVER_NAMESPACE] create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f -\n Note:\n Create the driver secret as you would normally except update/add the connection information for communicating with the sidecar instead of the backend storage array and scrub the username and password For PowerScale, the systemID will be the clusterName of the array.  The isilon-creds secret has a mountEndpoint parameter which must be set to the hostname or IP address of the PowerScale OneFS API server, for example, 10.0.0.1.      Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n [CSI_DRIVER_NAMESPACE] create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f -\nOtherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n [CSI_DRIVER_NAMESPACE] create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f -\n   Note: Follow the steps below for additional configurations to one or more of the supported CSI drivers.\n PowerFlex Please refer to step 5 in the installation steps for PowerFlex to edit the parameters in samples/config.yaml file to communicate with the sidecar.\n  Update endpoint to match the endpoint set in samples/secret/karavi-authorization-config.json\n  Create vxflexos-config secret using the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml -o yaml --dry-run=client | kubectl apply -f -\n  Please refer to step 9 in the installation steps for PowerFlex to edit the parameters in myvalues.yaml file to communicate with the sidecar.\n Enable CSM for Authorization and provide proxyHost address\n  Install the CSI PowerFlex driver\n  PowerMax Please refer to step 7 in the installation steps for PowerMax to edit the parameters in my-powermax-settings.yaml to communicate with the sidecar.\n  Update endpoint to match the endpoint set in samples/secret/karavi-authorization-config.json\n  Enable CSM for Authorization and provide proxyHost address\n  Install the CSI PowerMax driver\n  PowerScale Please refer to step 5 in the installation steps for PowerScale to edit the parameters in my-isilon-settings.yaml to communicate with the sidecar.\n Update endpointPort to match the endpoint port number set in samples/secret/karavi-authorization-config.json  Notes:\n  In my-isilon-settings.yaml, endpointPort acts as a default value. If endpointPort is not specified in my-isilon-settings.yaml, then it should be specified in the endpoint parameter of samples/secret/secret.yaml. The isilon-creds secret has a mountEndpoint parameter which must be set to the hostname or IP address of the PowerScale OneFS API server, for example, 10.0.0.1.   Enable CSM for Authorization and provide proxyHost address  Please refer to step 6 in the installation steps for PowerScale to edit the parameters in samples/secret/secret.yaml file to communicate with the sidecar.\nUpdate endpoint to match the endpoint set in samples/secret/karavi-authorization-config.json   Note: Only add the endpoint port if it has not been set in my-isilon-settings.yaml.\n  Create the isilon-creds secret using the following command:\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\n  Install the CSI PowerScale driver\n  Updating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\n   Parameter Type Default Description     web.jwtsigningsecret String “secret” The secret used to sign JWT tokens    Updating configuration parameters can be done by editing the karavi-config-secret on the CSM for the Authorization Server. The secret can be queried using k3s and kubectl like so:\nk3s kubectl -n karavi get secret/karavi-config-secret\nTo update or add parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nk3s kubectl -n karavi get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d\nSave the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64\nCopy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nk3s kubectl -n karavi edit secret/karavi-config-secret\nReplace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM for Authorization will read the changed secret.\n Note: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command like so. The --insecure flag is only necessary if certificates were not provided in $HOME/.karavi/config.json\n karavictl generate token --tenant $TenantName --insecure --addr \"grpc.${AuthorizationProxyHost}\" | jq -r '.Token' \u003e kubectl -n $namespace apply -f -\nCSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM for Authorization logging settings during runtime, run the below command on the K3s cluster, make your changes, and save the updated configmap data.\nk3s kubectl -n karavi edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","excerpt":"This section outlines the deployment steps for Container Storage …","ref":"/csm-docs/docs/authorization/deployment/rpm/","title":"RPM"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Authorization.\nUninstalling the RPM To uninstall the rpm package on the system, run the below command:\nrpm -e \u003crpm_file_name\u003e Uninstalling the sidecar-proxy in the CSI Driver To uninstall the sidecar-proxy in the CSI Driver, uninstall the driver and reinstall the driver using the original configuration secret.\n","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/docs/authorization/uninstallation/","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Resiliency.\nUninstalling the sidecar in the CSI Driver To uninstall the sidecar in the CSI Driver, uninstall the driver and reinstall the driver with the podmon feature disabled.\n","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/docs/resiliency/uninstallation/","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Authorization.\nUninstalling the RPM To uninstall the rpm package on the system, run the below command:\nrpm -e \u003crpm_file_name\u003e Uninstalling the sidecar-proxy in the CSI Driver To uninstall the sidecar-proxy in the CSI Driver, uninstall the driver and reinstall the driver using the original configuration secret.\n","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/v1/authorization/uninstallation/","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Resiliency.\nUninstalling the sidecar in the CSI Driver To uninstall the sidecar in the CSI Driver, uninstall the driver and reinstall the driver with the podmon feature disabled.\n","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/v1/resiliency/uninstallation/","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Authorization.\nUninstalling the RPM To uninstall the rpm package on the system, run the below command:\nrpm -e \u003crpm_file_name\u003e Uninstalling the sidecar-proxy in the CSI Driver To uninstall the sidecar-proxy in the CSI Driver, uninstall the driver and reinstall the driver using the original configuration secret.\n","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/v2/authorization/uninstallation/","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Resiliency.\nUninstalling the sidecar in the CSI Driver To uninstall the sidecar in the CSI Driver, uninstall the driver and reinstall the driver with the podmon feature disabled.\n","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/v2/resiliency/uninstallation/","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Authorization.\nUninstalling the RPM To uninstall the rpm package on the system, run the below command:\nrpm -e \u003crpm_file_name\u003e Uninstalling the sidecar-proxy in the CSI Driver To uninstall the sidecar-proxy in the CSI Driver, uninstall the driver and reinstall the driver using the original configuration secret.\n","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/v3/authorization/uninstallation/","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Resiliency.\nUninstalling the sidecar in the CSI Driver To uninstall the sidecar in the CSI Driver, uninstall the driver and reinstall the driver with the podmon feature disabled.\n","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/v3/resiliency/uninstallation/","title":"Uninstallation"},{"body":"CSM for Resiliency is primarily designed to detect pod failures due to some kind of node failure or node communication failure. The diagram below shows the hardware environment that is assumed in the design.\nA Kubernetes Control Plane is assumed to exist that provides the K8S API service used by CSM for Resiliency. There is an arbitrary number of worker nodes (two are shown in the diagram) that are connected to the Control Plane through a K8S Control Plane IP Network.\nThe worker nodes (e.g. Node1 and Node2) can run a mix of CSM for Resiliency monitored Application Pods as well as unmonitored Application Pods. Monitored Pods are designated by a specific label that is applied to each monitored pod. The label key and value are configurable for each driver type when CSM for Resiliency is installed and must be unique for each driver instance.\nThe Worker Nodes are assumed to also have a connection to a Storage System Array (such as PowerFlex.) It is often preferred that a separate network be used for storage access from the network used by the K8S control plane, and CSM for Resiliency takes advantage of the separate networks when available.\nAnti Use-Cases CSM for Resiliency does not generally try to handle any of the following errors:\n  Failure of the Kubernetes control plane, the etcd database used by Kubernetes, or the like. Kubernetes is generally designed to provide a highly available container orchestration system, and it is assumed clients follow the standard and/or best practices in configuring their Kubernetes deployments.\n  CSM for Resiliency is generally not designed to take action upon a failure solely of the Application Pod(s). Applications are still responsible for detecting and providing recovery mechanisms should their application fail. There are some specific recommendations for applications to be monitored by CSM for Resiliency that are described later.\n  Failure Model CSM for Resiliency’s design is focused on detecting the following types of hardware failures, and when they occur, moving protected pods to hardware that is functioning correctly:\n  Node failure. Node failure is defined to be similar to a Power Failure to the node which causes it to cease operation. This is differentiated from Node Communication Failures which require different treatments. Node failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\nGenerally, it is difficult to distinguish from the outside if a node is truly down (not executing) versus if it has lost connectivity on all its interfaces. (We might add capabilities in the future to query BIOS interfaces such as iDRAC, or perhaps periodically writing to file systems mounted in node-podmon to detect I/O failures, in order to get additional insight as to node status.) However, if the node has simply lost all outside communication paths, the protected pods are possibly still running. We refer to these pods as “zombie pods”. CSM for Resiliency is designed to deal with zombie pods in a way that prevents them from interfering with replacement pods it may have made by fencing the failed nodes and when communication is re-established to the node, going through a cleaning procedure to remove the zombie pod artifacts before allowing the node to go back into service.\n  K8S Control Plane Network Failure. Control Plane Network Failure often has the same K8S failure signature (the node is tainted with NoSchedule or NoExecute). However, if there is a separate Array I/O interface, CSM for Resiliency can often detect that the Array I/O Network may be active even though the Control Plane Network is down.\n  Array I/O Network failure is detected by polling the array to determine if the array has a healthy connection to the node. The capabilities to do this vary greatly by array and communication protocol type (Fibre Channel, iSCSI, NFS, NVMe, or PowerFlex SDC IP protocol). By monitoring the Array I/O Network separately from the Control Plane Network, CSM for Resiliency has two different indicators of whether the node is healthy or not.\n  K8S Control Plane Failure. Control Plane Failure is defined as failure of kubelet in a given node. K8S Control Plane failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\n  CSI Driver node pods. CSM for Resiliency monitors CSI driver node pods.If for any reason the CSI Driver node pods fail and enter the Not Ready state, it will taint the node with NoSchedule value. This will disable kubernetes scheduler to schedule new workloads on the given node, hence avoid workloads that needed CSI Driver pods to be in Ready state.\n  ","excerpt":"CSM for Resiliency is primarily designed to detect pod failures due to …","ref":"/csm-docs/docs/resiliency/usecases/","title":"Use Cases"},{"body":"CSM for Resiliency is primarily designed to detect pod failures due to some kind of node failure or node communication failure. The diagram below shows the hardware environment that is assumed in the design.\nA Kubernetes Control Plane is assumed to exist that provides the K8S API service used by CSM for Resiliency. There is an arbitrary number of worker nodes (two are shown in the diagram) that are connected to the Control Plane through a K8S Control Plane IP Network.\nThe worker nodes (e.g. Node1 and Node2) can run a mix of CSM for Resiliency monitored Application Pods as well as unmonitored Application Pods. Monitored Pods are designated by a specific label that is applied to each monitored pod. The label key and value are configurable for each driver type when CSM for Resiliency is installed and must be unique for each driver instance.\nThe Worker Nodes are assumed to also have a connection to a Storage System Array (such as PowerFlex.) It is often preferred that a separate network be used for storage access from the network used by the K8S control plane, and CSM for Resiliency takes advantage of the separate networks when available.\nAnti Use-Cases CSM for Resiliency does not generally try to handle any of the following errors:\n  Failure of the Kubernetes control plane, the etcd database used by Kubernetes, or the like. Kubernetes is generally designed to provide a highly available container orchestration system, and it is assumed clients follow the standard and/or best practices in configuring their Kubernetes deployments.\n  CSM for Resiliency is generally not designed to take action upon a failure solely of the Application Pod(s). Applications are still responsible for detecting and providing recovery mechanisms should their application fail. There are some specific recommendations for applications to be monitored by CSM for Resiliency that are described later.\n  Failure Model CSM for Resiliency’s design is focused on detecting the following types of hardware failures, and when they occur, moving protected pods to hardware that is functioning correctly:\n  Node failure. Node failure is defined to be similar to a Power Failure to the node which causes it to cease operation. This is differentiated from Node Communication Failures which require different treatments. Node failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\nGenerally, it is difficult to distinguish from the outside if a node is truly down (not executing) versus if it has lost connectivity on all its interfaces. (We might add capabilities in the future to query BIOS interfaces such as iDRAC, or perhaps periodically writing to file systems mounted in node-podmon to detect I/O failures, in order to get additional insight as to node status.) However, if the node has simply lost all outside communication paths, the protected pods are possibly still running. We refer to these pods as “zombie pods”. CSM for Resiliency is designed to deal with zombie pods in a way that prevents them from interfering with replacement pods it may have made by fencing the failed nodes and when communication is re-established to the node, going through a cleaning procedure to remove the zombie pod artifacts before allowing the node to go back into service.\n  K8S Control Plane Network Failure. Control Plane Network Failure often has the same K8S failure signature (the node is tainted with NoSchedule or NoExecute). However, if there is a separate Array I/O interface, CSM for Resiliency can often detect that the Array I/O Network may be active even though the Control Plane Network is down.\n  Array I/O Network failure is detected by polling the array to determine if the array has a healthy connection to the node. The capabilities to do this vary greatly by array and communication protocol type (Fibre Channel, iSCSI, NFS, NVMe, or PowerFlex SDC IP protocol). By monitoring the Array I/O Network separately from the Control Plane Network, CSM for Resiliency has two different indicators of whether the node is healthy or not.\n  K8S Control Plane Failure. Control Plane Failure is defined as failure of kubelet in a given node. K8S Control Plane failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\n  ","excerpt":"CSM for Resiliency is primarily designed to detect pod failures due to …","ref":"/csm-docs/v1/resiliency/usecases/","title":"Use Cases"},{"body":"CSM for Resiliency is primarily designed to detect pod failures due to some kind of node failure or node communication failure. The diagram below shows the hardware environment that is assumed in the design.\nA Kubernetes Control Plane is assumed to exist that provides the K8S API service used by CSM for Resiliency. There is an arbitrary number of worker nodes (two are shown in the diagram) that are connected to the Control Plane through a K8S Control Plane IP Network.\nThe worker nodes (e.g. Node1 and Node2) can run a mix of CSM for Resiliency monitored Application Pods as well as unmonitored Application Pods. Monitored Pods are designated by a specific label that is applied to each monitored pod. The label key and value are configurable for each driver type when CSM for Resiliency is installed and must be unique for each driver instance.\nThe Worker Nodes are assumed to also have a connection to a Storage System Array (such as PowerFlex.) It is often preferred that a separate network be used for storage access from the network used by the K8S control plane, and CSM for Resiliency takes advantage of the separate networks when available.\nAnti Use-Cases CSM for Resiliency does not generally try to handle any of the following errors:\n  Failure of the Kubernetes control plane, the etcd database used by Kubernetes, or the like. Kubernetes is generally designed to provide a highly available container orchestration system, and it is assumed clients follow the standard and/or best practices in configuring their Kubernetes deployments.\n  CSM for Resiliency is generally not designed to take action upon a failure solely of the Application Pod(s). Applications are still responsible for detecting and providing recovery mechanisms should their application fail. There are some specific recommendations for applications to be monitored by CSM for Resiliency that are described later.\n  Failure Model CSM for Resiliency’s design is focused on detecting the following types of hardware failures, and when they occur, moving protected pods to hardware that is functioning correctly:\n  Node failure. Node failure is defined to be similar to a Power Failure to the node which causes it to cease operation. This is differentiated from Node Communication Failures which require different treatments. Node failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\nGenerally, it is difficult to distinguish from the outside if a node is truly down (not executing) versus if it has lost connectivity on all its interfaces. (We might add capabilities in the future to query BIOS interfaces such as iDRAC, or perhaps periodically writing to file systems mounted in node-podmon to detect I/O failures, in order to get additional insight as to node status.) However, if the node has simply lost all outside communication paths, the protected pods are possibly still running. We refer to these pods as “zombie pods”. CSM for Resiliency is designed to deal with zombie pods in a way that prevents them from interfering with replacement pods it may have made by fencing the failed nodes and when communication is re-established to the node, going through a cleaning procedure to remove the zombie pod artifacts before allowing the node to go back into service.\n  K8S Control Plane Network Failure. Control Plane Network Failure often has the same K8S failure signature (the node is tainted with NoSchedule or NoExecute). However, if there is a separate Array I/O interface, CSM for Resiliency can often detect that the Array I/O Network may be active even though the Control Plane Network is down.\n  Array I/O Network failure is detected by polling the array to determine if the array has a healthy connection to the node. The capabilities to do this vary greatly by array and communication protocol type (Fibre Channel, iSCSI, NFS, NVMe, or PowerFlex SDC IP protocol). By monitoring the Array I/O Network separately from the Control Plane Network, CSM for Resiliency has two different indicators of whether the node is healthy or not.\n  K8S Control Plane Failure. Control Plane Failure is defined as failure of kubelet in a given node. K8S Control Plane failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\n  ","excerpt":"CSM for Resiliency is primarily designed to detect pod failures due to …","ref":"/csm-docs/v2/resiliency/usecases/","title":"Use Cases"},{"body":"CSM for Resiliency is primarily designed to detect pod failures due to some kind of node failure or node communication failure. The diagram below shows the hardware environment that is assumed in the design.\nA Kubernetes Control Plane is assumed to exist that provides the K8S API service used by CSM for Resiliency. There is an arbitrary number of worker nodes (two are shown in the diagram) that are connected to the Control Plane through a K8S Control Plane IP Network.\nThe worker nodes (e.g. Node1 and Node2) can run a mix of CSM for Resiliency monitored Application Pods as well as unmonitored Application Pods. Monitored Pods are designated by a specific label that is applied to each monitored pod. The label key and value are configurable for each driver type when CSM for Resiliency is installed and must be unique for each driver instance.\nThe Worker Nodes are assumed to also have a connection to a Storage System Array (such as PowerFlex.) It is often preferred that a separate network be used for storage access from the network used by the K8S control plane, and CSM for Resiliency takes advantage of the separate networks when available.\nAnti Use-Cases CSM for Resiliency does not generally try to handle any of the following errors:\n  Failure of the Kubernetes control plane, the etcd database used by Kubernetes, or the like. Kubernetes is generally designed to provide a highly available container orchestration system, and it is assumed clients follow the standard and/or best practices in configuring their Kubernetes deployments.\n  CSM for Resiliency is generally not designed to take action upon a failure solely of the Application Pod(s). Applications are still responsible for detecting and providing recovery mechanisms should their application fail. There are some specific recommendations for applications to be monitored by CSM for Resiliency that are described later.\n  Failure Model CSM for Resiliency’s design is focused on detecting the following types of hardware failures, and when they occur, moving protected pods to hardware that is functioning correctly:\n  Node failure. Node failure is defined to be similar to a Power Failure to the node which causes it to cease operation. This is differentiated from Node Communication Failures which require different treatments. Node failures are generally discovered by receipt of a Node event with a NoSchedule or NoExecute taint, or detection of such a taint when retrieving the Node via the K8S API.\nGenerally, it is difficult to distinguish from the outside if a node is truly down (not executing) versus if it has lost connectivity on all its interfaces. (We might add capabilities in the future to query BIOS interfaces such as iDRAC, or perhaps periodically writing to file systems mounted in node-podmon to detect I/O failures, in order to get additional insight as to node status.) However, if the node has simply lost all outside communication paths, the protected pods are possibly still running. We refer to these pods as “zombie pods”. CSM for Resiliency is designed to deal with zombie pods in a way that prevents them from interfering with replacement pods it may have made by fencing the failed nodes and when communication is re-established to the node, going through a cleaning procedure to remove the zombie pod artifacts before allowing the node to go back into service.\n  K8S Control Plane Network Failure. Control Plane Network Failure often has the same K8S failure signature (the node is tainted with NoSchedule or NoExecute). However, if there is a separate Array I/O interface, CSM for Resiliency can often detect that the Array I/O Network may be active even though the Control Plane Network is down.\n  Array I/O Network failure is detected by polling the array to determine if the array has a healthy connection to the node. The capabilities to do this vary greatly by array and communication protocol type (Fibre Channel, iSCSI, NFS, NVMe, or PowerFlex SDC IP protocol). By monitoring the Array I/O Network separately from the Control Plane Network, CSM for Resiliency has two different indicators of whether the node is healthy or not.\n  ","excerpt":"CSM for Resiliency is primarily designed to detect pod failures due to …","ref":"/csm-docs/v3/resiliency/usecases/","title":"Use Cases"},{"body":"This section provides the details and instructions on how to install the CSI Driver components using the provided Helm charts and in the case of the CSI drivers, the Dell CSI Helm Installer.\nDependencies Installing any of the CSI Driver components using Helm requires a few utilities to be installed on the system running the installation.\n   Dependency Usage     kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver.   helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3.   sshpass sshpass is used to check certain pre-requisites in worker nodes (in chosen drivers).    Note: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","excerpt":"This section provides the details and instructions on how to install …","ref":"/csm-docs/docs/csidriver/installation/helm/","title":"CSI Driver installation using Helm"},{"body":"This section provides the details and instructions on how to install the CSI Driver components using the provided Helm charts and in the case of the CSI drivers, the Dell CSI Helm Installer.\nDependencies Installing any of the CSI Driver components using Helm requires a few utilities to be installed on the system running the installation.\n   Dependency Usage     kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver.   helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3.   sshpass sshpass is used to check certain pre-requisites in worker nodes (in chosen drivers).    Note: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","excerpt":"This section provides the details and instructions on how to install …","ref":"/csm-docs/v1/csidriver/installation/helm/","title":"CSI Driver installation using Helm"},{"body":"This section provides the details and instructions on how to install the CSI Driver components using the provided Helm charts and in the case of the CSI drivers, the Dell CSI Helm Installer.\nDependencies Installing any of the CSI Driver components using Helm requires a few utilities to be installed on the system running the installation.\n   Dependency Usage     kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver.   helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3.   sshpass sshpass is used to check certain pre-requisites in worker nodes (in chosen drivers).    Note: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","excerpt":"This section provides the details and instructions on how to install …","ref":"/csm-docs/v2/csidriver/installation/helm/","title":"CSI Driver installation using Helm"},{"body":"This section provides the details and instructions on how to install the CSI Driver components using the provided Helm charts and in the case of the CSI drivers, the Dell CSI Helm Installer.\nDependencies Installing any of the CSI Driver components using Helm requires a few utilities to be installed on the system running the installation.\n   Dependency Usage     kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver.   helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3.   sshpass sshpass is used to check certain pre-requisites in worker nodes (in chosen drivers).    Note: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","excerpt":"This section provides the details and instructions on how to install …","ref":"/csm-docs/v3/csidriver/installation/helm/","title":"CSI Driver installation using Helm"},{"body":"Replication Cluster Topologies Container Storage Modules (CSM) for Replication project supports the replication of volumes within a single Kubernetes cluster or between two different Kubernetes clusters. The replication controller can support multiple clusters at once, but a single volume can be replicated to a maximum of two clusters.\nEach cluster should be assigned the unique identifier clusterId. The rules for naming are as follows:\n must be 63 characters or fewer (cannot be empty) must begin and end with an alphanumeric character ([a-z, 0-9, A-Z]) could contain dashes (-), underscores (_), dots (.), and alphanumerics between must be unique across clusters ``  Single Cluster Replication Cluster Configuration When configuring replication within a single cluster, you need to create a ConfigMap with at least the clusterId field configured to point to the current cluster:\napiVersion:v1data:config.yaml:| clusterId: cluster-Atargets:[]kind:ConfigMapmetadata:name:dell-replication-controller-confignamespace:dell-replication-controllerNote that the targets parameter is left empty since we don’t require any target clusters to work within a single cluster. This also means that you don’t need to create any Secrets that contain connection information to such clusters, since in this use case, we are limited to a single cluster.\nYou can find more info about configs and secrets for cluster communication in configmaps-secrets\nStorage Class Configuration To create volumes that would be replicated within a single cluster, you need to create a special StorageClass. This StorageClass should contain the usual replication parameter replication.storage.dell.com/remoteClusterID, and it should be set to self to indicate that we want to replicate the volume inside the current cluster.\nAlso, you would need to create another storage class in the same cluster that would serve as a target storage class. This means that all replicated volumes would be derived from it. Its replication.storage.dell.com/remoteClusterID parameter should be also set to self.\nYou can find out more about replication StorageClasses and replication specific parameters in storageclasses\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a single cluster replication, replicated resources (PersistentVolumes, ReplicationGroups) would be created in the same cluster with the replicated- prefix added to them.\nExample:\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s replicated-csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication-tgt 23s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:23:18Z rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z Multiple Cluster Replication Cluster Configuration Similar to a single cluster scenario, you need to create ConfigMap, but this time you need to provide at least one target cluster. You can provide as many as you like but be mindful that a single volume can be replicated to only one of them.\nFor example:\napiVersion:v1data:config.yaml:| clusterId: cluster-Atargets:- clusterId:cluster-Baddress:192.168.111.21secretRef:secretClusterBkind:ConfigMapmetadata:name:dell-replication-controller-confignamespace:dell-replication-controllerNote that target cluster information contains a field called secretRef. This field points to a secret available in the current cluster that contains connection information of cluster-B in the form of a kubeconfig file.\nYou can find more information about how to create such secrets in configmaps-secrets\nStorage Class Configuration To create replicated volumes in the multi-cluster configuration you still need to have a special storage class. Replication parameter replication.storage.dell.com/remoteClusterID should be set to the cluster-id of whatever cluster you want to replicate your volumes.\nFor multi-cluster replication, we can choose one of the target cluster ids we specified in ConfigMap. In our example replication parameter, the target cluster id should be equal to cluster-B.\nYou can find more information about other replication parameters available in storage classes here\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a multi-cluster replication, replicated resources would be created in both source and target clusters under the same names.\nExample:\n[CLUSTER-A] $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z [CLUSTER-B] $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication 18s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 30s Ready SYNCHRONIZED 2021-08-03T11:22:18Z ","excerpt":"Replication Cluster Topologies Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/replication/cluster-topologies/","title":"Cluster Topologies"},{"body":"Replication Cluster Topologies Container Storage Modules (CSM) for Replication project supports the replication of volumes within a single Kubernetes cluster or between two different Kubernetes clusters. The replication controller can support multiple clusters at once, but a single volume can be replicated to a maximum of two clusters.\nEach cluster should be assigned the unique identifier clusterId. The rules for naming are as follows:\n must be 63 characters or fewer (cannot be empty) must begin and end with an alphanumeric character ([a-z, 0-9, A-Z]) could contain dashes (-), underscores (_), dots (.), and alphanumerics between must be unique across clusters ``  Single Cluster Replication Cluster Configuration When configuring replication within a single cluster, you need to create a ConfigMap with at least the clusterId field configured to point to the current cluster:\napiVersion:v1data:config.yaml:| clusterId: cluster-Atargets:[]kind:ConfigMapmetadata:name:dell-replication-controller-confignamespace:dell-replication-controllerNote that the targets parameter is left empty since we don’t require any target clusters to work within a single cluster. This also means that you don’t need to create any Secrets that contain connection information to such clusters, since in this use case, we are limited to a single cluster.\nYou can find more info about configs and secrets for cluster communication in configmaps-secrets\nStorage Class Configuration To create volumes that would be replicated within a single cluster, you need to create a special StorageClass. This StorageClass should contain the usual replication parameter replication.storage.dell.com/remoteClusterID, and it should be set to self to indicate that we want to replicate the volume inside the current cluster.\nAlso, you would need to create another storage class in the same cluster that would serve as a target storage class. This means that all replicated volumes would be derived from it. Its replication.storage.dell.com/remoteClusterID parameter should be also set to self.\nYou can find out more about replication StorageClasses and replication specific parameters in storageclasses\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a single cluster replication, replicated resources (PersistentVolumes, ReplicationGroups) would be created in the same cluster with the replicated- prefix added to them.\nExample:\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s replicated-csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication-tgt 23s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:23:18Z rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z Multiple Cluster Replication Cluster Configuration Similar to a single cluster scenario, you need to create ConfigMap, but this time you need to provide at least one target cluster. You can provide as many as you like but be mindful that a single volume can be replicated to only one of them.\nFor example:\napiVersion:v1data:config.yaml:| clusterId: cluster-Atargets:- clusterId:cluster-Baddress:192.168.111.21secretRef:secretClusterBkind:ConfigMapmetadata:name:dell-replication-controller-confignamespace:dell-replication-controllerNote that target cluster information contains a field called secretRef. This field points to a secret available in the current cluster that contains connection information of cluster-B in the form of a kubeconfig file.\nYou can find more information about how to create such secrets in configmaps-secrets\nStorage Class Configuration To create replicated volumes in the multi-cluster configuration you still need to have a special storage class. Replication parameter replication.storage.dell.com/remoteClusterID should be set to the cluster-id of whatever cluster you want to replicate your volumes.\nFor multi-cluster replication, we can choose one of the target cluster ids we specified in ConfigMap. In our example replication parameter, the target cluster id should be equal to cluster-B.\nYou can find more information about other replication parameters available in storage classes here\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a multi-cluster replication, replicated resources would be created in both source and target clusters under the same names.\nExample:\n[CLUSTER-A] $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z [CLUSTER-B] $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication 18s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 30s Ready SYNCHRONIZED 2021-08-03T11:22:18Z ","excerpt":"Replication Cluster Topologies Container Storage Modules (CSM) for …","ref":"/csm-docs/v1/replication/cluster-topologies/","title":"Cluster Topologies"},{"body":"Replication Cluster Topologies Container Storage Modules (CSM) for Replication project supports the replication of volumes within a single Kubernetes cluster or between two different Kubernetes clusters. The replication controller can support multiple clusters at once, but a single volume can be replicated to a maximum of two clusters.\nEach cluster should be assigned the unique identifier clusterId. The rules for naming are as follows:\n must be 63 characters or fewer (cannot be empty) must begin and end with an alphanumeric character ([a-z, 0-9, A-Z]) could contain dashes (-), underscores (_), dots (.), and alphanumerics between must be unique across clusters ``  Single Cluster Replication Cluster Configuration When configuring replication within a single cluster, you need to create a ConfigMap with at least the clusterId field configured to point to the current cluster:\napiVersion:v1data:config.yaml:| clusterId: cluster-Atargets:[]kind:ConfigMapmetadata:name:dell-replication-controller-confignamespace:dell-replication-controllerNote that the targets parameter is left empty since we don’t require any target clusters to work within a single cluster. This also means that you don’t need to create any Secrets that contain connection information to such clusters, since in this use case, we are limited to a single cluster.\nYou can find more info about configs and secrets for cluster communication in configmaps-secrets\nStorage Class Configuration To create volumes that would be replicated within a single cluster, you need to create a special StorageClass. This StorageClass should contain the usual replication parameter replication.storage.dell.com/remoteClusterID, and it should be set to self to indicate that we want to replicate the volume inside the current cluster.\nAlso, you would need to create another storage class in the same cluster that would serve as a target storage class. This means that all replicated volumes would be derived from it. Its replication.storage.dell.com/remoteClusterID parameter should be also set to self.\nYou can find out more about replication StorageClasses and replication specific parameters in storageclasses\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a single cluster replication, replicated resources (PersistentVolumes, ReplicationGroups) would be created in the same cluster with the replicated- prefix added to them.\nExample:\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s replicated-csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication-tgt 23s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:23:18Z rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z Multiple Cluster Replication Cluster Configuration Similar to a single cluster scenario, you need to create ConfigMap, but this time you need to provide at least one target cluster. You can provide as many as you like but be mindful that a single volume can be replicated to only one of them.\nFor example:\napiVersion:v1data:config.yaml:| clusterId: cluster-Atargets:- clusterId:cluster-Baddress:192.168.111.21secretRef:secretClusterBkind:ConfigMapmetadata:name:dell-replication-controller-confignamespace:dell-replication-controllerNote that target cluster information contains a field called secretRef. This field points to a secret available in the current cluster that contains connection information of cluster-B in the form of a kubeconfig file.\nYou can find more information about how to create such secrets in configmaps-secrets\nStorage Class Configuration To create replicated volumes in the multi-cluster configuration you still need to have a special storage class. Replication parameter replication.storage.dell.com/remoteClusterID should be set to the cluster-id of whatever cluster you want to replicate your volumes.\nFor multi-cluster replication, we can choose one of the target cluster ids we specified in ConfigMap. In our example replication parameter, the target cluster id should be equal to cluster-B.\nYou can find more information about other replication parameters available in storage classes here\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a multi-cluster replication, replicated resources would be created in both source and target clusters under the same names.\nExample:\n[CLUSTER-A] $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z [CLUSTER-B] $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication 18s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 30s Ready SYNCHRONIZED 2021-08-03T11:22:18Z ","excerpt":"Replication Cluster Topologies Container Storage Modules (CSM) for …","ref":"/csm-docs/v2/replication/cluster-topologies/","title":"Cluster Topologies"},{"body":"Replication Cluster Topologies Container Storage Modules (CSM) for Replication project supports the replication of volumes within a single Kubernetes cluster or between two different Kubernetes clusters. The replication controller can support multiple clusters at once, but a single volume can be replicated to a maximum of two clusters.\nEach cluster should be assigned the unique identifier clusterId. The rules for naming are as follows:\n must be 63 characters or fewer (cannot be empty) must begin and end with an alphanumeric character ([a-z, 0-9, A-Z]) could contain dashes (-), underscores (_), dots (.), and alphanumerics between must be unique across clusters ``  Single Cluster Replication Cluster Configuration When configuring replication within a single cluster, you need to create a ConfigMap with at least the clusterId field configured to point to the current cluster:\napiVersion:v1data:config.yaml:| clusterId: cluster-Atargets:[]kind:ConfigMapmetadata:name:dell-replication-controller-confignamespace:dell-replication-controllerNote that the targets parameter is left empty since we don’t require any target clusters to work within a single cluster. This also means that you don’t need to create any Secrets that contain connection information to such clusters, since in this use case, we are limited to a single cluster.\nYou can find more info about configs and secrets for cluster communication in configmaps-secrets\nStorage Class Configuration To create volumes that would be replicated within a single cluster, you need to create a special StorageClass. This StorageClass should contain the usual replication parameter replication.storage.dell.com/remoteClusterID, and it should be set to self to indicate that we want to replicate the volume inside the current cluster.\nAlso, you would need to create another storage class in the same cluster that would serve as a target storage class. This means that all replicated volumes would be derived from it. Its replication.storage.dell.com/remoteClusterID parameter should be also set to self.\nYou can find out more about replication StorageClasses and replication specific parameters in storageclasses\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a single cluster replication, replicated resources (PersistentVolumes, ReplicationGroups) would be created in the same cluster with the replicated- prefix added to them.\nExample:\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s replicated-csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication-tgt 23s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:23:18Z rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z Multiple Cluster Replication Cluster Configuration Similar to a single cluster scenario, you need to create ConfigMap, but this time you need to provide at least one target cluster. You can provide as many as you like but be mindful that a single volume can be replicated to only one of them.\nFor example:\napiVersion:v1data:config.yaml:| clusterId: cluster-Atargets:- clusterId:cluster-Baddress:192.168.111.21secretRef:secretClusterBkind:ConfigMapmetadata:name:dell-replication-controller-confignamespace:dell-replication-controllerNote that target cluster information contains a field called secretRef. This field points to a secret available in the current cluster that contains connection information of cluster-B in the form of a kubeconfig file.\nYou can find more information about how to create such secrets in configmaps-secrets\nStorage Class Configuration To create replicated volumes in the multi-cluster configuration you still need to have a special storage class. Replication parameter replication.storage.dell.com/remoteClusterID should be set to the cluster-id of whatever cluster you want to replicate your volumes.\nFor multi-cluster replication, we can choose one of the target cluster ids we specified in ConfigMap. In our example replication parameter, the target cluster id should be equal to cluster-B.\nYou can find more information about other replication parameters available in storage classes here\nReplicated Resources When creating PersistentVolumeClaims using StorageClass for a multi-cluster replication, replicated resources would be created in both source and target clusters under the same names.\nExample:\n[CLUSTER-A] $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Bound powerstore-replication 23s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 34s Ready SYNCHRONIZED 2021-08-03T11:22:18Z [CLUSTER-B] $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS STORAGECLASS AGE csivol-06d51bfcc5 3Gi RWO Retain Available powerstore-replication 18s $ kubectl get rg NAME AGE STATE LINK STATE LAST LINKSTATE UPDATE rg-240721b0-12fb-4151-8dd8-94794ae2493e 30s Ready SYNCHRONIZED 2021-08-03T11:22:18Z ","excerpt":"Replication Cluster Topologies Container Storage Modules (CSM) for …","ref":"/csm-docs/v3/replication/cluster-topologies/","title":"Cluster Topologies"},{"body":"Communication between clusters Container Storage Modules (CSM) for Replication Controller requires access to remote clusters for replicating various objects. There are two ways to set up this communication -\n Using Normal Kubernetes users Using ServiceAccount token  You need to create secrets (using either of the two methods) in each cluster involved in replication and provide their references in ConfigMap objects which are used to configure the respective CSM Replication Controllers.\n Important: Direct network visibility between clusters required for CSM-Replication to work. Cluster-1’s API URL has to be pingable from cluster-2 pods and vice versa. If private networks are used and/or DNS is not set up properly - you may need to modify /etc/hosts file from within controller’s pod. This can be achieved by using helm installation method. Refer to the link\n  Note: If you are using a single stretched cluster, then you can skip all the following steps\n Inject configuration using repctl This is the simplest way to configure CSM Replication Controller. repctl simplifies the complex configuration process greatly by enabling creation of secrets and updating their references in multiple clusters.\nRecommended method Use repctl to create secrets using service account tokens and update ConfigMaps in multiple clusters in one command. Run the following command -\nrepctl cluster inject --use-sa This will create secrets using the token for the default ServiceAccount and update the ConfigMap in all the clusters which have been configured for repctl\nInject KubeConfigs from repctl configuration repctl is usually configured to communicate with multiple Kubernetes clusters and is provided with a set of KubeConfig files for each cluster. You can use repctl to inject secrets created using these files in each of the configured cluster. Run the following command -\nrepctl cluster inject  Note: For a detailed walkthrough of the simplified installation process using repctl, please refer this link\n Understanding the Config file If you are setting up replication between two clusters - Cluster A \u0026 Cluster B, then the configuration file (deploy/config.yaml) should look like this:\nCluster A clusterId:cluster-A# This cluster's Identifiertargets:- clusterId:cluster-B# Identifier for the remote cluster Baddress:192.168.111.21# Address of the remote clustersecretRef:secretClusterB# Name of the secret required for communication with Cluster BCluster B clusterId:cluster-B# This cluster's Identifiertargets:- clusterId:cluster-A# Identifier for the remote cluster Aaddress:192.168.111.20# Address of the remote clustersecretRef:secretClusterA# Name of the secret required for communication with Cluster AManual configuration Generating KubeConfig We provide a helper script which can help create KubeConfig files for a normal user as well as a Service Account.\n Using a Certificate Signing Request for a user  cd scripts ./gen-kubeconfig.sh -u \u003cCN user\u003e -c \u003cCSR\u003e -k \u003ckey\u003e # where \"CN user\" is the name of the user \u0026 key is the private key of the user  Create kubeconfig file for a Service Account  cd scripts ./gen-kubeconfig.sh -s \u003csa-name\u003e -n \u003cnamespace\u003e Once you have created the KubeConfig file, you can use it to create the secret.\nSecrets using normal Kubernetes users You can create a normal Kubernetes user for your remote Kubernetes cluster and use it for inter cluster communication. The process of creating users is outside the scope of this document. Once you have the user created, you can provide it the RBAC privileges required by the controller.\nExample Continuing from our earlier example with Cluster A \u0026 Cluster B -\n Create a user in Cluster B \u0026 generate a kubeconfig file for it using the helper script Create a ClusterRole in Cluster B using the following command: kubectl apply -f deploy/role.yaml  Create a ClusterRoleBinding in Cluster B for the user: kubectl create clusterrolebinding \u003cname\u003e --clusterrole=dell-replication-manager-role --user=\u003cuser-name\u003e  Create a secret in Cluster A using the kubeconfig file for this user  kubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller Secrets using ServiceAccount tokens You can use service account tokens to establish communication between various clusters. We recommend using the token for the default service account in the dell-replication-controller namespace after the installation as it already has all the required RBAC privileges.\nExample Use the following command to first create a KubeConfig file using the helper script in Cluster B -\n./gen-kubeconfig.sh -s default -n dell-replication-controller Once the KubeConfig file has been generated successfully, use the following command in Cluster A to to create the secret:\nkubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller ","excerpt":"Communication between clusters Container Storage Modules (CSM) for …","ref":"/csm-docs/docs/replication/deployment/configmap-secrets/","title":"ConfigMap \u0026 Secrets"},{"body":"Communication between clusters Container Storage Modules (CSM) for Replication Controller requires access to remote clusters for replicating various objects. There are two ways to set up this communication -\n Using Normal Kubernetes users Using ServiceAccount token  You need to create secrets (using either of the two methods) in each cluster involved in replication and provide their references in ConfigMap objects which are used to configure the respective CSM Replication Controllers.\n Important: Direct network visibility between clusters required for CSM-Replication to work. Cluster-1’s API URL has to be pingable from cluster-2 pods and vice versa. If private networks are used and/or DNS is not set up properly - you may need to modify /etc/hosts file from within controller’s pod. This can be achieved by using helm installation method. Refer to the link\n  Note: If you are using a single stretched cluster, then you can skip all the following steps\n Inject configuration using repctl This is the simplest way to configure CSM Replication Controller. repctl simplifies the complex configuration process greatly by enabling creation of secrets and updating their references in multiple clusters.\nRecommended method Use repctl to create secrets using service account tokens and update ConfigMaps in multiple clusters in one command. Run the following command -\nrepctl cluster inject --use-sa This will create secrets using the token for the default ServiceAccount and update the ConfigMap in all the clusters which have been configured for repctl\nInject KubeConfigs from repctl configuration repctl is usually configured to communicate with multiple Kubernetes clusters and is provided with a set of KubeConfig files for each cluster. You can use repctl to inject secrets created using these files in each of the configured cluster. Run the following command -\nrepctl cluster inject  Note: For a detailed walkthrough of the simplified installation process using repctl, please refer this link\n Understanding the Config file If you are setting up replication between two clusters - Cluster A \u0026 Cluster B, then the configuration file (deploy/config.yaml) should look like this:\nCluster A clusterId:cluster-A# This cluster's Identifiertargets:- clusterId:cluster-B# Identifier for the remote cluster Baddress:192.168.111.21# Address of the remote clustersecretRef:secretClusterB# Name of the secret required for communication with Cluster BCluster B clusterId:cluster-B# This cluster's Identifiertargets:- clusterId:cluster-A# Identifier for the remote cluster Aaddress:192.168.111.20# Address of the remote clustersecretRef:secretClusterA# Name of the secret required for communication with Cluster AManual configuration Generating KubeConfig We provide a helper script which can help create KubeConfig files for a normal user as well as a Service Account.\n Using a Certificate Signing Request for a user  cd scripts ./gen-kubeconfig.sh -u \u003cCN user\u003e -c \u003cCSR\u003e -k \u003ckey\u003e # where \"CN user\" is the name of the user \u0026 key is the private key of the user  Create kubeconfig file for a Service Account  cd scripts ./gen-kubeconfig.sh -s \u003csa-name\u003e -n \u003cnamespace\u003e Once you have created the KubeConfig file, you can use it to create the secret.\nSecrets using normal Kubernetes users You can create a normal Kubernetes user for your remote Kubernetes cluster and use it for inter cluster communication. The process of creating users is outside the scope of this document. Once you have the user created, you can provide it the RBAC privileges required by the controller.\nExample Continuing from our earlier example with Cluster A \u0026 Cluster B -\n Create a user in Cluster B \u0026 generate a kubeconfig file for it using the helper script Create a ClusterRole in Cluster B using the following command: kubectl apply -f deploy/role.yaml  Create a ClusterRoleBinding in Cluster B for the user: kubectl create clusterrolebinding \u003cname\u003e --clusterrole=dell-replication-manager-role --user=\u003cuser-name\u003e  Create a secret in Cluster A using the kubeconfig file for this user  kubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller Secrets using ServiceAccount tokens You can use service account tokens to establish communication between various clusters. We recommend using the token for the default service account in the dell-replication-controller namespace after the installation as it already has all the required RBAC privileges.\nExample Use the following command to first create a KubeConfig file using the helper script in Cluster B -\n./gen-kubeconfig.sh -s default -n dell-replication-controller Once the KubeConfig file has been generated successfully, use the following command in Cluster A to to create the secret:\nkubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller ","excerpt":"Communication between clusters Container Storage Modules (CSM) for …","ref":"/csm-docs/v1/replication/deployment/configmap-secrets/","title":"ConfigMap \u0026 Secrets"},{"body":"Communication between clusters Container Storage Modules (CSM) for Replication Controller requires access to remote clusters for replicating various objects. There are two ways to set up this communication -\n Using Normal Kubernetes users Using ServiceAccount token  You need to create secrets (using either of the two methods) in each cluster involved in replication and provide their references in ConfigMap objects which are used to configure the respective CSM Replication Controllers.\n Important: Direct network visibility between clusters required for CSM-Replication to work. Cluster-1’s API URL has to be pingable from cluster-2 pods and vice versa. If private networks are used and/or DNS is not set up properly - you may need to modify /etc/hosts file from within controller’s pod. This can be achieved by using helm installation method. Refer to the link\n  Note: If you are using a single stretched cluster, then you can skip all the following steps\n Inject configuration using repctl This is the simplest way to configure CSM Replication Controller. repctl simplifies the complex configuration process greatly by enabling creation of secrets and updating their references in multiple clusters.\nRecommended method Use repctl to create secrets using service account tokens and update ConfigMaps in multiple clusters in one command. Run the following command -\nrepctl cluster inject --use-sa This will create secrets using the token for the default ServiceAccount and update the ConfigMap in all the clusters which have been configured for repctl\nInject KubeConfigs from repctl configuration repctl is usually configured to communicate with multiple Kubernetes clusters and is provided with a set of KubeConfig files for each cluster. You can use repctl to inject secrets created using these files in each of the configured cluster. Run the following command -\nrepctl cluster inject  Note: For a detailed walkthrough of the simplified installation process using repctl, please refer this link\n Understanding the Config file If you are setting up replication between two clusters - Cluster A \u0026 Cluster B, then the configuration file (deploy/config.yaml) should look like this:\nCluster A clusterId:cluster-A# This cluster's Identifiertargets:- clusterId:cluster-B# Identifier for the remote cluster Baddress:192.168.111.21# Address of the remote clustersecretRef:secretClusterB# Name of the secret required for communication with Cluster BCluster B clusterId:cluster-B# This cluster's Identifiertargets:- clusterId:cluster-A# Identifier for the remote cluster Aaddress:192.168.111.20# Address of the remote clustersecretRef:secretClusterA# Name of the secret required for communication with Cluster AManual configuration Generating KubeConfig We provide a helper script which can help create KubeConfig files for a normal user as well as a Service Account.\n Using a Certificate Signing Request for a user  cd scripts ./gen-kubeconfig.sh -u \u003cCN user\u003e -c \u003cCSR\u003e -k \u003ckey\u003e # where \"CN user\" is the name of the user \u0026 key is the private key of the user  Create kubeconfig file for a Service Account  cd scripts ./gen-kubeconfig.sh -s \u003csa-name\u003e -n \u003cnamespace\u003e Once you have created the KubeConfig file, you can use it to create the secret.\nSecrets using normal Kubernetes users You can create a normal Kubernetes user for your remote Kubernetes cluster and use it for inter cluster communication. The process of creating users is outside the scope of this document. Once you have the user created, you can provide it the RBAC privileges required by the controller.\nExample Continuing from our earlier example with Cluster A \u0026 Cluster B -\n Create a user in Cluster B \u0026 generate a kubeconfig file for it using the helper script Create a ClusterRole in Cluster B using the following command: kubectl apply -f deploy/role.yaml  Create a ClusterRoleBinding in Cluster B for the user: kubectl create clusterrolebinding \u003cname\u003e --clusterrole=dell-replication-manager-role --user=\u003cuser-name\u003e  Create a secret in Cluster A using the kubeconfig file for this user  kubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller Secrets using ServiceAccount tokens You can use service account tokens to establish communication between various clusters. We recommend using the token for the default service account in the dell-replication-controller namespace after the installation as it already has all the required RBAC privileges.\nExample Use the following command to first create a KubeConfig file using the helper script in Cluster B -\n./gen-kubeconfig.sh -s default -n dell-replication-controller Once the KubeConfig file has been generated successfully, use the following command in Cluster A to to create the secret:\nkubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller ","excerpt":"Communication between clusters Container Storage Modules (CSM) for …","ref":"/csm-docs/v2/replication/deployment/configmap-secrets/","title":"ConfigMap \u0026 Secrets"},{"body":"Communication between clusters Container Storage Modules (CSM) for Replication Controller requires access to remote clusters for replicating various objects. There are two ways to set up this communication -\n Using Normal Kubernetes users Using ServiceAccount token  You need to create secrets (using either of the two methods) in each cluster involved in replication and provide their references in ConfigMap objects which are used to configure the respective CSM Replication Controllers.\n Important: Direct network visibility between clusters required for CSM-Replication to work. Cluster-1’s API URL has to be pingable from cluster-2 pods and vice versa.\n  Note: If you are using a single stretched cluster, then you can skip all the following steps\n Inject configuration using repctl This is the simplest way to configure CSM Replication Controller. repctl simplifies the complex configuration process greatly by enabling creation of secrets and updating their references in multiple clusters.\nRecommended method Use repctl to create secrets using service account tokens and update ConfigMaps in multiple clusters in one command. Run the following command -\nrepctl cluster inject --use-sa This will create secrets using the token for the default ServiceAccount and update the ConfigMap in all the clusters which have been configured for repctl\nInject KubeConfigs from repctl configuration repctl is usually configured to communicate with multiple Kubernetes clusters and is provided with a set of KubeConfig files for each cluster. You can use repctl to inject secrets created using these files in each of the configured cluster. Run the following command -\nrepctl cluster inject  Note: For a detailed walkthrough of the simplified installation process using repctl, please refer this link\n Understanding the Config file If you are setting up replication between two clusters - Cluster A \u0026 Cluster B, then the configuration file (deploy/config.yaml) should look like this:\nCluster A clusterId:cluster-A# This cluster's Identifiertargets:- clusterId:cluster-B# Identifier for the remote cluster Baddress:192.168.111.21# Address of the remote clustersecretRef:secretClusterB# Name of the secret required for communication with Cluster BCluster B clusterId:cluster-B# This cluster's Identifiertargets:- clusterId:cluster-A# Identifier for the remote cluster Aaddress:192.168.111.20# Address of the remote clustersecretRef:secretClusterA# Name of the secret required for communication with Cluster AManual configuration Generating KubeConfig We provide a helper script which can help create KubeConfig files for a normal user as well as a Service Account.\n Using a Certificate Signing Request for a user  cd scripts ./gen-kubeconfig.sh -u \u003cCN user\u003e -c \u003cCSR\u003e -k \u003ckey\u003e # where \"CN user\" is the name of the user \u0026 key is the private key of the user  Create kubeconfig file for a Service Account  cd scripts ./gen-kubeconfig.sh -s \u003csa-name\u003e -n \u003cnamespace\u003e Once you have created the KubeConfig file, you can use it to create the secret.\nSecrets using normal Kubernetes users You can create a normal Kubernetes user for your remote Kubernetes cluster and use it for inter cluster communication. The process of creating users is outside the scope of this document. Once you have the user created, you can provide it the RBAC privileges required by the controller.\nExample Continuing from our earlier example with Cluster A \u0026 Cluster B -\n Create a user in Cluster B \u0026 generate a kubeconfig file for it using the helper script Create a ClusterRole in Cluster B using the following command: kubectl apply -f deploy/role.yaml  Create a ClusterRoleBinding in Cluster B for the user: kubectl create clusterrolebinding \u003cname\u003e --clusterrole=dell-replication-manager-role --user=\u003cuser-name\u003e  Create a secret in Cluster A using the kubeconfig file for this user  kubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller Secrets using ServiceAccount tokens You can use service account tokens to establish communication between various clusters. We recommend using the token for the default service account in the dell-replication-controller namespace after the installation as it already has all the required RBAC privileges.\nExample Use the following command to first create a KubeConfig file using the helper script in Cluster B -\n./gen-kubeconfig.sh -s default -n dell-replication-controller Once the KubeConfig file has been generated successfully, use the following command in Cluster A to to create the secret:\nkubectl create secret generic \u003csecret-name\u003e --from-file=data=\u003ckubeconfig_file_user\u003e --namespace dell-replication-controller ","excerpt":"Communication between clusters Container Storage Modules (CSM) for …","ref":"/csm-docs/v3/replication/deployment/configmap-secrets/","title":"ConfigMap \u0026 Secrets"},{"body":"The CSI Drivers by Dell implement an interface between CSI (CSI spec v1.5) enabled Container Orchestrator (CO) and Dell Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nFeatures and capabilities Supported Operating Systems/Container Orchestrator Platforms    PowerMax PowerFlex Unity XT PowerScale PowerStore     Kubernetes 1.22, 1.23, 1.24 1.22, 1.23, 1.24 1.22, 1.23, 1.24 1.22, 1.23, 1.24 1.22, 1.23, 1.24   RHEL 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x   Ubuntu 20.04 20.04 18.04, 20.04 18.04, 20.04 20.04   CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9   SLES 15SP3 15SP3 15SP3 15SP3 15SP3   Red Hat OpenShift 4.9, 4.10, 4.10 EUS 4.9, 4.10, 4.10 EUS 4.9, 4.10, 4.10 EUS 4.9, 4.10, 4.10 EUS 4.9, 4.10, 4.10 EUS   Mirantis Kubernetes Engine 3.5.x 3.5.x 3.5.x 3.5.x 3.5.x   Google Anthos 1.9 1.8 no 1.9 1.9   VMware Tanzu no no NFS NFS NFS   Rancher Kubernetes Engine yes yes yes yes yes   Amazon Elastic Kubernetes Service\nAnywhere no yes no no yes    CSI Driver Capabilities   Features PowerMax PowerFlex Unity XT PowerScale PowerStore     CSI Driver version 2.3.0 2.3.0 2.3.0 2.3.0 2.3.0   Static Provisioning yes yes yes yes yes   Dynamic Provisioning yes yes yes yes yes   Expand Persistent Volume yes yes yes yes yes   Create VolumeSnapshot yes yes yes yes yes   Create Volume from Snapshot yes yes yes yes yes   Delete Snapshot yes yes yes yes yes   Access Mode FC/iSCSI: RWO/\nRWOP\nRaw block: RWO/\nRWX/\nROX/\nRWOP RWO/ROX/RWOP\nRWX (Raw block only) RWO/ROX/RWOP\nRWX (Raw block \u0026 NFS only) RWO/RWX/ROX/\nRWOP RWO/RWOP\n(FC/iSCSI)\nRWO/\nRWX/\nROX/\nRWOP\n(RawBlock, NFS)   CSI Volume Cloning yes yes yes yes yes   CSI Raw Block Volume yes yes yes no yes   CSI Ephemeral Volume no yes yes yes yes   Topology yes yes yes yes yes   Multi-array yes yes yes yes yes   Volume Health Monitoring yes yes yes yes yes    Supported Storage Platforms    PowerMax PowerFlex Unity XT PowerScale PowerStore     Storage Array 5978.479.479, 5978.711.711\nUnisphere 9.2 3.5.x, 3.6.x 5.0.7, 5.1.0, 5.1.2 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4 1.0.x, 2.0.x, 2.1.x, 3.0    Backend Storage Details   Features PowerMax PowerFlex Unity XT PowerScale PowerStore     Fibre Channel yes N/A yes N/A yes   iSCSI yes N/A yes N/A yes   NVMeTCP N/A N/A N/A N/A yes   NVMeFC N/A N/A N/A N/A yes   NFS N/A N/A yes yes yes   Other N/A ScaleIO protocol N/A N/A N/A   Supported FS ext4 / xfs ext4 / xfs ext3 / ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS   Thin / Thick provisioning Thin Thin Thin/Thick N/A Thin   Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering Policy\nNFS Host IO size\nSnapshot Retention duration Access Zone\nNFS version (3 or 4);Configurable Export IPs iSCSI CHAP    ","excerpt":"The CSI Drivers by Dell implement an interface between CSI (CSI spec …","ref":"/csm-docs/docs/csidriver/","title":"CSI Drivers"},{"body":"The CSI Drivers by Dell implement an interface between CSI (CSI spec v1.5) enabled Container Orchestrator (CO) and Dell Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nFeatures and capabilities Supported Operating Systems/Container Orchestrator Platforms    PowerMax PowerFlex Unity PowerScale PowerStore     Kubernetes 1.21, 1.22, 1.23 1.21, 1.22, 1.23 1.21, 1.22, 1.23 1.21, 1.22, 1.23 1.21, 1.22, 1.23   RHEL 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x   Ubuntu 20.04 20.04 18.04, 20.04 18.04, 20.04 20.04   CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9   SLES 15SP3 15SP3 15SP3 15SP3 15SP3   Red Hat OpenShift 4.8, 4.8 EUS, 4.9 4.8, 4.8 EUS, 4.9 4.8, 4.8 EUS, 4.9 4.8, 4.8 EUS, 4.9 4.8, 4.8 EUS, 4.9   Mirantis Kubernetes Engine 3.4.x 3.4.x 3.5.x 3.4.x 3.4.x   Google Anthos 1.6 1.8 no 1.9 1.9   VMware Tanzu no no NFS NFS NFS   Rancher Kubernetes Engine yes yes yes yes yes   Amazon Elastic Kubernetes Service\nAnywhere no yes no no yes    CSI Driver Capabilities   Features PowerMax PowerFlex Unity PowerScale PowerStore     CSI Driver version 2.2.0 2.2.0 2.2.0 2.2.0 2.2.0   Static Provisioning yes yes yes yes yes   Dynamic Provisioning yes yes yes yes yes   Expand Persistent Volume yes yes yes yes yes   Create VolumeSnapshot yes yes yes yes yes   Create Volume from Snapshot yes yes yes yes yes   Delete Snapshot yes yes yes yes yes   Access Mode RWO/\nRWOP(FC/iSCSI)\nRWO/\nRWX/\nROX/\nRWOP(Raw block) RWO/ROX/RWOP\nRWX (Raw block only) RWO/ROX/RWOP\nRWX (Raw block \u0026 NFS only) RWO/RWX/ROX/\nRWOP RWO/RWOP\n(FC/iSCSI)\nRWO/\nRWX/\nROX/\nRWOP\n(RawBlock, NFS)   CSI Volume Cloning yes yes yes yes yes   CSI Raw Block Volume yes yes yes no yes   CSI Ephemeral Volume no yes yes yes yes   Topology yes yes yes yes yes   Multi-array yes yes yes yes yes   Volume Health Monitoring yes yes yes yes yes    Supported Storage Platforms    PowerMax PowerFlex Unity PowerScale PowerStore     Storage Array 5978.479.479, 5978.711.711\nUnisphere 9.2 3.5.x, 3.6.x 5.0.7, 5.1.0, 5.1.2 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3 1.0.x, 2.0.x, 2.1.x    Backend Storage Details   Features PowerMax PowerFlex Unity PowerScale PowerStore     Fibre Channel yes N/A yes N/A yes   iSCSI yes N/A yes N/A yes   NVMeTCP N/A N/A N/A N/A yes   NFS N/A N/A yes yes yes   Other N/A ScaleIO protocol N/A N/A N/A   Supported FS ext4 / xfs ext4 / xfs ext3 / ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS   Thin / Thick provisioning Thin Thin Thin/Thick N/A Thin   Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering Policy\nNFS Host IO size\nSnapshot Retention duration Access Zone\nNFS version (3 or 4);Configurable Export IPs iSCSI CHAP    ","excerpt":"The CSI Drivers by Dell implement an interface between CSI (CSI spec …","ref":"/csm-docs/v1/csidriver/","title":"CSI Drivers"},{"body":"The CSI Drivers by Dell implement an interface between CSI (CSI spec v1.5) enabled Container Orchestrator (CO) and Dell Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nFeatures and capabilities Supported Operating Systems/Container Orchestrator Platforms    PowerMax PowerFlex Unity PowerScale PowerStore     Kubernetes 1.21, 1.22, 1.23 1.21, 1.22, 1.23 1.21, 1.22, 1.23 1.21, 1.22, 1.23 1.21, 1.22, 1.23   RHEL 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x   Ubuntu 20.04 20.04 18.04, 20.04 18.04, 20.04 20.04   CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9   SLES 15SP3 15SP3 15SP3 15SP3 15SP3   Red Hat OpenShift 4.8, 4.8 EUS, 4.9 4.8, 4.8 EUS, 4.9 4.8, 4.8 EUS, 4.9 4.8, 4.8 EUS, 4.9 4.8, 4.8 EUS, 4.9   Mirantis Kubernetes Engine 3.4.x 3.4.x 3.5.x 3.4.x 3.4.x   Google Anthos 1.6 1.8 no 1.9 1.9   VMware Tanzu no no NFS NFS NFS   Rancher Kubernetes Engine yes yes yes yes yes   Amazon Elastic Kubernetes Service\nAnywhere no yes no no yes    CSI Driver Capabilities   Features PowerMax PowerFlex Unity PowerScale PowerStore     CSI Driver version 2.2.0 2.2.0 2.2.0 2.2.0 2.2.0   Static Provisioning yes yes yes yes yes   Dynamic Provisioning yes yes yes yes yes   Expand Persistent Volume yes yes yes yes yes   Create VolumeSnapshot yes yes yes yes yes   Create Volume from Snapshot yes yes yes yes yes   Delete Snapshot yes yes yes yes yes   Access Mode RWO/\nRWOP(FC/iSCSI)\nRWO/\nRWX/\nROX/\nRWOP(Raw block) RWO/ROX/RWOP\nRWX (Raw block only) RWO/ROX/RWOP\nRWX (Raw block \u0026 NFS only) RWO/RWX/ROX/\nRWOP RWO/RWOP\n(FC/iSCSI)\nRWO/\nRWX/\nROX/\nRWOP\n(RawBlock, NFS)   CSI Volume Cloning yes yes yes yes yes   CSI Raw Block Volume yes yes yes no yes   CSI Ephemeral Volume no yes yes yes yes   Topology yes yes yes yes yes   Multi-array yes yes yes yes yes   Volume Health Monitoring yes yes yes yes yes    Supported Storage Platforms    PowerMax PowerFlex Unity PowerScale PowerStore     Storage Array 5978.479.479, 5978.711.711\nUnisphere 9.2 3.5.x, 3.6.x 5.0.7, 5.1.0, 5.1.2 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3 1.0.x, 2.0.x, 2.1.x    Backend Storage Details   Features PowerMax PowerFlex Unity PowerScale PowerStore     Fibre Channel yes N/A yes N/A yes   iSCSI yes N/A yes N/A yes   NVMeTCP N/A N/A N/A N/A yes   NFS N/A N/A yes yes yes   Other N/A ScaleIO protocol N/A N/A N/A   Supported FS ext4 / xfs ext4 / xfs ext3 / ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS   Thin / Thick provisioning Thin Thin Thin/Thick N/A Thin   Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering Policy\nNFS Host IO size\nSnapshot Retention duration Access Zone\nNFS version (3 or 4);Configurable Export IPs iSCSI CHAP    ","excerpt":"The CSI Drivers by Dell implement an interface between CSI (CSI spec …","ref":"/csm-docs/v2/csidriver/","title":"CSI Drivers"},{"body":"The CSI Drivers by Dell EMC implement an interface between CSI (CSI spec v1.5) enabled Container Orchestrator (CO) and Dell EMC Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nFeatures and capabilities Supported Operating Systems/Container Orchestrator Platforms    PowerMax PowerFlex Unity PowerScale PowerStore     Kubernetes 1.20, 1.21, 1.22 1.20, 1.21, 1.22 1.20, 1.21, 1.22 1.20, 1.21, 1.22 1.20, 1.21, 1.22   RHEL 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x 7.x,8.x   Ubuntu 20.04 20.04 18.04, 20.04 18.04, 20.04 20.04   CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9   SLES 15SP3 15SP3 15SP3 15SP3 15SP3   Red Hat OpenShift 4.8, 4.8 EUS, 4.9 4.8, 4.8 EUS, 4.9 4.8, 4.8 EUS, 4.9 4.8, 4.8 EUS, 4.9 4.8, 4.8 EUS, 4.9   Mirantis Kubernetes Engine 3.4.x 3.4.x 3.4.x 3.4.x 3.4.x   Google Anthos 1.6 1.8 no 1.9 1.9   VMware Tanzu no no NFS NFS NFS   Rancher Kubernetes Engine yes yes yes yes yes    CSI Driver Capabilities   Features PowerMax PowerFlex Unity PowerScale PowerStore     CSI Specification v1.5 v1.5 v1.5 v1.5 v1.5   Static Provisioning yes yes yes yes yes   Dynamic Provisioning yes yes yes yes yes   Expand Persistent Volume yes yes yes yes yes   Create VolumeSnapshot yes yes yes yes yes   Create Volume from Snapshot yes yes yes yes yes   Delete Snapshot yes yes yes yes yes   Access Mode RWO\n(FC/iSCSI)\nRWO/\nRWX/\nROX\n(Raw block) RWO\nRWO/\nRWX/\nROX/\nRWOP\n(Raw block) RWO/RWOP\n(FC/iSCSI)\nRWO/RWX/\nRWOP\n(RawBlock)\nRWO/RWX/ROX/\nRWOP\n(NFS) RWO/RWX/ROX/\nRWOP RWO/RWOP\n(FC/iSCSI)\nRWO/\nRWX/\nROX/\nRWOP\n(RawBlock, NFS)   CSI Volume Cloning yes yes yes yes yes   CSI Raw Block Volume yes yes yes no yes   CSI Ephemeral Volume no yes yes yes yes   Topology yes yes yes yes yes   Multi-array yes yes yes yes yes   Volume Health Monitoring no yes yes yes yes    Supported Storage Platforms    PowerMax PowerFlex Unity PowerScale PowerStore     Storage Array 5978.479.479, 5978.669.669, 5978.711.711, Unisphere 9.2 3.5.x, 3.6.x 5.0.5, 5.0.6, 5.0.7, 5.1.0 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3 1.0.x, 2.0.x    Backend Storage Details   Features PowerMax PowerFlex Unity PowerScale PowerStore     Fibre Channel yes N/A yes N/A yes   iSCSI yes N/A yes N/A yes   NFS N/A N/A yes yes yes   Other N/A ScaleIO protocol N/A N/A N/A   Supported FS ext4 / xfs ext4 / xfs ext3 / ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS   Thin / Thick provisioning Thin Thin Thin/Thick N/A Thin   Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering Policy\nNFS Host IO size\nSnapshot Retention duration Access Zone\nNFS version (3 or 4);Configurable Export IPs iSCSI CHAP    ","excerpt":"The CSI Drivers by Dell EMC implement an interface between CSI (CSI …","ref":"/csm-docs/v3/csidriver/","title":"CSI Drivers"},{"body":"csm is a command-line client for installation of Dell EMC Container Storage Modules and CSI Drivers for Kubernetes clusters.\nPre-requisites  Deploy the Container Storage Modules Installer Download/Install the csm binary from Github: https://github.com/dell/csm. Alternatively, you can build the binary by:  cloning the csm repository changing into csm/cmd/csm directory running make build   create a cli_env.sh file that contains the correct values for the below variables. And export the variables by running source ./cli_env.sh  # Change this to CSM API Server IP export API_SERVER_IP=\"127.0.0.1\" # Change this to CSM API Server Port export API_SERVER_PORT=\"31313\" # CSM API Server protocol - allowed values are https \u0026 http export SCHEME=\"https\" # Path to store JWT \u003ctoken\u003e export AUTH_CONFIG_PATH=\"/home/user/installer-token/\" Usage ~$ ./csm -h csm is command line tool for csm application Usage: csm [flags] csm [command] Available Commands: add add cluster, configuration or storage approve-task approve task for application authenticate authenticate user change change - subcommand is password create create application delete delete storage, cluster, configuration or application get get storage, cluster, application, configuration, supported driver, module, storage type help Help about any command reject-task reject task for an application update update storage, configuration or cluster Flags: -h, --help help for csm-cli Use \"csm [command] --help\" for more information about a command. Authenticate the User To begin with, you need to authenticate the user who will be managing the CSM Installer and its components.\n./csm authenticate --username=\u003cadmin-username\u003e --password=\u003cadmin-password\u003e Or more securely, run the above command without --password to be prompted for one\n./csm authenticate --username=\u003cadmin-username\u003e Enter user's password: Change Password To change password follow below command\n./csm change password --username=\u003cadmin-username\u003e View Supported Platforms You can now view the supported Dell emcCSI Drivers\n./csm get supported-drivers You can also view the supported Modules\n./csm get supported-modules And also view the supported Storage Array Types\n./csm get supported-storage-arrays Add a Cluster You can now add a cluster by providing cluster detail name and Kubeconfig path\n./csm add cluster --clustername \u003cdesire-cluster-name\u003e --configfilepath \u003cpath-to-kubeconfig\u003e Upload Configuration Files You can now add a configuration file that can be used for creating application by providing filename and path\n./csm add configuration --filename \u003cname-of-the-desire-file\u003e --filepath \u003cpath-to-the-desired-file\u003e Add a Storage System You can now add storage endpoints, array type and its unique id\n./csm add storage --endpoint \u003cstorage-array-endpoint\u003e --storage-type \u003cstorage-array-type\u003e --unique-id \u003cstorage-array-unique-id\u003e --username \u003cstorage-array-username\u003e The optional --meta-data flag can be used to provide additional meta-data for the storage system that is used when creating Secrets for the CSI Driver. These fields include:\n isDefault: Set to true if this storage system is used as default for multi-array configuration skipCertificateValidation: Set to true to skip certificate validation mdmId: Comma separated list of MDM IPs for PowerFlex nasName: NAS Name for PowerStore blockProtocol: Block Protocol for PowerStore port: Port for PowerScale portGroups: Comma separated list of port group names for PowerMax  Create an Application You may now create an application depending on the specific use case. Below are the common use cases:\n CSI Driver ./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-type\u003e   CSI Driver with CSM Authorization CSM Authorization requires a token.yaml issued by storage Admin from the CSM Authorization Server, a certificate file, and the  of the authorization server. The token.yaml and cert should be added by following the steps in adding configuration file. CSM Authorization does not yet support all CSI Drivers/platforms(See supported platforms documentation or supported platforms via CLI)). Finally, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type authorization:\u003ctag\u003e \\ --module-configuration \"karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSM Observability(Standalone) CSM Observability depends on driver config secret(s) corresponding to the metric(s) you want to enable. Please see CSM Observability for all Supported Metrics. For the sake of demonstration, assuming we want to enable CSM Metrics for PowerFlex, the PowerFlex secret yaml should be added by following the steps in adding configuration file. Once this is done, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type observability:\u003ctag\u003e \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true\"   CSM Observability(Standalone) with CSM Authorization See the individual steps for configuaration file pre-requisites for CSM Observability (Standalone) with CSM Authorization\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type \"observability:\u003ctag\u003e,authorization:\u003ctag\u003e\" \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true,karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSI Driver for Dell EMC PowerMax with reverse proxy module To deploy CSI Driver for Dell EMC PowerMax with reverse proxy module, first upload reverse proxy tls crt and tls key via adding configuration file. Then, use the below command to create application:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powermax:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cpowermax-unique-id\u003e \\ --module-type reverse-proxy:\u003ctag\u003e \\ --module-configuration reverseProxy.tlsSecretKeyFile=\u003crevprotlskey\u003e,reverseProxy.tlsSecretCertFile=\u003crevprotlscert\u003e   CSI Driver with replication module To deploy CSI driver with replication module, first add a target cluster through adding cluster. Then, use the below command(this command is an example to deploy CSI Driver for Dell EMC PowerStore with replication module) to create application::\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerstore:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-configuration target_cluster=\u003ccreated-target-cluster-name\u003e \\ --module-type replication:\u003ctag\u003e   CSI Driver with other module(s) not covered above Assuming you want to deploy a driver with module A and module B. If they have specific configurations of A.image=\"docker:v1\",A.filename=hello, and B.namespace=world.\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type \"module A:\u003ctag\u003e,module B:\u003ctag\u003e\" \\ --module-configuration \"A.image=docker:v1,A.filename=hello,B.namespace=world\"\t  Note:\n  --driver-type and --module-type flags in create application command MUST match the values from the supported CSM platforms Replication module supports only using a pair of clusters at a time (source and a target/or single cluster) from CSM installer, However repctl can be used if needed to add multiple pairs of target clusters. Using replication module with other modules during application creation is not yet supported.  Approve application/task You may now approve the task so that you can continue to work with the application\n./csm approve-task --applicationname \u003ccreated-application-name\u003e Reject application/task You may want to reject a task or application to discontinue the ongoing process\n./csm reject-task --applicationname \u003ccreated-application-name\u003e Delete application/task If you want to delete an application\n./csm delete application --name \u003ccreated-application-name\u003e  Note: When deleting an application, the namespace and Secrets are not deleted. These resources need to be deleted manually. See more in Troubleshooting.\n  Note: All commands and associated syntax can be displayed with -h or –help\n ","excerpt":"csm is a command-line client for installation of Dell EMC Container …","ref":"/csm-docs/v1/deployment/csmcli/","title":"CSM CLI"},{"body":"csm is a command-line client for installation of Dell EMC Container Storage Modules and CSI Drivers for Kubernetes clusters.\nPre-requisites  Deploy the Container Storage Modules Installer Download/Install the csm binary from Github: https://github.com/dell/csm. Alternatively, you can build the binary by:  cloning the csm repository changing into csm/cmd/csm directory running make build   create a cli_env.sh file that contains the correct values for the below variables. And export the variables by running source ./cli_env.sh  # Change this to CSM API Server IP export API_SERVER_IP=\"127.0.0.1\" # Change this to CSM API Server Port export API_SERVER_PORT=\"31313\" # CSM API Server protocol - allowed values are https \u0026 http export SCHEME=\"https\" # Path to store JWT \u003ctoken\u003e export AUTH_CONFIG_PATH=\"/home/user/installer-token/\" Usage ~$ ./csm -h csm is command line tool for csm application Usage: csm [flags] csm [command] Available Commands: add add cluster, configuration or storage approve-task approve task for application authenticate authenticate user change change - subcommand is password create create application delete delete storage, cluster, configuration or application get get storage, cluster, application, configuration, supported driver, module, storage type help Help about any command reject-task reject task for an application update update storage, configuration or cluster Flags: -h, --help help for csm-cli Use \"csm [command] --help\" for more information about a command. Authenticate the User To begin with, you need to authenticate the user who will be managing the CSM Installer and its components.\n./csm authenticate --username=\u003cadmin-username\u003e --password=\u003cadmin-password\u003e Or more securely, run the above command without --password to be prompted for one\n./csm authenticate --username=\u003cadmin-username\u003e Enter user's password: Change Password To change password follow below command\n./csm change password --username=\u003cadmin-username\u003e View Supported Platforms You can now view the supported Dell emcCSI Drivers\n./csm get supported-drivers You can also view the supported Modules\n./csm get supported-modules And also view the supported Storage Array Types\n./csm get supported-storage-arrays Add a Cluster You can now add a cluster by providing cluster detail name and Kubeconfig path\n./csm add cluster --clustername \u003cdesire-cluster-name\u003e --configfilepath \u003cpath-to-kubeconfig\u003e Upload Configuration Files You can now add a configuration file that can be used for creating application by providing filename and path\n./csm add configuration --filename \u003cname-of-the-desire-file\u003e --filepath \u003cpath-to-the-desired-file\u003e Add a Storage System You can now add storage endpoints, array type and its unique id\n./csm add storage --endpoint \u003cstorage-array-endpoint\u003e --storage-type \u003cstorage-array-type\u003e --unique-id \u003cstorage-array-unique-id\u003e --username \u003cstorage-array-username\u003e The optional --meta-data flag can be used to provide additional meta-data for the storage system that is used when creating Secrets for the CSI Driver. These fields include:\n isDefault: Set to true if this storage system is used as default for multi-array configuration skipCertificateValidation: Set to true to skip certificate validation mdmId: Comma separated list of MDM IPs for PowerFlex nasName: NAS Name for PowerStore blockProtocol: Block Protocol for PowerStore port: Port for PowerScale portGroups: Comma separated list of port group names for PowerMax  Create an Application You may now create an application depending on the specific use case. Below are the common use cases:\n CSI Driver ./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-type\u003e   CSI Driver with CSM Authorization CSM Authorization requires a token.yaml issued by storage Admin from the CSM Authorization Server, a certificate file, and the  of the authorization server. The token.yaml and cert should be added by following the steps in adding configuration file. CSM Authorization does not yet support all CSI Drivers/platforms(See supported platforms documentation or supported platforms via CLI)). Finally, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type authorization:\u003ctag\u003e \\ --module-configuration \"karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSM Observability(Standalone) CSM Observability depends on driver config secret(s) corresponding to the metric(s) you want to enable. Please see CSM Observability for all Supported Metrics. For the sake of demonstration, assuming we want to enable CSM Metrics for PowerFlex, the PowerFlex secret yaml should be added by following the steps in adding configuration file. Once this is done, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type observability:\u003ctag\u003e \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true\"   CSM Observability(Standalone) with CSM Authorization See the individual steps for configuaration file pre-requisites for CSM Observability (Standalone) with CSM Authorization\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type \"observability:\u003ctag\u003e,authorization:\u003ctag\u003e\" \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true,karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSI Driver for Dell EMC PowerMax with reverse proxy module To deploy CSI Driver for Dell EMC PowerMax with reverse proxy module, first upload reverse proxy tls crt and tls key via adding configuration file. Then, use the below command to create application:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powermax:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cpowermax-unique-id\u003e \\ --module-type reverse-proxy:\u003ctag\u003e \\ --module-configuration reverseProxy.tlsSecretKeyFile=\u003crevprotlskey\u003e,reverseProxy.tlsSecretCertFile=\u003crevprotlscert\u003e   CSI Driver with replication module To deploy CSI driver with replication module, first add a target cluster through adding cluster. Then, use the below command(this command is an example to deploy CSI Driver for Dell EMC PowerStore with replication module) to create application::\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerstore:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-configuration target_cluster=\u003ccreated-target-cluster-name\u003e \\ --module-type replication:\u003ctag\u003e   CSI Driver with other module(s) not covered above Assuming you want to deploy a driver with module A and module B. If they have specific configurations of A.image=\"docker:v1\",A.filename=hello, and B.namespace=world.\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type \"module A:\u003ctag\u003e,module B:\u003ctag\u003e\" \\ --module-configuration \"A.image=docker:v1,A.filename=hello,B.namespace=world\"\t  Note:\n  --driver-type and --module-type flags in create application command MUST match the values from the supported CSM platforms Replication module supports only using a pair of clusters at a time (source and a target/or single cluster) from CSM installer, However repctl can be used if needed to add multiple pairs of target clusters. Using replication module with other modules during application creation is not yet supported.  Approve application/task You may now approve the task so that you can continue to work with the application\n./csm approve-task --applicationname \u003ccreated-application-name\u003e Reject application/task You may want to reject a task or application to discontinue the ongoing process\n./csm reject-task --applicationname \u003ccreated-application-name\u003e Delete application/task If you want to delete an application\n./csm delete application --name \u003ccreated-application-name\u003e  Note: When deleting an application, the namespace and Secrets are not deleted. These resources need to be deleted manually. See more in Troubleshooting.\n  Note: All commands and associated syntax can be displayed with -h or –help\n ","excerpt":"csm is a command-line client for installation of Dell EMC Container …","ref":"/csm-docs/v2/deployment/csmcli/","title":"CSM CLI"},{"body":"csm is a command-line client for installation of Dell EMC Container Storage Modules and CSI Drivers for Kubernetes clusters.\nPre-requisites  Deploy the Container Storage Modules Installer Download/Install the csm binary from Github: https://github.com/dell/csm. Alternatively, you can build the binary by:  cloning the csm repository changing into csm/cmd/csm directory running make build   create a cli_env.sh file that contains the correct values for the below variables. And export the variables by running source ./cli_env.sh  # Change this to CSM API Server IP export API_SERVER_IP=\"127.0.0.1\" # Change this to CSM API Server Port export API_SERVER_PORT=\"31313\" # CSM API Server protocol - allowed values are https \u0026 http export SCHEME=\"https\" # Path to store JWT \u003ctoken\u003e export AUTH_CONFIG_PATH=\"/home/user/installer-token/\" Usage ~$ ./csm -h csm is command line tool for csm application Usage: csm [flags] csm [command] Available Commands: add add cluster, configuration or storage approve-task approve task for application authenticate authenticate user change change - subcommand is password create create application delete delete storage, cluster, configuration or application get get storage, cluster, application, configuration, supported driver, module, storage type help Help about any command reject-task reject task for an application update update storage, configuration or cluster Flags: -h, --help help for csm-cli Use \"csm [command] --help\" for more information about a command. Authenticate the User To begin with, you need to authenticate the user who will be managing the CSM Installer and its components.\n./csm authenticate --username=\u003cadmin-username\u003e --password=\u003cadmin-password\u003e Or more securely, run the above command without --password to be prompted for one\n./csm authenticate --username=\u003cadmin-username\u003e Enter user's password: Change Password To change password follow below command\n./csm change password --username=\u003cadmin-username\u003e View Supported Platforms You can now view the supported Dell emcCSI Drivers\n./csm get supported-drivers You can also view the supported Modules\n./csm get supported-modules And also view the supported Storage Array Types\n./csm get supported-storage-arrays Add a Cluster You can now add a cluster by providing cluster detail name and Kubeconfig path\n./csm add cluster --clustername \u003cdesire-cluster-name\u003e --configfilepath \u003cpath-to-kubeconfig\u003e Upload Configuration Files You can now add a configuration file that can be used for creating application by providing filename and path\n./csm add configuration --filename \u003cname-of-the-desire-file\u003e --filepath \u003cpath-to-the-desired-file\u003e Add a Storage System You can now add storage endpoints, array type and its unique id\n./csm add storage --endpoint \u003cstorage-array-endpoint\u003e --storage-type \u003cstorage-array-type\u003e --unique-id \u003cstorage-array-unique-id\u003e --username \u003cstorage-array-username\u003e The optional --meta-data flag can be used to provide additional meta-data for the storage system that is used when creating Secrets for the CSI Driver. These fields include:\n isDefault: Set to true if this storage system is used as default for multi-array configuration skipCertificateValidation: Set to true to skip certificate validation mdmId: Comma separated list of MDM IPs for PowerFlex nasName: NAS Name for PowerStore blockProtocol: Block Protocol for PowerStore port: Port for PowerScale portGroups: Comma separated list of port group names for PowerMax  Create an Application You may now create an application depending on the specific use case. Below are the common use cases:\n CSI Driver ./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-type\u003e   CSI Driver with CSM Authorization CSM Authorization requires a token.yaml issued by storage Admin from the CSM Authorization Server, a certificate file, and the  of the authorization server. The token.yaml and cert should be added by following the steps in adding configuration file. CSM Authorization does not yet support all CSI Drivers/platforms(See supported platforms documentation or supported platforms via CLI)). Finally, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type authorization:\u003ctag\u003e \\ --module-configuration \"karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSM Observability(Standalone) CSM Observability depends on driver config secret(s) corresponding to the metric(s) you want to enable. Please see CSM Observability for all Supported Metrics. For the sake of demonstration, assuming we want to enable CSM Metrics for PowerFlex, the PowerFlex secret yaml should be added by following the steps in adding configuration file. Once this is done, run the command below:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type observability:\u003ctag\u003e \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true\"   CSM Observability(Standalone) with CSM Authorization See the individual steps for configuaration file pre-requisites for CSM Observability (Standalone) with CSM Authorization\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --name \u003cdesired-application-name\u003e \\ --module-type \"observability:\u003ctag\u003e,authorization:\u003ctag\u003e\" \\ --module-configuration \"karaviMetricsPowerflex.driverConfig.filename=\u003cfilename-for-powerflex-config\u003e,karaviMetricsPowerflex.enabled=true,karaviAuthorizationProxy.proxyAuthzToken.filename=\u003cfilename-for-token\u003e,karaviAuthorizationProxy.rootCertificate.filename=\u003cfilename-for-cert\u003e,karaviAuthorizationProxy.proxyHost=\u003cproxyHost-address\u003e\"   CSI Driver for Dell EMC PowerMax with reverse proxy module To deploy CSI Driver for Dell EMC PowerMax with reverse proxy module, first upload reverse proxy tls crt and tls key via adding configuration file. Then, use the below command to create application:\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powermax:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cpowermax-unique-id\u003e \\ --module-type reverse-proxy:\u003ctag\u003e \\ --module-configuration reverseProxy.tlsSecretKeyFile=\u003crevprotlskey\u003e,reverseProxy.tlsSecretCertFile=\u003crevprotlscert\u003e   CSI Driver with replication module To deploy CSI driver with replication module, first add a target cluster through adding cluster. Then, use the below command(this command is an example to deploy CSI Driver for Dell EMC PowerStore with replication module) to create application::\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerstore:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-configuration target_cluster=\u003ccreated-target-cluster-name\u003e \\ --module-type replication:\u003ctag\u003e   CSI Driver with other module(s) not covered above Assuming you want to deploy a driver with module A and module B. If they have specific configurations of A.image=\"docker:v1\",A.filename=hello, and B.namespace=world.\n./csm create application --clustername \u003ccreated-cluster-name\u003e \\ --driver-type powerflex:\u003ctag\u003e --name \u003cdesired-application-name\u003e \\ --storage-arrays \u003cstorage-array-unique-id\u003e \\ --module-type \"module A:\u003ctag\u003e,module B:\u003ctag\u003e\" \\ --module-configuration \"A.image=docker:v1,A.filename=hello,B.namespace=world\"\t  Note:\n  --driver-type and --module-type flags in create application command MUST match the values from the supported CSM platforms Replication module supports only using a pair of clusters at a time (source and a target/or single cluster) from CSM installer, However repctl can be used if needed to add multiple pairs of target clusters. Using replication module with other modules during application creation is not yet supported.  Approve application/task You may now approve the task so that you can continue to work with the application\n./csm approve-task --applicationname \u003ccreated-application-name\u003e Reject application/task You may want to reject a task or application to discontinue the ongoing process\n./csm reject-task --applicationname \u003ccreated-application-name\u003e Delete application/task If you want to delete an application\n./csm delete application --name \u003ccreated-application-name\u003e  Note: When deleting an application, the namespace and Secrets are not deleted. These resources need to be deleted manually. See more in Troubleshooting.\n  Note: All commands and associated syntax can be displayed with -h or –help\n ","excerpt":"csm is a command-line client for installation of Dell EMC Container …","ref":"/csm-docs/v3/deployment/csmcli/","title":"CSM CLI"},{"body":"CSM for Observability can be deployed in one of three ways:\n Helm CSM for Observability Installer CSM for Observability Offline Installer  Post Installation Dependencies The following third-party components are required in the same Kubernetes cluster where CSM for Observability has been deployed:\n Prometheus Grafana Other Deployment Methods  There are various ways to deploy these components. We recommend following the Helm deployments according to the specifications defined below.\nTip: CSM for Observability must be deployed first. Once the module has been deployed, you can proceed to deploying/configuring Prometheus and Grafana.\nPrometheus The Prometheus service should be running on the same Kubernetes cluster as the CSM for Observability services. As part of the CSM for Observability deployment, the OpenTelemetry Collector gets deployed. CSM for Observability pushes metrics to the OpenTelemetry Collector where the metrics are consumed by Prometheus. Prometheus must be configured to scrape the metrics data from the OpenTelemetry Collector.\n   Supported Version Image Helm Chart     2.34.0 prom/prometheus:v2.34.0 Prometheus Helm chart    Note: It is the user’s responsibility to provide persistent storage for Prometheus if they want to preserve historical data.\nPrometheus Helm Deployment Here is a sample minimal configuration for Prometheus. Please note that the configuration below uses insecure skip verify. If you wish to properly configure TLS, you will need to provide a ca_file in the Prometheus configuration. The certificate provided as part of the CSM for Observability deployment should be signed by this same CA. For more information about Prometheus configuration, see Prometheus configuration.\n  Create a values file named prometheus-values.yaml.\n# prometheus-values.yamlalertmanager:enabled:falsenodeExporter:enabled:falsepushgateway:enabled:falsekubeStateMetrics:enabled:falseconfigmapReload:prometheus:enabled:falseserver:enabled:trueimage:repository:quay.io/prometheus/prometheustag:v2.34.0pullPolicy:IfNotPresentpersistentVolume:enabled:falseservice:type:NodePortservicePort:9090extraScrapeConfigs:| - job_name: 'karavi-metrics-[CSI-DRIVER]'scrape_interval:5sscheme:httpsstatic_configs:- targets:['otel-collector:8443']tls_config:insecure_skip_verify:true  If using Rancher, create a ServiceMonitor.\napiVersion:monitoring.coreos.com/v1kind:ServiceMonitormetadata:name:otel-collectornamespace:powerflexspec:endpoints:- path:/metricsport:exporter-httpsscheme:httpstlsConfig:insecureSkipVerify:trueselector:matchLabels:app.kubernetes.io/instance:karavi-observabilityapp.kubernetes.io/name:otel-collector  Add the Prometheus Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add stable https://charts.helm.sh/stable helm repo update   Install the Helm chart.\nOn your terminal, run the command below:\nhelm install prometheus prometheus-community/prometheus -n [CSM_NAMESPACE] -f prometheus-values.yaml   Grafana The Grafana dashboards require Grafana to be deployed in the same Kubernetes cluster as CSM for Observability. Below are the configuration details required to properly set up Grafana to work with CSM for Observability.\n   Supported Version Helm Chart     8.5.0 Grafana Helm chart    Grafana must be configured with the following data sources/plugins:\n   Name Additional Information     Prometheus data source Prometheus data source   Data Table plugin Data Table plugin   Pie Chart plugin Pie Chart plugin   SimpleJson data source SimpleJson data source    Settings for the Grafana Prometheus data source:\n   Setting Value Additional Information     Name Prometheus    Type prometheus    URL http://PROMETHEUS_IP:PORT The IP/PORT of your running Prometheus instance   Access Proxy     Settings for the Grafana SimpleJson data source:\n   Setting Value     Name Karavi-Topology   URL Access CSM for Observability Topology service at https://karavi-topology.namespace.svc.cluster.local:8443   Skip TLS Verify Enabled (If not using CA certificate)   With CA Cert Enabled (If using CA certificate)    Grafana Helm Deployment Below are the steps to deploy a new Grafana instance into your Kubernetes cluster:\n  Create a ConfigMap.\nWhen using a network that requires a decryption certificate, the Grafana server MUST be configured with the necessary certificate. If no certificate is required, skip to step 2.\n Create a Config file named grafana-configmap.yaml The file should look like this:  # grafana-configmap.yamlapiVersion:v1kind:ConfigMapmetadata:name:certs-configmapnamespace:[CSM_NAMESPACE]labels:certs-configmap:\"1\"data:ca-certificates.crt:|- -----BEGIN CERTIFICATE-----ReplaceMeWithActualCaCERT=-----ENDCERTIFICATE-----NOTE: you need an actual CA Cert for it to work\nOn your terminal, run the commands below:\nkubectl create -f grafana-configmap.yaml   Create a values file.\nCreate a Config file named grafana-values.yaml The file should look like this:\n# grafana-values.yaml image:repository:grafana/grafanatag:8.5.0sha:\"\"pullPolicy:IfNotPresentservice:type:NodePort## Administrator credentials when not using an existing SecretadminUser:adminadminPassword:admin## Pass the plugins you want to be installed as a list.##plugins:- grafana-simple-json-datasource- briangann-datatable-panel- grafana-piechart-panel## Configure grafana datasources## ref: http://docs.grafana.org/administration/provisioning/#datasources##datasources:datasources.yaml:apiVersion:1datasources:- name:Karavi-Topologytype:grafana-simple-json-datasourceaccess:proxyurl:'https://karavi-topology:8443'isDefault:nullversion:1editable:truejsonData:tlsSkipVerify:true- name:Prometheustype:prometheusaccess:proxyurl:'http://prometheus-server:9090'isDefault:nullversion:1editable:truetestFramework:enabled:falsesidecar:datasources:enabled:truedashboards:enabled:true## Additional grafana server CofigMap mounts## Defines additional mounts with CofigMap. CofigMap must be manually created in the namespace.extraConfigmapMounts:[]# If you created a ConfigMap on the previous step, delete [] and uncomment the lines below # - name: certs-configmap# mountPath: /etc/ssl/certs/ca-certificates.crt# subPath: ca-certificates.crt# configMap: certs-configmap# readOnly: true  Add the Grafana Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo update   Install the Helm chart.\nOn your terminal, run the commands below:\nhelm install grafana grafana/grafana -n [CSM_NAMESPACE] -f grafana-values.yaml   Other Deployment Methods  Grafana Labs Operator Deployment Rancher Monitoring and Alerting Deployment  Importing CSM for Observability Dashboards Once Grafana is properly configured, you can import the pre-built observability dashboards. Log into Grafana and click the + icon in the side menu. Then click Import. From here you can upload the JSON files or paste the JSON text directly into the text area. Below are the locations of the dashboards that can be imported:\n   Dashboard Description     PowerFlex: I/O Performance by Kubernetes Node Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by Kubernetes node   PowerFlex: I/O Performance by Provisioned Volume Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume   PowerFlex: Storage Pool Consumption By CSI Driver Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.   PowerStore: I/O Performance by Provisioned Volume As of Release 0.4.0: Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume   CSI Driver Provisioned Volume Topology Provides visibility into Dell CSI (Container Storage Interface) driver provisioned volume characteristics in Kubernetes correlated with volumes on the storage system.    Dynamic Configuration Some parameters can be configured/updated during runtime without restarting the CSM for Observability services. These parameters will be stored in ConfigMaps that can be updated on the Kubernetes cluster. This will automatically change the settings on the services.\n   ConfigMap Observability Service Parameters     karavi-metrics-powerflex-configmap karavi-metrics-powerflex COLLECTOR_ADDRPROVISIONER_NAMESPOWERFLEX_SDC_METRICS_ENABLEDPOWERFLEX_SDC_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_POLL_FREQUENCYPOWERFLEX_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMAT   karavi-metrics-powerstore-configmap karavi-metrics-powerstore COLLECTOR_ADDRPROVISIONER_NAMESPOWERSTORE_VOLUME_METRICS_ENABLEDPOWERSTORE_VOLUME_IO_POLL_FREQUENCYPOWERSTORE_SPACE_POLL_FREQUENCYPOWERSTORE_ARRAY_POLL_FREQUENCYPOWERSTORE_FILE_SYSTEM_POLL_FREQUENCYPOWERSTORE_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY   karavi-topology-configmap karavi-topology PROVISIONER_NAMESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY    To update any of these settings, run the following command on the Kubernetes cluster then save the updated ConfigMap data.\nkubectl edit configmap [CONFIG_MAP_NAME] -n [CSM_NAMESPACE] Tracing CSM for Observability is instrumented to report trace data to Zipkin. This helps gather timing data needed to troubleshoot latency problems with CSM for Observability. Follow the instructions below to enable the reporting of trace data:\n  Deploy a Zipkin instance in the CSM namespace and expose the service as NodePort for external access.\napiVersion: apps/v1 kind: Deployment metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance template: metadata: labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance spec: containers: - name: zipkin image: \"openzipkin/zipkin\" imagePullPolicy: IfNotPresent env: - name: \"STORAGE_TYPE\" value: \"mem\" - name: \"TRANSPORT_TYPE\" value: \"http\" --- apiVersion: v1 kind: Service metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: ports: - port: 9411 targetPort: 9411 protocol: TCP type: \"NodePort\" selector: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance   Add the Zipkin URI to the CSM for Observability ConfigMaps. Based on the manifest above, Zipkin will be running on port 9411.\nNote: Zipkin tracing is currently not supported for the collection of PowerFlex metrics.\nUpdate the ConfigMaps from the table above. Here is an example updating the karavi-topology-configmap based on the deployment manifest above.\nkubectl edit configmap/karavi-topology-configmap -n [CSM_NAMESPACE] Update the ZIPKIN_URI and ZIPKIN_PROBABILITY values and save the ConfigMap.\nZIPKIN_URI: \"http://zipkin:9411/api/v2/spans\" ZIPKIN_SERVICE_NAME: \"karavi-topology\" ZIPKIN_PROBABILITY: \"1.0\" Once the ConfigMaps are updated, the changes will automatically be applied and tracing can be seen by accessing Zipkin on the exposed port.\n  Updating Storage System Credentials If the storage system credentials have been updated in the relevant CSI Driver, CSM for Observability must be updated with those new credentials as follows:\nWhen CSM for Observability uses the Authorization module In this case, all storage system requests made by CSM for Observability will be routed through the Authorization module. The following must be performed:\nUpdate the Authorization Module Token   Delete the current proxy-authz-tokens Secret from the CSM namespace.\n$ kubectl delete secret proxy-authz-tokens -n [CSM_NAMESPACE]   Copy the proxy-authz-tokens Secret from a CSI Driver to the CSM namespace.\n$ kubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSM_CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   Update Storage Systems If the list of storage systems managed by a Dell CSI Driver have changed, the following steps can be performed to update CSM for Observability to reference the updated systems:\n  Delete the current karavi-authorization-config Secret from the CSM namespace.\n$ kubectl delete secret proxy-authz-tokens -n [CSM_NAMESPACE]   Copy the karavi-authorization-config Secret from the CSI Driver namespace to CSM for Observability namespace.\n$ kubectl get secret karavi-authorization-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSM_CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   When CSM for Observability does not use the Authorization module In this case all storage system requests made by CSM for Observability will not be routed through the Authorization module. The following must be performed:\nCSI Driver for Dell PowerFlex   Delete the current vxflexos-config Secret from the CSM namespace.\n$ kubectl delete secret vxflexos-config -n [CSM_NAMESPACE]   Copy the vxflexos-config Secret from the CSI Driver for Dell PowerFlex namespace to the CSM namespace.\n$ kubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   CSI Driver for Dell PowerStore   Delete the current powerstore-config Secret from the CSM namespace.\n$ kubectl delete secret powerstore-config -n [CSM_NAMESPACE]   Copy the powerstore-config Secret from the CSI Driver for Dell PowerStore namespace to the CSM namespace.\n$ kubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   ","excerpt":"CSM for Observability can be deployed in one of three ways:\n Helm CSM …","ref":"/csm-docs/docs/observability/deployment/","title":"Deployment"},{"body":"CSM for Resiliency is installed as part of the Dell CSI driver installation. The drivers can be installed either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart installation is supported.\nFor information on the PowerFlex CSI driver, see PowerFlex CSI Driver.\nFor information on the Unity XT CSI driver, see Unity XT CSI Driver.\nFor information on the PowerScale CSI driver, see PowerScale CSI Driver.\nConfigure all the helm chart parameters described below before installing the drivers.\nHelm Chart Installation The drivers that support Helm chart installation allow CSM for Resiliency to be optionally installed by variables in the chart. There is a podmon block specified in the values.yaml file of the chart that will look similar to the text below by default:\n# Podmon is an optional feature under development and tech preview. # Enable this feature only after contact support for additional information podmon: enabled: true image: dellemc/podmon:v1.2.0 controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" - \"--driverPodLabelValue=dell-storage\" To install CSM for Resiliency with the driver, the following changes are required:\n Enable CSM for Resiliency by changing the podmon.enabled boolean to true. This will enable both controller-podmon and node-podmon. Specify the podmon image to be used as podmon.image. Specify arguments to controller-podmon in the podmon.controller.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon. Specify arguments to node-podmon in the podmon.node.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon.  Podmon Arguments    Argument Required Description Applicability     enabled Required Boolean “true” enables CSM for Resiliency installation with the driver in a helm installation. top level   image Required Must be set to a repository where the podmon image can be pulled. controller \u0026 node   mode Required Must be set to “controller” for controller-podmon and “node” for node-podmon. controller \u0026 node   csisock Required This should be left as set in the helm template for the driver. For controller: -csisock=unix:/var/run/csi/csi.sock For node it will vary depending on the driver’s identity: -csisock=unix:/var/lib/kubelet/plugins\n/vxflexos.emc.dell.com/csi_sock controller \u0026 node   leaderelection Required Boolean value that should be set true for controller and false for node. The default value is true. controller \u0026 node   skipArrayConnectionValidation Optional Boolean value that if set to true will cause controllerPodCleanup to skip the validation that no I/O is ongoing before cleaning up the pod. If set to true will cause controllerPodCleanup on K8S Control Plane failure (kubelet service down). controller   labelKey Optional String value that sets the label key used to denote pods to be monitored by CSM for Resiliency. It will make life easier if this key is the same for all driver types, and drivers are differentiated by different labelValues (see below). If the label keys are the same across all drivers you can do kubectl get pods -A -l labelKey to find all the CSM for Resiliency protected pods. labelKey defaults to “podmon.dellemc.com/driver”. controller \u0026 node   labelValue Required String that sets the value that denotes pods to be monitored by CSM for Resiliency. This must be specific for each driver. Defaults to “csi-vxflexos” for CSI Driver for Dell PowerFlex and “csi-unity” for CSI Driver for Dell Unity XT controller \u0026 node   arrayConnectivityPollRate Optional The minimum polling rate in seconds to determine if the array has connectivity to a node. Should not be set to less than 5 seconds. See the specific section for each array type for additional guidance. controller \u0026 node   arrayConnectivityConnectionLossThreshold Optional Gives the number of failed connection polls that will be deemed to indicate array connectivity loss. Should not be set to less than 3. See the specific section for each array type for additional guidance. controller   driver-config-params Required String that set the path to a file containing configuration parameter(for instance, Log levels) for a driver. controller \u0026 node    PowerFlex Specific Recommendations PowerFlex supports a very robust array connection validation mechanism that can detect changes in connectivity in about two seconds and can detect whether I/O has occurred over a five-second sample. For that reason it is recommended to set “skipArrayConnectionValidation=false” (which is the default) and to set “arrayConnectivityPollRate=5” (5 seconds) and “arrayConnectivityConnectionLossThreshold=3” to 3 or more.\nHere is a typical installation used for testing:\npodmon:image:dellemc/podmonenabled:truecontroller:args:- \"--csisock=unix:/var/run/csi/csi.sock\"- \"--labelvalue=csi-vxflexos\"- \"--mode=controller\"- \"--arrayConnectivityPollRate=5\"- \"--arrayConnectivityConnectionLossThreshold=3\"- \"--skipArrayConnectionValidation=false\"- \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\"- \"--driverPodLabelValue=dell-storage\"node:args:- \"--csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\"- \"--labelvalue=csi-vxflexos\"- \"--mode=node\"- \"--leaderelection=false\"- \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\"- \"--driverPodLabelValue=dell-storage\"Unity XT Specific Recommendations Here is a typical installation used for testing:\npodmon:image:dellemc/podmonenabled:truecontroller:args:- \"--csisock=unix:/var/run/csi/csi.sock\"- \"--labelvalue=csi-unity\"- \"--driverPath=csi-unity.dellemc.com\"- \"--mode=controller\"- \"--skipArrayConnectionValidation=false\"- \"--driver-config-params=/unity-config/driver-config-params.yaml\"- \"--driverPodLabelValue=dell-storage\"node:args:- \"--csisock=unix:/var/lib/kubelet/plugins/unity.emc.dell.com/csi_sock\"- \"--labelvalue=csi-unity\"- \"--driverPath=csi-unity.dellemc.com\"- \"--mode=node\"- \"--leaderelection=false\"- \"--driver-config-params=/unity-config/driver-config-params.yaml\"- \"--driverPodLabelValue=dell-storage\"PowerScale Specific Recommendations Here is a typical installation used for testing:\npodmon:image:dellemc/podmonenabled:truecontroller:args:- \"--csisock=unix:/var/run/csi/csi.sock\"- \"--labelvalue=csi-isilon\"- \"--arrayConnectivityPollRate=60\"- \"--driverPath=csi-isilon.dellemc.com\"- \"--mode=controller\"- \"--skipArrayConnectionValidation=false\"- \"--driver-config-params=/csi-isilon-config-params/driver-config-params.yaml\"- \"--driverPodLabelValue=dell-storage\"node:args:- \"--csisock=unix:/var/lib/kubelet/plugins/csi-isilon/csi_sock\"- \"--labelvalue=csi-isilon\"- \"--arrayConnectivityPollRate=60\"- \"--driverPath=csi-isilon.dellemc.com\"- \"--mode=node\"- \"--leaderelection=false\"- \"--driver-config-params=/csi-isilon-config-params/driver-config-params.yaml\"- \"--driverPodLabelValue=dell-storage\"Dynamic parameters CSM for Resiliency has configuration parameters that can be updated dynamically, such as the logging level and format. This can be done by editing the Dell CSI Driver’s parameters ConfigMap. The ConfigMap can be queried using kubectl. For example, the Dell Powerflex CSI Driver ConfigMaps can be found using this command: kubectl get -n vxflexos configmap. The ConfigMap to edit will have this pattern: -config-params (e.g., vxflexos-config-params).\nTo update or add parameters, you can use the kubectl edit command. For example, kubectl edit -n vxflexos configmap vxflexos-config-params.\nThis is a list of parameters that can be adjusted for CSM for Resiliency:\n   Parameter Type Default Description     PODMON_CONTROLLER_LOG_FORMAT String “text” Logging format output for the controller podmon sidecar. Should be “text” or “json”   PODMON_CONTROLLER_LOG_LEVEL String “debug” Logging level for the controller podmon sidecar. Standard values: ‘info’, ‘error’, ‘warning’, ‘debug’, ‘trace’   PODMON_NODE_LOG_FORMAT String “text” Logging format output for the node podmon sidecar. Should be “text” or “json”   PODMON_NODE_LOG_LEVEL String “debug” Logging level for the node podmon sidecar. Standard values: ‘info’, ‘error’, ‘warning’, ‘debug’, ‘trace’   PODMON_ARRAY_CONNECTIVITY_POLL_RATE Integer (\u003e0) 15 An interval in seconds to poll the underlying array   PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD Integer (\u003e0) 3 A value representing the number of failed connection poll intervals before marking the array connectivity as lost   PODMON_SKIP_ARRAY_CONNECTION_VALIDATION Boolean false Flag to disable the array connectivity check, set to true for NoSchedule or NoExecute taint due to K8S Control Plane failure (kubelet failure)    Here is an example of the parameters:\nPODMON_CONTROLLER_LOG_FORMAT:\"text\"PODMON_CONTROLLER_LOG_LEVEL:\"info\"PODMON_NODE_LOG_FORMAT:\"text\"PODMON_NODE_LOG_LEVEL:\"info\"PODMON_ARRAY_CONNECTIVITY_POLL_RATE:20PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD:2PODMON_SKIP_ARRAY_CONNECTION_VALIDATION:true","excerpt":"CSM for Resiliency is installed as part of the Dell CSI driver …","ref":"/csm-docs/docs/resiliency/deployment/","title":"Deployment"},{"body":"CSM for Observability can be deployed in one of three ways:\n Helm CSM for Observability Installer CSM for Observability Offline Installer  Post Installation Dependencies The following third-party components are required in the same Kubernetes cluster where CSM for Observability has been deployed:\n Prometheus Grafana Other Deployment Methods  There are various ways to deploy these components. We recommend following the Helm deployments according to the specifications defined below.\nTip: CSM for Observability must be deployed first. Once the module has been deployed, you can proceed to deploying/configuring Prometheus and Grafana.\nPrometheus The Prometheus service should be running on the same Kubernetes cluster as the CSM for Observability services. As part of the CSM for Observability deployment, the OpenTelemetry Collector gets deployed. CSM for Observability pushes metrics to the OpenTelemetry Collector where the metrics are consumed by Prometheus. Prometheus must be configured to scrape the metrics data from the OpenTelemetry Collector.\n   Supported Version Image Helm Chart     2.23.0 prom/prometheus:v2.23.0 Prometheus Helm chart    Note: It is the user’s responsibility to provide persistent storage for Prometheus if they want to preserve historical data.\nPrometheus Helm Deployment Here is a sample minimal configuration for Prometheus. Please note that the configuration below uses insecure skip verify. If you wish to properly configure TLS, you will need to provide a ca_file in the Prometheus configuration. The certificate provided as part of the CSM for Observability deployment should be signed by this same CA. For more information about Prometheus configuration, see Prometheus configuration.\n  Create a values file named prometheus-values.yaml.\n# prometheus-values.yamlalertmanager:enabled:falsenodeExporter:enabled:falsepushgateway:enabled:falsekubeStateMetrics:enabled:falseconfigmapReload:prometheus:enabled:falseserver:enabled:trueimage:repository:quay.io/prometheus/prometheustag:v2.23.0pullPolicy:IfNotPresentpersistentVolume:enabled:falseservice:type:NodePortservicePort:9090extraScrapeConfigs:| - job_name: 'karavi-metrics-[CSI-DRIVER]'scrape_interval:5sscheme:httpsstatic_configs:- targets:['otel-collector:8443']tls_config:insecure_skip_verify:true  If using Rancher, create a ServiceMonitor.\napiVersion:monitoring.coreos.com/v1kind:ServiceMonitormetadata:name:otel-collectornamespace:powerflexspec:endpoints:- path:/metricsport:exporter-httpsscheme:httpstlsConfig:insecureSkipVerify:trueselector:matchLabels:app.kubernetes.io/instance:karavi-observabilityapp.kubernetes.io/name:otel-collector  Add the Prometheus Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add stable https://charts.helm.sh/stable helm repo update   Install the Helm chart.\nOn your terminal, run the command below:\nhelm install prometheus prometheus-community/prometheus -n [CSM_NAMESPACE] -f prometheus-values.yaml   Grafana The Grafana dashboards require Grafana to be deployed in the same Kubernetes cluster as CSM for Observability. Below are the configuration details required to properly set up Grafana to work with CSM for Observability.\n   Supported Version Helm Chart     7.3.0-7.3.2 Grafana Helm chart    Grafana must be configured with the following data sources/plugins:\n   Name Additional Information     Prometheus data source Prometheus data source   Data Table plugin Data Table plugin   Pie Chart plugin Pie Chart plugin   SimpleJson data source SimpleJson data source    Settings for the Grafana Prometheus data source:\n   Setting Value Additional Information     Name Prometheus    Type prometheus    URL http://PROMETHEUS_IP:PORT The IP/PORT of your running Prometheus instance   Access Proxy     Settings for the Grafana SimpleJson data source:\n   Setting Value     Name Karavi-Topology   URL Access CSM for Observability Topology service at https://karavi-topology.namespace.svc.cluster.local:8443   Skip TLS Verify Enabled (If not using CA certificate)   With CA Cert Enabled (If using CA certificate)    Grafana Helm Deployment Below are the steps to deploy a new Grafana instance into your Kubernetes cluster:\n  Create a ConfigMap.\nWhen using a network that requires a decryption certificate, the Grafana server MUST be configured with the necessary certificate. If no certificate is required, skip to step 2.\n Create a Config file named grafana-configmap.yaml The file should look like this:  # grafana-configmap.yamlapiVersion:v1kind:ConfigMapmetadata:name:certs-configmapnamespace:[CSM_NAMESPACE]labels:certs-configmap:\"1\"data:ca-certificates.crt:|- -----BEGIN CERTIFICATE-----ReplaceMeWithActualCaCERT=-----ENDCERTIFICATE-----NOTE: you need an actual CA Cert for it to work\nOn your terminal, run the commands below:\nkubectl create -f grafana-configmap.yaml   Create a values file.\nCreate a Config file named grafana-values.yaml The file should look like this:\n# grafana-values.yaml image:repository:grafana/grafanatag:7.3.0sha:\"\"pullPolicy:IfNotPresentservice:type:NodePort## Administrator credentials when not using an existing SecretadminUser:adminadminPassword:admin## Pass the plugins you want to be installed as a list.##plugins:- grafana-simple-json-datasource- briangann-datatable-panel- grafana-piechart-panel## Configure grafana datasources## ref: http://docs.grafana.org/administration/provisioning/#datasources##datasources:datasources.yaml:apiVersion:1datasources:- name:Karavi-Topologytype:grafana-simple-json-datasourceaccess:proxyurl:'https://karavi-topology:8443'isDefault:nullversion:1editable:truejsonData:tlsSkipVerify:true- name:Prometheustype:prometheusaccess:proxyurl:'http://prometheus-server:9090'isDefault:nullversion:1editable:truetestFramework:enabled:falsesidecar:datasources:enabled:truedashboards:enabled:true## Additional grafana server CofigMap mounts## Defines additional mounts with CofigMap. CofigMap must be manually created in the namespace.extraConfigmapMounts:[]# If you created a ConfigMap on the previous step, delete [] and uncomment the lines below # - name: certs-configmap# mountPath: /etc/ssl/certs/ca-certificates.crt# subPath: ca-certificates.crt# configMap: certs-configmap# readOnly: true  Add the Grafana Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo update   Install the Helm chart.\nOn your terminal, run the commands below:\nhelm install grafana grafana/grafana -n [CSM_NAMESPACE] -f grafana-values.yaml   Other Deployment Methods  Grafana Labs Operator Deployment Rancher Monitoring and Alerting Deployment  Importing CSM for Observability Dashboards Once Grafana is properly configured, you can import the pre-built observability dashboards. Log into Grafana and click the + icon in the side menu. Then click Import. From here you can upload the JSON files or paste the JSON text directly into the text area. Below are the locations of the dashboards that can be imported:\n   Dashboard Description     PowerFlex: I/O Performance by Kubernetes Node Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by Kubernetes node   PowerFlex: I/O Performance by Provisioned Volume Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume   PowerFlex: Storage Pool Consumption By CSI Driver Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.   PowerStore: I/O Performance by Provisioned Volume As of Release 0.4.0: Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume   CSI Driver Provisioned Volume Topology Provides visibility into Dell CSI (Container Storage Interface) driver provisioned volume characteristics in Kubernetes correlated with volumes on the storage system.    Dynamic Configuration Some parameters can be configured/updated during runtime without restarting the CSM for Observability services. These parameters will be stored in ConfigMaps that can be updated on the Kubernetes cluster. This will automatically change the settings on the services.\n   ConfigMap Observability Service Parameters     karavi-metrics-powerflex-configmap karavi-metrics-powerflex COLLECTOR_ADDRPROVISIONER_NAMESPOWERFLEX_SDC_METRICS_ENABLEDPOWERFLEX_SDC_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_POLL_FREQUENCYPOWERFLEX_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMAT   karavi-metrics-powerstore-configmap karavi-metrics-powerstore COLLECTOR_ADDRPROVISIONER_NAMESPOWERSTORE_VOLUME_METRICS_ENABLEDPOWERSTORE_VOLUME_IO_POLL_FREQUENCYPOWERSTORE_SPACE_POLL_FREQUENCYPOWERSTORE_ARRAY_POLL_FREQUENCYPOWERSTORE_FILE_SYSTEM_POLL_FREQUENCYPOWERSTORE_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY   karavi-topology-configmap karavi-topology PROVISIONER_NAMESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY    To update any of these settings, run the following command on the Kubernetes cluster then save the updated ConfigMap data.\nkubectl edit configmap [CONFIG_MAP_NAME] -n [CSM_NAMESPACE] Tracing CSM for Observability is instrumented to report trace data to Zipkin. This helps gather timing data needed to troubleshoot latency problems with CSM for Observability. Follow the instructions below to enable the reporting of trace data:\n  Deploy a Zipkin instance in the CSM namespace and expose the service as NodePort for external access.\napiVersion: apps/v1 kind: Deployment metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance template: metadata: labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance spec: containers: - name: zipkin image: \"openzipkin/zipkin\" imagePullPolicy: IfNotPresent env: - name: \"STORAGE_TYPE\" value: \"mem\" - name: \"TRANSPORT_TYPE\" value: \"http\" --- apiVersion: v1 kind: Service metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: ports: - port: 9411 targetPort: 9411 protocol: TCP type: \"NodePort\" selector: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance   Add the Zipkin URI to the CSM for Observability ConfigMaps. Based on the manifest above, Zipkin will be running on port 9411.\nNote: Zipkin tracing is currently not supported for the collection of PowerFlex metrics.\nUpdate the ConfigMaps from the table above. Here is an example updating the karavi-topology-configmap based on the deployment manifest above.\nkubectl edit configmap/karavi-topology-configmap -n [CSM_NAMESPACE] Update the ZIPKIN_URI and ZIPKIN_PROBABILITY values and save the ConfigMap.\nZIPKIN_URI: \"http://zipkin:9411/api/v2/spans\" ZIPKIN_SERVICE_NAME: \"karavi-topology\" ZIPKIN_PROBABILITY: \"1.0\" Once the ConfigMaps are updated, the changes will automatically be applied and tracing can be seen by accessing Zipkin on the exposed port.\n  Updating Storage System Credentials If the storage system credentials have been updated in the relevant CSI Driver, CSM for Observability must be updated with those new credentials as follows:\nWhen CSM for Observability uses the Authorization module In this case, all storage system requests made by CSM for Observability will be routed through the Authorization module. The following must be performed:\nUpdate the Authorization Module Token   Delete the current proxy-authz-tokens Secret from the CSM namespace.\n$ kubectl delete secret proxy-authz-tokens -n [CSM_NAMESPACE]   Copy the proxy-authz-tokens Secret from a CSI Driver to the CSM namespace.\n$ kubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSM_CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   Update Storage Systems If the list of storage systems managed by a Dell CSI Driver have changed, the following steps can be performed to update CSM for Observability to reference the updated systems:\n  Delete the current karavi-authorization-config Secret from the CSM namespace.\n$ kubectl delete secret proxy-authz-tokens -n [CSM_NAMESPACE]   Copy the karavi-authorization-config Secret from the CSI Driver namespace to CSM for Observability namespace.\n$ kubectl get secret karavi-authorization-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSM_CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   When CSM for Observability does not use the Authorization module In this case all storage system requests made by CSM for Observability will not be routed through the Authorization module. The following must be performed:\nCSI Driver for Dell PowerFlex   Delete the current vxflexos-config Secret from the CSM namespace.\n$ kubectl delete secret vxflexos-config -n [CSM_NAMESPACE]   Copy the vxflexos-config Secret from the CSI Driver for Dell PowerFlex namespace to the CSM namespace.\n$ kubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   CSI Driver for Dell PowerStore   Delete the current powerstore-config Secret from the CSM namespace.\n$ kubectl delete secret powerstore-config -n [CSM_NAMESPACE]   Copy the powerstore-config Secret from the CSI Driver for Dell PowerStore namespace to the CSM namespace.\n$ kubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   ","excerpt":"CSM for Observability can be deployed in one of three ways:\n Helm CSM …","ref":"/csm-docs/v1/observability/deployment/","title":"Deployment"},{"body":"CSM for Resiliency is installed as part of the Dell CSI driver installation. The drivers can be installed either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart installation is supported.\nFor information on the PowerFlex CSI driver, see PowerFlex CSI Driver.\nFor information on the Unity CSI driver, see Unity CSI Driver.\nConfigure all the helm chart parameters described below before installing the drivers.\nHelm Chart Installation The drivers that support Helm chart installation allow CSM for Resiliency to be optionally installed by variables in the chart. There is a podmon block specified in the values.yaml file of the chart that will look similar to the text below by default:\n# Podmon is an optional feature under development and tech preview. # Enable this feature only after contact support for additional information podmon: enabled: true image: dellemc/podmon:v1.1.0 controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" To install CSM for Resiliency with the driver, the following changes are required:\n Enable CSM for Resiliency by changing the podmon.enabled boolean to true. This will enable both controller-podmon and node-podmon. Specify the podmon image to be used as podmon.image. Specify arguments to controller-podmon in the podmon.controller.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon. Specify arguments to node-podmon in the podmon.node.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon.  Podmon Arguments    Argument Required Description Applicability     enabled Required Boolean “true” enables CSM for Resiliency installation with the driver in a helm installation. top level   image Required Must be set to a repository where the podmon image can be pulled. controller \u0026 node   mode Required Must be set to “controller” for controller-podmon and “node” for node-podmon. controller \u0026 node   csisock Required This should be left as set in the helm template for the driver. For controller: -csisock=unix:/var/run/csi/csi.sock For node it will vary depending on the driver’s identity: -csisock=unix:/var/lib/kubelet/plugins\n/vxflexos.emc.dell.com/csi_sock controller \u0026 node   leaderelection Required Boolean value that should be set true for controller and false for node. The default value is true. controller \u0026 node   skipArrayConnectionValidation Optional Boolean value that if set to true will cause controllerPodCleanup to skip the validation that no I/O is ongoing before cleaning up the pod. If set to true will cause controllerPodCleanup on K8S Control Plane failure (kubelet service down). controller   labelKey Optional String value that sets the label key used to denote pods to be monitored by CSM for Resiliency. It will make life easier if this key is the same for all driver types, and drivers are differentiated by different labelValues (see below). If the label keys are the same across all drivers you can do kubectl get pods -A -l labelKey to find all the CSM for Resiliency protected pods. labelKey defaults to “podmon.dellemc.com/driver”. controller \u0026 node   labelValue Required String that sets the value that denotes pods to be monitored by CSM for Resiliency. This must be specific for each driver. Defaults to “csi-vxflexos” for CSI Driver for Dell PowerFlex and “csi-unity” for CSI Driver for Dell Unity controller \u0026 node   arrayConnectivityPollRate Optional The minimum polling rate in seconds to determine if the array has connectivity to a node. Should not be set to less than 5 seconds. See the specific section for each array type for additional guidance. controller   arrayConnectivityConnectionLossThreshold Optional Gives the number of failed connection polls that will be deemed to indicate array connectivity loss. Should not be set to less than 3. See the specific section for each array type for additional guidance. controller   driver-config-params Required String that set the path to a file containing configuration parameter(for instance, Log levels) for a driver. controller \u0026 node    PowerFlex Specific Recommendations PowerFlex supports a very robust array connection validation mechanism that can detect changes in connectivity in about two seconds and can detect whether I/O has occurred over a five-second sample. For that reason it is recommended to set “skipArrayConnectionValidation=false” (which is the default) and to set “arrayConnectivityPollRate=5” (5 seconds) and “arrayConnectivityConnectionLossThreshold=3” to 3 or more.\nHere is a typical installation used for testing:\npodmon:image:dellemc/podmonenabled:truecontroller:args:- \"-csisock=unix:/var/run/csi/csi.sock\"- \"-labelvalue=csi-vxflexos\"- \"-mode=controller\"- \"-arrayConnectivityPollRate=5\"- \"-arrayConnectivityConnectionLossThreshold=3\"- \"--skipArrayConnectionValidation=false\"- \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\"node:args:- \"-csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\"- \"-labelvalue=csi-vxflexos\"- \"-mode=node\"- \"-leaderelection=false\"- \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\"Unity Specific Recommendations Here is a typical installation used for testing:\npodmon:image:dellemc/podmonenabled:truecontroller:args:- \"-csisock=unix:/var/run/csi/csi.sock\"- \"-labelvalue=csi-unity\"- \"-driverPath=csi-unity.dellemc.com\"- \"-mode=controller\"- \"--skipArrayConnectionValidation=false\"- \"--driver-config-params=/unity-config/driver-config-params.yaml\"node:args:- \"-csisock=unix:/var/lib/kubelet/plugins/unity.emc.dell.com/csi_sock\"- \"-labelvalue=csi-unity\"- \"-driverPath=csi-unity.dellemc.com\"- \"-mode=node\"- \"-leaderelection=false\"- \"--driver-config-params=/unity-config/driver-config-params.yaml\"Dynamic parameters CSM for Resiliency has configuration parameters that can be updated dynamically, such as the logging level and format. This can be done by editing the DellEMC CSI Driver’s parameters ConfigMap. The ConfigMap can be queried using kubectl. For example, the DellEMC Powerflex CSI Driver ConfigMaps can be found using the following command: kubectl get -n vxflexos configmap. The ConfigMap to edit will have this pattern: -config-params (e.g., vxflexos-config-params).\nTo update or add parameters, you can use the kubectl edit command. For example, kubectl edit -n vxflexos configmap vxflexos-config-params.\nThis is a list of parameters that can be adjusted for CSM for Resiliency:\n   Parameter Type Default Description     PODMON_CONTROLLER_LOG_FORMAT String “text” Logging format output for the controller podmon sidecar. Should be “text” or “json”   PODMON_CONTROLLER_LOG_LEVEL String “debug” Logging level for the controller podmon sidecar. Standard values: ‘info’, ‘error’, ‘warning’, ‘debug’, ‘trace’   PODMON_NODE_LOG_FORMAT String “text” Logging format output for the node podmon sidecar. Should be “text” or “json”   PODMON_NODE_LOG_LEVEL String “debug” Logging level for the node podmon sidecar. Standard values: ‘info’, ‘error’, ‘warning’, ‘debug’, ‘trace’   PODMON_ARRAY_CONNECTIVITY_POLL_RATE Integer (\u003e0) 15 An interval in seconds to poll the underlying array   PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD Integer (\u003e0) 3 A value representing the number of failed connection poll intervals before marking the array connectivity as lost   PODMON_SKIP_ARRAY_CONNECTION_VALIDATION Boolean false Flag to disable the array connectivity check, set to true for NoSchedule or NoExecute taint due to K8S Control Plane failure (kubelet failure)    Here is an example of the parameters:\nPODMON_CONTROLLER_LOG_FORMAT:\"text\"PODMON_CONTROLLER_LOG_LEVEL:\"info\"PODMON_NODE_LOG_FORMAT:\"text\"PODMON_NODE_LOG_LEVEL:\"info\"PODMON_ARRAY_CONNECTIVITY_POLL_RATE:20PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD:2PODMON_SKIP_ARRAY_CONNECTION_VALIDATION:true","excerpt":"CSM for Resiliency is installed as part of the Dell CSI driver …","ref":"/csm-docs/v1/resiliency/deployment/","title":"Deployment"},{"body":"CSM for Observability can be deployed in one of three ways:\n Helm CSM for Observability Installer CSM for Observability Offline Installer  Post Installation Dependencies The following third-party components are required in the same Kubernetes cluster where CSM for Observability has been deployed:\n Prometheus Grafana Other Deployment Methods  There are various ways to deploy these components. We recommend following the Helm deployments according to the specifications defined below.\nTip: CSM for Observability must be deployed first. Once the module has been deployed, you can proceed to deploying/configuring Prometheus and Grafana.\nPrometheus The Prometheus service should be running on the same Kubernetes cluster as the CSM for Observability services. As part of the CSM for Observability deployment, the OpenTelemetry Collector gets deployed. CSM for Observability pushes metrics to the OpenTelemetry Collector where the metrics are consumed by Prometheus. Prometheus must be configured to scrape the metrics data from the OpenTelemetry Collector.\n   Supported Version Image Helm Chart     2.22.0 prom/prometheus:v2.22.0 Prometheus Helm chart    Note: It is the user’s responsibility to provide persistent storage for Prometheus if they want to preserve historical data.\nPrometheus Helm Deployment Here is a sample minimal configuration for Prometheus. Please note that the configuration below uses insecure skip verify. If you wish to properly configure TLS, you will need to provide a ca_file in the Prometheus configuration. The certificate provided as part of the CSM for Observability deployment should be signed by this same CA. For more information about Prometheus configuration, see Prometheus configuration.\n  Create a values file named prometheus-values.yaml.\n# prometheus-values.yamlalertmanager:enabled:falsenodeExporter:enabled:falsepushgateway:enabled:falsekubeStateMetrics:enabled:falseconfigmapReload:prometheus:enabled:falseserver:enabled:trueimage:repository:quay.io/prometheus/prometheustag:v2.23.0pullPolicy:IfNotPresentpersistentVolume:enabled:falseservice:type:NodePortservicePort:9090extraScrapeConfigs:|- job_name:'karavi-metrics-powerflex'scrape_interval:5sscheme:httpsstatic_configs:- targets:['otel-collector:8443']tls_config:insecure_skip_verify:true  If using Rancher, create a ServiceMonitor.\napiVersion:monitoring.coreos.com/v1kind:ServiceMonitormetadata:name:otel-collectornamespace:powerflexspec:endpoints:- path:/metricsport:exporter-httpsscheme:httpstlsConfig:insecureSkipVerify:trueselector:matchLabels:app.kubernetes.io/instance:karavi-observabilityapp.kubernetes.io/name:otel-collector  Add the Prometheus Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add stable https://charts.helm.sh/stable helm repo update   Install the Helm chart.\nOn your terminal, run the command below:\nhelm install prometheus prometheus-community/prometheus -n [CSM_NAMESPACE] -f prometheus-values.yaml   Grafana The Grafana dashboards require Grafana to be deployed in the same Kubernetes cluster as CSM for Observability. Below are the configuration details required to properly set up Grafana to work with CSM for Observability.\n   Supported Version Helm Chart     7.3.0-7.3.2 Grafana Helm chart    Grafana must be configured with the following data sources/plugins:\n   Name Additional Information     Prometheus data source Prometheus data source   Data Table plugin Data Table plugin   Pie Chart plugin Pie Chart plugin   SimpleJson data source SimpleJson data source    Settings for the Grafana Prometheus data source:\n   Setting Value Additional Information     Name Prometheus    Type prometheus    URL http://PROMETHEUS_IP:PORT The IP/PORT of your running Prometheus instance   Access Proxy     Settings for the Grafana SimpleJson data source:\n   Setting Value     Name Karavi-Topology   URL Access CSM for Observability Topology service at https://karavi-topology.namespace.svc.cluster.local:8443   Skip TLS Verify Enabled (If not using CA certificate)   With CA Cert Enabled (If using CA certificate)    Grafana Helm Deployment Below are the steps to deploy a new Grafana instance into your Kubernetes cluster:\n  Create a ConfigMap.\nWhen using a network that requires a decryption certificate, the Grafana server MUST be configured with the necessary certificate. If no certificate is required, skip to step 2.\n Create a Config file named grafana-configmap.yaml The file should look like this:  # grafana-configmap.yamlapiVersion:v1kind:ConfigMapmetadata:name:certs-configmapnamespace:[CSM_NAMESPACE]labels:certs-configmap:\"1\"data:ca-certificates.crt:|- -----BEGIN CERTIFICATE-----ReplaceMeWithActualCaCERT=-----ENDCERTIFICATE-----NOTE: you need an actual CA Cert for it to work\nOn your terminal, run the commands below:\nkubectl create -f grafana-configmap.yaml   Create a values file.\nCreate a Config file named grafana-values.yaml The file should look like this:\n# grafana-values.yaml image:repository:grafana/grafanatag:7.3.0sha:\"\"pullPolicy:IfNotPresentservice:type:NodePort## Administrator credentials when not using an existing SecretadminUser:adminadminPassword:admin## Pass the plugins you want to be installed as a list.##plugins:- grafana-simple-json-datasource- briangann-datatable-panel- grafana-piechart-panel## Configure grafana datasources## ref: http://docs.grafana.org/administration/provisioning/#datasources##datasources:datasources.yaml:apiVersion:1datasources:- name:Karavi-Topologytype:grafana-simple-json-datasourceaccess:proxyurl:'https://karavi-topology:8443'isDefault:nullversion:1editable:truejsonData:tlsSkipVerify:true- name:Prometheustype:prometheusaccess:proxyurl:'http://prometheus:9090'isDefault:nullversion:1editable:truetestFramework:enabled:falsesidecar:datasources:enabled:truedashboards:enabled:true## Additional grafana server CofigMap mounts## Defines additional mounts with CofigMap. CofigMap must be manually created in the namespace.extraConfigmapMounts:[]# If you created a ConfigMap on the previous step, delete [] and uncomment the lines below # - name: certs-configmap# mountPath: /etc/ssl/certs/ca-certificates.crt# subPath: ca-certificates.crt# configMap: certs-configmap# readOnly: true  Add the Grafana Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo update   Install the Helm chart.\nOn your terminal, run the commands below:\nhelm install grafana grafana/grafana -n [CSM_NAMESPACE] -f grafana-values.yaml   Other Deployment Methods  Grafana Labs Operator Deployment Rancher Monitoring and Alerting Deployment  Importing CSM for Observability Dashboards Once Grafana is properly configured, you can import the pre-built observability dashboards. Log into Grafana and click the + icon in the side menu. Then click Import. From here you can upload the JSON files or paste the JSON text directly into the text area. Below are the locations of the dashboards that can be imported:\n   Dashboard Description     PowerFlex: I/O Performance by Kubernetes Node Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by Kubernetes node   PowerFlex: I/O Performance by Provisioned Volume Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume   PowerFlex: Storage Pool Consumption By CSI Driver Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.   PowerStore: I/O Performance by Provisioned Volume As of Release 0.4.0: Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume   CSI Driver Provisioned Volume Topology Provides visibility into Dell CSI (Container Storage Interface) driver provisioned volume characteristics in Kubernetes correlated with volumes on the storage system.    Dynamic Configuration Some parameters can be configured/updated during runtime without restarting the CSM for Observability services. These parameters will be stored in ConfigMaps that can be updated on the Kubernetes cluster. This will automatically change the settings on the services.\n   ConfigMap Observability Service Parameters     karavi-metrics-powerflex-configmap karavi-metrics-powerflex COLLECTOR_ADDRPROVISIONER_NAMESPOWERFLEX_SDC_METRICS_ENABLEDPOWERFLEX_SDC_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_POLL_FREQUENCYPOWERFLEX_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMAT   karavi-metrics-powerstore-configmap karavi-metrics-powerstore COLLECTOR_ADDRPROVISIONER_NAMESPOWERSTORE_VOLUME_METRICS_ENABLEDPOWERSTORE_VOLUME_IO_POLL_FREQUENCYPOWERSTORE_SPACE_POLL_FREQUENCYPOWERSTORE_ARRAY_POLL_FREQUENCYPOWERSTORE_FILE_SYSTEM_POLL_FREQUENCYPOWERSTORE_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY   karavi-topology-configmap karavi-topology PROVISIONER_NAMESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY    To update any of these settings, run the following command on the Kubernetes cluster then save the updated ConfigMap data.\nkubectl edit configmap [CONFIG_MAP_NAME] -n [CSM_NAMESPACE] Tracing CSM for Observability is instrumented to report trace data to Zipkin. This helps gather timing data needed to troubleshoot latency problems with CSM for Observability. Follow the instructions below to enable the reporting of trace data:\n  Deploy a Zipkin instance in the CSM namespace and expose the service as NodePort for external access.\napiVersion: apps/v1 kind: Deployment metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance template: metadata: labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance spec: containers: - name: zipkin image: \"openzipkin/zipkin\" imagePullPolicy: IfNotPresent env: - name: \"STORAGE_TYPE\" value: \"mem\" - name: \"TRANSPORT_TYPE\" value: \"http\" --- apiVersion: v1 kind: Service metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: ports: - port: 9411 targetPort: 9411 protocol: TCP type: \"NodePort\" selector: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance   Add the Zipkin URI to the CSM for Observability ConfigMaps. Based on the manifest above, Zipkin will be running on port 9411.\nNote: Zipkin tracing is currently not supported for the collection of PowerFlex metrics.\nUpdate the ConfigMaps from the table above. Here is an example updating the karavi-topology-configmap based on the deployment manifest above.\nkubectl edit configmap/karavi-topology-configmap -n [CSM_NAMESPACE] Update the ZIPKIN_URI and ZIPKIN_PROBABILITY values and save the ConfigMap.\nZIPKIN_URI: \"http://zipkin:9411/api/v2/spans\" ZIPKIN_SERVICE_NAME: \"karavi-topology\" ZIPKIN_PROBABILITY: \"1.0\" Once the ConfigMaps are updated, the changes will automatically be applied and tracing can be seen by accessing Zipkin on the exposed port.\n  Updating Storage System Credentials If the storage system credentials have been updated in the relevant CSI Driver, CSM for Observability must be updated with those new credentials as follows:\nWhen CSM for Observability uses the Authorization module In this case, all storage system requests made by CSM for Observability will be routed through the Authorization module. The following must be performed:\nUpdate the Authorization Module Token   Delete the current proxy-authz-tokens Secret from the CSM namespace.\n$ kubectl delete secret proxy-authz-tokens -n [CSM_NAMESPACE]   Copy the proxy-authz-tokens Secret from a CSI Driver to the CSM namespace.\n$ kubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSM_CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   Update Storage Systems If the list of storage systems managed by a Dell CSI Driver have changed, the following steps can be performed to update CSM for Observability to reference the updated systems:\n  Delete the current karavi-authorization-config Secret from the CSM namespace.\n$ kubectl delete secret proxy-authz-tokens -n [CSM_NAMESPACE]   Copy the karavi-authorization-config Secret from the CSI Driver namespace to CSM for Observability namespace.\n$ kubectl get secret karavi-authorization-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSM_CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   When CSM for Observability does not use the Authorization module In this case all storage system requests made by CSM for Observability will not be routed through the Authorization module. The following must be performed:\nCSI Driver for Dell PowerFlex   Delete the current vxflexos-config Secret from the CSM namespace.\n$ kubectl delete secret vxflexos-config -n [CSM_NAMESPACE]   Copy the vxflexos-config Secret from the CSI Driver for Dell PowerFlex namespace to the CSM namespace.\n$ kubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   CSI Driver for Dell PowerStore   Delete the current powerstore-config Secret from the CSM namespace.\n$ kubectl delete secret powerstore-config -n [CSM_NAMESPACE]   Copy the powerstore-config Secret from the CSI Driver for Dell PowerStore namespace to the CSM namespace.\n$ kubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   ","excerpt":"CSM for Observability can be deployed in one of three ways:\n Helm CSM …","ref":"/csm-docs/v2/observability/deployment/","title":"Deployment"},{"body":"CSM for Resiliency is installed as part of the Dell CSI driver installation. The drivers can be installed either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart installation is supported.\nFor information on the PowerFlex CSI driver, see PowerFlex CSI Driver.\nFor information on the Unity CSI driver, see Unity CSI Driver.\nConfigure all the helm chart parameters described below before installing the drivers.\nHelm Chart Installation The drivers that support Helm chart installation allow CSM for Resiliency to be optionally installed by variables in the chart. There is a podmon block specified in the values.yaml file of the chart that will look similar to the text below by default:\n# Podmon is an optional feature under development and tech preview. # Enable this feature only after contact support for additional information podmon: enabled: true image: dellemc/podmon:v1.1.0 controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=controller\" - \"--skipArrayConnectionValidation=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" To install CSM for Resiliency with the driver, the following changes are required:\n Enable CSM for Resiliency by changing the podmon.enabled boolean to true. This will enable both controller-podmon and node-podmon. Specify the podmon image to be used as podmon.image. Specify arguments to controller-podmon in the podmon.controller.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon. Specify arguments to node-podmon in the podmon.node.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon.  Podmon Arguments    Argument Required Description Applicability     enabled Required Boolean “true” enables CSM for Resiliency installation with the driver in a helm installation. top level   image Required Must be set to a repository where the podmon image can be pulled. controller \u0026 node   mode Required Must be set to “controller” for controller-podmon and “node” for node-podmon. controller \u0026 node   csisock Required This should be left as set in the helm template for the driver. For controller: -csisock=unix:/var/run/csi/csi.sock For node it will vary depending on the driver’s identity: -csisock=unix:/var/lib/kubelet/plugins\n/vxflexos.emc.dell.com/csi_sock controller \u0026 node   leaderelection Required Boolean value that should be set true for controller and false for node. The default value is true. controller \u0026 node   skipArrayConnectionValidation Optional Boolean value that if set to true will cause controllerPodCleanup to skip the validation that no I/O is ongoing before cleaning up the pod. If set to true will cause controllerPodCleanup on K8S Control Plane failure (kubelet service down). controller   labelKey Optional String value that sets the label key used to denote pods to be monitored by CSM for Resiliency. It will make life easier if this key is the same for all driver types, and drivers are differentiated by different labelValues (see below). If the label keys are the same across all drivers you can do kubectl get pods -A -l labelKey to find all the CSM for Resiliency protected pods. labelKey defaults to “podmon.dellemc.com/driver”. controller \u0026 node   labelValue Required String that sets the value that denotes pods to be monitored by CSM for Resiliency. This must be specific for each driver. Defaults to “csi-vxflexos” for CSI Driver for Dell PowerFlex and “csi-unity” for CSI Driver for Dell Unity controller \u0026 node   arrayConnectivityPollRate Optional The minimum polling rate in seconds to determine if the array has connectivity to a node. Should not be set to less than 5 seconds. See the specific section for each array type for additional guidance. controller   arrayConnectivityConnectionLossThreshold Optional Gives the number of failed connection polls that will be deemed to indicate array connectivity loss. Should not be set to less than 3. See the specific section for each array type for additional guidance. controller   driver-config-params Required String that set the path to a file containing configuration parameter(for instance, Log levels) for a driver. controller \u0026 node    PowerFlex Specific Recommendations PowerFlex supports a very robust array connection validation mechanism that can detect changes in connectivity in about two seconds and can detect whether I/O has occurred over a five-second sample. For that reason it is recommended to set “skipArrayConnectionValidation=false” (which is the default) and to set “arrayConnectivityPollRate=5” (5 seconds) and “arrayConnectivityConnectionLossThreshold=3” to 3 or more.\nHere is a typical installation used for testing:\npodmon:image:dellemc/podmonenabled:truecontroller:args:- \"-csisock=unix:/var/run/csi/csi.sock\"- \"-labelvalue=csi-vxflexos\"- \"-mode=controller\"- \"-arrayConnectivityPollRate=5\"- \"-arrayConnectivityConnectionLossThreshold=3\"- \"--skipArrayConnectionValidation=false\"- \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\"node:args:- \"-csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\"- \"-labelvalue=csi-vxflexos\"- \"-mode=node\"- \"-leaderelection=false\"- \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\"Unity Specific Recommendations Here is a typical installation used for testing:\npodmon:image:dellemc/podmonenabled:truecontroller:args:- \"-csisock=unix:/var/run/csi/csi.sock\"- \"-labelvalue=csi-unity\"- \"-driverPath=csi-unity.dellemc.com\"- \"-mode=controller\"- \"--skipArrayConnectionValidation=false\"- \"--driver-config-params=/unity-config/driver-config-params.yaml\"node:args:- \"-csisock=unix:/var/lib/kubelet/plugins/unity.emc.dell.com/csi_sock\"- \"-labelvalue=csi-unity\"- \"-driverPath=csi-unity.dellemc.com\"- \"-mode=node\"- \"-leaderelection=false\"- \"--driver-config-params=/unity-config/driver-config-params.yaml\"Dynamic parameters CSM for Resiliency has configuration parameters that can be updated dynamically, such as the logging level and format. This can be done by editing the DellEMC CSI Driver’s parameters ConfigMap. The ConfigMap can be queried using kubectl. For example, the DellEMC Powerflex CSI Driver ConfigMaps can be found using the following command: kubectl get -n vxflexos configmap. The ConfigMap to edit will have this pattern: -config-params (e.g., vxflexos-config-params).\nTo update or add parameters, you can use the kubectl edit command. For example, kubectl edit -n vxflexos configmap vxflexos-config-params.\nThis is a list of parameters that can be adjusted for CSM for Resiliency:\n   Parameter Type Default Description     PODMON_CONTROLLER_LOG_FORMAT String “text” Logging format output for the controller podmon sidecar. Should be “text” or “json”   PODMON_CONTROLLER_LOG_LEVEL String “debug” Logging level for the controller podmon sidecar. Standard values: ‘info’, ‘error’, ‘warning’, ‘debug’, ‘trace’   PODMON_NODE_LOG_FORMAT String “text” Logging format output for the node podmon sidecar. Should be “text” or “json”   PODMON_NODE_LOG_LEVEL String “debug” Logging level for the node podmon sidecar. Standard values: ‘info’, ‘error’, ‘warning’, ‘debug’, ‘trace’   PODMON_ARRAY_CONNECTIVITY_POLL_RATE Integer (\u003e0) 15 An interval in seconds to poll the underlying array   PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD Integer (\u003e0) 3 A value representing the number of failed connection poll intervals before marking the array connectivity as lost   PODMON_SKIP_ARRAY_CONNECTION_VALIDATION Boolean false Flag to disable the array connectivity check, set to true for NoSchedule or NoExecute taint due to K8S Control Plane failure (kubelet failure)    Here is an example of the parameters:\nPODMON_CONTROLLER_LOG_FORMAT:\"text\"PODMON_CONTROLLER_LOG_LEVEL:\"info\"PODMON_NODE_LOG_FORMAT:\"text\"PODMON_NODE_LOG_LEVEL:\"info\"PODMON_ARRAY_CONNECTIVITY_POLL_RATE:20PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD:2PODMON_SKIP_ARRAY_CONNECTION_VALIDATION:true","excerpt":"CSM for Resiliency is installed as part of the Dell CSI driver …","ref":"/csm-docs/v2/resiliency/deployment/","title":"Deployment"},{"body":"CSM for Observability can be deployed in one of three ways:\n CSM Installer (Recommended installation method) Helm CSM for Observability Installer CSM for Observability Offline Installer  Prerequisites  Helm 3.3 The deployment of one or more supported Dell EMC CSI drivers  Post Installation Dependencies The following third-party components are required in the same Kubernetes cluster where CSM for Observability has been deployed:\n Prometheus Grafana  These components must be deployed according to the specifications defined below.\nTip: CSM for Observability must be deployed first. Once the module has been deployed, you can proceed to deploying/configuring Prometheus and Grafana.\nPrometheus The Prometheus service should be running on the same Kubernetes cluster as the CSM for Observability services. As part of the CSM for Observability deployment, the OpenTelemetry Collector gets deployed. The OpenTelemetry Collector is what CSM for Observability pushes metrics so that the metrics can be consumed by Prometheus. This means that Prometheus must be configured to scrape the metrics data from the OpenTelemetry Collector.\n   Supported Version Image Helm Chart     2.22.0 prom/prometheus:v2.22.0 Prometheus Helm chart    Note: It is the user’s responsibility to provide persistent storage for Prometheus if they want to preserve historical data.\nPrometheus Deployment Here is a sample minimal configuration for Prometheus. Please note that the configuration below uses insecure skip verify. If you wish to properly configure TLS, you will need to provide a ca_file in the Prometheus configuration. The certificate provided as part of the CSM for Observability deployment should be signed by this same CA. For more information about Prometheus configuration, see Prometheus configuration.\n  Create a values file named prometheus-values.yaml.\n# prometheus-values.yamlalertmanager:enabled:falsenodeExporter:enabled:falsepushgateway:enabled:falsekubeStateMetrics:enabled:falseconfigmapReload:prometheus:enabled:falseserver:enabled:trueimage:repository:quay.io/prometheus/prometheustag:v2.22.0pullPolicy:IfNotPresentpersistentVolume:enabled:falseservice:type:NodePortservicePort:9090serverFiles:prometheus.yml:scrape_configs:- job_name:'karavi-metrics-powerflex'scrape_interval:5sscheme:httpsstatic_configs:- targets:['otel-collector:8443']tls_config:insecure_skip_verify:true  If using Rancher, create a ServiceMonitor.\napiVersion:monitoring.coreos.com/v1kind:ServiceMonitormetadata:name:otel-collectornamespace:powerflexspec:endpoints:- path:/metricsport:exporter-httpsscheme:httpstlsConfig:insecureSkipVerify:trueselector:matchLabels:app.kubernetes.io/instance:karavi-observabilityapp.kubernetes.io/name:otel-collector  Add the Prometheus Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add stable https://charts.helm.sh/stable helm repo update   Install the Helm chart.\nOn your terminal, run the command below:\nhelm install prometheus prometheus-community/prometheus -n [CSM_NAMESPACE] --create-namespace -f prometheus-values.yaml   Grafana The Grafana dashboards require Grafana to be deployed in the same Kubernetes cluster as CSM for Observability. Below are the configuration details required to properly set up Grafana to work with CSM for Observability.\n   Supported Version Helm Chart     7.3.0-7.3.2 Grafana Helm chart    Grafana must be configured with the following data sources/plugins:\n   Name Additional Information     Prometheus data source Prometheus data source   Data Table plugin Data Table plugin   Pie Chart plugin Pie Chart plugin   SimpleJson data source SimpleJson data source    Settings for the Grafana Prometheus data source:\n   Setting Value Additional Information     Name Prometheus    Type prometheus    URL http://PROMETHEUS_IP:PORT The IP/PORT of your running Prometheus instance   Access Proxy     Settings for the Grafana SimpleJson data source:\n   Setting Value     Name Karavi-Topology   URL Access CSM for Observability Topology service at https://karavi-topology.namespace.svc.cluster.local:8443   Skip TLS Verify Enabled (If not using CA certificate)   With CA Cert Enabled (If using CA certificate)    Grafana Deployment Below are the steps to deploy a new Grafana instance into your Kubernetes cluster:\n  Create a ConfigMap.\nWhen using a network that requires a decryption certificate, the Grafana server MUST be configured with the necessary certificate. If no certificate is required, skip to step 2.\n Create a Config file named grafana-configmap.yaml The file should look like this:  # grafana-configmap.yamlapiVersion:v1kind:ConfigMapmetadata:name:certs-configmapnamespace:[CSM_NAMESPACE]labels:certs-configmap:\"1\"data:ca-certificates.crt:|- -----BEGIN CERTIFICATE-----ReplaceMeWithActualCaCERT=-----ENDCERTIFICATE-----NOTE: you need an actual CA Cert for it to work\nOn your terminal, run the commands below:\nkubectl create -f grafana-configmap.yaml   Create a values file.\nCreate a Config file named grafana-configmap.yaml The file should look like this:\n# grafana-values.yaml image:repository:grafana/grafanatag:7.3.0sha:\"\"pullPolicy:IfNotPresentservice:type:NodePort## Administrator credentials when not using an existing SecretadminUser:adminadminPassword:admin## Pass the plugins you want to be installed as a list.##plugins:- grafana-simple-json-datasource- briangann-datatable-panel- grafana-piechart-panel## Configure grafana datasources## ref: http://docs.grafana.org/administration/provisioning/#datasources##datasources:datasources.yaml:apiVersion:1datasources:- name:Karavi-Topologytype:grafana-simple-json-datasourceaccess:proxyurl:'https://karavi-topology:8443'isDefault:nullversion:1editable:truejsonData:tlsSkipVerify:true- name:Prometheustype:prometheusaccess:proxyurl:'http://prometheus:9090'isDefault:nullversion:1editable:truetestFramework:enabled:falsesidecar:datasources:enabled:truedashboards:enabled:true## Additional grafana server CofigMap mounts## Defines additional mounts with CofigMap. CofigMap must be manually created in the namespace.extraConfigmapMounts:[]# If you created a ConfigMap on the previous step, delete [] and uncomment the lines below # - name: certs-configmap# mountPath: /etc/ssl/certs/ca-certificates.crt# subPath: ca-certificates.crt# configMap: certs-configmap# readOnly: true  Add the Grafana Helm chart repository.\nOn your terminal, run each of the commands below:\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo update   Install the Helm chart.\nOn your terminal, run the commands below:\nhelm install grafana grafana/grafana -n [CSM_NAMESPACE] -f grafana-values.yaml   Importing CSM for Observability Dashboards Once Grafana is properly configured, you can import the pre-built observability dashboards. Log into Grafana and click the + icon in the side menu. Then click Import. From here you can upload the JSON files or paste the JSON text directly into the text area. Below are the locations of the dashboards that can be imported:\n   Dashboard Description     PowerFlex: I/O Performance by Kubernetes Node Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by Kubernetes node   PowerFlex: I/O Performance by Provisioned Volume Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume   PowerFlex: Storage Pool Consumption By CSI Driver Provides visibility into the total, used, and available capacity for a storage class and associated underlying storage construct.   PowerStore: I/O Performance by Provisioned Volume As of Release 0.4.0: Provides visibility into the I/O performance metrics (IOPS, bandwidth, latency) by volume   CSI Driver Provisioned Volume Topology Provides visibility into Dell EMC CSI (Container Storage Interface) driver provisioned volume characteristics in Kubernetes correlated with volumes on the storage system.    Dynamic Configuration Some parameters can be configured/updated during runtime without restarting the CSM for Observability services. These parameters will be stored in ConfigMaps that can be updated on the Kubernetes cluster. This will automatically change the settings on the services.\n   ConfigMap Observability Service Parameters     karavi-metrics-powerflex-configmap karavi-metrics-powerflex COLLECTOR_ADDRPROVISIONER_NAMESPOWERFLEX_SDC_METRICS_ENABLEDPOWERFLEX_SDC_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_IO_POLL_FREQUENCYPOWERFLEX_VOLUME_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_METRICS_ENABLEDPOWERFLEX_STORAGE_POOL_POLL_FREQUENCYPOWERFLEX_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMAT   karavi-metrics-powerstore-configmap karavi-metrics-powerstore COLLECTOR_ADDRPROVISIONER_NAMESPOWERSTORE_VOLUME_METRICS_ENABLEDPOWERSTORE_VOLUME_IO_POLL_FREQUENCYPOWERSTORE_SPACE_POLL_FREQUENCYPOWERSTORE_ARRAY_POLL_FREQUENCYPOWERSTORE_FILE_SYSTEM_POLL_FREQUENCYPOWERSTORE_MAX_CONCURRENT_QUERIESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY   karavi-topology-configmap karavi-topology PROVISIONER_NAMESLOG_LEVELLOG_FORMATZIPKIN_URIZIPKIN_SERVICE_NAMEZIPKIN_PROBABILITY    To update any of these settings, run the following command on the Kubernetes cluster then save the updated ConfigMap data.\nkubectl edit configmap [CONFIG_MAP_NAME] -n [CSM_NAMESPACE] Tracing CSM for Observability is instrumented to report trace data to Zipkin. This helps gather timing data needed to troubleshoot latency problems with CSM for Observability. Follow the instructions below to enable the reporting of trace data:\n  Deploy a Zipkin instance in the CSM namespace and expose the service as NodePort for external access.\napiVersion: apps/v1 kind: Deployment metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance template: metadata: labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance spec: containers: - name: zipkin image: \"openzipkin/zipkin\" imagePullPolicy: IfNotPresent env: - name: \"STORAGE_TYPE\" value: \"mem\" - name: \"TRANSPORT_TYPE\" value: \"http\" --- apiVersion: v1 kind: Service metadata: name: zipkin labels: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance app.kubernetes.io/managed-by: zipkin-service spec: ports: - port: 9411 targetPort: 9411 protocol: TCP type: \"NodePort\" selector: app.kubernetes.io/name: zipkin app.kubernetes.io/instance: zipkin-instance   Add the Zipkin URI to the CSM for Observability ConfigMaps. Based on the manifest above, Zipkin will be running on port 9411.\nNote: Zipkin tracing is currently not supported for the collection of PowerFlex metrics.\nUpdate the ConfigMaps from the table above. Here is an example updating the karavi-topology-configmap based on the deployment manifest above.\nkubectl edit configmap/karavi-topology-configmap -n [CSM_NAMESPACE] Update the ZIPKIN_URI and ZIPKIN_PROBABILITY values and save the ConfigMap.\nZIPKIN_URI: \"http://zipkin:9411/api/v2/spans\" ZIPKIN_SERVICE_NAME: \"karavi-topology\" ZIPKIN_PROBABILITY: \"1.0\" Once the ConfigMaps are updated, the changes will automatically be applied and tracing can be seen by accessing Zipkin on the exposed port.\n  Updating Storage System Credentials If the storage system credentials have been updated in the relevant CSI Driver, CSM for Observability must be updated with those new credentials as follows:\nWhen CSM for Observability uses the Authorization module In this case, all storage system requests made by CSM for Observability will be routed through the Authorization module. The following must be performed:\nUpdate the Authorization Module Token   Delete the current proxy-authz-tokens Secret from the CSM namespace.\n$ kubectl delete secret proxy-authz-tokens -n [CSM_NAMESPACE]   Copy the proxy-authz-tokens Secret from a CSI Driver to the CSM namespace.\n$ kubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSM_CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   Update Storage Systems If the list of storage systems managed by a Dell EMC CSI Driver have changed, the following steps can be performed to update CSM for Observability to reference the updated systems:\n  Delete the current karavi-authorization-config Secret from the CSM namespace.\n$ kubectl delete secret proxy-authz-tokens -n [CSM_NAMESPACE]   Copy the karavi-authorization-config Secret from the CSI Driver namespace to CSM for Observability namespace.\n$ kubectl get secret karavi-authorization-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSM_CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   When CSM for Observability does not use the Authorization module In this case all storage system requests made by CSM for Observability will not be routed through the Authorization module. The following must be performed:\nCSI Driver for Dell EMC PowerFlex   Delete the current vxflexos-config Secret from the CSM namespace.\n$ kubectl delete secret vxflexos-config -n [CSM_NAMESPACE]   Copy the vxflexos-config Secret from the CSI Driver for Dell EMC PowerFlex namespace to the CSM namespace.\n$ kubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   CSI Driver for Dell EMC PowerStore   Delete the current powerstore-config Secret from the CSM namespace.\n$ kubectl delete secret powerstore-config -n [CSM_NAMESPACE]   Copy the powerstore-config Secret from the CSI Driver for Dell EMC PowerStore namespace to the CSM namespace.\n$ kubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   ","excerpt":"CSM for Observability can be deployed in one of three ways:\n CSM …","ref":"/csm-docs/v3/observability/deployment/","title":"Deployment"},{"body":"CSM for Resiliency is installed as part of the Dell CSI driver installation. The drivers can be installed either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart installation is supported.\nFor information on the PowerFlex CSI driver, see PowerFlex CSI Driver.\nFor information on the Unity CSI driver, see Unity CSI Driver.\nConfigure all the helm chart parameters described below before installing the drivers.\nHelm Chart Installation The drivers that support Helm chart installation allow CSM for Resiliency to be optionally installed by variables in the chart. There is a podmon block specified in the values.yaml file of the chart that will look similar to the text below by default:\n# Podmon is an optional feature under development and tech preview. # Enable this feature only after contact support for additional information podmon: enabled: true image: dellemc/podmon:v1.0.0 controller: args: - \"--csisock=unix:/var/run/csi/csi.sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=controller\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" node: args: - \"--csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\" - \"--labelvalue=csi-vxflexos\" - \"--mode=node\" - \"--leaderelection=false\" - \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\" To install CSM for Resiliency with the driver, the following changes are required:\n Enable CSM for Resiliency by changing the podmon.enabled boolean to true. This will enable both controller-podmon and node-podmon. Specify the podmon image to be used as podmon.image. Specify arguments to controller-podmon in the podmon.controller.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon. Specify arguments to node-podmon in the podmon.node.args block. See “Podmon Arguments” below. Note that some arguments are required. Note that the arguments supplied to controller-podmon are different from those supplied to node-podmon.  Podmon Arguments    Argument Required Description Applicability     enabled Required Boolean “true” enables CSM for Resiliency installation with the driver in a helm installation. top level   image Required Must be set to a repository where the podmon image can be pulled. controller \u0026 node   mode Required Must be set to “controller” for controller-podmon and “node” for node-podmon. controller \u0026 node   csisock Required This should be left as set in the helm template for the driver. For controller: -csisock=unix:/var/run/csi/csi.sock For node it will vary depending on the driver’s identity: -csisock=unix:/var/lib/kubelet/plugins\n/vxflexos.emc.dell.com/csi_sock controller \u0026 node   leaderelection Required Boolean value that should be set true for controller and false for node. The default value is true. controller \u0026 node   skipArrayConnectionValidation Optional Boolean value that if set to true will cause controllerPodCleanup to skip the validation that no I/O is ongoing before cleaning up the pod. controller   labelKey Optional String value that sets the label key used to denote pods to be monitored by CSM for Resiliency. It will make life easier if this key is the same for all driver types, and drivers are differentiated by different labelValues (see below). If the label keys are the same across all drivers you can do kubectl get pods -A -l labelKey to find all the CSM for Resiliency protected pods. labelKey defaults to “podmon.dellemc.com/driver”. controller \u0026 node   labelValue Required String that sets the value that denotes pods to be monitored by CSM for Resiliency. This must be specific for each driver. Defaults to “csi-vxflexos” for CSI Driver for Dell EMC PowerFlex and “csi-unity” for CSI Driver for Dell EMC Unity controller \u0026 node   arrayConnectivityPollRate Optional The minimum polling rate in seconds to determine if the array has connectivity to a node. Should not be set to less than 5 seconds. See the specific section for each array type for additional guidance. controller   arrayConnectivityConnectionLossThreshold Optional Gives the number of failed connection polls that will be deemed to indicate array connectivity loss. Should not be set to less than 3. See the specific section for each array type for additional guidance. controller   driver-config-params Required String that set the path to a file containing configuration parameter(for instance, Log levels) for a driver. controller \u0026 node    PowerFlex Specific Recommendations PowerFlex supports a very robust array connection validation mechanism that can detect changes in connectivity in about two seconds and can detect whether I/O has occurred over a five-second sample. For that reason it is recommended to set “skipArrayConnectionValidation=false” (which is the default) and to set “arrayConnectivityPollRate=5” (5 seconds) and “arrayConnectivityConnectionLossThreshold=3” to 3 or more.\nHere is a typical installation used for testing:\npodmon:image:dellemc/podmonenabled:truecontroller:args:- \"-csisock=unix:/var/run/csi/csi.sock\"- \"-labelvalue=csi-vxflexos\"- \"-mode=controller\"- \"-arrayConnectivityPollRate=5\"- \"-arrayConnectivityConnectionLossThreshold=3\"- \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\"node:args:- \"-csisock=unix:/var/lib/kubelet/plugins/vxflexos.emc.dell.com/csi_sock\"- \"-labelvalue=csi-vxflexos\"- \"-mode=node\"- \"-leaderelection=false\"- \"--driver-config-params=/vxflexos-config-params/driver-config-params.yaml\"Unity Specific Recommendations Here is a typical installation used for testing:\npodmon:image:dellemc/podmonenabled:truecontroller:args:- \"-csisock=unix:/var/run/csi/csi.sock\"- \"-labelvalue=csi-unity\"- \"-driverPath=csi-unity.dellemc.com\"- \"-mode=controller\"- \"--driver-config-params=/unity-config/driver-config-params.yaml\"node:args:- \"-csisock=unix:/var/lib/kubelet/plugins/unity.emc.dell.com/csi_sock\"- \"-labelvalue=csi-unity\"- \"-driverPath=csi-unity.dellemc.com\"- \"-mode=node\"- \"-leaderelection=false\"- \"--driver-config-params=/unity-config/driver-config-params.yaml\"Dynamic parameters CSM for Resiliency has configuration parameters that can be updated dynamically, such as the logging level and format. This can be done by editing the DellEMC CSI Driver’s parameters ConfigMap. The ConfigMap can be queried using kubectl. For example, the DellEMC Powerflex CSI Driver ConfigMaps can be found using the following command: kubectl get -n vxflexos configmap. The ConfigMap to edit will have this pattern: -config-params (e.g., vxflexos-config-params).\nTo update or add parameters, you can use the kubectl edit command. For example, kubectl edit -n vxflexos configmap vxflexos-config-params.\nThis is a list of parameters that can be adjusted for CSM for Resiliency:\n   Parameter Type Default Description     PODMON_CONTROLLER_LOG_FORMAT String “text” Logging format output for the controller podmon sidecar. Should be “text” or “json”   PODMON_CONTROLLER_LOG_LEVEL String “debug” Logging level for the controller podmon sidecar. Standard values: ‘info’, ‘error’, ‘warning’, ‘debug’, ‘trace’   PODMON_NODE_LOG_FORMAT String “text” Logging format output for the node podmon sidecar. Should be “text” or “json”   PODMON_NODE_LOG_LEVEL String “debug” Logging level for the node podmon sidecar. Standard values: ‘info’, ‘error’, ‘warning’, ‘debug’, ‘trace’   PODMON_ARRAY_CONNECTIVITY_POLL_RATE Integer (\u003e0) 15 An interval in seconds to poll the underlying array   PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD Integer (\u003e0) 3 A value representing the number of failed connection poll intervals before marking the array connectivity as lost   PODMON_SKIP_ARRAY_CONNECTION_VALIDATION Boolean false Flag to disable the array connectivity check    Here is an example of the parameters:\nPODMON_CONTROLLER_LOG_FORMAT:\"text\"PODMON_CONTROLLER_LOG_LEVEL:\"info\"PODMON_NODE_LOG_FORMAT:\"text\"PODMON_NODE_LOG_LEVEL:\"info\"PODMON_ARRAY_CONNECTIVITY_POLL_RATE:20PODMON_ARRAY_CONNECTIVITY_CONNECTION_LOSS_THRESHOLD:2PODMON_SKIP_ARRAY_CONNECTION_VALIDATION:true","excerpt":"CSM for Resiliency is installed as part of the Dell CSI driver …","ref":"/csm-docs/v3/resiliency/deployment/","title":"Deployment"},{"body":"The Container Storage Modules (CSM) for Observability Helm chart bootstraps an Observability deployment on a Kubernetes cluster using the Helm package manager.\nPrerequisites  Helm 3.3 The deployment of one or more supported Dell CSI drivers  Install the CSM for Observability Helm Chart Steps\n  Create a namespace where you want to install the module kubectl create namespace karavi\n  Install cert-manager CRDs kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.crds.yaml\n  Add the Dell Helm Charts repo helm repo add dell https://dell.github.io/helm-charts\n  Copy only the deployed CSI driver entities to the Observability namespace\nPowerFlex   Copy the config Secret from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\n  If CSM for Authorization is enabled for CSI PowerFlex, perform the following steps:\n Copy the driver configuration parameters ConfigMap from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get configmap vxflexos-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\n  Copy the karavi-authorization-config, proxy-server-root-certificate, proxy-authz-tokens Secret from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\n  PowerStore   Copy the config Secret from the CSI PowerStore namespace into the CSM for Observability namespace:\nkubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\n    Configure the parameters and install the CSM for Observability Helm Chart\nA default values.yaml file is located here that can be used for installation. This can be copied into a file named myvalues.yaml and either used as is or modified accordingly.\nNote:\n The default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured in your values file for CSM Observability.  $ helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] -f myvalues.yaml Alternatively, you can specify each parameter using the ‘–set key=value[,key=value]’ and/or ‘–set-file key=value[,key=value] arguments to ‘helm install’. For example:\n$ helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] \\ --set-file karaviTopology.certificateFile=\u003clocation-of-karavi-topology-certificate-file\u003e \\ --set-file karaviTopology.privateKeyFile=\u003clocation-of-karavi-topology-private-key-file\u003e \\ --set-file otelCollector.certificateFile=\u003clocation-of-otel-collector-certificate-file\u003e \\ --set-file otelCollector.privateKeyFile=\u003clocation-of-otel-collector-private-key-file\u003e   Configuration The following table lists the configurable parameters of the CSM for Observability Helm chart and their default values.\n   Parameter Description Default     karaviTopology.image Location of the csm-topology Docker image dellemc/csm-topology:v1.0   karaviTopology.enabled Enable the CSM for Observability Topology service true   karaviTopology.provisionerNames Provisioner Names used to filter the Persistent Volumes created on the Kubernetes cluster (must be a comma-separated list)  csi-vxflexos.dellemc.com   karaviTopology.service.type Kubernetes service type ClusterIP   karaviTopology.certificateFile Optional valid CA public certificate file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’.    karaviTopology.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’.    karaviTopology.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO   karaviTopology.logFormat Output logs in the specified format (Valid values: text, json) text   otelCollector.certificateFile Optional valid CA public certificate file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’.    otelCollector.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’.    otelCollector.service.type Kubernetes service type ClusterIP   karaviMetricsPowerflex.image CSM Metrics for PowerFlex Service image dellemc/csm-metrics-powerflex:v1.0   karaviMetricsPowerflex.enabled Enable CSM Metrics for PowerFlex service true   karaviMetricsPowerflex.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680   karaviMetricsPowerflex.provisionerNames Provisioner Names used to filter for determining PowerFlex SDC nodes( Must be a Comma-separated list)  csi-vxflexos.dellemc.com   karaviMetricsPowerflex.sdcPollFrequencySeconds The polling frequency (in seconds) to gather SDC metrics 10   karaviMetricsPowerflex.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10   karaviMetricsPowerflex.storageClassPoolPollFrequencySeconds The polling frequency (in seconds) to gather storage class/pool metrics 10   karaviMetricsPowerflex.concurrentPowerflexQueries The number of simultaneous metrics queries to make to Powerflex(MUST be less than 10; otherwise, several request errors from Powerflex will ensue. 10   karaviMetricsPowerflex.authorization.enabled Authorization is an optional feature to apply credential shielding of the backend PowerFlex. false   karaviMetricsPowerflex.authorization.proxyHost Hostname of the csm-authorization server.    karaviMetricsPowerflex.authorization.skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server.    karaviMetricsPowerflex.sdcMetricsEnabled Enable PowerFlex SDC Metrics Collection true   karaviMetricsPowerflex.volumeMetricsEnabled Enable PowerFlex Volume Metrics Collection true   karaviMetricsPowerflex.storageClassPoolMetricsEnabled Enable PowerFlex Storage Class/Pool Metrics Collection true   karaviMetricsPowerflex.endpoint Endpoint for pod leader election karavi-metrics-powerflex   karaviMetricsPowerflex.service.type Kubernetes service type ClusterIP   karaviMetricsPowerflex.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO   karaviMetricsPowerflex.logFormat Output logs in the specified format (Valid values: text, json) text   karaviMetricsPowerstore.image CSM Metrics for PowerStore Service image dellemc/csm-metrics-powerstore:v1.0   karaviMetricsPowerstore.enabled Enable CSM Metrics for PowerStore service true   karaviMetricsPowerstore.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680   karaviMetricsPowerstore.provisionerNames Provisioner Names used to filter for determining PowerStore volumes (must be a Comma-separated list) csi-powerstore.dellemc.com   karaviMetricsPowerstore.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10   karaviMetricsPowerstore.concurrentPowerflexQueries The number of simultaneous metrics queries to make to PowerStore (must be less than 10; otherwise, several request errors from PowerStore will ensue.) 10   karaviMetricsPowerstore.volumeMetricsEnabled Enable PowerStore Volume Metrics Collection true   karaviMetricsPowerstore.endpoint Endpoint for pod leader election karavi-metrics-powerstore   karaviMetricsPowerstore.service.type Kubernetes service type ClusterIP   karaviMetricsPowerstore.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO   karaviMetricsPowerstore.logFormat Output logs in the specified format (Valid values: text, json) text   karaviMetricsPowerstore.zipkin.uri URI of a Zipkin instance where tracing data can be forwarded    karaviMetricsPowerstore.zipkin.serviceName Service name used for Zipkin tracing data metrics-powerstore   karaviMetricsPowerstore.zipkin.probability Percentage of trace information to send to Zipkin (Valid range: 0.0 to 1.0) 0    ","excerpt":"The Container Storage Modules (CSM) for Observability Helm chart …","ref":"/csm-docs/docs/observability/deployment/helm/","title":"Helm"},{"body":"The Container Storage Modules (CSM) for Observability Helm chart bootstraps an Observability deployment on a Kubernetes cluster using the Helm package manager.\nPrerequisites  Helm 3.3 The deployment of one or more supported Dell CSI drivers  Install the CSM for Observability Helm Chart Steps\n  Create a namespace where you want to install the module kubectl create namespace karavi\n  Install cert-manager CRDs kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.crds.yaml\n  Add the Dell Helm Charts repo helm repo add dell https://dell.github.io/helm-charts\n  Copy only the deployed CSI driver entities to the Observability namespace\nPowerFlex   Copy the config Secret from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\n  If CSM for Authorization is enabled for CSI PowerFlex, perform the following steps:\n Copy the driver configuration parameters ConfigMap from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get configmap vxflexos-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\n  Copy the karavi-authorization-config, proxy-server-root-certificate, proxy-authz-tokens Secret from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\n  PowerStore   Copy the config Secret from the CSI PowerStore namespace into the CSM for Observability namespace:\nkubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\n    Configure the parameters and install the CSM for Observability Helm Chart\nA default values.yaml file is located here that can be used for installation. This can be copied into a file named myvalues.yaml and either used as is or modified accordingly.\nNote:\n The default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured in your values file for CSM Observability.  $ helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] -f myvalues.yaml Alternatively, you can specify each parameter using the ‘–set key=value[,key=value]’ and/or ‘–set-file key=value[,key=value] arguments to ‘helm install’. For example:\n$ helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] \\ --set-file karaviTopology.certificateFile=\u003clocation-of-karavi-topology-certificate-file\u003e \\ --set-file karaviTopology.privateKeyFile=\u003clocation-of-karavi-topology-private-key-file\u003e \\ --set-file otelCollector.certificateFile=\u003clocation-of-otel-collector-certificate-file\u003e \\ --set-file otelCollector.privateKeyFile=\u003clocation-of-otel-collector-private-key-file\u003e   Configuration The following table lists the configurable parameters of the CSM for Observability Helm chart and their default values.\n   Parameter Description Default     karaviTopology.image Location of the csm-topology Docker image dellemc/csm-topology:v1.0   karaviTopology.enabled Enable the CSM for Observability Topology service true   karaviTopology.provisionerNames Provisioner Names used to filter the Persistent Volumes created on the Kubernetes cluster (must be a comma-separated list)  csi-vxflexos.dellemc.com   karaviTopology.service.type Kubernetes service type ClusterIP   karaviTopology.certificateFile Optional valid CA public certificate file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’.    karaviTopology.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’.    karaviTopology.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO   karaviTopology.logFormat Output logs in the specified format (Valid values: text, json) text   otelCollector.certificateFile Optional valid CA public certificate file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’.    otelCollector.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’.    otelCollector.service.type Kubernetes service type ClusterIP   karaviMetricsPowerflex.image CSM Metrics for PowerFlex Service image dellemc/csm-metrics-powerflex:v1.0   karaviMetricsPowerflex.enabled Enable CSM Metrics for PowerFlex service true   karaviMetricsPowerflex.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680   karaviMetricsPowerflex.provisionerNames Provisioner Names used to filter for determining PowerFlex SDC nodes( Must be a Comma-separated list)  csi-vxflexos.dellemc.com   karaviMetricsPowerflex.sdcPollFrequencySeconds The polling frequency (in seconds) to gather SDC metrics 10   karaviMetricsPowerflex.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10   karaviMetricsPowerflex.storageClassPoolPollFrequencySeconds The polling frequency (in seconds) to gather storage class/pool metrics 10   karaviMetricsPowerflex.concurrentPowerflexQueries The number of simultaneous metrics queries to make to Powerflex(MUST be less than 10; otherwise, several request errors from Powerflex will ensue. 10   karaviMetricsPowerflex.authorization.enabled Authorization is an optional feature to apply credential shielding of the backend PowerFlex. false   karaviMetricsPowerflex.authorization.proxyHost Hostname of the csm-authorization server.    karaviMetricsPowerflex.authorization.skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server.    karaviMetricsPowerflex.sdcMetricsEnabled Enable PowerFlex SDC Metrics Collection true   karaviMetricsPowerflex.volumeMetricsEnabled Enable PowerFlex Volume Metrics Collection true   karaviMetricsPowerflex.storageClassPoolMetricsEnabled Enable PowerFlex Storage Class/Pool Metrics Collection true   karaviMetricsPowerflex.endpoint Endpoint for pod leader election karavi-metrics-powerflex   karaviMetricsPowerflex.service.type Kubernetes service type ClusterIP   karaviMetricsPowerflex.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO   karaviMetricsPowerflex.logFormat Output logs in the specified format (Valid values: text, json) text   karaviMetricsPowerstore.image CSM Metrics for PowerStore Service image dellemc/csm-metrics-powerstore:v1.0   karaviMetricsPowerstore.enabled Enable CSM Metrics for PowerStore service true   karaviMetricsPowerstore.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680   karaviMetricsPowerstore.provisionerNames Provisioner Names used to filter for determining PowerStore volumes (must be a Comma-separated list) csi-powerstore.dellemc.com   karaviMetricsPowerstore.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10   karaviMetricsPowerstore.concurrentPowerflexQueries The number of simultaneous metrics queries to make to PowerStore (must be less than 10; otherwise, several request errors from PowerStore will ensue.) 10   karaviMetricsPowerstore.volumeMetricsEnabled Enable PowerStore Volume Metrics Collection true   karaviMetricsPowerstore.endpoint Endpoint for pod leader election karavi-metrics-powerstore   karaviMetricsPowerstore.service.type Kubernetes service type ClusterIP   karaviMetricsPowerstore.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO   karaviMetricsPowerstore.logFormat Output logs in the specified format (Valid values: text, json) text   karaviMetricsPowerstore.zipkin.uri URI of a Zipkin instance where tracing data can be forwarded    karaviMetricsPowerstore.zipkin.serviceName Service name used for Zipkin tracing data metrics-powerstore   karaviMetricsPowerstore.zipkin.probability Percentage of trace information to send to Zipkin (Valid range: 0.0 to 1.0) 0    ","excerpt":"The Container Storage Modules (CSM) for Observability Helm chart …","ref":"/csm-docs/v1/observability/deployment/helm/","title":"Helm"},{"body":"The Container Storage Modules (CSM) for Observability Helm chart bootstraps an Observability deployment on a Kubernetes cluster using the Helm package manager.\nPrerequisites  Helm 3.3 The deployment of one or more supported Dell CSI drivers  Install the CSM for Observability Helm Chart Steps\n  Create a namespace where you want to install the module kubectl create namespace karavi\n  Install cert-manager CRDs kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.crds.yaml\n  Add the Dell Helm Charts repo helm repo add dell https://dell.github.io/helm-charts\n  Copy only the deployed CSI driver entities to the Observability namespace\nPowerFlex   Copy the config Secret from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\n  If CSM for Authorization is enabled for CSI PowerFlex, perform the following steps:\n Copy the driver configuration parameters ConfigMap from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get configmap vxflexos-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\n  Copy the karavi-authorization-config, proxy-server-root-certificate, proxy-authz-tokens Secret from the CSI PowerFlex namespace into the CSM for Observability namespace:\nkubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\n  PowerStore   Copy the config Secret from the CSI PowerStore namespace into the CSM for Observability namespace:\nkubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -\n    Configure the parameters and install the CSM for Observability Helm Chart\nA default values.yaml file is located here that can be used for installation. This can be copied into a file named myvalues.yaml and either used as is or modified accordingly.\nNote:\n The default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured in your values file for CSM Observability.  $ helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] -f myvalues.yaml Alternatively, you can specify each parameter using the ‘–set key=value[,key=value]’ and/or ‘–set-file key=value[,key=value] arguments to ‘helm install’. For example:\n$ helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] \\ --set-file karaviTopology.certificateFile=\u003clocation-of-karavi-topology-certificate-file\u003e \\ --set-file karaviTopology.privateKeyFile=\u003clocation-of-karavi-topology-private-key-file\u003e \\ --set-file otelCollector.certificateFile=\u003clocation-of-otel-collector-certificate-file\u003e \\ --set-file otelCollector.privateKeyFile=\u003clocation-of-otel-collector-private-key-file\u003e   Configuration The following table lists the configurable parameters of the CSM for Observability Helm chart and their default values.\n   Parameter Description Default     karaviTopology.image Location of the csm-topology Docker image dellemc/csm-topology:v1.0   karaviTopology.enabled Enable the CSM for Observability Topology service true   karaviTopology.provisionerNames Provisioner Names used to filter the Persistent Volumes created on the Kubernetes cluster (must be a comma-separated list)  csi-vxflexos.dellemc.com   karaviTopology.service.type Kubernetes service type ClusterIP   karaviTopology.certificateFile Optional valid CA public certificate file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’.    karaviTopology.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’.    karaviTopology.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO   karaviTopology.logFormat Output logs in the specified format (Valid values: text, json) text   otelCollector.certificateFile Optional valid CA public certificate file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’.    otelCollector.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’.    otelCollector.service.type Kubernetes service type ClusterIP   karaviMetricsPowerflex.image CSM Metrics for PowerFlex Service image dellemc/csm-metrics-powerflex:v1.0   karaviMetricsPowerflex.enabled Enable CSM Metrics for PowerFlex service true   karaviMetricsPowerflex.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680   karaviMetricsPowerflex.provisionerNames Provisioner Names used to filter for determining PowerFlex SDC nodes( Must be a Comma-separated list)  csi-vxflexos.dellemc.com   karaviMetricsPowerflex.sdcPollFrequencySeconds The polling frequency (in seconds) to gather SDC metrics 10   karaviMetricsPowerflex.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10   karaviMetricsPowerflex.storageClassPoolPollFrequencySeconds The polling frequency (in seconds) to gather storage class/pool metrics 10   karaviMetricsPowerflex.concurrentPowerflexQueries The number of simultaneous metrics queries to make to Powerflex(MUST be less than 10; otherwise, several request errors from Powerflex will ensue. 10   karaviMetricsPowerflex.authorization.enabled Authorization is an optional feature to apply credential shielding of the backend PowerFlex. false   karaviMetricsPowerflex.authorization.proxyHost Hostname of the csm-authorization server.    karaviMetricsPowerflex.authorization.skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server.    karaviMetricsPowerflex.sdcMetricsEnabled Enable PowerFlex SDC Metrics Collection true   karaviMetricsPowerflex.volumeMetricsEnabled Enable PowerFlex Volume Metrics Collection true   karaviMetricsPowerflex.storageClassPoolMetricsEnabled Enable PowerFlex Storage Class/Pool Metrics Collection true   karaviMetricsPowerflex.endpoint Endpoint for pod leader election karavi-metrics-powerflex   karaviMetricsPowerflex.service.type Kubernetes service type ClusterIP   karaviMetricsPowerflex.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO   karaviMetricsPowerflex.logFormat Output logs in the specified format (Valid values: text, json) text   karaviMetricsPowerstore.image CSM Metrics for PowerStore Service image dellemc/csm-metrics-powerstore:v1.0   karaviMetricsPowerstore.enabled Enable CSM Metrics for PowerStore service true   karaviMetricsPowerstore.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680   karaviMetricsPowerstore.provisionerNames Provisioner Names used to filter for determining PowerStore volumes (must be a Comma-separated list) csi-powerstore.dellemc.com   karaviMetricsPowerstore.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10   karaviMetricsPowerstore.concurrentPowerflexQueries The number of simultaneous metrics queries to make to PowerStore (must be less than 10; otherwise, several request errors from PowerStore will ensue.) 10   karaviMetricsPowerstore.volumeMetricsEnabled Enable PowerStore Volume Metrics Collection true   karaviMetricsPowerstore.endpoint Endpoint for pod leader election karavi-metrics-powerstore   karaviMetricsPowerstore.service.type Kubernetes service type ClusterIP   karaviMetricsPowerstore.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO   karaviMetricsPowerstore.logFormat Output logs in the specified format (Valid values: text, json) text   karaviMetricsPowerstore.zipkin.uri URI of a Zipkin instance where tracing data can be forwarded    karaviMetricsPowerstore.zipkin.serviceName Service name used for Zipkin tracing data metrics-powerstore   karaviMetricsPowerstore.zipkin.probability Percentage of trace information to send to Zipkin (Valid range: 0.0 to 1.0) 0    ","excerpt":"The Container Storage Modules (CSM) for Observability Helm chart …","ref":"/csm-docs/v2/observability/deployment/helm/","title":"Helm"},{"body":"The Container Storage Modules (CSM) for Observability Helm chart bootstraps an Observability deployment on a Kubernetes cluster using the Helm package manager.\nPrerequisites   A supported CSI Driver is deployed\n  The cert-manager CustomResourceDefinition resources are created.\n$ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.5.3/cert-manager.crds.yaml   Copy the CSI Driver Secret Copy the config Secret from the Dell CSI Driver namespace into the namespace where CSM for Observability is deployed.\nPowerFlex $ kubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Note: The target namespace must exist before executing this command.\nPowerStore $ kubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Note: The target namespace must exist before executing this command.\nAdd the Repo $ helm repo add dell https://dell.github.io/helm-charts Installing the Chart $ helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] --create-namespace The configuration section below lists all the parameters that can be configured during installation\nConfiguration The following table lists the configurable parameters of the CSM for Observability Helm chart and their default values.\n   Parameter Description Default     karaviTopology.image Location of the csm-topology Docker image dellemc/csm-topology:v1.0   karaviTopology.enabled Enable the CSM for Observability Topology service true   karaviTopology.provisionerNames Provisioner Names used to filter the Persistent Volumes created on the Kubernetes cluster (must be a comma-separated list)  csi-vxflexos.dellemc.com   karaviTopology.service.type Kubernetes service type ClusterIP   karaviTopology.certificateFile Optional valid CA public certificate file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’.    karaviTopology.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the Topology service. Must use domain name ‘karavi-topology’.    karaviTopology.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO   karaviTopology.logFormat Output logs in the specified format (Valid values: text, json) text   otelCollector.certificateFile Optional valid CA public certificate file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’.    otelCollector.privateKeyFile Optional public certificate’s associated private key file that will be used to deploy the OpenTelemetry Collector. Must use domain name ‘otel-collector’.    otelCollector.service.type Kubernetes service type ClusterIP   karaviMetricsPowerflex.image CSM Metrics for PowerFlex Service image dellemc/csm-metrics-powerflex:v1.0   karaviMetricsPowerflex.enabled Enable CSM Metrics for PowerFlex service true   karaviMetricsPowerflex.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680   karaviMetricsPowerflex.provisionerNames Provisioner Names used to filter for determining PowerFlex SDC nodes( Must be a Comma-separated list)  csi-vxflexos.dellemc.com   karaviMetricsPowerflex.sdcPollFrequencySeconds The polling frequency (in seconds) to gather SDC metrics 10   karaviMetricsPowerflex.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10   karaviMetricsPowerflex.storageClassPoolPollFrequencySeconds The polling frequency (in seconds) to gather storage class/pool metrics 10   karaviMetricsPowerflex.concurrentPowerflexQueries The number of simultaneous metrics queries to make to Powerflex(MUST be less than 10; otherwise, several request errors from Powerflex will ensue. 10   karaviMetricsPowerflex.sdcMetricsEnabled Enable PowerFlex SDC Metrics Collection true   karaviMetricsPowerflex.volumeMetricsEnabled Enable PowerFlex Volume Metrics Collection true   karaviMetricsPowerflex.storageClassPoolMetricsEnabled Enable PowerFlex Storage Class/Pool Metrics Collection true   karaviMetricsPowerflex.endpoint Endpoint for pod leader election karavi-metrics-powerflex   karaviMetricsPowerflex.service.type Kubernetes service type ClusterIP   karaviMetricsPowerflex.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO   karaviMetricsPowerflex.logFormat Output logs in the specified format (Valid values: text, json) text   karaviMetricsPowerstore.image CSM Metrics for PowerStore Service image dellemc/csm-metrics-powerstore:v1.0   karaviMetricsPowerstore.enabled Enable CSM Metrics for PowerStore service true   karaviMetricsPowerstore.collectorAddr Metrics Collector accessible from the Kubernetes cluster otel-collector:55680   karaviMetricsPowerstore.provisionerNames Provisioner Names used to filter for determining PowerStore volumes (must be a Comma-separated list) csi-powerstore.dellemc.com   karaviMetricsPowerstore.volumePollFrequencySeconds The polling frequency (in seconds) to gather volume metrics 10   karaviMetricsPowerstore.concurrentPowerflexQueries The number of simultaneous metrics queries to make to PowerStore (must be less than 10; otherwise, several request errors from PowerStore will ensue.) 10   karaviMetricsPowerstore.volumeMetricsEnabled Enable PowerStore Volume Metrics Collection true   karaviMetricsPowerstore.endpoint Endpoint for pod leader election karavi-metrics-powerstore   karaviMetricsPowerstore.service.type Kubernetes service type ClusterIP   karaviMetricsPowerstore.logLevel Output logs that are at or above the given log level severity (Valid values: TRACE, DEBUG, INFO, WARN, ERROR, FATAL, PANIC) INFO   karaviMetricsPowerstore.logFormat Output logs in the specified format (Valid values: text, json) text   karaviMetricsPowerstore.zipkin.uri URI of a Zipkin instance where tracing data can be forwarded    karaviMetricsPowerstore.zipkin.serviceName Service name used for Zipkin tracing data metrics-powerstore   karaviMetricsPowerstore.zipkin.probability Percentage of trace information to send to Zipkin (Valid range: 0.0 to 1.0) 0    Specify each parameter using the ‘–set key=value[,key=value]’ and/or ‘–set-file key=value[,key=value] arguments to ‘helm install’. For example:\n$ helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] --create-namespace \\ --set-file karaviTopology.certificateFile=\u003clocation-of-karavi-topology-certificate-file\u003e \\ --set-file karaviTopology.privateKeyFile=\u003clocation-of-karavi-topology-private-key-file\u003e \\ --set-file otelCollector.certificateFile=\u003clocation-of-otel-collector-certificate-file\u003e \\ --set-file otelCollector.privateKeyFile=\u003clocation-of-otel-collector-private-key-file\u003e Alternatively, a YAML file that specifies the values for the above parameters can be provided while installing the chart. For example:\n$ helm install karavi-observability dell/karavi-observability -n [CSM_NAMESPACE] --create-namespace -f values.yaml Note: You can use the default values.yaml\n","excerpt":"The Container Storage Modules (CSM) for Observability Helm chart …","ref":"/csm-docs/v3/observability/deployment/helm/","title":"Helm"},{"body":"The Container Storage Modules (CSM) for Observability installer bootstraps Helm to create a more simplified and robust deployment option that does the following:\n Verifies CSM for Observability is not yet installed Verifies the Kubernetes/Openshift versions are supported Verifies the Helm version is supported Adds the Dell Helm chart repository Refreshes the Helm chart repositories to download any recent changes Creates the CSM namespace (if not already created) Copies the secrets from the CSI driver namespaces into the CSM namespace (if not already copied) Installs the CertManager CRDs (if not already installed) Installs the CSM for Observability Helm chart Waits for the CSM for Observability pods to become ready  If the Authorization module is enabled for the CSI drivers installed in the same Kubernetes cluster, the installer will perform the current steps to enable CSM for Observability to use the same Authorization instance:\n Verifies the karavictl binary is available. Verifies the appropriate Secrets and ConfigMap exist in the CSI driver namespace. Updates the CSM for Observability deployment to use the existing Authorization instance if not already enabled during the initial installation of CSM for Observability.  Prerequisites  Helm 3.3 The deployment of one or more supported Dell CSI drivers  Online Installer The following instructions can be followed to install CSM for Observability in an environment that has an internet connection and is capable of downloading the required Helm chart and Docker images.\nDependencies A Linux-based system, with internet access, will be used to execute the script to install CSM for Observability into a Kubernetes/Openshift environment that also has internet access.\n   Dependency Usage     kubectl kubectl will be used to verify the Kubernetes/OpenShift environment   helm helm will be used to install the CSM for Observability helm chart   jq jq will be used to parse the CSM for Authorization configuration file during installation    Installer Usage [user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh --help Help for ./karavi-observability-install.sh Usage: ./karavi-observability-install.sh mode options... Mode: install Installs Karavi Observability and enables Karavi Authorization if already installed enable-authorization Updates existing installation of Karavi Observability with Karavi Authorization upgrade Upgrades existing installation of Karavi Observability to the latest release Options: Required --namespace[=]\u003cnamespace\u003e Namespace where Karavi Observability will be installed Optional --csi-powerflex-namespace[=]\u003ccsi powerflex namespace\u003e Namespace where CSI PowerFlex is installed, default is 'vxflexos' --set-file Set values from files used during helm installation (can be specified multiple times) --skip-verify Skip verification of the environment --values[=]\u003cvalues.yaml\u003e Values file, which defines configuration values --verbose Display verbose logging --version[=]\u003chelm chart version\u003e Helm chart version to install, default value will be latest --help Help Note: CSM for Authorization currently does not support the Observability module for PowerStore. Therefore setting enable-authorization is not supported in this case.\nExecuting the Installer To perform an online installation of CSM for Observability, the following steps should be performed:\n  Clone the GitHub repository:\n[user@system /home/user]# git clone https://github.com/dell/karavi-observability.git   Change to the installer directory:\n[user@system /home/user]# cd karavi-observability/installer   Execute the installation script. The following example will install CSM for Observability into the CSM namespace.\nA sample values.yaml file is located here. This can be copied into a file named myvalues.yaml and modified accordingly for the installer command below. Configuration options are outlined in the Helm chart deployment section.\nNote:\n The default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured in myvalues.yaml for CSM Observability.  [user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh install --namespace [CSM_NAMESPACE] --values myvalues.yaml --------------------------------------------------------------------------------- \u003e Installing Karavi Observability in namespace karavi on 1.21 --------------------------------------------------------------------------------- | |- Karavi Observability is not installed Success | |- Karavi Authorization will be enabled during installation | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Configure helm chart repository | |--\u003e Adding helm repository https://dell.github.io/helm-charts Success | |--\u003e Updating helm repositories Success | |- Creating namespace karavi Success | |- CSI Driver for PowerFlex is installed Success | |- Copying Secret from vxflexos to karavi Success | |- CSI Driver for PowerStore is installed Success | |- Copying Secret from powerstore to karavi Success | |- Installing CertManager CRDs Success | |- Enabling Karavi Authorization for Karavi Observability | |--\u003e Copying ConfigMap from vxflexos to karavi Success | |--\u003e Copying Karavi Authorization Secrets from vxflexos to karavi Success | |- Installing Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success   ","excerpt":"The Container Storage Modules (CSM) for Observability installer …","ref":"/csm-docs/docs/observability/deployment/online/","title":"Installer"},{"body":"The Container Storage Modules (CSM) for Observability installer bootstraps Helm to create a more simplified and robust deployment option that does the following:\n Verifies CSM for Observability is not yet installed Verifies the Kubernetes/Openshift versions are supported Verifies the Helm version is supported Adds the Dell Helm chart repository Refreshes the Helm chart repositories to download any recent changes Creates the CSM namespace (if not already created) Copies the secrets from the CSI driver namespaces into the CSM namespace (if not already copied) Installs the CertManager CRDs (if not already installed) Installs the CSM for Observability Helm chart Waits for the CSM for Observability pods to become ready  If the Authorization module is enabled for the CSI drivers installed in the same Kubernetes cluster, the installer will perform the current steps to enable CSM for Observability to use the same Authorization instance:\n Verifies the karavictl binary is available. Verifies the appropriate Secrets and ConfigMap exist in the CSI driver namespace. Updates the CSM for Observability deployment to use the existing Authorization instance if not already enabled during the initial installation of CSM for Observability.  Prerequisites  Helm 3.3 The deployment of one or more supported Dell CSI drivers  Online Installer The following instructions can be followed to install CSM for Observability in an environment that has an internet connection and is capable of downloading the required Helm chart and Docker images.\nDependencies A Linux-based system, with internet access, will be used to execute the script to install CSM for Observability into a Kubernetes/Openshift environment that also has internet access.\n   Dependency Usage     kubectl kubectl will be used to verify the Kubernetes/OpenShift environment   helm helm will be used to install the CSM for Observability helm chart   jq jq will be used to parse the CSM for Authorization configuration file during installation    Installer Usage [user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh --help Help for ./karavi-observability-install.sh Usage: ./karavi-observability-install.sh mode options... Mode: install Installs Karavi Observability and enables Karavi Authorization if already installed enable-authorization Updates existing installation of Karavi Observability with Karavi Authorization upgrade Upgrades existing installation of Karavi Observability to the latest release Options: Required --namespace[=]\u003cnamespace\u003e Namespace where Karavi Observability will be installed Optional --csi-powerflex-namespace[=]\u003ccsi powerflex namespace\u003e Namespace where CSI PowerFlex is installed, default is 'vxflexos' --set-file Set values from files used during helm installation (can be specified multiple times) --skip-verify Skip verification of the environment --values[=]\u003cvalues.yaml\u003e Values file, which defines configuration values --verbose Display verbose logging --version[=]\u003chelm chart version\u003e Helm chart version to install, default value will be latest --help Help Note: CSM for Authorization currently does not support the Observability module for PowerStore. Therefore setting enable-authorization is not supported in this case.\nExecuting the Installer To perform an online installation of CSM for Observability, the following steps should be performed:\n  Clone the GitHub repository:\n[user@system /home/user]# git clone https://github.com/dell/karavi-observability.git   Change to the installer directory:\n[user@system /home/user]# cd karavi-observability/installer   Execute the installation script. The following example will install CSM for Observability into the CSM namespace.\nA sample values.yaml file is located here. This can be copied into a file named myvalues.yaml and modified accordingly for the installer command below. Configuration options are outlined in the Helm chart deployment section.\nNote:\n The default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured in myvalues.yaml for CSM Observability.  [user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh install --namespace [CSM_NAMESPACE] --values myvalues.yaml --------------------------------------------------------------------------------- \u003e Installing Karavi Observability in namespace karavi on 1.21 --------------------------------------------------------------------------------- | |- Karavi Observability is not installed Success | |- Karavi Authorization will be enabled during installation | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Configure helm chart repository | |--\u003e Adding helm repository https://dell.github.io/helm-charts Success | |--\u003e Updating helm repositories Success | |- Creating namespace karavi Success | |- CSI Driver for PowerFlex is installed Success | |- Copying Secret from vxflexos to karavi Success | |- CSI Driver for PowerStore is installed Success | |- Copying Secret from powerstore to karavi Success | |- Installing CertManager CRDs Success | |- Enabling Karavi Authorization for Karavi Observability | |--\u003e Copying ConfigMap from vxflexos to karavi Success | |--\u003e Copying Karavi Authorization Secrets from vxflexos to karavi Success | |- Installing Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success   ","excerpt":"The Container Storage Modules (CSM) for Observability installer …","ref":"/csm-docs/v1/observability/deployment/online/","title":"Installer"},{"body":"The Container Storage Modules (CSM) for Observability installer bootstraps Helm to create a more simplified and robust deployment option that does the following:\n Verifies CSM for Observability is not yet installed Verifies the Kubernetes/Openshift versions are supported Verifies the Helm version is supported Adds the Dell Helm chart repository Refreshes the Helm chart repositories to download any recent changes Creates the CSM namespace (if not already created) Copies the secrets from the CSI driver namespaces into the CSM namespace (if not already copied) Installs the CertManager CRDs (if not already installed) Installs the CSM for Observability Helm chart Waits for the CSM for Observability pods to become ready  If the Authorization module is enabled for the CSI drivers installed in the same Kubernetes cluster, the installer will perform the current steps to enable CSM for Observability to use the same Authorization instance:\n Verifies the karavictl binary is available. Verifies the appropriate Secrets and ConfigMap exist in the CSI driver namespace. Updates the CSM for Observability deployment to use the existing Authorization instance if not already enabled during the initial installation of CSM for Observability.  Prerequisites  Helm 3.3 The deployment of one or more supported Dell CSI drivers  Online Installer The following instructions can be followed to install CSM for Observability in an environment that has an internet connection and is capable of downloading the required Helm chart and Docker images.\nDependencies A Linux-based system, with internet access, will be used to execute the script to install CSM for Observability into a Kubernetes/Openshift environment that also has internet access.\n   Dependency Usage     kubectl kubectl will be used to verify the Kubernetes/OpenShift environment   helm helm will be used to install the CSM for Observability helm chart   jq jq will be used to parse the CSM for Authorization configuration file during installation    Installer Usage [user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh --help Help for ./karavi-observability-install.sh Usage: ./karavi-observability-install.sh mode options... Mode: install Installs Karavi Observability and enables Karavi Authorization if already installed enable-authorization Updates existing installation of Karavi Observability with Karavi Authorization upgrade Upgrades existing installation of Karavi Observability to the latest release Options: Required --namespace[=]\u003cnamespace\u003e Namespace where Karavi Observability will be installed Optional --csi-powerflex-namespace[=]\u003ccsi powerflex namespace\u003e Namespace where CSI PowerFlex is installed, default is 'vxflexos' --set-file Set values from files used during helm installation (can be specified multiple times) --skip-verify Skip verification of the environment --values[=]\u003cvalues.yaml\u003e Values file, which defines configuration values --verbose Display verbose logging --version[=]\u003chelm chart version\u003e Helm chart version to install, default value will be latest --help Help Note: CSM for Authorization currently does not support the Observability module for PowerStore. Therefore setting enable-authorization is not supported in this case.\nExecuting the Installer To perform an online installation of CSM for Observability, the following steps should be performed:\n  Clone the GitHub repository:\n[user@system /home/user]# git clone https://github.com/dell/karavi-observability.git   Change to the installer directory:\n[user@system /home/user]# cd karavi-observability/installer   Execute the installation script. The following example will install CSM for Observability into the CSM namespace.\nA sample values.yaml file is located here. This can be copied into a file named myvalues.yaml and modified accordingly for the installer command below. Configuration options are outlined in the Helm chart deployment section.\nNote:\n The default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured in myvalues.yaml for CSM Observability.  [user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh install --namespace [CSM_NAMESPACE] --values myvalues.yaml --------------------------------------------------------------------------------- \u003e Installing Karavi Observability in namespace karavi on 1.21 --------------------------------------------------------------------------------- | |- Karavi Observability is not installed Success | |- Karavi Authorization will be enabled during installation | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Configure helm chart repository | |--\u003e Adding helm repository https://dell.github.io/helm-charts Success | |--\u003e Updating helm repositories Success | |- Creating namespace karavi Success | |- CSI Driver for PowerFlex is installed Success | |- Copying Secret from vxflexos to karavi Success | |- CSI Driver for PowerStore is installed Success | |- Copying Secret from powerstore to karavi Success | |- Installing CertManager CRDs Success | |- Enabling Karavi Authorization for Karavi Observability | |--\u003e Copying ConfigMap from vxflexos to karavi Success | |--\u003e Copying Karavi Authorization Secrets from vxflexos to karavi Success | |- Installing Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success   ","excerpt":"The Container Storage Modules (CSM) for Observability installer …","ref":"/csm-docs/v2/observability/deployment/online/","title":"Installer"},{"body":"The Container Storage Modules (CSM) for Observability installer bootstraps Helm to create a more simplified and robust deployment option that does the following:\n Verifies CSM for Observability is not yet installed Verifies the Kubernetes/Openshift versions are supported Verifies the Helm version is supported Adds the Dell Helm chart repository Refreshes the Helm chart repositories to download any recent changes Creates the CSM namespace (if not already created) Copies the secrets from the CSI driver namespaces into the CSM namespace (if not already copied) Installs the CertManager CRDs (if not already installed) Installs the CSM for Observability Helm chart Waits for the CSM for Observability pods to become ready  If the Authorization module is enabled for the CSI drivers installed in the same Kubernetes cluster, the installer will perform the current steps to enable CSM for Observability to use the same Authorization instance:\n Verifies the karavictl binary is available. Verifies the appropriate Secret exists in the CSI driver namespace. Queries the CSI driver environment to get references to the Authorization module sidecar-proxy Docker image and URL of the proxy server. Updates the CSM for Observability deployment to use the existing Authorization instance.  Online Installer The following instructions can be followed to install CSM for Observability in an environment that has an internet connection and is capable of downloading the required Helm chart and Docker images.\nDependencies A Linux-based system, with internet access, will be used to execute the script to install CSM for Observability into a Kubernetes/Openshift environment that also has internet access.\n   Dependency Usage     kubectl kubectl will be used to verify the Kubernetes/OpenShift environment   helm helm will be used to install the CSM for Observability helm chart   jq jq will be used to parse the CSM for Authorization configuration file during installation    Installer Usage [user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh --help Help for ./karavi-observability-install.sh Usage: ./karavi-observability-install.sh mode options... Mode: install Installs Karavi Observability and enables Karavi Authorization if already installed enable-authorization Updates existing installation of Karavi Observability with Karavi Authorization Options: Required --namespace[=]\u003cnamespace\u003e Namespace where Karavi Observability will be installed Optional --auth-image-addr Docker registry location of the Karavi Authorization sidecar proxy image --auth-proxy-host Host address of the Karavi Authorization proxy server --csi-powerflex-namespace[=]\u003ccsi powerflex namespace\u003e Namespace where CSI PowerFlex is installed, default is 'vxflexos' --set-file Set values from files used during helm installation (can be specified multiple times) --skip-verify Skip verification of the environment --values[=]\u003cvalues.yaml\u003e Values file, which defines configuration values --verbose Display verbose logging --version[=]\u003chelm chart version\u003e Helm chart version to install, default value will be latest --help Help Note: CSM for Authorization currently does not support the Observability module for PowerStore. Therefore setting enable-authorization is not supported in this case.\nExecuting the Installer To perform an online installation of CSM for Observability, the following steps should be performed:\n  Clone the GitHub repository:\n[user@system /home/user]# git clone https://github.com/dell/karavi-observability.git   Change to the installer directory:\n[user@system /home/user]# cd karavi-observability/installer   Execute the installation script. The following example will install CSM for Observability into the CSM namespace.\nA sample values.yaml file is located here. This can be copied into a file named myvalues.yaml and modified accordingly for the installer command below. Configuration options are outlined in the Helm chart deployment section.\n[user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh install --namespace [CSM_NAMESPACE] --values myvalues.yaml --------------------------------------------------------------------------------- \u003e Installing Karavi Observability in namespace karavi on 1.19 --------------------------------------------------------------------------------- | |- Karavi Observability is not installed Success | |- Karavi Authorization will be enabled during installation | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Configure helm chart repository | |--\u003e Adding helm repository https://dell.github.io/helm-charts Success | |--\u003e Updating helm repositories Success | |- Creating namespace karavi Success | |- Copying Secret from vxflexos to karavi Success | |- Installing CertManager CRDs Success | |- Installing Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success | |- Copying Secret from vxflexos to karavi Success | |- Enabling Karavi Authorization for Karavi Observability Success | |- Waiting for pods in namespace karavi to be ready Success   ","excerpt":"The Container Storage Modules (CSM) for Observability installer …","ref":"/csm-docs/v3/observability/deployment/online/","title":"Installer"},{"body":"The following instructions can be followed when a Helm chart will be installed in an environment that does not have an internet connection and will be unable to download the Helm chart and related Docker images.\nPrerequisites  Helm 3.3 The deployment of one or more supported Dell CSI drivers  Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\n One Linux-based system, with internet access, will be used to create the bundle. This involves the user invoking a script that utilizes docker to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker to restore container images from file and push them to a registry  If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker docker will be used to pull images from public image registries, tag them, and push them to a private registry.\nRequired on both the system building the offline bundle as well as the system preparing for installation. Tested version is docker 18.09    Executing the Installer To perform an offline installation of a Helm chart, the following steps should be performed:\n Build an offline bundle. Unpack the offline bundle and prepare for installation. Perform a Helm installation.  Build the Offline Bundle   Copy the offline-installer.sh script to a local Linux system using curl or wget:\n[user@anothersystem /home/user]# curl https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh --output offline-installer.sh or\n[user@anothersystem /home/user]# wget -O offline-installer.sh https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh   Set the file as executable.\n[user@anothersystem /home/user]# chmod +x offline-installer.sh   Build the bundle by providing the Helm chart name as the argument:\n[user@anothersystem /home/user]# ./offline-installer.sh -c dell/karavi-observability * * Adding Helm repository https://dell.github.io/helm-charts * * Downloading Helm chart dell/karavi-observability to directory /home/user/offline-karavi-observability-bundle/helm-original * * Downloading and saving Docker images dellemc/csm-topology:v0.3.0 dellemc/csm-metrics-powerflex:v0.3.0 otel/opentelemetry-collector:0.9.0 nginxinc/nginx-unprivileged:1.18 * * Compressing offline-karavi-observability-bundle.tar.gz   Unpack the Offline Bundle   Copy the bundle file to another Linux system that has access to the internal Docker registry and that can install the Helm chart. From that Linux system, unpack the bundle.\n[user@anothersystem /home/user]# tar -xzf offline-karavi-observability-bundle.tar.gz   Change directory into the new directory created from unpacking the bundle:\n[user@anothersystem /home/user]# cd offline-karavi-observability-bundle   Prepare the bundle by providing the internal Docker registry URL.\n[user@anothersystem /home/user/offline-karavi-observability-bundle]# ./offline-installer.sh -p \u003cmy-registry\u003e:5000 * * Loading, tagging, and pushing Docker images to registry \u003cmy-registry\u003e:5000/ dellemc/csm-topology:v0.3.0 -\u003e \u003cmy-registry\u003e:5000/csm-topology:v0.3.0 dellemc/csm-metrics-powerflex:v0.3.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerflex:v0.3.0 otel/opentelemetry-collector:0.9.0 -\u003e \u003cmy-registry\u003e:5000/opentelemetry-collector:0.9.0 nginxinc/nginx-unprivileged:1.18 -\u003e \u003cmy-registry\u003e:5000/nginx-unprivileged:1.18   Perform Helm installation   Change directory to helm which contains the updated Helm chart directory:\n[user@anothersystem /home/user/offline-karavi-observability-bundle]# cd helm   Install necessary cert-manager CustomResourceDefinitions provided:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl apply --validate=false -f cert-manager.crds.yaml   Copy the CSI Driver Secret(s)\nCopy the CSI Driver Secret from the namespace where CSI Driver is installed to the namespace where CSM for Observability is to be installed.\nCSI Driver for PowerFlex:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerFlex, perform these steps:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get configmap vxflexos-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - [user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for PowerStore\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   Now that the required images have been made available and the Helm chart’s configuration updated with references to the internal registry location, installation can proceed by following the instructions that are documented within the Helm chart’s repository.\nNote:\n Optionally, you could provide your own configurations. A sample values.yaml file is located here. The default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured.  [user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# helm install -n install-namespace app-name karavi-observability NAME: app-name LAST DEPLOYED: Fri Nov 6 08:48:13 2020 NAMESPACE: install-namespace STATUS: deployed REVISION: 1 TEST SUITE: None   ","excerpt":"The following instructions can be followed when a Helm chart will be …","ref":"/csm-docs/docs/observability/deployment/offline/","title":"Offline Installer"},{"body":"The following instructions can be followed when a Helm chart will be installed in an environment that does not have an internet connection and will be unable to download the Helm chart and related Docker images.\nPrerequisites  Helm 3.3 The deployment of one or more supported Dell CSI drivers  Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\n One Linux-based system, with internet access, will be used to create the bundle. This involves the user invoking a script that utilizes docker to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker to restore container images from file and push them to a registry  If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker docker will be used to pull images from public image registries, tag them, and push them to a private registry.\nRequired on both the system building the offline bundle as well as the system preparing for installation. Tested version is docker 18.09    Executing the Installer To perform an offline installation of a Helm chart, the following steps should be performed:\n Build an offline bundle. Unpack the offline bundle and prepare for installation. Perform a Helm installation.  Build the Offline Bundle   Copy the offline-installer.sh script to a local Linux system using curl or wget:\n[user@anothersystem /home/user]# curl https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh --output offline-installer.sh or\n[user@anothersystem /home/user]# wget -O offline-installer.sh https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh   Set the file as executable.\n[user@anothersystem /home/user]# chmod +x offline-installer.sh   Build the bundle by providing the Helm chart name as the argument:\n[user@anothersystem /home/user]# ./offline-installer.sh -c dell/karavi-observability * * Adding Helm repository https://dell.github.io/helm-charts * * Downloading Helm chart dell/karavi-observability to directory /home/user/offline-karavi-observability-bundle/helm-original * * Downloading and saving Docker images dellemc/csm-topology:v0.3.0 dellemc/csm-metrics-powerflex:v0.3.0 otel/opentelemetry-collector:0.9.0 nginxinc/nginx-unprivileged:1.18 * * Compressing offline-karavi-observability-bundle.tar.gz   Unpack the Offline Bundle   Copy the bundle file to another Linux system that has access to the internal Docker registry and that can install the Helm chart. From that Linux system, unpack the bundle.\n[user@anothersystem /home/user]# tar -xzf offline-karavi-observability-bundle.tar.gz   Change directory into the new directory created from unpacking the bundle:\n[user@anothersystem /home/user]# cd offline-karavi-observability-bundle   Prepare the bundle by providing the internal Docker registry URL.\n[user@anothersystem /home/user/offline-karavi-observability-bundle]# ./offline-installer.sh -p \u003cmy-registry\u003e:5000 * * Loading, tagging, and pushing Docker images to registry \u003cmy-registry\u003e:5000/ dellemc/csm-topology:v0.3.0 -\u003e \u003cmy-registry\u003e:5000/csm-topology:v0.3.0 dellemc/csm-metrics-powerflex:v0.3.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerflex:v0.3.0 otel/opentelemetry-collector:0.9.0 -\u003e \u003cmy-registry\u003e:5000/opentelemetry-collector:0.9.0 nginxinc/nginx-unprivileged:1.18 -\u003e \u003cmy-registry\u003e:5000/nginx-unprivileged:1.18   Perform Helm installation   Change directory to helm which contains the updated Helm chart directory:\n[user@anothersystem /home/user/offline-karavi-observability-bundle]# cd helm   Install necessary cert-manager CustomResourceDefinitions provided:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl apply --validate=false -f cert-manager.crds.yaml   Copy the CSI Driver Secret(s)\nCopy the CSI Driver Secret from the namespace where CSI Driver is installed to the namespace where CSM for Observability is to be installed.\nCSI Driver for PowerFlex:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerFlex, perform the following steps:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get configmap vxflexos-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - [user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for PowerStore\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   Now that the required images have been made available and the Helm chart’s configuration updated with references to the internal registry location, installation can proceed by following the instructions that are documented within the Helm chart’s repository.\nNote:\n Optionally, you could provide your own configurations. A sample values.yaml file is located here. The default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured.  [user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# helm install -n install-namespace app-name karavi-observability NAME: app-name LAST DEPLOYED: Fri Nov 6 08:48:13 2020 NAMESPACE: install-namespace STATUS: deployed REVISION: 1 TEST SUITE: None   ","excerpt":"The following instructions can be followed when a Helm chart will be …","ref":"/csm-docs/v1/observability/deployment/offline/","title":"Offline Installer"},{"body":"The following instructions can be followed when a Helm chart will be installed in an environment that does not have an internet connection and will be unable to download the Helm chart and related Docker images.\nPrerequisites  Helm 3.3 The deployment of one or more supported Dell CSI drivers  Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\n One Linux-based system, with internet access, will be used to create the bundle. This involves the user invoking a script that utilizes docker to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker to restore container images from file and push them to a registry  If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker docker will be used to pull images from public image registries, tag them, and push them to a private registry.\nRequired on both the system building the offline bundle as well as the system preparing for installation. Tested version is docker 18.09    Executing the Installer To perform an offline installation of a Helm chart, the following steps should be performed:\n Build an offline bundle. Unpack the offline bundle and prepare for installation. Perform a Helm installation.  Build the Offline Bundle   Copy the offline-installer.sh script to a local Linux system using curl or wget:\n[user@anothersystem /home/user]# curl https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh --output offline-installer.sh or\n[user@anothersystem /home/user]# wget -O offline-installer.sh https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh   Set the file as executable.\n[user@anothersystem /home/user]# chmod +x offline-installer.sh   Build the bundle by providing the Helm chart name as the argument:\n[user@anothersystem /home/user]# ./offline-installer.sh -c dell/karavi-observability * * Adding Helm repository https://dell.github.io/helm-charts * * Downloading Helm chart dell/karavi-observability to directory /home/user/offline-karavi-observability-bundle/helm-original * * Downloading and saving Docker images dellemc/csm-topology:v0.3.0 dellemc/csm-metrics-powerflex:v0.3.0 otel/opentelemetry-collector:0.9.0 nginxinc/nginx-unprivileged:1.18 * * Compressing offline-karavi-observability-bundle.tar.gz   Unpack the Offline Bundle   Copy the bundle file to another Linux system that has access to the internal Docker registry and that can install the Helm chart. From that Linux system, unpack the bundle.\n[user@anothersystem /home/user]# tar -xzf offline-karavi-observability-bundle.tar.gz   Change directory into the new directory created from unpacking the bundle:\n[user@anothersystem /home/user]# cd offline-karavi-observability-bundle   Prepare the bundle by providing the internal Docker registry URL.\n[user@anothersystem /home/user/offline-karavi-observability-bundle]# ./offline-installer.sh -p \u003cmy-registry\u003e:5000 * * Loading, tagging, and pushing Docker images to registry \u003cmy-registry\u003e:5000/ dellemc/csm-topology:v0.3.0 -\u003e \u003cmy-registry\u003e:5000/csm-topology:v0.3.0 dellemc/csm-metrics-powerflex:v0.3.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerflex:v0.3.0 otel/opentelemetry-collector:0.9.0 -\u003e \u003cmy-registry\u003e:5000/opentelemetry-collector:0.9.0 nginxinc/nginx-unprivileged:1.18 -\u003e \u003cmy-registry\u003e:5000/nginx-unprivileged:1.18   Perform Helm installation   Change directory to helm which contains the updated Helm chart directory:\n[user@anothersystem /home/user/offline-karavi-observability-bundle]# cd helm   Install necessary cert-manager CustomResourceDefinitions provided:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl apply --validate=false -f cert-manager.crds.yaml   Copy the CSI Driver Secret(s)\nCopy the CSI Driver Secret from the namespace where CSI Driver is installed to the namespace where CSM for Observability is to be installed.\nCSI Driver for PowerFlex:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - If CSM for Authorization is enabled for CSI PowerFlex, perform the following steps:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get configmap vxflexos-config-params -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - [user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret karavi-authorization-config proxy-server-root-certificate proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for PowerStore\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   Now that the required images have been made available and the Helm chart’s configuration updated with references to the internal registry location, installation can proceed by following the instructions that are documented within the Helm chart’s repository.\nNote:\n Optionally, you could provide your own configurations. A sample values.yaml file is located here. The default values.yaml is configured to deploy the CSM for Observability Topology service on install. If CSM for Authorization is enabled for CSI PowerFlex, the karaviMetricsPowerflex.authorization parameters must be properly configured.  [user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# helm install -n install-namespace app-name karavi-observability NAME: app-name LAST DEPLOYED: Fri Nov 6 08:48:13 2020 NAMESPACE: install-namespace STATUS: deployed REVISION: 1 TEST SUITE: None   ","excerpt":"The following instructions can be followed when a Helm chart will be …","ref":"/csm-docs/v2/observability/deployment/offline/","title":"Offline Installer"},{"body":"The following instructions can be followed when a Helm chart will be installed in an environment that does not have an internet connection and will be unable to download the Helm chart and related Docker images.\nDependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\n One Linux-based system, with internet access, will be used to create the bundle. This involves the user invoking a script that utilizes docker to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker to restore container images from file and push them to a registry  If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker docker will be used to pull images from public image registries, tag them, and push them to a private registry.\nRequired on both the system building the offline bundle as well as the system preparing for installation. Tested version is docker 18.09    Executing the Installer To perform an offline installation of a Helm chart, the following steps should be performed:\n Build an offline bundle. Unpack the offline bundle and prepare for installation. Perform a Helm installation.  Build the Offline Bundle   Copy the offline-installer.sh script to a local Linux system using curl or wget:\n[user@anothersystem /home/user]# curl https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh --output offline-installer.sh or\n[user@anothersystem /home/user]# wget -O offline-installer.sh https://raw.githubusercontent.com/dell/karavi-observability/main/installer/offline-installer.sh   Set the file as executable.\n[user@anothersystem /home/user]# chmod +x offline-installer.sh   Build the bundle by providing the Helm chart name as the argument:\n[user@anothersystem /home/user]# ./offline-installer.sh -c dell/karavi-observability * * Adding Helm repository https://dell.github.io/helm-charts * * Downloading Helm chart dell/karavi-observability to directory /home/user/offline-karavi-observability-bundle/helm-original * * Downloading and saving Docker images dellemc/csm-topology:v0.3.0 dellemc/csm-metrics-powerflex:v0.3.0 otel/opentelemetry-collector:0.9.0 nginxinc/nginx-unprivileged:1.18 * * Compressing offline-karavi-observability-bundle.tar.gz   Unpack the Offline Bundle   Copy the bundle file to another Linux system that has access to the internal Docker registry and that can install the Helm chart. From that Linux system, unpack the bundle.\n[user@anothersystem /home/user]# tar -xzf offline-karavi-observability-bundle.tar.gz   Change directory into the new directory created from unpacking the bundle:\n[user@anothersystem /home/user]# cd offline-karavi-observability-bundle   Prepare the bundle by providing the internal Docker registry URL.\n[user@anothersystem /home/user/offline-karavi-observability-bundle]# ./offline-installer.sh -p \u003cmy-registry\u003e:5000 * * Loading, tagging, and pushing Docker images to registry \u003cmy-registry\u003e:5000/ dellemc/csm-topology:v0.3.0 -\u003e \u003cmy-registry\u003e:5000/csm-topology:v0.3.0 dellemc/csm-metrics-powerflex:v0.3.0 -\u003e \u003cmy-registry\u003e:5000/csm-metrics-powerflex:v0.3.0 otel/opentelemetry-collector:0.9.0 -\u003e \u003cmy-registry\u003e:5000/opentelemetry-collector:0.9.0 nginxinc/nginx-unprivileged:1.18 -\u003e \u003cmy-registry\u003e:5000/nginx-unprivileged:1.18   Perform Helm installation   Change directory to helm which contains the updated Helm chart directory:\n[user@anothersystem /home/user/offline-karavi-observability-bundle]# cd helm   Install necessary cert-manager CustomResourceDefinitions provided:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl apply --validate=false -f cert-manager.crds.yaml   Copy the CSI Driver Secret(s)\nCopy the CSI Driver Secret from the namespace where CSI Driver is installed to the namespace where CSM for Observability is to be installed.\nCSI Driver for PowerFlex:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret vxflexos-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - CSI Driver for PowerStore\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret powerstore-config -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f -   Now that the required images have been made available and the Helm chart’s configuration updated with references to the internal registry location, installation can proceed by following the instructions that are documented within the Helm chart’s repository.\nNote: Optionally, you could provide your own configurations. A sample values.yaml file is located here.\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# helm install -n install-namespace app-name karavi-observability NAME: app-name LAST DEPLOYED: Fri Nov 6 08:48:13 2020 NAMESPACE: install-namespace STATUS: deployed REVISION: 1 TEST SUITE: None   (Optional) The following steps can be performed to enable CSM for Observability to use an existing instance of Authorization for accessing the REST API for the given storage systems.\nNote: CSM for Authorization currently does not support the Observability module for PowerStore.\nCopy the proxy Secret into the CSM for Observability namespace:\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secret proxy-authz-tokens -n [CSI_DRIVER_NAMESPACE] -o yaml | sed 's/namespace: [CSI_DRIVER_NAMESPACE]/namespace: [CSM_NAMESPACE]/' | kubectl create -f - Use karavictl to update the Observability module deployment to use the Authorization module. Required parameters are the location of the sidecar-proxy Docker image and the URL of the Authorization module proxy. If the Authorization module was installed using certificates, the flags --insecure=false and --root-certificate \u003clocation-of-root-certificate\u003e must be also be provided. If certificates were not provided during installation, the flag --insecure=true must be provided.\n[user@anothersystem /home/user/offline-karavi-observability-bundle/helm]# kubectl get secrets,deployments -n [CSM_NAMESPACE] -o yaml | karavictl inject --insecure=false --root-certificate \u003clocation-of-root-certificate\u003e --image-addr \u003csidecar-proxy-image-location\u003e --proxy-host \u003cproxy-host\u003e | kubectl apply -f -   ","excerpt":"The following instructions can be followed when a Helm chart will be …","ref":"/csm-docs/v3/observability/deployment/offline/","title":"Offline Installer"},{"body":"Users can install the Dell CSI Operator via Operatorhub.io on Kubernetes. The following outlines the process to do so:\nSteps\n Search dell in the storage category in Operatorhub.io.  Click Dell Operator.  Check the desired version is selected and click Install. Follow the provided instructions.  Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","excerpt":"Users can install the Dell CSI Operator via Operatorhub.io on …","ref":"/csm-docs/docs/csidriver/partners/operator/","title":"OperatorHub.io"},{"body":"Users can install the Dell CSI Operator via Operatorhub.io on Kubernetes. The following outlines the process to do so:\nSteps\n Search dell in the storage category in Operatorhub.io.  Click DellEMC Operator.  Check the desired version is selected and click Install. Follow the provided instructions.  Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","excerpt":"Users can install the Dell CSI Operator via Operatorhub.io on …","ref":"/csm-docs/v1/csidriver/partners/operator/","title":"OperatorHub.io"},{"body":"Users can install the Dell CSI Operator via Operatorhub.io on Kubernetes. The following outlines the process to do so:\nSteps\n Search dell in the storage category in Operatorhub.io.  Click DellEMC Operator.  Check the desired version is selected and click Install. Follow the provided instructions.  Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","excerpt":"Users can install the Dell CSI Operator via Operatorhub.io on …","ref":"/csm-docs/v2/csidriver/partners/operator/","title":"OperatorHub.io"},{"body":"Users can install the Dell CSI Operator via Operatorhub.io on Kubernetes. The following outlines the process to do so:\nSteps\n Search dell in the storage category in Operatorhub.io.  Click DellEMC Operator.  Check the desired version is selected and click Install. Follow the provided instructions.  Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","excerpt":"Users can install the Dell CSI Operator via Operatorhub.io on …","ref":"/csm-docs/v3/csidriver/partners/operator/","title":"OperatorHub.io"},{"body":"The Dell CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\n Type “Dell” in the OperatorHub section under Operators, to get the list of available Dell CSI Operators.  Check the version you want to install from the list, you can check the details by clicking it.  Once selected, click “Install” to proceed with the installation process.  You can verify the list of available operators by selecting the “Installed Operator” section.  Select the Dell CSI Operator for further details.  Install CSI Drivers via Operator Steps\n Select the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected.  After clicking the “Create CSIUnity” option in the above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters.  You can check the driver installed and node and controller pods running in the Pods section under Workloads.  ","excerpt":"The Dell CSI Drivers support Red Hat OpenShift. Please see the …","ref":"/csm-docs/docs/csidriver/partners/redhat/","title":"Red Hat OpenShift"},{"body":"The Dell CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\n Type “Dell” in the OperatorHub section under Operators, to get the list of available Dell CSI Operators.  Check the version you want to install from the list, you can check the details by clicking it.  Once selected, click “Install” to proceed with the installation process.  You can verify the list of available operators by selecting the “Installed Operator” section.  Select the Dell CSI Operator for further details.  Install CSI Drivers via Operator Steps\n Select the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected.  After clicking the “Create CSIUnity” option in the above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters.  You can check the driver installed and node and controller pods running in the Pods section under Workloads.  ","excerpt":"The Dell CSI Drivers support Red Hat OpenShift. Please see the …","ref":"/csm-docs/v1/csidriver/partners/redhat/","title":"Red Hat OpenShift"},{"body":"The Dell CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\n Type “Dell” in the OperatorHub section under Operators, to get the list of available Dell CSI Operators.  Check the version you want to install from the list, you can check the details by clicking it.  Once selected, click “Install” to proceed with the installation process.  You can verify the list of available operators by selecting the “Installed Operator” section.  Select the Dell CSI Operator for further details.  Install CSI Drivers via Operator Steps\n Select the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected.  After clicking the “Create CSIUnity” option in the above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters.  You can check the driver installed and node and controller pods running in the Pods section under Workloads.  ","excerpt":"The Dell CSI Drivers support Red Hat OpenShift. Please see the …","ref":"/csm-docs/v2/csidriver/partners/redhat/","title":"Red Hat OpenShift"},{"body":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\n Type “Dell” in the OperatorHub section under Operators, to get the list of available Dell CSI Operators.  Check the version you want to install from the list, you can check the details by clicking it.  Once selected, click “Install” to proceed with the installation process.  You can verify the list of available operators by selecting the “Installed Operator” section.  Select the Dell CSI Operator for further details.  Install CSI Drivers via Operator Steps\n Select the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected.  After clicking the “Create CSIUnity” option in the above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters.  You can check the driver installed and node and controller pods running in the Pods section under Workloads.  ","excerpt":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the …","ref":"/csm-docs/v3/csidriver/partners/redhat/","title":"Red Hat OpenShift"},{"body":"Frequently Asked Questions  Why does the installation fail due to an invalid cipherKey value? Why does the cluster-init pod show the error “cluster has already been initialized”? Why does the precheck fail when creating an application? How can I view detailed logs for the CSM Installer? After deleting an application, why can’t I re-create the same application? How can I upgrade CSM if I’ve used the CSM Installer to deploy CSM 1.0?  Why does the installation fail due to an invalid cipherKey value? The cipherKey value used during deployment of the CSM Installer must be exactly 32 characters in length and contained within quotes.\nWhy does the cluster-init pod show the error “cluster has already been initialized”? During the initial start-up of the CSM Installer, the database will be initialized by the cluster-init job. If the CSM Installer is uninstalled and then re-installed on the same cluster, this error may be shown due to the Persistent Volume for the database already containing an initialized database. The CSM Installer will function as normal and the cluster-init job can be ignored.\nIf a clean installation of the CSM Installer is required, the dbVolumeDirectory (default location /var/lib/cockroachdb) must be deleted from the worker node which is hosting the Persistent Volume. After this directory is deleted, the CSM Installer can be re-installed.\nCaution: Deleting the dbVolumeDirectory location will remove any data persisted by the CSM Installer including clusters, storage systems, and installed applications.\nWhy does the precheck fail when creating an application? Each CSI Driver and CSM Module has required software or CRDs that must be installed before the application can be deployed in the cluster. These prechecks are verified when the csm create application command is executed. If the error message “create application failed” is displayed, review the CSM Installer logs to view details about the failed prechecks.\nIf the precheck fails due to required software (e.g. iSCSI, NFS, SDC) not installed on the cluster nodes, follow these steps to address the issue:\n Delete the cluster from the CSM Installer using the csm delete cluster command. Update the nodes in the cluster by installing required software. Add the cluster to the CSM Installer using the csm add cluster command.  How can I view detailed logs for the CSM Installer? Detailed logs of the CSM Installer can be displayed using the following command:\nkubectl logs -f -n \u003cnamespace\u003e deploy/dell-csm-installer After deleting an application, why can’t I re-create the same application? After deleting an application using the csm delete application command, the namespace and other non-application resources including Secrets are not deleted from the cluster. This is to prevent removing any resources that may not have been created by the CSM Installer. The namespace must be manually deleted before attempting to re-create the same application using the CSM Installer.\nHow can I upgrade CSM if I’ve used the CSM Installer to deploy CSM 1.0? The CSM Installer currently does not support upgrade. If you used the CSM Installer to deploy CSM 1.0 you will need to perform the following steps to upgrade:\n Using the CSM installer, delete any driver/module applications that were installed (ex: csm delete application --name \u003ccreated-application-name\u003e). Uninstall the CSM Installer (ex: helm delete -n   ) Follow the deployment instructions here to redeploy the CSI driver and modules.  ","excerpt":"Frequently Asked Questions  Why does the installation fail due to an …","ref":"/csm-docs/docs/deployment/csminstaller/troubleshooting/","title":"Troubleshooting"},{"body":" Can CSM Operator manage existing drivers installed using Helm charts or the Dell CSI Operator? Why does some of the Custom Resource fields show up as invalid or unsupported in the OperatorHub GUI? How can I view detailed logs for the CSM Operator? My Dell CSI Driver install failed. How do I fix it?  Can CSM Operator manage existing drivers installed using Helm charts or the Dell CSI Operator? The Dell CSM Operator is unable to manage any existing driver installed using Helm charts or the Dell CSI Operator. If you already have installed one of the Dell CSI driver in your cluster and want to use the CSM operator based deployment, uninstall the driver and then redeploy the driver via Dell CSM Operator\nWhy does some of the Custom Resource fields show up as invalid or unsupported in the OperatorHub GUI? The Dell CSM Operator is not fully compliant with the OperatorHub React UI elements.Due to this, some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSM Operator.\nHow can I view detailed logs for the CSM Operator? Detailed logs of the CSM Operator can be displayed using the following command:\nkubectl logs \u003ccsm-operator-controller-podname\u003e -n \u003cnamespace\u003e My Dell CSI Driver install failed. How do I fix it? Describe the current state by issuing: kubectl describe csm \u003ccustom-resource-name\u003e -n \u003cnamespace\u003e\nIn the output refer to the status and events section. If status shows pods that are in the failed state, refer to the CSI Driver Troubleshooting guide.\nExample:\nStatus: Controller Status: Available: 0 Desired: 2 Failed: 2 Node Status: Available: 0 Desired: 2 Failed: 2 State: Failed Events Warning Updated 67s (x15 over 2m4s) csm (combined from similar events): at 1646848059520359167 Pod error details ControllerError: ErrImagePull= pull access denied for dellem/csi-isilon, repository does not exist or may require 'docker login': denied: requested access to the resource is denied, Daemonseterror: ErrImagePull= pull access denied for dellem/csi-isilon, repository does not exist or may require 'docker login': denied: requested access to the resource is denied The above event shows dellem/csi-isilon does not exist, to resolve this user can kubectl edit the csm and update to correct image.\nTo get details of driver installation: kubectl logs \u003cdell-csm-operator-controller-manager-pod\u003e -n dell-csm-operator.\nTypical reasons for errors:\n Incorrect driver version Incorrect driver type Incorrect driver Spec env, args for containers Incorrect RBAC permissions  ","excerpt":" Can CSM Operator manage existing drivers installed using Helm charts …","ref":"/csm-docs/docs/deployment/csmoperator/troubleshooting/","title":"Troubleshooting"},{"body":"Frequently Asked Questions  Why does the installation fail due to an invalid cipherKey value? Why does the cluster-init pod show the error “cluster has already been initialized”? Why does the precheck fail when creating an application? How can I view detailed logs for the CSM Installer? After deleting an application, why can’t I re-create the same application? How can I upgrade CSM if I’ve used the CSM Installer to deploy CSM 1.0?  Why does the installation fail due to an invalid cipherKey value? The cipherKey value used during deployment of the CSM Installer must be exactly 32 characters in length and contained within quotes.\nWhy does the cluster-init pod show the error “cluster has already been initialized”? During the initial start-up of the CSM Installer, the database will be initialized by the cluster-init job. If the CSM Installer is uninstalled and then re-installed on the same cluster, this error may be shown due to the Persistent Volume for the database already containing an initialized database. The CSM Installer will function as normal and the cluster-init job can be ignored.\nIf a clean installation of the CSM Installer is required, the dbVolumeDirectory (default location /var/lib/cockroachdb) must be deleted from the worker node which is hosting the Persistent Volume. After this directory is deleted, the CSM Installer can be re-installed.\nCaution: Deleting the dbVolumeDirectory location will remove any data persisted by the CSM Installer including clusters, storage systems, and installed applications.\nWhy does the precheck fail when creating an application? Each CSI Driver and CSM Module has required software or CRDs that must be installed before the application can be deployed in the cluster. These prechecks are verified when the csm create application command is executed. If the error message “create application failed” is displayed, review the CSM Installer logs to view details about the failed prechecks.\nIf the precheck fails due to required software (e.g. iSCSI, NFS, SDC) not installed on the cluster nodes, follow these steps to address the issue:\n Delete the cluster from the CSM Installer using the csm delete cluster command. Update the nodes in the cluster by installing required software. Add the cluster to the CSM Installer using the csm add cluster command.  How can I view detailed logs for the CSM Installer? Detailed logs of the CSM Installer can be displayed using the following command:\nkubectl logs -f -n \u003cnamespace\u003e deploy/dell-csm-installer After deleting an application, why can’t I re-create the same application? After deleting an application using the csm delete application command, the namespace and other non-application resources including Secrets are not deleted from the cluster. This is to prevent removing any resources that may not have been created by the CSM Installer. The namespace must be manually deleted before attempting to re-create the same application using the CSM Installer.\nHow can I upgrade CSM if I’ve used the CSM Installer to deploy CSM 1.0? The CSM Installer currently does not support upgrade. If you used the CSM Installer to deploy CSM 1.0 you will need to perform the following steps to upgrade:\n Using the CSM installer, delete any driver/module applications that were installed (ex: csm delete application --name \u003ccreated-application-name\u003e). Uninstall the CSM Installer (ex: helm delete -n   ) Follow the deployment instructions here to redeploy the CSI driver and modules.  ","excerpt":"Frequently Asked Questions  Why does the installation fail due to an …","ref":"/csm-docs/v1/deployment/csminstaller/troubleshooting/","title":"Troubleshooting"},{"body":" Can CSM Operator manage existing drivers installed using Helm charts or the Dell CSI Operator? Why does some of the Custom Resource fields show up as invalid or unsupported in the OperatorHub GUI? How can I view detailed logs for the CSM Operator? My Dell CSI Driver install failed. How do I fix it?  Can CSM Operator manage existing drivers installed using Helm charts or the Dell CSI Operator? The Dell CSM Operator is unable to manage any existing driver installed using Helm charts or the Dell CSI Operator. If you already have installed one of the Dell CSI driver in your cluster and want to use the CSM operator based deployment, uninstall the driver and then redeploy the driver via Dell CSM Operator\nWhy does some of the Custom Resource fields show up as invalid or unsupported in the OperatorHub GUI? The Dell CSM Operator is not fully compliant with the OperatorHub React UI elements.Due to this, some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSM Operator.\nHow can I view detailed logs for the CSM Operator? Detailed logs of the CSM Operator can be displayed using the following command:\nkubectl logs \u003ccsm-operator-controller-podname\u003e -n \u003cnamespace\u003e My Dell CSI Driver install failed. How do I fix it? Describe the current state by issuing: kubectl describe csm \u003ccustom-resource-name\u003e -n \u003cnamespace\u003e\nIn the output refer to the status and events section. If status shows pods that are in the failed state, refer to the CSI Driver Troubleshooting guide.\nExample:\nStatus: Controller Status: Available: 0 Desired: 2 Failed: 2 Node Status: Available: 0 Desired: 2 Failed: 2 State: Failed Events Warning Updated 67s (x15 over 2m4s) csm (combined from similar events): at 1646848059520359167 Pod error details ControllerError: ErrImagePull= pull access denied for dellem/csi-isilon, repository does not exist or may require 'docker login': denied: requested access to the resource is denied, Daemonseterror: ErrImagePull= pull access denied for dellem/csi-isilon, repository does not exist or may require 'docker login': denied: requested access to the resource is denied The above event shows dellem/csi-isilon does not exist, to resolve this user can kubectl edit the csm and update to correct image.\nTo get details of driver installation: kubectl logs \u003cdell-csm-operator-controller-manager-pod\u003e -n dell-csm-operator.\nTypical reasons for errors:\n Incorrect driver version Incorrect driver type Incorrect driver Spec env, args for containers Incorrect RBAC permissions  ","excerpt":" Can CSM Operator manage existing drivers installed using Helm charts …","ref":"/csm-docs/v1/deployment/csmoperator/troubleshooting/","title":"Troubleshooting"},{"body":"Frequently Asked Questions  Why does the installation fail due to an invalid cipherKey value? Why does the cluster-init pod show the error “cluster has already been initialized”? Why does the precheck fail when creating an application? How can I view detailed logs for the CSM Installer? After deleting an application, why can’t I re-create the same application? How can I upgrade CSM if I’ve used the CSM Installer to deploy CSM 1.0?  Why does the installation fail due to an invalid cipherKey value? The cipherKey value used during deployment of the CSM Installer must be exactly 32 characters in length and contained within quotes.\nWhy does the cluster-init pod show the error “cluster has already been initialized”? During the initial start-up of the CSM Installer, the database will be initialized by the cluster-init job. If the CSM Installer is uninstalled and then re-installed on the same cluster, this error may be shown due to the Persistent Volume for the database already containing an initialized database. The CSM Installer will function as normal and the cluster-init job can be ignored.\nIf a clean installation of the CSM Installer is required, the dbVolumeDirectory (default location /var/lib/cockroachdb) must be deleted from the worker node which is hosting the Persistent Volume. After this directory is deleted, the CSM Installer can be re-installed.\nCaution: Deleting the dbVolumeDirectory location will remove any data persisted by the CSM Installer including clusters, storage systems, and installed applications.\nWhy does the precheck fail when creating an application? Each CSI Driver and CSM Module has required software or CRDs that must be installed before the application can be deployed in the cluster. These prechecks are verified when the csm create application command is executed. If the error message “create application failed” is displayed, review the CSM Installer logs to view details about the failed prechecks.\nIf the precheck fails due to required software (e.g. iSCSI, NFS, SDC) not installed on the cluster nodes, follow these steps to address the issue:\n Delete the cluster from the CSM Installer using the csm delete cluster command. Update the nodes in the cluster by installing required software. Add the cluster to the CSM Installer using the csm add cluster command.  How can I view detailed logs for the CSM Installer? Detailed logs of the CSM Installer can be displayed using the following command:\nkubectl logs -f -n \u003cnamespace\u003e deploy/dell-csm-installer After deleting an application, why can’t I re-create the same application? After deleting an application using the csm delete application command, the namespace and other non-application resources including Secrets are not deleted from the cluster. This is to prevent removing any resources that may not have been created by the CSM Installer. The namespace must be manually deleted before attempting to re-create the same application using the CSM Installer.\nHow can I upgrade CSM if I’ve used the CSM Installer to deploy CSM 1.0? The CSM Installer currently does not support upgrade. If you used the CSM Installer to deploy CSM 1.0 you will need to perform the following steps to upgrade:\n Using the CSM installer, delete any driver/module applications that were installed (ex: csm delete application --name \u003ccreated-application-name\u003e). Uninstall the CSM Installer (ex: helm delete -n   ) Follow the deployment instructions here to redeploy the CSI driver and modules.  ","excerpt":"Frequently Asked Questions  Why does the installation fail due to an …","ref":"/csm-docs/v2/deployment/csminstaller/troubleshooting/","title":"Troubleshooting"},{"body":" Can CSM Operator manage existing drivers installed using Helm charts or the Dell CSI Operator? Why does some of the Custom Resource fields show up as invalid or unsupported in the OperatorHub GUI? How can I view detailed logs for the CSM Operator? My Dell CSI Driver install failed. How do I fix it?  Can CSM Operator manage existing drivers installed using Helm charts or the Dell CSI Operator? The Dell CSM Operator is unable to manage any existing driver installed using Helm charts or the Dell CSI Operator. If you already have installed one of the Dell CSI driver in your cluster and want to use the CSM operator based deployment, uninstall the driver and then redeploy the driver via Dell CSM Operator\nWhy does some of the Custom Resource fields show up as invalid or unsupported in the OperatorHub GUI? The Dell CSM Operator is not fully compliant with the OperatorHub React UI elements.Due to this, some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSM Operator.\nHow can I view detailed logs for the CSM Operator? Detailed logs of the CSM Operator can be displayed using the following command:\nkubectl logs \u003ccsm-operator-controller-podname\u003e -n \u003cnamespace\u003e My Dell CSI Driver install failed. How do I fix it? Describe the current state by issuing: kubectl describe csm \u003ccustom-resource-name\u003e -n \u003cnamespace\u003e\nIn the output refer to the status and events section. If status shows pods that are in the failed state, refer to the CSI Driver Troubleshooting guide.\nExample:\nStatus: Controller Status: Available: 0 Desired: 2 Failed: 2 Node Status: Available: 0 Desired: 2 Failed: 2 State: Failed Events Warning Updated 67s (x15 over 2m4s) csm (combined from similar events): at 1646848059520359167 Pod error details ControllerError: ErrImagePull= pull access denied for dellem/csi-isilon, repository does not exist or may require 'docker login': denied: requested access to the resource is denied, Daemonseterror: ErrImagePull= pull access denied for dellem/csi-isilon, repository does not exist or may require 'docker login': denied: requested access to the resource is denied The above event shows dellem/csi-isilon does not exist, to resolve this user can kubectl edit the csm and update to correct image.\nTo get details of driver installation: kubectl logs \u003cdell-csm-operator-controller-manager-pod\u003e -n dell-csm-operator.\nTypical reasons for errors:\n Incorrect driver version Incorrect driver type Incorrect driver Spec env, args for containers Incorrect RBAC permissions  ","excerpt":" Can CSM Operator manage existing drivers installed using Helm charts …","ref":"/csm-docs/v2/deployment/csmoperator/troubleshooting/","title":"Troubleshooting"},{"body":"Frequently Asked Questions  Why does the installation fail due to an invalid cipherKey value? Why does the cluster-init pod show the error “cluster has already been initialized”? Why does the precheck fail when creating an application? How can I view detailed logs for the CSM Installer? After deleting an application, why can’t I re-create the same application? How can I upgrade CSM if I’ve used the CSM Installer to deploy CSM 1.0?  Why does the installation fail due to an invalid cipherKey value? The cipherKey value used during deployment of the CSM Installer must be exactly 32 characters in length and contained within quotes.\nWhy does the cluster-init pod show the error “cluster has already been initialized”? During the initial start-up of the CSM Installer, the database will be initialized by the cluster-init job. If the CSM Installer is uninstalled and then re-installed on the same cluster, this error may be shown due to the Persistent Volume for the database already containing an initialized database. The CSM Installer will function as normal and the cluster-init job can be ignored.\nIf a clean installation of the CSM Installer is required, the dbVolumeDirectory (default location /var/lib/cockroachdb) must be deleted from the worker node which is hosting the Persistent Volume. After this directory is deleted, the CSM Installer can be re-installed.\nCaution: Deleting the dbVolumeDirectory location will remove any data persisted by the CSM Installer including clusters, storage systems, and installed applications.\nWhy does the precheck fail when creating an application? Each CSI Driver and CSM Module has required software or CRDs that must be installed before the application can be deployed in the cluster. These prechecks are verified when the csm create application command is executed. If the error message “create application failed” is displayed, review the CSM Installer logs to view details about the failed prechecks.\nIf the precheck fails due to required software (e.g. iSCSI, NFS, SDC) not installed on the cluster nodes, follow these steps to address the issue:\n Delete the cluster from the CSM Installer using the csm delete cluster command. Update the nodes in the cluster by installing required software. Add the cluster to the CSM Installer using the csm add cluster command.  How can I view detailed logs for the CSM Installer? Detailed logs of the CSM Installer can be displayed using the following command:\nkubectl logs -f -n \u003cnamespace\u003e deploy/dell-csm-installer After deleting an application, why can’t I re-create the same application? After deleting an application using the csm delete application command, the namespace and other non-application resources including Secrets are not deleted from the cluster. This is to prevent removing any resources that may not have been created by the CSM Installer. The namespace must be manually deleted before attempting to re-create the same application using the CSM Installer.\nHow can I upgrade CSM if I’ve used the CSM Installer to deploy CSM 1.0? The CSM Installer currently does not support upgrade. If you used the CSM Installer to deploy CSM 1.0 you will need to perform the following steps to upgrade:\n Using the CSM installer, delete any driver/module applications that were installed (ex: csm delete application --name \u003ccreated-application-name\u003e). Uninstall the CSM Installer (ex: helm delete -n   ) Follow the deployment instructions here to redeploy the CSI driver and modules.  ","excerpt":"Frequently Asked Questions  Why does the installation fail due to an …","ref":"/csm-docs/v3/deployment/csminstaller/troubleshooting/","title":"Troubleshooting"},{"body":"Uninstall a CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the driver:\n./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e For usage information:\n[dell-csi-helm-installer]# ./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed by the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall the driver installed via the operator, delete the Custom Resource(CR)\n# Replace driver-type, driver-name and driver-namespace with their respective values kubectl delete \u003cdriver-type\u003e/\u003cdriver-name\u003e -n \u003cdriver-namespace\u003e ","excerpt":"Uninstall a CSI driver installed via Helm To uninstall a driver, the …","ref":"/csm-docs/docs/csidriver/uninstall/","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Observability.\nUninstall the CSM for Observability Helm Chart The command below removes all the Kubernetes components associated with the chart.\n$ helm delete karavi-observability --namespace [CSM_NAMESPACE] You may also want to uninstall the CRDs created for cert-manager.\n$ kubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.crds.yaml ","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/docs/observability/uninstall/","title":"Uninstallation"},{"body":"Uninstall a CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the driver:\n./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e For usage information:\n[dell-csi-helm-installer]# ./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed by the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall the driver installed via the operator, delete the Custom Resource(CR)\n# Replace driver-type, driver-name and driver-namespace with their respective values kubectl delete \u003cdriver-type\u003e/\u003cdriver-name\u003e -n \u003cdriver-namespace\u003e ","excerpt":"Uninstall a CSI driver installed via Helm To uninstall a driver, the …","ref":"/csm-docs/v1/csidriver/uninstall/","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Observability.\nUninstall the CSM for Observability Helm Chart The command below removes all the Kubernetes components associated with the chart.\n$ helm delete karavi-observability --namespace [CSM_NAMESPACE] You may also want to uninstall the CRDs created for cert-manager.\n$ kubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.crds.yaml ","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/v1/observability/uninstall/","title":"Uninstallation"},{"body":"Uninstall a CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the driver:\n./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e For usage information:\n[dell-csi-helm-installer]# ./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed by the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall the driver installed via the operator, delete the Custom Resource(CR)\n# Replace driver-type, driver-name and driver-namespace with their respective values kubectl delete \u003cdriver-type\u003e/\u003cdriver-name\u003e -n \u003cdriver-namespace\u003e ","excerpt":"Uninstall a CSI driver installed via Helm To uninstall a driver, the …","ref":"/csm-docs/v2/csidriver/uninstall/","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Observability.\nUninstall the CSM for Observability Helm Chart The command below removes all the Kubernetes components associated with the chart.\n$ helm delete karavi-observability --namespace [CSM_NAMESPACE] You may also want to uninstall the CRDs created for cert-manager.\n$ kubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.crds.yaml ","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/v2/observability/uninstall/","title":"Uninstallation"},{"body":"Uninstall a CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the driver:\n./csi-uninstall.sh --namespace \u003cdriver-namespace\u003e For usage information:\n[dell-csi-helm-installer]# ./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u003cnamespace\u003e Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u003chelm release\u003e Name to register with helm, default value will match the driver name -h Help Uninstall a CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed by the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall the driver installed via the operator, delete the Custom Resource(CR)\n# Replace driver-type, driver-name and driver-namespace with their respective values kubectl delete \u003cdriver-type\u003e/\u003cdriver-name\u003e -n \u003cdriver-namespace\u003e ","excerpt":"Uninstall a CSI driver installed via Helm To uninstall a driver, the …","ref":"/csm-docs/v3/csidriver/uninstall/","title":"Uninstallation"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Observability.\nUninstall the CSM for Observability Helm Chart The command below removes all the Kubernetes components associated with the chart.\n$ helm delete karavi-observability --namespace [CSM_NAMESPACE] You may also want to uninstall the CRDs created for cert-manager.\n$ kubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.5.3/cert-manager.crds.yaml ","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/v3/observability/uninstall/","title":"Uninstallation"},{"body":"This section outlines the upgrade steps for Container Storage Modules (CSM) for Authorization. The upgrade of CSM for Authorization is handled in 2 parts:\n Upgrading the CSM for Authorization proxy server Upgrading the Dell CSI drivers with CSM for Authorization enabled  Upgrading CSM for Authorization proxy server Obtain the latest single binary installer RPM by following one of our two options here.\nTo update the rpm package on the system, run the below command:\nrpm -Uvh karavi-authorization-\u003cnew_version\u003e.x86_64.rpm --nopreun --nopostun To verify that the new version of the rpm is installed and K3s has been updated, run the below commands:\nrpm -qa | grep karavi k3s kubectl version  Note: The above steps manage install and upgrade of all dependencies that are required by the CSM for Authorization proxy server.\n Upgrading Dell CSI Driver(s) with CSM for Authorization enabled Given a setup where the CSM for Authorization proxy server is already upgraded to the latest version, follow the upgrade instructions for the applicable CSI Driver(s) to upgrade the driver and the CSM for Authorization sidecar\n Upgrade PowerFlex CSI driver Upgrade PowerMax CSI driver Upgrade PowerScale CSI driver  Rollback This section outlines the rollback steps for Container Storage Modules (CSM) for Authorization.\nRollback CSM for Authorization proxy server To rollback the rpm package on the system, run the below command:\nrpm -Uvh --oldpackage karavi-authorization-\u003cold_version\u003e.x86_64.rpm --nopreun --nopostun ","excerpt":"This section outlines the upgrade steps for Container Storage Modules …","ref":"/csm-docs/docs/authorization/upgrade/","title":"Upgrade"},{"body":"","excerpt":"","ref":"/csm-docs/docs/csidriver/upgradation/","title":"Upgrade"},{"body":"This section outlines the upgrade steps for Container Storage Modules (CSM) for Observability. CSM for Observability upgrade can be achieved in one of two ways:\n Helm Chart Upgrade Online Installer Upgrade  Helm Chart Upgrade CSM for Observability Helm upgrade supports Helm, Online Installer, and Offline Installer deployments.\nTo upgrade an existing Helm installation of CSM for Observability to the latest release, download the latest Helm charts.\nhelm repo update Check if the latest Helm chart version is available:\nhelm search repo dell NAME CHART VERSION APP VERSION DESCRIPTION dell/karavi-observability 1.0.1 1.0.0 CSM for Observability is part of the [Container...  Note: If using cert-manager CustomResourceDefinitions older than v1.5.3, delete the old CRDs and install v1.5.3 of the CRDs prior to upgrade. See Prerequisites for location of CRDs.\n Upgrade to the latest CSM for Observability release:\nUpgrade Helm and Online Installer deployments: $ helm upgrade --version $latest_chart_version --values values.yaml karavi-observability dell/karavi-observability -n $namespace Upgrade Offline Installer deployment: $ helm upgrade --version $latest_chart_version karavi-observability dell/karavi-observability -n $namespace The configuration section lists all the parameters that can be configured using the values.yaml file.\nOnline Installer Upgrade CSM for Observability online installer upgrade can be used if the initial deployment was performed using the Online Installer or Helm.\n  Change to the installer directory:\n[user@system /home/user]# cd karavi-observability/installer   Update values.yaml file as needed. Configuration options are outlined in the Helm chart deployment section.\n  Execute the ./karavi-observability-install.sh script:\n[user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh upgrade --namespace $namespace --values myvalues.yaml --version $latest_chart_version --------------------------------------------------------------------------------- \u003e Upgrading Karavi Observability in namespace karavi on 1.21 --------------------------------------------------------------------------------- | |- Karavi Observability is installed. Upgrade can continue Success | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Upgrading CertManager CRDs Success | |- Updating helm repositories Success | |- Upgrading Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success   ","excerpt":"This section outlines the upgrade steps for Container Storage Modules …","ref":"/csm-docs/docs/observability/upgrade/","title":"Upgrade"},{"body":"CSM for Resiliency can be upgraded as part of the Dell CSI driver upgrade process. The drivers can be upgraded either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart upgrade is supported for CSM for Resiliency.\nFor information on the PowerFlex CSI driver upgrade process, see PowerFlex CSI Driver.\nFor information on the Unity XT CSI driver upgrade process, see Unity XT CSI Driver.\nFor information on the PowerScale CSI driver upgrade process, see PowerScale CSI Driver.\nHelm Chart Upgrade To upgrade CSM for Resiliency with the driver, the following steps are required.\n Note: These steps refer to the values file and csi-install.sh script that were used during initial installation of the Dell CSI driver.\n Steps\n Update the podmon.image value in the values files to reference the new podmon image. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  ","excerpt":"CSM for Resiliency can be upgraded as part of the Dell CSI driver …","ref":"/csm-docs/docs/resiliency/upgrade/","title":"Upgrade"},{"body":"This section outlines the upgrade steps for Container Storage Modules (CSM) for Authorization. The upgrade of CSM for Authorization is handled in 2 parts:\n Upgrading the CSM for Authorization proxy server Upgrading the Dell CSI drivers with CSM for Authorization enabled  Upgrading CSM for Authorization proxy server Obtain the latest single binary installer RPM by following one of our two options here.\nTo update the rpm package on the system, run the below command:\nrpm -Uvh karavi-authorization-\u003cnew_version\u003e.x86_64.rpm --nopreun --nopostun To verify that the new version of the rpm is installed and K3s has been updated, run the below commands:\nrpm -qa | grep karavi k3s kubectl version  Note: The above steps manage install and upgrade of all dependencies that are required by the CSM for Authorization proxy server.\n Upgrading Dell CSI Driver(s) with CSM for Authorization enabled Given a setup where the CSM for Authorization proxy server is already upgraded to the latest version, follow the upgrade instructions for the applicable CSI Driver(s) to upgrade the driver and the CSM for Authorization sidecar\n Upgrade PowerFlex CSI driver Upgrade PowerMax CSI driver Upgrade PowerScale CSI driver  Rollback This section outlines the rollback steps for Container Storage Modules (CSM) for Authorization.\nRollback CSM for Authorization proxy server To rollback the rpm package on the system, run the below command:\nrpm -Uvh --oldpackage karavi-authorization-\u003cold_version\u003e.x86_64.rpm --nopreun --nopostun ","excerpt":"This section outlines the upgrade steps for Container Storage Modules …","ref":"/csm-docs/v1/authorization/upgrade/","title":"Upgrade"},{"body":"","excerpt":"","ref":"/csm-docs/v1/csidriver/upgradation/","title":"Upgrade"},{"body":"This section outlines the upgrade steps for Container Storage Modules (CSM) for Observability. CSM for Observability upgrade can be achieved in one of two ways:\n Helm Chart Upgrade Online Installer Upgrade  Helm Chart Upgrade CSM for Observability Helm upgrade supports Helm, Online Installer, and Offline Installer deployments.\nTo upgrade an existing Helm installation of CSM for Observability to the latest release, download the latest Helm charts.\nhelm repo update Check if the latest Helm chart version is available:\nhelm search repo dell NAME CHART VERSION APP VERSION DESCRIPTION dell/karavi-observability 1.0.1 1.0.0 CSM for Observability is part of the [Container...  Note: If using cert-manager CustomResourceDefinitions older than v1.5.3, delete the old CRDs and install v1.5.3 of the CRDs prior to upgrade. See Prerequisites for location of CRDs.\n Upgrade to the latest CSM for Observability release:\nUpgrade Helm and Online Installer deployments: $ helm upgrade --version $latest_chart_version --values values.yaml karavi-observability dell/karavi-observability -n $namespace Upgrade Offline Installer deployment: $ helm upgrade --version $latest_chart_version karavi-observability dell/karavi-observability -n $namespace The configuration section lists all the parameters that can be configured using the values.yaml file.\nOnline Installer Upgrade CSM for Observability online installer upgrade can be used if the initial deployment was performed using the Online Installer or Helm.\n  Change to the installer directory:\n[user@system /home/user]# cd karavi-observability/installer   Update values.yaml file as needed. Configuration options are outlined in the Helm chart deployment section.\n  Execute the ./karavi-observability-install.sh script:\n[user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh upgrade --namespace $namespace --values myvalues.yaml --version $latest_chart_version --------------------------------------------------------------------------------- \u003e Upgrading Karavi Observability in namespace karavi on 1.21 --------------------------------------------------------------------------------- | |- Karavi Observability is installed. Upgrade can continue Success | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Upgrading CertManager CRDs Success | |- Updating helm repositories Success | |- Upgrading Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success   ","excerpt":"This section outlines the upgrade steps for Container Storage Modules …","ref":"/csm-docs/v1/observability/upgrade/","title":"Upgrade"},{"body":"CSM for Resiliency can be upgraded as part of the Dell CSI driver upgrade process. The drivers can be upgraded either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart upgrade is supported for CSM for Resiliency.\nFor information on the PowerFlex CSI driver upgrade process, see PowerFlex CSI Driver.\nFor information on the Unity CSI driver upgrade process, see Unity CSI Driver.\nHelm Chart Upgrade To upgrade CSM for Resiliency with the driver, the following steps are required.\n Note: These steps refer to the values file and csi-install.sh script that were used during initial installation of the Dell CSI driver.\n Steps\n Update the podmon.image value in the values files to reference the new podmon image. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  ","excerpt":"CSM for Resiliency can be upgraded as part of the Dell CSI driver …","ref":"/csm-docs/v1/resiliency/upgrade/","title":"Upgrade"},{"body":"This section outlines the upgrade steps for Container Storage Modules (CSM) for Authorization. The upgrade of CSM for Authorization is handled in 2 parts:\n Upgrading the CSM for Authorization proxy server Upgrading the Dell CSI drivers with CSM for Authorization enabled  Upgrading CSM for Authorization proxy server Obtain the latest single binary installer RPM by following one of our two options here.\nTo update the rpm package on the system, run the below command:\nrpm -Uvh karavi-authorization-\u003cnew_version\u003e.x86_64.rpm --nopreun --nopostun To verify that the new version of the rpm is installed and K3s has been updated, run the below commands:\nrpm -qa | grep karavi k3s kubectl version  Note: The above steps manage install and upgrade of all dependencies that are required by the CSM for Authorization proxy server.\n Upgrading Dell CSI Driver(s) with CSM for Authorization enabled Given a setup where the CSM for Authorization proxy server is already upgraded to the latest version, follow the upgrade instructions for the applicable CSI Driver(s) to upgrade the driver and the CSM for Authorization sidecar\n Upgrade PowerFlex CSI driver Upgrade PowerMax CSI driver Upgrade PowerScale CSI driver  Rollback This section outlines the rollback steps for Container Storage Modules (CSM) for Authorization.\nRollback CSM for Authorization proxy server To rollback the rpm package on the system, run the below command:\nrpm -Uvh --oldpackage karavi-authorization-\u003cold_version\u003e.x86_64.rpm --nopreun --nopostun ","excerpt":"This section outlines the upgrade steps for Container Storage Modules …","ref":"/csm-docs/v2/authorization/upgrade/","title":"Upgrade"},{"body":"","excerpt":"","ref":"/csm-docs/v2/csidriver/upgradation/","title":"Upgrade"},{"body":"This section outlines the upgrade steps for Container Storage Modules (CSM) for Observability. CSM for Observability upgrade can be achieved in one of two ways:\n Helm Chart Upgrade Online Installer Upgrade  Helm Chart Upgrade CSM for Observability Helm upgrade supports Helm, Online Installer, and Offline Installer deployments.\nTo upgrade an existing Helm installation of CSM for Observability to the latest release, download the latest Helm charts.\nhelm repo update Check if the latest Helm chart version is available:\nhelm search repo dell NAME CHART VERSION APP VERSION DESCRIPTION dell/karavi-observability 1.0.1 1.0.0 CSM for Observability is part of the [Container...  Note: If using cert-manager CustomResourceDefinitions older than v1.5.3, delete the old CRDs and install v1.5.3 of the CRDs prior to upgrade. See Prerequisites for location of CRDs.\n Upgrade to the latest CSM for Observability release:\nUpgrade Helm and Online Installer deployments: $ helm upgrade --version $latest_chart_version --values values.yaml karavi-observability dell/karavi-observability -n $namespace Upgrade Offline Installer deployment: $ helm upgrade --version $latest_chart_version karavi-observability dell/karavi-observability -n $namespace The configuration section lists all the parameters that can be configured using the values.yaml file.\nOnline Installer Upgrade CSM for Observability online installer upgrade can be used if the initial deployment was performed using the Online Installer or Helm.\n  Change to the installer directory:\n[user@system /home/user]# cd karavi-observability/installer   Update values.yaml file as needed. Configuration options are outlined in the Helm chart deployment section.\n  Execute the ./karavi-observability-install.sh script:\n[user@system /home/user/karavi-observability/installer]# ./karavi-observability-install.sh upgrade --namespace $namespace --values myvalues.yaml --version $latest_chart_version --------------------------------------------------------------------------------- \u003e Upgrading Karavi Observability in namespace karavi on 1.21 --------------------------------------------------------------------------------- | |- Karavi Observability is installed. Upgrade can continue Success | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying helm version Success | |- Upgrading CertManager CRDs Success | |- Updating helm repositories Success | |- Upgrading Karavi Observability helm chart Success | |- Waiting for pods in namespace karavi to be ready Success   ","excerpt":"This section outlines the upgrade steps for Container Storage Modules …","ref":"/csm-docs/v2/observability/upgrade/","title":"Upgrade"},{"body":"CSM for Resiliency can be upgraded as part of the Dell CSI driver upgrade process. The drivers can be upgraded either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart upgrade is supported for CSM for Resiliency.\nFor information on the PowerFlex CSI driver upgrade process, see PowerFlex CSI Driver.\nFor information on the Unity CSI driver upgrade process, see Unity CSI Driver.\nHelm Chart Upgrade To upgrade CSM for Resiliency with the driver, the following steps are required.\n Note: These steps refer to the values file and csi-install.sh script that were used during initial installation of the Dell CSI driver.\n Steps\n Update the podmon.image value in the values files to reference the new podmon image. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  ","excerpt":"CSM for Resiliency can be upgraded as part of the Dell CSI driver …","ref":"/csm-docs/v2/resiliency/upgrade/","title":"Upgrade"},{"body":"This section outlines the upgrade steps for Container Storage Modules (CSM) for Authorization. The upgrade of CSM for Authorization is handled in 2 parts:\n Upgrading the CSM for Authorization proxy server Upgrading the Dell EMC CSI drivers with CSM for Authorization enabled  Upgrading CSM for Authorization proxy server Obtain the latest single binary installer RPM by following one of our two options here.\nTo update the rpm package on the system, run the below command:\nrpm -Uvh karavi-authorization-\u003cnew_version\u003e.x86_64.rpm --nopreun --nopostun To verify that the new version of the rpm is installed and K3s has been updated, run the below commands:\nrpm -qa | grep karavi k3s kubectl version  Note: The above steps manage install and upgrade of all dependencies that are required by the CSM for Authorization proxy server.\n Upgrading Dell EMC CSI Driver(s) with CSM for Authorization enabled Given a setup where the CSM for Authorization proxy server is already upgraded to the latest version, follow the upgrade instructions for the applicable CSI Driver(s) to upgrade the driver and the CSM for Authorization sidecar\n Upgrade PowerFlex CSI driver Upgrade PowerMax CSI driver Upgrade PowerScale CSI driver  Rollback This section outlines the rollback steps for Container Storage Modules (CSM) for Authorization.\nRollback CSM for Authorization proxy server To rollback the rpm package on the system, run the below command:\nrpm -Uvh --oldpackage karavi-authorization-\u003cold_version\u003e.x86_64.rpm --nopreun --nopostun ","excerpt":"This section outlines the upgrade steps for Container Storage Modules …","ref":"/csm-docs/v3/authorization/upgrade/","title":"Upgrade"},{"body":"","excerpt":"","ref":"/csm-docs/v3/csidriver/upgradation/","title":"Upgrade"},{"body":"CSM for Observability can only be upgraded via the Helm chart following the instructions below.\nCSM for Observability Helm upgrade can be used if the initial deployment was performed using the Helm chart or Online Installer.\n Note: The Offline Installer does not support upgrade.\n Helm Chart Upgrade To upgrade an existing Helm installation of CSM for Observability to the latest release, download the latest Helm charts.\nhelm repo update Check if the latest Helm chart version is available:\nhelm search repo dell NAME CHART VERSION APP VERSION DESCRIPTION dell/karavi-observability 1.0.1 1.0.0 CSM for Observability is part of the [Container...  Note: If using cert-manager CustomResourceDefinitions older than v1.5.3, delete the old CRDs and install v1.5.3 of the CRDs prior to upgrade. See Prerequisites for location of CRDs.\n Upgrade to the latest CSM for Observability release:\n$ helm upgrade --version $latest_chart_version --values values.yaml karavi-observability dell/karavi-observability -n $namespace The configuration section lists all the parameters that can be configured using the values.yaml file.\n","excerpt":"CSM for Observability can only be upgraded via the Helm chart …","ref":"/csm-docs/v3/observability/upgrade/","title":"Upgrade"},{"body":"CSM for Resiliency can be upgraded as part of the Dell CSI driver upgrade process. The drivers can be upgraded either by a helm chart or by the Dell CSI Operator. Currently, only Helm chart upgrade is supported for CSM for Resiliency.\nFor information on the PowerFlex CSI driver upgrade process, see PowerFlex CSI Driver.\nFor information on the Unity CSI driver upgrade process, see Unity CSI Driver.\nHelm Chart Upgrade To upgrade CSM for Resiliency with the driver, the following steps are required.\n Note: These steps refer to the values file and csi-install.sh script that were used during initial installation of the Dell CSI driver.\n Steps\n Update the podmon.image value in the values files to reference the new podmon image. Run the csi-install script with the option –upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  ","excerpt":"CSM for Resiliency can be upgraded as part of the Dell CSI driver …","ref":"/csm-docs/v3/resiliency/upgrade/","title":"Upgrade"},{"body":"Container Storage Modules (CSM) for Authorization is part of the open-source suite of Kubernetes storage enablers for Dell products.\nCSM for Authorization provides storage and Kubernetes administrators the ability to apply RBAC for Dell CSI Drivers. It does this by deploying a proxy between the CSI driver and the storage system to enforce role-based access and usage rules.\nStorage administrators of compatible storage platforms will be able to apply quota and RBAC rules that instantly and automatically restrict cluster tenants usage of storage resources. Users of storage through CSM for Authorization do not need to have storage admin root credentials to access the storage system.\nKubernetes administrators will have an interface to create, delete, and manage roles/groups that storage rules may be applied. Administrators and/or users may then generate authentication tokens that may be used by tenants to use storage with proper access policies being automatically enforced.\nThe following diagram shows a high-level overview of CSM for Authorization with a tenant-app that is using a CSI driver to perform storage operations through the CSM for Authorization proxy-server to access the a Dell storage system. All requests from the CSI driver will contain the token for the given tenant that was granted by the Storage Administrator.\nCSM for Authorization Capabilities   Feature PowerFlex PowerMax PowerScale Unity XT PowerStore     Ability to set storage quota limits to ensure k8s tenants are not overconsuming storage Yes Yes No (natively supported) No No   Ability to create access control policies to ensure k8s tenant clusters are not accessing storage that does not belong to them Yes Yes No (natively supported) No No   Ability to shield storage credentials from Kubernetes administrators ensuring credentials are only handled by storage admins Yes Yes Yes No No    NOTE: PowerScale OneFS implements its own form of Role-Based Access Control (RBAC). CSM for Authorization does not enforce any role-based restrictions for PowerScale. To configure RBAC for PowerScale, refer to the PowerScale OneFS documentation.\nSupported Operating Systems/Container Orchestrator Platforms   COP/OS Supported Versions     Kubernetes 1.22, 1.23, 1.24   RHEL 7.x, 8.x   CentOS 7.8, 7.9    Supported Storage Platforms    PowerMax PowerFlex PowerScale     Storage Array 5978.479.479, 5978.711.711, Unisphere 9.2 3.5.x, 3.6.x OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3    Supported CSI Drivers CSM for Authorization supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell PowerFlex csi-powerflex v2.0 +   CSI Driver for Dell PowerMax csi-powermax v2.0 +   CSI Driver for Dell PowerScale csi-powerscale v2.0 +    NOTE: If the deployed CSI driver has a number of controller pods equal to the number of schedulable nodes in your cluster, CSM for Authorization may not be able to inject properly into the driver’s controller pod. To resolve this, please refer to our troubleshooting guide on the topic.\nAuthorization Components Support Matrix CSM for Authorization consists of 2 components - the Authorization sidecar and the Authorization proxy server. It is important that the version of the Authorization sidecar image maps to a supported version of the Authorization proxy server.\n  Authorization Sidecar Image Tag Authorization Proxy Server Version     dellemc/csm-authorization-sidecar:v1.0.0 v1.0.0, v1.1.0   dellemc/csm-authorization-sidecar:v1.2.0 v1.1.0, v1.2.0   dellemc/csm-authorization-sidecar:v1.3.0 v1.1.0, v1.2.0, v1.3.0    Roles and Responsibilities The CSM for Authorization CLI can be executed in the context of the following roles:\n Storage Administrators Kubernetes Tenant Administrators  Storage Administrators Storage Administrators can perform the following operations within CSM for Authorization\n Tenant Management (create, get, list, delete, bind roles, unbind roles) Token Management (generate, revoke) Storage System Management (create, get, list, update, delete) Storage Access Roles Management (assign to a storage system with an optional quota)  Tenant Administrators Tenants of CSM for Authorization can use the token provided by the Storage Administrators in their storage requests.\nWorkflow  Tenant Admin requests storage from a Storage Admin. Storage Admin uses CSM Authorization CLI to:\na) Create a tenant resource.\nb) Create a role permitting desired storage access.\nc) Assign the role to the tenant and generate a token.\n Storage Admin returns a token to the Tenant Admin. Tenant Admin inputs the Token into their Kubernetes cluster as a Secret. Tenant Admin updates CSI driver with CSM Authorization sidecar module.  ","excerpt":"Container Storage Modules (CSM) for Authorization is part of the …","ref":"/csm-docs/docs/authorization/","title":"Authorization"},{"body":"Container Storage Modules (CSM) for Authorization is part of the open-source suite of Kubernetes storage enablers for Dell products.\nCSM for Authorization provides storage and Kubernetes administrators the ability to apply RBAC for Dell CSI Drivers. It does this by deploying a proxy between the CSI driver and the storage system to enforce role-based access and usage rules.\nStorage administrators of compatible storage platforms will be able to apply quota and RBAC rules that instantly and automatically restrict cluster tenants usage of storage resources. Users of storage through CSM for Authorization do not need to have storage admin root credentials to access the storage system.\nKubernetes administrators will have an interface to create, delete, and manage roles/groups that storage rules may be applied. Administrators and/or users may then generate authentication tokens that may be used by tenants to use storage with proper access policies being automatically enforced.\nThe following diagram shows a high-level overview of CSM for Authorization with a tenant-app that is using a CSI driver to perform storage operations through the CSM for Authorization proxy-server to access the a Dell storage system. All requests from the CSI driver will contain the token for the given tenant that was granted by the Storage Administrator.\nCSM for Authorization Capabilities   Feature PowerFlex PowerMax PowerScale Unity PowerStore     Ability to set storage quota limits to ensure k8s tenants are not overconsuming storage Yes Yes No (natively supported) No No   Ability to create access control policies to ensure k8s tenant clusters are not accessing storage that does not belong to them Yes Yes No (natively supported) No No   Ability to shield storage credentials from Kubernetes administrators ensuring credentials are only handled by storage admins Yes Yes Yes No No    NOTE: PowerScale OneFS implements its own form of Role-Based Access Control (RBAC). CSM for Authorization does not enforce any role-based restrictions for PowerScale. To configure RBAC for PowerScale, refer to the PowerScale OneFS documentation.\nSupported Operating Systems/Container Orchestrator Platforms   COP/OS Supported Versions     Kubernetes 1.21, 1.22, 1.23   Red Hat OpenShift 4.8, 4.9   RHEL 7.x, 8.x   CentOS 7.8, 7.9    Supported Storage Platforms    PowerMax PowerFlex PowerScale     Storage Array 5978.479.479, 5978.711.711, Unisphere 9.2 3.5.x, 3.6.x OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3    Supported CSI Drivers CSM for Authorization supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell PowerFlex csi-powerflex v2.0, v2.1, v2.2   CSI Driver for Dell PowerMax csi-powermax v2.0, v2.1 ,v2.2   CSI Driver for Dell PowerScale csi-powerscale v2.0, v2.1, v2.2    NOTE: If the deployed CSI driver has a number of controller pods equal to the number of schedulable nodes in your cluster, CSM for Authorization may not be able to inject properly into the driver’s controller pod. To resolve this, please refer to our troubleshooting guide on the topic.\nAuthorization Components Support Matrix CSM for Authorization consists of 2 components - the Authorization sidecar and the Authorization proxy server. It is important that the version of the Authorization sidecar image maps to a supported version of the Authorization proxy server.\n  Authorization Sidecar Image Tag Authorization Proxy Server Version     dellemc/csm-authorization-sidecar:v1.0.0 v1.0.0, v1.1.0   dellemc/csm-authorization-sidecar:v1.2.0 v1.1.0, v1.2.0    Roles and Responsibilities The CSM for Authorization CLI can be executed in the context of the following roles:\n Storage Administrators Kubernetes Tenant Administrators  Storage Administrators Storage Administrators can perform the following operations within CSM for Authorization\n Tenant Management (create, get, list, delete, bind roles, unbind roles) Token Management (generate, revoke) Storage System Management (create, get, list, update, delete) Storage Access Roles Management (assign to a storage system with an optional quota)  Tenant Administrators Tenants of CSM for Authorization can use the token provided by the Storage Administrators in their storage requests.\nWorkflow  Tenant Admin requests storage from a Storage Admin. Storage Admin uses CSM Authorization CLI to:\na) Create a tenant resource.\nb) Create a role permitting desired storage access.\nc) Assign the role to the tenant and generate a token.\n Storage Admin returns a token to the Tenant Admin. Tenant Admin inputs the Token into their Kubernetes cluster as a Secret. Tenant Admin updates CSI driver with CSM Authorization sidecar module.  ","excerpt":"Container Storage Modules (CSM) for Authorization is part of the …","ref":"/csm-docs/v1/authorization/","title":"Authorization"},{"body":"Container Storage Modules (CSM) for Authorization is part of the open-source suite of Kubernetes storage enablers for Dell products.\nCSM for Authorization provides storage and Kubernetes administrators the ability to apply RBAC for Dell CSI Drivers. It does this by deploying a proxy between the CSI driver and the storage system to enforce role-based access and usage rules.\nStorage administrators of compatible storage platforms will be able to apply quota and RBAC rules that instantly and automatically restrict cluster tenants usage of storage resources. Users of storage through CSM for Authorization do not need to have storage admin root credentials to access the storage system.\nKubernetes administrators will have an interface to create, delete, and manage roles/groups that storage rules may be applied. Administrators and/or users may then generate authentication tokens that may be used by tenants to use storage with proper access policies being automatically enforced.\nThe following diagram shows a high-level overview of CSM for Authorization with a tenant-app that is using a CSI driver to perform storage operations through the CSM for Authorization proxy-server to access the a Dell storage system. All requests from the CSI driver will contain the token for the given tenant that was granted by the Storage Administrator.\nCSM for Authorization Capabilities   Feature PowerFlex PowerMax PowerScale Unity PowerStore     Ability to set storage quota limits to ensure k8s tenants are not overconsuming storage Yes Yes No (natively supported) No No   Ability to create access control policies to ensure k8s tenant clusters are not accessing storage that does not belong to them Yes Yes No (natively supported) No No   Ability to shield storage credentials from Kubernetes administrators ensuring credentials are only handled by storage admins Yes Yes Yes No No    NOTE: PowerScale OneFS implements its own form of Role-Based Access Control (RBAC). CSM for Authorization does not enforce any role-based restrictions for PowerScale. To configure RBAC for PowerScale, refer to the PowerScale OneFS documentation.\nSupported Operating Systems/Container Orchestrator Platforms   COP/OS Supported Versions     Kubernetes 1.21, 1.22, 1.23   Red Hat OpenShift 4.8, 4.9   RHEL 7.x, 8.x   CentOS 7.8, 7.9    Supported Storage Platforms    PowerMax PowerFlex PowerScale     Storage Array 5978.479.479, 5978.711.711, Unisphere 9.2 3.5.x, 3.6.x OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3    Supported CSI Drivers CSM for Authorization supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell PowerFlex csi-powerflex v2.0, v2.1, v2.2   CSI Driver for Dell PowerMax csi-powermax v2.0, v2.1 ,v2.2   CSI Driver for Dell PowerScale csi-powerscale v2.0, v2.1, v2.2    NOTE: If the deployed CSI driver has a number of controller pods equal to the number of schedulable nodes in your cluster, CSM for Authorization may not be able to inject properly into the driver’s controller pod. To resolve this, please refer to our troubleshooting guide on the topic.\nAuthorization Components Support Matrix CSM for Authorization consists of 2 components - the Authorization sidecar and the Authorization proxy server. It is important that the version of the Authorization sidecar image maps to a supported version of the Authorization proxy server.\n  Authorization Sidecar Image Tag Authorization Proxy Server Version     dellemc/csm-authorization-sidecar:v1.0.0 v1.0.0, v1.1.0   dellemc/csm-authorization-sidecar:v1.2.0 v1.1.0, v1.2.0    Roles and Responsibilities The CSM for Authorization CLI can be executed in the context of the following roles:\n Storage Administrators Kubernetes Tenant Administrators  Storage Administrators Storage Administrators can perform the following operations within CSM for Authorization\n Tenant Management (create, get, list, delete, bind roles, unbind roles) Token Management (generate, revoke) Storage System Management (create, get, list, update, delete) Storage Access Roles Management (assign to a storage system with an optional quota)  Tenant Administrators Tenants of CSM for Authorization can use the token provided by the Storage Administrators in their storage requests.\nWorkflow  Tenant Admin requests storage from a Storage Admin. Storage Admin uses CSM Authorization CLI to:\na) Create a tenant resource.\nb) Create a role permitting desired storage access.\nc) Assign the role to the tenant and generate a token.\n Storage Admin returns a token to the Tenant Admin. Tenant Admin inputs the Token into their Kubernetes cluster as a Secret. Tenant Admin updates CSI driver with CSM Authorization sidecar module.  ","excerpt":"Container Storage Modules (CSM) for Authorization is part of the …","ref":"/csm-docs/v2/authorization/","title":"Authorization"},{"body":"Container Storage Modules (CSM) for Authorization is part of the open-source suite of Kubernetes storage enablers for Dell EMC products.\nCSM for Authorization provides storage and Kubernetes administrators the ability to apply RBAC for Dell EMC CSI Drivers. It does this by deploying a proxy between the CSI driver and the storage system to enforce role-based access and usage rules.\nStorage administrators of compatible storage platforms will be able to apply quota and RBAC rules that instantly and automatically restrict cluster tenants usage of storage resources. Users of storage through CSM for Authorization do not need to have storage admin root credentials to access the storage system.\nKubernetes administrators will have an interface to create, delete, and manage roles/groups that storage rules may be applied. Administrators and/or users may then generate authentication tokens that may be used by tenants to use storage with proper access policies being automatically enforced.\nThe following diagram shows a high-level overview of CSM for Authorization with a tenant-app that is using a CSI driver to perform storage operations through the CSM for Authorization proxy-server to access the a Dell EMC storage system. All requests from the CSI driver will contain the token for the given tenant that was granted by the Storage Administrator.\nCSM for Authorization Capabilities   Feature PowerFlex PowerMax PowerScale Unity PowerStore     Ability to set storage quota limits to ensure k8s tenants are not overconsuming storage Yes Yes No (natively supported) No No   Ability to create access control policies to ensure k8s tenant clusters are not accessing storage that does not belong to them Yes Yes No (natively supported) No No   Ability to shield storage credentials from Kubernetes administrators ensuring credentials are only handled by storage admins Yes Yes Yes No No    NOTE: PowerScale OneFS implements its own form of Role-Based Access Control (RBAC). CSM for Authorization does not enforce any role-based restrictions for PowerScale. To configure RBAC for PowerScale, refer to the PowerScale OneFS documentation.\nSupported Operating Systems/Container Orchestrator Platforms   COP/OS Supported Versions     Kubernetes 1.20, 1.21, 1.22   Red Hat OpenShift 4.8, 4.9   RHEL 7.x, 8.x   CentOS 7.8, 7.9    Supported Storage Platforms    PowerMax PowerFlex PowerScale     Storage Array 5978.479.479, 5978.669.669, 5978.711.711, Unisphere 9.2 3.5.x, 3.6.x OneFS 8.1, 8.2, 9.0, 9.1, 9.2    Supported CSI Drivers CSM for Authorization supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell EMC PowerFlex csi-powerflex v2.0,v2.1   CSI Driver for Dell EMC PowerMax csi-powermax v2.0,v2.1   CSI Driver for Dell EMC PowerScale csi-powerscale v2.0,v2.1    Note: If the deployed CSI driver has a number of controller pods equal to the number of schedulable nodes in your cluster, CSM for Authorization may not be able to inject properly into the driver’s controller pod. To resolve this, please refer to our troubleshooting guide on the topic.\nAuthorization Components Support Matrix CSM for Authorization consists of 2 components - the Authorization sidecar and the Authorization proxy server. It is important that the version of the Authorization sidecar image maps to a supported version of the Authorization proxy server.\n  Authorization Sidecar Image Tag Authorization Proxy Server Version     dellemc/csm-authorization-sidecar:v1.0.0 v1.0.0, v1.1.0    Roles and Responsibilities The CSM for Authorization CLI can be executed in the context of the following roles:\n Storage Administrators Kubernetes Tenant Administrators  Storage Administrators Storage Administrators can perform the following operations within CSM for Authorization\n Tenant Management (create, get, list, delete, bind roles, unbind roles) Token Management (generate, revoke) Storage System Management (create, get, list, update, delete) Storage Access Roles Management (assign to a storage system with an optional quota)  Tenant Administrators Tenants of CSM for Authorization can use the token provided by the Storage Administrators in their storage requests.\nWorkflow  Tenant Admin requests storage from a Storage Admin. Storage Admin uses CSM Authorization CLI to:\na) Create a tenant resource.\nb) Create a role permitting desired storage access.\nc) Assign the role to the tenant and generate a token.\n Storage Admin returns a token to the Tenant Admin. Tenant Admin inputs the Token into their Kubernetes cluster as a Secret. Tenant Admin updates CSI driver with CSM Authorization sidecar module.  ","excerpt":"Container Storage Modules (CSM) for Authorization is part of the …","ref":"/csm-docs/v3/authorization/","title":"Authorization"},{"body":"karavictl is a command-line interface (CLI) used to interact with and manage your Container Storage Modules (CSM) Authorization deployment. This document outlines all karavictl commands, their intended use, options that can be provided to alter their execution, and expected output from those commands.\nIf you feel that something is unclear or missing in this document, please open up an issue.\n   Command Description     karavictl karavictl is used to interact with CSM Authorization Server   karavictl cluster-info Display the state of resources within the cluster   karavictl generate Generate resources for use with CSM   karavictl generate token Generate tokens   karavictl role Manage role   karavictl role get Get role   karavictl role list List roles   karavictl role create Create one or more CSM roles   karavictl role update Update one or more CSM roles   karavictl role delete Delete role   karavictl rolebinding Manage role bindings   karavictl rolebinding create Create a rolebinding between role and tenant   karavictl rolebinding delete Delete a rolebinding between role and tenant   karavictl storage Manage storage systems   karavictl storage get Get details on a registered storage system   karavictl storage list List registered storage systems   karavictl storage create Create and register a storage system   karavictl storage update Update a registered storage system   karavictl storage delete Delete a registered storage system   karavictl tenant  Manage tenants   karavictl tenant create Create a tenant resource within CSM   karavictl tenant get Get a tenant resource within CSM   karavictl tenant list Lists tenant resources within CSM   karavictl tenant revoke Get a tenant resource within CSM   karavictl tenant delete Deletes a tenant resource within CSM    General Commands karavictl karavictl is used to interact with CSM Authorization Server\nSynopsis karavictl provides security, RBAC, and quota limits for accessing Dell storage products from Kubernetes clusters\nOptions  --config string config file (default is $HOME/.karavictl.yaml) -h, --help help for karavictl -t, --toggle Help message for toggle Output Outputs help text\n karavictl cluster-info Display the state of resources within the cluster\nSynopsis Prints table of resources within the cluster, including their readiness\nkaravictl cluster-info [flags] Options  -h, --help help for cluster-info -w, --watch Watch for changes Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl cluster-info NAME READY UP-TO-DATE AVAILABLE AGE github-auth-provider 1/1 1 1 59m tenant-service 1/1 1 1 59m redis-primary 1/1 1 1 59m proxy-server 1/1 1 1 59m redis-commander 1/1 1 1 59m  karavictl generate Generate resources for use with CSM\nSynopsis Generates resources for use with CSM\nkaravictl generate [flags] Options  -h, --help help for generate Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl generate token Generate tokens\nSynopsis Generate tokens for use with the CSI Driver when in proxy mode The tokens are output as a Kubernetes Secret resource, so the results may be piped directly to kubectl:\nExample: karavictl generate token | kubectl apply -f -\nkaravictl generate token [flags] Options  --addr string host:port address (default \"grpc.gatekeeper.cluster:443\") --from-config string File providing self-generated token information -h, --help help for token --tenant Tenant name --shared-secret string Shared secret for token signing Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl generate token --shared-secret supersecret apiVersion: v1 kind: Secret metadata: name: proxy-authz-tokens namespace: vxflexos type: Opaque data: access: \u003cACCESS-TOKEN\u003e refresh: \u003cREFRESH-TOKEN\u003e Usually, you will want to pipe the output to kubectl to apply the secret\n$ karavictl generate token --shared-secret supersecret | kubectl apply -f - Role Commands karavictl role Manage roles\nSynopsis Manage roles\nkaravictl role [flags] Options  -h, --help help for role Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl role get Get role\nSynopsis Get role\nkaravictl role get [flags] Options  -h, --help help for get Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role get CSISilver { \"Name\": \"CSISilver\", \"StorageSystem\": \"3000000000011111\", \"PoolQuotas\": [ { \"Pool\": \"mypool\", \"Quota\": \"16 GB\" } ] }  karavictl role list List roles\nSynopsis List roles\nkaravictl role list [flags] Options  -h, --help help for list Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role list { \"CSIGold\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 32000000 } ] } ], \"CSISilver\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 16000000 } ] } ] }  karavictl role create Create one or more CSM roles\nSynopsis Creates one or more CSM roles\nkaravictl role create [flags] Options  -f, --from-file string role data from a file --role strings role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -h, --help help for create NOTE:\n For PowerScale, set the quota to 0 as CSM for Authorization does not enforce quota limits.  Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role create --from-file roles.json On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the creation occurred.\nAlternatively, you can create a role in-line using:\n$ karavictl role create --role=role-name=system-type=000000000001=mypool=200000000  karavictl role update Update one or more CSM roles\nSynopsis Updates one or more CSM roles\nkaravictl role update [flags] Options  -f, --from-file string role data from a file --role strings role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -h, --help help for update Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role update --from-file roles.json On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the update occurred.\nAlternatively, you can update existing roles in-line using:\n$ karavictl role update --role=role-name=system-type=000000000001=mypool=400000000  karavictl role delete Delete role\nSynopsis Delete role\nkaravictl role delete \u003crole-name\u003e [flags] Options  -h, --help help for delete Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role delete CSISilver On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the deletion occurred.\n karavictl rolebinding Manage role bindings\nSynopsis Management for role bindings\nkaravictl rolebinding [flags] Options  -h, --help help for rolebinding Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl rolebinding create Create a rolebinding between role and tenant\nSynopsis Creates a rolebinding between role and tenant\nkaravictl rolebinding create [flags] Options  -h, --help help for create -r, --role string Role name -t, --tenant string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl rolebinding create --role CSISilver --tenant Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the rolebinding creation occurred.\n karavictl rolebinding delete Delete a rolebinding between role and tenant\nSynopsis Deletes a rolebinding between role and tenant\nkaravictl rolebinding delete [flags] Options  -h, --help help for create -r, --role string Role name -t, --tenant string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl rolebinding delete --role CSISilver --tenant Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the rolebinding deletion occurred.\nStorage Commands karavictl storage Manage storage systems\nSynopsis Manages storage systems\nkaravictl storage [flags] Options  -h, --help help for storage Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl storage get Get details on a registered storage system.\nSynopsis Gets details on a registered storage system.\nkaravictl storage get [flags] Options  -h, --help help for get -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage get --type powerflex --system-id 3000000000011111 { \"User\": \"admin\", \"Password\": \"(omitted)\", \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true }  karavictl storage list List registered storage systems.\nSynopsis Lists registered storage systems.\nkaravictl storage list [flags] Options  -h, --help help for list Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage list { \"storage\": { \"powerflex\": { \"3000000000011111\": { \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true, \"Password\": \"(omitted)\", \"User\": \"admin\" } } } }  karavictl storage create Create and register a storage system.\nSynopsis Creates and registers a storage system.\nkaravictl storage create [flags] Options  -e, --endpoint string Endpoint of REST API gateway -h, --help help for create -i, --insecure Insecure skip verify -p, --password string Password (default \"****\") -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") -u, --user string Username (default \"admin\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage create --endpoint https://1.1.1.1 --insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the creation occurred.\n karavictl storage update Update a registered storage system.\nSynopsis Updates a registered storage system.\nkaravictl storage update [flags] Options  -e, --endpoint string Endpoint of REST API gateway -h, --help help for update -i, --insecure Insecure skip verify -p, --pass string Password (default \"****\") -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") -u, --user string Username (default \"admin\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage update --endpoint https://1.1.1.1 --insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the update occurred.\n karavictl storage delete Delete a registered storage system.\nSynopsis Deletes a registered storage system.\nkaravictl storage delete [flags] Options  -h, --help help for delete -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage delete --type powerflex --system-id 3000000000011111 On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the deletion occurred.\nTenant Commands karavictl tenant Manage tenants\nSynopsis Management for tenants\nkaravictl tenant [flags] Options  -h, --help help for tenant Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl tenant create Create a tenant resource within CSM\nSynopsis Creates a tenant resource within CSM\nkaravictl tenant create [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant create --name Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the creation occurred.\n karavictl tenant get Get a tenant resource within CSM\nSynopsis Gets a tenant resource and its assigned roles within CSM\nkaravictl tenant get [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant get --name Alice { \"name\": \"Alice\" \"roles\": \"role-1,role-2\" }  karavictl tenant list Lists tenant resources within CSM\nSynopsis Lists tenant resources within CSM\nkaravictl tenant list [flags] Options  -h, --help help for create Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant list { \"tenants\": [ { \"name\": \"Alice\" } ] }  karavictl tenant revoke Revokes access for a tenant\nSynopsis Revokes access to storage resources for a tenant\nkaravictl tenant revoke [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant revoke --name Alice On success, there will be no output.\n karavictl tenant delete Deletes a tenant resource within CSM\nSynopsis Deletes a tenant resource within CSM\nkaravictl tenant delete [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant delete --name Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the deletion occurred.\n","excerpt":"karavictl is a command-line interface (CLI) used to interact with and …","ref":"/csm-docs/docs/authorization/cli/","title":"CLI"},{"body":"karavictl is a command-line interface (CLI) used to interact with and manage your Container Storage Modules (CSM) Authorization deployment. This document outlines all karavictl commands, their intended use, options that can be provided to alter their execution, and expected output from those commands.\nIf you feel that something is unclear or missing in this document, please open up an issue.\n   Command Description     karavictl karavictl is used to interact with CSM Authorization Server   karavictl cluster-info Display the state of resources within the cluster   karavictl generate Generate resources for use with CSM   karavictl generate token Generate tokens   karavictl role Manage role   karavictl role get Get role   karavictl role list List roles   karavictl role create Create one or more CSM roles   karavictl role update Update one or more CSM roles   karavictl role delete Delete role   karavictl rolebinding Manage role bindings   karavictl rolebinding create Create a rolebinding between role and tenant   karavictl storage Manage storage systems   karavictl storage get Get details on a registered storage system   karavictl storage list List registered storage systems   karavictl storage create Create and register a storage system   karavictl storage update Update a registered storage system   karavictl storage delete Delete a registered storage system   karavictl tenant  Manage tenants   karavictl tenant create Create a tenant resource within CSM   karavictl tenant get Get a tenant resource within CSM   karavictl tenant list Lists tenant resources within CSM   karavictl tenant get Get a tenant resource within CSM   karavictl tenant delete Deletes a tenant resource within CSM    General Commands karavictl karavictl is used to interact with CSM Authorization Server\nSynopsis karavictl provides security, RBAC, and quota limits for accessing Dell storage products from Kubernetes clusters\nOptions  --config string config file (default is $HOME/.karavictl.yaml) -h, --help help for karavictl -t, --toggle Help message for toggle Output Outputs help text\n karavictl cluster-info Display the state of resources within the cluster\nSynopsis Prints table of resources within the cluster, including their readiness\nkaravictl cluster-info [flags] Options  -h, --help help for cluster-info -w, --watch Watch for changes Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl cluster-info NAME READY UP-TO-DATE AVAILABLE AGE github-auth-provider 1/1 1 1 59m tenant-service 1/1 1 1 59m redis-primary 1/1 1 1 59m proxy-server 1/1 1 1 59m redis-commander 1/1 1 1 59m  karavictl generate Generate resources for use with CSM\nSynopsis Generates resources for use with CSM\nkaravictl generate [flags] Options  -h, --help help for generate Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl generate token Generate tokens\nSynopsis Generate tokens for use with the CSI Driver when in proxy mode The tokens are output as a Kubernetes Secret resource, so the results may be piped directly to kubectl:\nExample: karavictl generate token | kubectl apply -f -\nkaravictl generate token [flags] Options  --addr string host:port address (default \"grpc.gatekeeper.cluster:443\") --from-config string File providing self-generated token information -h, --help help for token --tenant Tenant name --shared-secret string Shared secret for token signing Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl generate token --shared-secret supersecret apiVersion: v1 kind: Secret metadata: name: proxy-authz-tokens namespace: vxflexos type: Opaque data: access: \u003cACCESS-TOKEN\u003e refresh: \u003cREFRESH-TOKEN\u003e Usually, you will want to pipe the output to kubectl to apply the secret\n$ karavictl generate token --shared-secret supersecret | kubectl apply -f - Role Commands karavictl role Manage roles\nSynopsis Manage roles\nkaravictl role [flags] Options  -h, --help help for role Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl role get Get role\nSynopsis Get role\nkaravictl role get [flags] Options  -h, --help help for get Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role get CSISilver { \"Name\": \"CSISilver\", \"StorageSystem\": \"3000000000011111\", \"PoolQuotas\": [ { \"Pool\": \"mypool\", \"Quota\": \"16 GB\" } ] }  karavictl role list List roles\nSynopsis List roles\nkaravictl role list [flags] Options  -h, --help help for list Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role list { \"CSIGold\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 32000000 } ] } ], \"CSISilver\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 16000000 } ] } ] }  karavictl role create Create one or more CSM roles\nSynopsis Creates one or more CSM roles\nkaravictl role create [flags] Options  -f, --from-file string role data from a file --role strings role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -h, --help help for create NOTE:\n For PowerScale, set the quota to 0 as CSM for Authorization does not enforce quota limits.  Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role create --from-file roles.json On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the creation occurred.\nAlternatively, you can create a role in-line using:\n$ karavictl role create --role=role-name=system-type=000000000001=mypool=200000000  karavictl role update Update one or more CSM roles\nSynopsis Updates one or more CSM roles\nkaravictl role update [flags] Options  -f, --from-file string role data from a file --role strings role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -h, --help help for update Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role update --from-file roles.json On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the update occurred.\nAlternatively, you can update existing roles in-line using:\n$ karavictl role update --role=role-name=system-type=000000000001=mypool=400000000  karavictl role delete Delete role\nSynopsis Delete role\nkaravictl role delete \u003crole-name\u003e [flags] Options  -h, --help help for delete Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role delete CSISilver On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the deletion occurred.\n karavictl rolebinding Manage role bindings\nSynopsis Management for role bindings\nkaravictl rolebinding [flags] Options  -h, --help help for rolebinding Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl rolebinding create Create a rolebinding between role and tenant\nSynopsis Creates a rolebinding between role and tenant\nkaravictl rolebinding create [flags] Options  -h, --help help for create -r, --role string Role name -t, --tenant string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl rolebinding create --role CSISilver --tenant Alice On success, there will be no output. You may run karavictl tenant get \u003ctenant-name\u003e to confirm the rolebinding creation occurred.\nStorage Commands karavictl storage Manage storage systems\nSynopsis Manages storage systems\nkaravictl storage [flags] Options  -h, --help help for storage Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl storage get Get details on a registered storage system.\nSynopsis Gets details on a registered storage system.\nkaravictl storage get [flags] Options  -h, --help help for get -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage get --type powerflex --system-id 3000000000011111 { \"User\": \"admin\", \"Password\": \"(omitted)\", \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true }  karavictl storage list List registered storage systems.\nSynopsis Lists registered storage systems.\nkaravictl storage list [flags] Options  -h, --help help for list Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage list { \"storage\": { \"powerflex\": { \"3000000000011111\": { \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true, \"Password\": \"(omitted)\", \"User\": \"admin\" } } } }  karavictl storage create Create and register a storage system.\nSynopsis Creates and registers a storage system.\nkaravictl storage create [flags] Options  -e, --endpoint string Endpoint of REST API gateway -h, --help help for create -i, --insecure Insecure skip verify -p, --password string Password (default \"****\") -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") -u, --user string Username (default \"admin\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage create --endpoint https://1.1.1.1 --insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the creation occurred.\n karavictl storage update Update a registered storage system.\nSynopsis Updates a registered storage system.\nkaravictl storage update [flags] Options  -e, --endpoint string Endpoint of REST API gateway -h, --help help for update -i, --insecure Insecure skip verify -p, --pass string Password (default \"****\") -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") -u, --user string Username (default \"admin\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage update --endpoint https://1.1.1.1 --insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the update occurred.\n karavictl storage delete Delete a registered storage system.\nSynopsis Deletes a registered storage system.\nkaravictl storage delete [flags] Options  -h, --help help for delete -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage delete --type powerflex --system-id 3000000000011111 On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the deletion occurred.\nTenant Commands karavictl tenant Manage tenants\nSynopsis Management fortenants\nkaravictl tenant [flags] Options  -h, --help help for tenant Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl tenant create Create a tenant resource within CSM\nSynopsis Creates a tenant resource within CSM\nkaravictl tenant create [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant create --name Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the creation occurred.\n karavictl tenant get Get a tenant resource within CSM\nSynopsis Gets a tenant resource within CSM\nkaravictl tenant get [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant get --name Alice { \"name\": \"Alice\" }  karavictl tenant list Lists tenant resources within CSM\nSynopsis Lists tenant resources within CSM\nkaravictl tenant list [flags] Options  -h, --help help for create Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant list { \"tenants\": [ { \"name\": \"Alice\" } ] }  karavictl tenant delete Deletes a tenant resource within CSM\nSynopsis Deletes a tenant resource within CSM\nkaravictl tenant delete [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant delete --name Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the deletion occurred.\n","excerpt":"karavictl is a command-line interface (CLI) used to interact with and …","ref":"/csm-docs/v1/authorization/cli/","title":"CLI"},{"body":"karavictl is a command-line interface (CLI) used to interact with and manage your Container Storage Modules (CSM) Authorization deployment. This document outlines all karavictl commands, their intended use, options that can be provided to alter their execution, and expected output from those commands.\nIf you feel that something is unclear or missing in this document, please open up an issue.\n   Command Description     karavictl karavictl is used to interact with CSM Authorization Server   karavictl cluster-info Display the state of resources within the cluster   karavictl generate Generate resources for use with CSM   karavictl generate token Generate tokens   karavictl role Manage role   karavictl role get Get role   karavictl role list List roles   karavictl role create Create one or more CSM roles   karavictl role update Update one or more CSM roles   karavictl role delete Delete role   karavictl rolebinding Manage role bindings   karavictl rolebinding create Create a rolebinding between role and tenant   karavictl storage Manage storage systems   karavictl storage get Get details on a registered storage system   karavictl storage list List registered storage systems   karavictl storage create Create and register a storage system   karavictl storage update Update a registered storage system   karavictl storage delete Delete a registered storage system   karavictl tenant  Manage tenants   karavictl tenant create Create a tenant resource within CSM   karavictl tenant get Get a tenant resource within CSM   karavictl tenant list Lists tenant resources within CSM   karavictl tenant get Get a tenant resource within CSM   karavictl tenant delete Deletes a tenant resource within CSM    General Commands karavictl karavictl is used to interact with CSM Authorization Server\nSynopsis karavictl provides security, RBAC, and quota limits for accessing Dell storage products from Kubernetes clusters\nOptions  --config string config file (default is $HOME/.karavictl.yaml) -h, --help help for karavictl -t, --toggle Help message for toggle Output Outputs help text\n karavictl cluster-info Display the state of resources within the cluster\nSynopsis Prints table of resources within the cluster, including their readiness\nkaravictl cluster-info [flags] Options  -h, --help help for cluster-info -w, --watch Watch for changes Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl cluster-info NAME READY UP-TO-DATE AVAILABLE AGE github-auth-provider 1/1 1 1 59m tenant-service 1/1 1 1 59m redis-primary 1/1 1 1 59m proxy-server 1/1 1 1 59m redis-commander 1/1 1 1 59m  karavictl generate Generate resources for use with CSM\nSynopsis Generates resources for use with CSM\nkaravictl generate [flags] Options  -h, --help help for generate Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl generate token Generate tokens\nSynopsis Generate tokens for use with the CSI Driver when in proxy mode The tokens are output as a Kubernetes Secret resource, so the results may be piped directly to kubectl:\nExample: karavictl generate token | kubectl apply -f -\nkaravictl generate token [flags] Options  --addr string host:port address (default \"grpc.gatekeeper.cluster:443\") --from-config string File providing self-generated token information -h, --help help for token --tenant Tenant name --shared-secret string Shared secret for token signing Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl generate token --shared-secret supersecret apiVersion: v1 kind: Secret metadata: name: proxy-authz-tokens namespace: vxflexos type: Opaque data: access: \u003cACCESS-TOKEN\u003e refresh: \u003cREFRESH-TOKEN\u003e Usually, you will want to pipe the output to kubectl to apply the secret\n$ karavictl generate token --shared-secret supersecret | kubectl apply -f - Role Commands karavictl role Manage roles\nSynopsis Manage roles\nkaravictl role [flags] Options  -h, --help help for role Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl role get Get role\nSynopsis Get role\nkaravictl role get [flags] Options  -h, --help help for get Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role get CSISilver { \"Name\": \"CSISilver\", \"StorageSystem\": \"3000000000011111\", \"PoolQuotas\": [ { \"Pool\": \"mypool\", \"Quota\": \"16 GB\" } ] }  karavictl role list List roles\nSynopsis List roles\nkaravictl role list [flags] Options  -h, --help help for list Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role list { \"CSIGold\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 32000000 } ] } ], \"CSISilver\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 16000000 } ] } ] }  karavictl role create Create one or more CSM roles\nSynopsis Creates one or more CSM roles\nkaravictl role create [flags] Options  -f, --from-file string role data from a file --role strings role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -h, --help help for create NOTE:\n For PowerScale, set the quota to 0 as CSM for Authorization does not enforce quota limits.  Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role create --from-file roles.json On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the creation occurred.\nAlternatively, you can create a role in-line using:\n$ karavictl role create --role=role-name=system-type=000000000001=mypool=200000000  karavictl role update Update one or more CSM roles\nSynopsis Updates one or more CSM roles\nkaravictl role update [flags] Options  -f, --from-file string role data from a file --role strings role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -h, --help help for update Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role update --from-file roles.json On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the update occurred.\nAlternatively, you can update existing roles in-line using:\n$ karavictl role update --role=role-name=system-type=000000000001=mypool=400000000  karavictl role delete Delete role\nSynopsis Delete role\nkaravictl role delete \u003crole-name\u003e [flags] Options  -h, --help help for delete Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role delete CSISilver On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the deletion occurred.\n karavictl rolebinding Manage role bindings\nSynopsis Management for role bindings\nkaravictl rolebinding [flags] Options  -h, --help help for rolebinding Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl rolebinding create Create a rolebinding between role and tenant\nSynopsis Creates a rolebinding between role and tenant\nkaravictl rolebinding create [flags] Options  -h, --help help for create -r, --role string Role name -t, --tenant string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl rolebinding create --role CSISilver --tenant Alice On success, there will be no output. You may run karavictl tenant get \u003ctenant-name\u003e to confirm the rolebinding creation occurred.\nStorage Commands karavictl storage Manage storage systems\nSynopsis Manages storage systems\nkaravictl storage [flags] Options  -h, --help help for storage Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl storage get Get details on a registered storage system.\nSynopsis Gets details on a registered storage system.\nkaravictl storage get [flags] Options  -h, --help help for get -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage get --type powerflex --system-id 3000000000011111 { \"User\": \"admin\", \"Password\": \"(omitted)\", \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true }  karavictl storage list List registered storage systems.\nSynopsis Lists registered storage systems.\nkaravictl storage list [flags] Options  -h, --help help for list Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage list { \"storage\": { \"powerflex\": { \"3000000000011111\": { \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true, \"Password\": \"(omitted)\", \"User\": \"admin\" } } } }  karavictl storage create Create and register a storage system.\nSynopsis Creates and registers a storage system.\nkaravictl storage create [flags] Options  -e, --endpoint string Endpoint of REST API gateway -h, --help help for create -i, --insecure Insecure skip verify -p, --password string Password (default \"****\") -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") -u, --user string Username (default \"admin\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage create --endpoint https://1.1.1.1 --insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the creation occurred.\n karavictl storage update Update a registered storage system.\nSynopsis Updates a registered storage system.\nkaravictl storage update [flags] Options  -e, --endpoint string Endpoint of REST API gateway -h, --help help for update -i, --insecure Insecure skip verify -p, --pass string Password (default \"****\") -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") -u, --user string Username (default \"admin\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage update --endpoint https://1.1.1.1 --insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the update occurred.\n karavictl storage delete Delete a registered storage system.\nSynopsis Deletes a registered storage system.\nkaravictl storage delete [flags] Options  -h, --help help for delete -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage delete --type powerflex --system-id 3000000000011111 On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the deletion occurred.\nTenant Commands karavictl tenant Manage tenants\nSynopsis Management fortenants\nkaravictl tenant [flags] Options  -h, --help help for tenant Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl tenant create Create a tenant resource within CSM\nSynopsis Creates a tenant resource within CSM\nkaravictl tenant create [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant create --name Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the creation occurred.\n karavictl tenant get Get a tenant resource within CSM\nSynopsis Gets a tenant resource within CSM\nkaravictl tenant get [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant get --name Alice { \"name\": \"Alice\" }  karavictl tenant list Lists tenant resources within CSM\nSynopsis Lists tenant resources within CSM\nkaravictl tenant list [flags] Options  -h, --help help for create Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant list { \"tenants\": [ { \"name\": \"Alice\" } ] }  karavictl tenant delete Deletes a tenant resource within CSM\nSynopsis Deletes a tenant resource within CSM\nkaravictl tenant delete [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant delete --name Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the deletion occurred.\n","excerpt":"karavictl is a command-line interface (CLI) used to interact with and …","ref":"/csm-docs/v2/authorization/cli/","title":"CLI"},{"body":"karavictl is a command-line interface (CLI) used to interact with and manage your Container Storage Modules (CSM) Authorization deployment. This document outlines all karavictl commands, their intended use, options that can be provided to alter their execution, and expected output from those commands.\nIf you feel that something is unclear or missing in this document, please open up an issue.\n   Command Description     karavictl karavictl is used to interact with CSM Authorization Server   karavictl cluster-info Display the state of resources within the cluster   karavictl inject Inject the sidecar proxy into a CSI driver pod   karavictl generate Generate resources for use with CSM   karavictl generate token Generate tokens   karavictl role Manage role   karavictl role get Get role   karavictl role list List roles   karavictl role create Create one or more CSM roles   karavictl role update Update one or more CSM roles   karavictl role delete Delete role   karavictl rolebinding Manage role bindings   karavictl rolebinding create Create a rolebinding between role and tenant   karavictl storage Manage storage systems   karavictl storage get Get details on a registered storage system   karavictl storage list List registered storage systems   karavictl storage create Create and register a storage system   karavictl storage update Update a registered storage system   karavictl storage delete Delete a registered storage system   karavictl tenant  Manage tenants   karavictl tenant create Create a tenant resource within CSM   karavictl tenant get Get a tenant resource within CSM   karavictl tenant list Lists tenant resources within CSM   karavictl tenant get Get a tenant resource within CSM   karavictl tenant delete Deletes a tenant resource within CSM    General Commands karavictl karavictl is used to interact with CSM Authorization Server\nSynopsis karavictl provides security, RBAC, and quota limits for accessing Dell EMC storage products from Kubernetes clusters\nOptions  --config string config file (default is $HOME/.karavictl.yaml) -h, --help help for karavictl -t, --toggle Help message for toggle Output Outputs help text\n karavictl cluster-info Display the state of resources within the cluster\nSynopsis Prints table of resources within the cluster, including their readiness\nkaravictl cluster-info [flags] Options  -h, --help help for cluster-info -w, --watch Watch for changes Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl cluster-info NAME READY UP-TO-DATE AVAILABLE AGE github-auth-provider 1/1 1 1 59m tenant-service 1/1 1 1 59m redis-primary 1/1 1 1 59m proxy-server 1/1 1 1 59m redis-commander 1/1 1 1 59m  karavictl inject Inject the sidecar proxy into a CSI driver pod\nSynopsis Injects the sidecar proxy into a CSI driver pod.\nYou can inject resources coming from stdin.\nkaravictl inject [flags] Options  -h, --help help for inject --image-addr string Help message for image-addr --proxy-host string Help message for proxy-host Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Examples: Inject into an existing vxflexos CSI driver\nkubectl get secrets,deployments,daemonsets -n vxflexos -o yaml \\ | karavictl inject --image-addr [IMAGE_REPO]:5000/sidecar-proxy:latest --proxy-host [PROXY_HOST_IP] \\ | kubectl apply -f - Output $ kubectl get secrets,deployments,daemonsets -n vxflexos -o yaml \\ | karavictl inject --image-addr [IMAGE_REPO]:5000/sidecar-proxy:latest --proxy-host [PROXY_HOST_IP] \\ | kubectl apply -f - secret/karavi-authorization-config created deployment.apps/vxflexos-controller configured daemonset.apps/vxflexos-node configured  karavictl generate Generate resources for use with CSM\nSynopsis Generates resources for use with CSM\nkaravictl generate [flags] Options  -h, --help help for generate Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl generate token Generate tokens\nSynopsis Generate tokens for use with the CSI Driver when in proxy mode The tokens are output as a Kubernetes Secret resource, so the results may be piped directly to kubectl:\nExample: karavictl generate token | kubectl apply -f -\nkaravictl generate token [flags] Options  --addr string host:port address (default \"grpc.gatekeeper.cluster:443\") --from-config string File providing self-generated token information -h, --help help for token --tenant Tenant name --shared-secret string Shared secret for token signing Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl generate token --shared-secret supersecret apiVersion: v1 kind: Secret metadata: name: proxy-authz-tokens namespace: vxflexos type: Opaque data: access: \u003cACCESS-TOKEN\u003e refresh: \u003cREFRESH-TOKEN\u003e Usually, you will want to pipe the output to kubectl to apply the secret\n$ karavictl generate token --shared-secret supersecret | kubectl apply -f - Role Commands karavictl role Manage roles\nSynopsis Manage roles\nkaravictl role [flags] Options  -h, --help help for role Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl role get Get role\nSynopsis Get role\nkaravictl role get [flags] Options  -h, --help help for get Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role get CSISilver { \"Name\": \"CSISilver\", \"StorageSystem\": \"3000000000011111\", \"PoolQuotas\": [ { \"Pool\": \"mypool\", \"Quota\": \"16 GB\" } ] }  karavictl role list List roles\nSynopsis List roles\nkaravictl role list [flags] Options  -h, --help help for list Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role list { \"CSIGold\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 32000000 } ] } ], \"CSISilver\": [ { \"storage_system_id\": \"3000000000011111\", \"pool_quotas\": [ { \"pool\": \"mypool\", \"quota\": 16000000 } ] } ] }  karavictl role create Create one or more CSM roles\nSynopsis Creates one or more CSM roles\nkaravictl role create [flags] Options  -f, --from-file string role data from a file --role strings role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -h, --help help for create NOTE:\n For PowerScale, set the quota to 0 as CSM for Authorization does not enforce quota limits.  Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role create --from-file roles.json On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the creation occurred.\nAlternatively, you can create a role in-line using:\n$ karavictl role create --role=role-name=system-type=000000000001=mypool=200000000  karavictl role update Update one or more CSM roles\nSynopsis Updates one or more CSM roles\nkaravictl role update [flags] Options  -f, --from-file string role data from a file --role strings role in the form \u003cname\u003e=\u003ctype\u003e=\u003cid\u003e=\u003cpool\u003e=\u003cquota\u003e -h, --help help for update Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role update --from-file roles.json On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the update occurred.\nAlternatively, you can update existing roles in-line using:\n$ karavictl role update --role=role-name=system-type=000000000001=mypool=400000000  karavictl role delete Delete role\nSynopsis Delete role\nkaravictl role delete \u003crole-name\u003e [flags] Options  -h, --help help for delete Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl role delete CSISilver On success, there will be no output. You may run karavictl role get \u003crole-name\u003e to confirm the deletion occurred.\n karavictl rolebinding Manage role bindings\nSynopsis Management for role bindings\nkaravictl rolebinding [flags] Options  -h, --help help for rolebinding Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl rolebinding create Create a rolebinding between role and tenant\nSynopsis Creates a rolebinding between role and tenant\nkaravictl rolebinding create [flags] Options  -h, --help help for create -r, --role string Role name -t, --tenant string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl rolebinding create --role CSISilver --tenant Alice On success, there will be no output. You may run karavictl tenant get \u003ctenant-name\u003e to confirm the rolebinding creation occurred.\nStorage Commands karavictl storage Manage storage systems\nSynopsis Manages storage systems\nkaravictl storage [flags] Options  -h, --help help for storage Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl storage get Get details on a registered storage system.\nSynopsis Gets details on a registered storage system.\nkaravictl storage get [flags] Options  -h, --help help for get -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage get --type powerflex --system-id 3000000000011111 { \"User\": \"admin\", \"Password\": \"(omitted)\", \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true }  karavictl storage list List registered storage systems.\nSynopsis Lists registered storage systems.\nkaravictl storage list [flags] Options  -h, --help help for list Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage list { \"storage\": { \"powerflex\": { \"3000000000011111\": { \"Endpoint\": \"https://1.1.1.1\", \"Insecure\": true, \"Password\": \"(omitted)\", \"User\": \"admin\" } } } }  karavictl storage create Create and register a storage system.\nSynopsis Creates and registers a storage system.\nkaravictl storage create [flags] Options  -e, --endpoint string Endpoint of REST API gateway -h, --help help for create -i, --insecure Insecure skip verify -p, --password string Password (default \"****\") -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") -u, --user string Username (default \"admin\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage create --endpoint https://1.1.1.1 --insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the creation occurred.\n karavictl storage update Update a registered storage system.\nSynopsis Updates a registered storage system.\nkaravictl storage update [flags] Options  -e, --endpoint string Endpoint of REST API gateway -h, --help help for update -i, --insecure Insecure skip verify -p, --pass string Password (default \"****\") -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") -u, --user string Username (default \"admin\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage update --endpoint https://1.1.1.1 --insecure --system-id 3000000000011111 --type powerflex --user admin --password ******** On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the update occurred.\n karavictl storage delete Delete a registered storage system.\nSynopsis Deletes a registered storage system.\nkaravictl storage delete [flags] Options  -h, --help help for delete -s, --system-id string System identifier (default \"systemid\") -t, --type string Type of storage system (\"powerflex\", \"powermax\") Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl storage delete --type powerflex --system-id 3000000000011111 On success, there will be no output. You may run karavictl storage get --type \u003cstorage-system-type\u003e --system-id \u003cstorage-system-id\u003e to confirm the deletion occurred.\nTenant Commands karavictl tenant Manage tenants\nSynopsis Management fortenants\nkaravictl tenant [flags] Options  -h, --help help for tenant Options inherited from parent commands  --config string config file (default is $HOME/.karavictl.yaml) Output Outputs help text\n karavictl tenant create Create a tenant resource within CSM\nSynopsis Creates a tenant resource within CSM\nkaravictl tenant create [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant create --name Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the creation occurred.\n karavictl tenant get Get a tenant resource within CSM\nSynopsis Gets a tenant resource within CSM\nkaravictl tenant get [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant get --name Alice { \"name\": \"Alice\" }  karavictl tenant list Lists tenant resources within CSM\nSynopsis Lists tenant resources within CSM\nkaravictl tenant list [flags] Options  -h, --help help for create Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant list { \"tenants\": [ { \"name\": \"Alice\" } ] }  karavictl tenant delete Deletes a tenant resource within CSM\nSynopsis Deletes a tenant resource within CSM\nkaravictl tenant delete [flags] Options  -h, --help help for create -n, --name string Tenant name Options inherited from parent commands  --addr string Address of the server (default \"localhost:443\") --config string config file (default is $HOME/.karavictl.yaml) Output $ karavictl tenant delete --name Alice On success, there will be no output. You may run karavictl tenant get --name \u003ctenant-name\u003e to confirm the deletion occurred.\n","excerpt":"karavictl is a command-line interface (CLI) used to interact with and …","ref":"/csm-docs/v3/authorization/cli/","title":"CLI"},{"body":"Disaster Recovery Workflows Once the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), users can exercise the general Disaster Recovery workflows.\nPlanned Migration to the target cluster/array This scenario is the choice when you want to try your disaster recovery plan or you need to switch activities from one site to another.\na. Execute \"failover\" action on selected ReplicationGroup using the cluster name ./repctl --rg rg-id failover --target target-cluster-name b. Execute \"reprotect\" action on selected ReplicationGroup which will resume the replication from new \"source\" ./repctl --rg rg-id reprotect --at new-source-cluster-name  Unplanned Migration to the target cluster/array This scenario is the choice when you lost a site.\na. Execute \"failover\" action on selected ReplicationGroup using the cluster name ./repctl --rg rg-id failover --target target-cluster-name --unplanned b. Execute \"swap\" action on selected ReplicationGroup which would swap personalities of R1 and R2 (only applicable for PowerMax driver) ./repctl --rg rg-id swap --at target-cluster-name **Note:** Unplanned migration usually happens when the original \"source\" cluster is unavailable. The following action makes sense when the cluster is back. c. Execute \"reprotect\" action on selected ReplicationGroup which will resume the replication. ./repctl --rg rg-id reprotect --at new-source-cluster-name   Note: When users do Failover and Failback, the tests pods on the source cluster may go “CrashLoopOff” state since it will try to remount the same volume which is already mounted. To get around this problem bring down the number of replicas to 0 and then after that is done, bring it up to 1.\n ","excerpt":"Disaster Recovery Workflows Once the DellCSIReplicationGroup \u0026 …","ref":"/csm-docs/docs/replication/disaster-recovery/","title":"Disaster Recovery"},{"body":"Disaster Recovery Workflows Once the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), users can exercise the general Disaster Recovery workflows.\nPlanned Migration to the target cluster/array This scenario is the choice when you want to try your disaster recovery plan or you need to switch activities from one site to another.\na. Execute \"failover\" action on selected ReplicationGroup using the cluster name ./repctl --rg rg-id failover --target target-cluster-name b. Execute \"reprotect\" action on selected ReplicationGroup which will resume the replication from new \"source\" ./repctl --rg rg-id reprotect --at new-source-cluster-name  Unplanned Migration to the target cluster/array This scenario is the choice when you lost a site.\na. Execute \"failover\" action on selected ReplicationGroup using the cluster name ./repctl --rg rg-id failover --target target-cluster-name --unplanned b. Execute \"swap\" action on selected ReplicationGroup which would swap personalities of R1 and R2 (only applicable for PowerMax driver) ./repctl --rg rg-id swap --at target-cluster-name **Note:** Unplanned migration usually happens when the original \"source\" cluster is unavailable. The following action makes sense when the cluster is back. c. Execute \"reprotect\" action on selected ReplicationGroup which will resume the replication. ./repctl --rg rg-id reprotect --at new-source-cluster-name   Note: When users do Failover and Failback, the tests pods on the source cluster may go “CrashLoopOff” state since it will try to remount the same volume which is already mounted. To get around this problem bring down the number of replicas to 0 and then after that is done, bring it up to 1.\n ","excerpt":"Disaster Recovery Workflows Once the DellCSIReplicationGroup \u0026 …","ref":"/csm-docs/v1/replication/disaster-recovery/","title":"Disaster Recovery"},{"body":"Disaster Recovery Workflows Once the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), users can exercise the general Disaster Recovery workflows.\nPlanned Migration to the target cluster/array This scenario is the choice when you want to try your disaster recovery plan or you need to switch activities from one site to another.\na. Execute \"failover\" action on selected ReplicationGroup using the cluster name ./repctl --rg rg-id failover --target target-cluster-name b. Execute \"reprotect\" action on selected ReplicationGroup which will resume the replication from new \"source\" ./repctl --rg rg-id reprotect --at new-source-cluster-name  Unplanned Migration to the target cluster/array This scenario is the choice when you lost a site.\na. Execute \"failover\" action on selected ReplicationGroup using the cluster name ./repctl --rg rg-id failover --target target-cluster-name --unplanned b. Execute \"swap\" action on selected ReplicationGroup which would swap personalities of R1 and R2 (only applicable for PowerMax driver) ./repctl --rg rg-id swap --at target-cluster-name **Note:** Unplanned migration usually happens when the original \"source\" cluster is unavailable. The following action makes sense when the cluster is back. c. Execute \"reprotect\" action on selected ReplicationGroup which will resume the replication. ./repctl --rg rg-id reprotect --at new-source-cluster-name   Note: When users do Failover and Failback, the tests pods on the source cluster may go “CrashLoopOff” state since it will try to remount the same volume which is already mounted. To get around this problem bring down the number of replicas to 0 and then after that is done, bring it up to 1.\n ","excerpt":"Disaster Recovery Workflows Once the DellCSIReplicationGroup \u0026 …","ref":"/csm-docs/v2/replication/disaster-recovery/","title":"Disaster Recovery"},{"body":"Disaster Recovery Workflows Once the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), users can exercise the general Disaster Recovery workflows.\nPlanned Migration to the target cluster/array This scenario is the choice when you want to try your disaster recovery plan or you need to switch activities from one site to another.\na. Execute \"failover\" action on selected ReplicationGroup using the cluster name ./repctl --rg rg-id failover --to-cluster target-cluster-name b. Execute \"reprotect\" action on selected ReplicationGroup which will resume the replication from new \"source\" ./repctl --rg rg-id reprotect --to-cluster new-source-cluster-name \u003cbr\u003e \u003cbr\u003e ![state_changes1](../state_changes1.png) \u003cbr\u003e \u003cbr\u003e  Unplanned Migration to the target cluster/array This scenario is the choice when you lost a site.\na. Execute \"failover\" action on selected ReplicationGroup using the cluster name ./repctl --rg rg-id failover --to-cluster target-cluster-name --unplanned b. Execute \"swap\" action on selected ReplicationGroup which would swap personalities of R1 and R2 (only applicable for PowerMax driver) ./repctl --rg rg-id swap --to-cluster target-cluster-name **Note:** Unplanned migration usually happens when the original \"source\" cluster is unavailable. The following action makes sense when the cluster is back. c. Execute \"reprotect\" action on selected ReplicationGroup which will resume the replication. ./repctl --rg rg-id reprotect --to-cluster new-source-cluster-name \u003cbr\u003e \u003cbr\u003e ![state_changes2](../state_changes2.png) \u003cbr\u003e \u003cbr\u003e   Note: When users do Failover and Failback, the tests pods on the source cluster may go “CrashLoopOff” state since it will try to remount the same volume which is already mounted. To get around this problem bring down the number of replicas to 0 and then after that is done, bring it up to 1.\n ","excerpt":"Disaster Recovery Workflows Once the DellCSIReplicationGroup \u0026 …","ref":"/csm-docs/v3/replication/disaster-recovery/","title":"Disaster Recovery"},{"body":"","excerpt":"","ref":"/csm-docs/docs/csidriver/features/","title":"Features"},{"body":"","excerpt":"","ref":"/csm-docs/v1/csidriver/features/","title":"Features"},{"body":"","excerpt":"","ref":"/csm-docs/v2/csidriver/features/","title":"Features"},{"body":"","excerpt":"","ref":"/csm-docs/v3/csidriver/features/","title":"Features"},{"body":"Install Replication Walkthrough You can start using Container Storage Modules (CSM) for Replication with help from repctl using these simple steps:\n Prepare admin Kubernetes clusters configs Add admin configs as clusters to repctl ./repctl cluster add -f \"/root/.kube/config-1\",\"/root/.kube/config-2\" -n \"cluster-1\",\"cluster-2\"  Install replication controller and CRDs ./repctl create -f ../deploy/replicationcrds.all.yaml ./repctl create -f ../deploy/controller.yaml  NOTE: The controller will report that configmap is invalid. This is expected behavior. The message should disappear once you inject the kubeconfigs (next step).\n  (Choose one)  (More secure) Inject service accounts’ configs into clusters ./repctl cluster inject --use-sa  (Less secure) Inject admin configs into clusters ./repctl cluster inject    Modify examples/\u003cstorage\u003e_example_values.yaml config with replication information  NOTE: clusterID should match names you gave to clusters in step 2\n  Create replication storage classes using config ./repctl create sc --from-config ./examples/\u003cstorage\u003e_example_values.yaml  Install CSI driver for your chosen storage in source cluster and provision replicated volumes (optional) Create PVCs on target cluster from Replication Group ./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false    Note: all repctl output is saved alongside with repctl binary in the repctl.log file and can be attached to any installation troubleshooting requests\n ","excerpt":"Install Replication Walkthrough You can start using Container Storage …","ref":"/csm-docs/docs/replication/deployment/install-repctl/","title":"Installation using repctl"},{"body":"Install Replication Walkthrough You can start using Container Storage Modules (CSM) for Replication with help from repctl using these simple steps:\n Prepare admin Kubernetes clusters configs Add admin configs as clusters to repctl ./repctl cluster add -f \"/root/.kube/config-1\",\"/root/.kube/config-2\" -n \"cluster-1\",\"cluster-2\"  Install replication controller and CRDs ./repctl create -f ../deploy/replicationcrds.all.yaml ./repctl create -f ../deploy/controller.yaml  NOTE: The controller will report that configmap is invalid. This is expected behavior. The message should disappear once you inject the kubeconfigs (next step).\n  (Choose one)  (More secure) Inject service accounts’ configs into clusters ./repctl cluster inject --use-sa  (Less secure) Inject admin configs into clusters ./repctl cluster inject    Modify examples/\u003cstorage\u003e_example_values.yaml config with replication information  NOTE: clusterID should match names you gave to clusters in step 2\n  Create replication storage classes using config ./repctl create sc --from-config ./examples/\u003cstorage\u003e_example_values.yaml  Install CSI driver for your chosen storage in source cluster and provision replicated volumes (optional) Create PVCs on target cluster from Replication Group ./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false    Note: all repctl output is saved alongside with repctl binary in the repctl.log file and can be attached to any installation troubleshooting requests\n ","excerpt":"Install Replication Walkthrough You can start using Container Storage …","ref":"/csm-docs/v1/replication/deployment/install-repctl/","title":"Installation using repctl"},{"body":"Install Replication Walkthrough You can start using Container Storage Modules (CSM) for Replication with help from repctl using these simple steps:\n Prepare admin Kubernetes clusters configs Add admin configs as clusters to repctl ./repctl cluster add -f \"/root/.kube/config-1\",\"/root/.kube/config-2\" -n \"cluster-1\",\"cluster-2\"  Install replication controller and CRDs ./repctl create -f ../deploy/replicationcrds.all.yaml ./repctl create -f ../deploy/controller.yaml  NOTE: The controller will report that configmap is invalid. This is expected behavior. The message should disappear once you inject the kubeconfigs (next step).\n  (Choose one)  (More secure) Inject service accounts’ configs into clusters ./repctl cluster inject --use-sa  (Less secure) Inject admin configs into clusters ./repctl cluster inject    Modify examples/\u003cstorage\u003e_example_values.yaml config with replication information  NOTE: clusterID should match names you gave to clusters in step 2\n  Create replication storage classes using config ./repctl create sc --from-config ./examples/\u003cstorage\u003e_example_values.yaml  Install CSI driver for your chosen storage in source cluster and provision replicated volumes (optional) Create PVCs on target cluster from Replication Group ./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false    Note: all repctl output is saved alongside with repctl binary in the repctl.log file and can be attached to any installation troubleshooting requests\n ","excerpt":"Install Replication Walkthrough You can start using Container Storage …","ref":"/csm-docs/v2/replication/deployment/install-repctl/","title":"Installation using repctl"},{"body":"Install Replication Walkthrough You can start using Container Storage Modules (CSM) for Replication with help from repctl using these simple steps:\n Prepare admin Kubernetes clusters configs Add admin configs as clusters to repctl ./repctl cluster add -f \"/root/.kube/config-1\",\"/root/.kube/config-2\" -n \"cluster-1\",\"cluster-2\"  Install replication controller and CRDs ./repctl create -f ../deploy/replicationcrds.all.yaml ./repctl create -f ../deploy/controller.yaml  NOTE: The controller will report that configmap is invalid. This is expected behavior. The message should disappear once you inject the kubeconfigs (next step).\n  (Choose one)  (More secure) Inject service accounts’ configs into clusters ./repctl cluster inject --use-sa  (Less secure) Inject admin configs into clusters ./repctl cluster inject    Modify examples/\u003cstorage\u003e_example_values.yaml config with replication information  NOTE: clusterID should match names you gave to clusters in step 2\n  Create replication storage classes using config ./repctl create sc --from-config ./examples/\u003cstorage\u003e_example_values.yaml  Install CSI driver for your chosen storage in source cluster and provision replicated volumes (optional) Create PVCs on target cluster from Replication Group ./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false    Note: all repctl output is saved alongside with repctl binary in the repctl.log file and can be attached to any installation troubleshooting requests\n ","excerpt":"Install Replication Walkthrough You can start using Container Storage …","ref":"/csm-docs/v3/replication/deployment/install-repctl/","title":"Installation using repctl"},{"body":"Frequently Asked Questions  Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? How can I diagnose an issue with Container Storage Modules (CSM) for Observability? How can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? How can I debug and troubleshoot issues with Kubernetes? How can I troubleshoot latency problems with CSM for Observability? Why does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Why do I see FailedMount warnings when describing pods in my cluster?  Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? This issue can arise when the topology service manifest is updated to expose the service as NodePort and a client makes a request to the service. Karavi-toplogy is configured with a self-signed or custom certificate and when a client does not recognize a server’s certificate, it shows an error and pings the server(karavi-topology) with the error. You would see the issue when accessing the service through a browser or curl:\nBrowser experience A user who tries to connect to karavi-topology on any browser may receive an error/warning message about the certificate. The message may vary depending on the browser. For instance, in Internet Explorer, you’ll see:\nThere is a problem with this website's security certificate. The security certificate presented by this website was not issued by a trusted certificate authority While this certificate problem may indicate an attempt to fool you or intercept data you send to the server, see resolution on how to fix it\nCurl experience A user who tries to connect to karavi-topology by using curl may receive the following warning or error message:\n[root@:~]$ curl -v https://\u003ckaravi-topology-cluster-IP\u003e:\u003cport?/query * Trying ***********... * TCP_NODELAY set * Connected to *********** (***********) port 31433 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/certs/ca-certificates.crt CApath: /etc/ssl/certs * TLSv1.3 (OUT), TLS handshake, Client hello (1): * TLSv1.3 (IN), TLS handshake, Server hello (2): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Unknown (8): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Certificate (11): * TLSv1.3 (OUT), TLS alert, Server hello (2): * SSL certificate problem: unable to get local issuer certificate * stopped the pause stream! * Closing connection 0 curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.haxx.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. Kubernetes Admin experience Due to the error above, the client pings the topology server with a TLS handshake error which is logged in karavi-topology pod. For instance,\n[root@:~]$ kubectl logs -n powerflex karavi-topology-5d4669d6dd-trzxw 2021/04/27 09:38:28 Set DriverNames to [csi-vxflexos.dellemc.com] 2021/04/28 07:15:05 http: TLS handshake error from 10.42.0.0:58450: local error: tls: bad record MAC 2021/04/28 07:16:14 http: TLS handshake error from 10.42.0.0:55311: local error: tls: bad record MAC Resolution To resolve this issue, we need to configure the client to be aware of the karavi-topology certificate (this includes all custom SSL certificate that are not issued from a trusted Certificate Authority (CA))\nGet a copy of the certificate used by karavi-topology If we supplied a custom certificate during installing karavi-topology, we can simply open the .crt and copy the text. However, if it was assigned by cert-manager, you can get a copy of the certificate by running the following kubectl command on the clusters.\n[root@:~]$ kubectl -n \u003cnamespace\u003e get secret karavi-topology-tls -o jsonpath='{.data.tls\\.crt}' | base64 -d -----BEGIN CERTIFICATE----- RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe -----END CERTIFICATE----- Configure your client to accept the above certificate A workaround on most browsers is to accept the karavi-topology certificate by clicking Continue to this website (not recommended). This will make all other successive communication to not cause any certificate error. Anyhow, you will need to read the documentation for your specific client to configure the above certificate. For Grafana, here are two ways to configure the karavi-topology datasource to use the above certificate:\n Deploy certificate with new Grafana instance Please follow the steps in Sample Grafana Deployment but attach the certificate to your `grafana-values.yaml` before deploying. The file should look like: # grafana-values.yaml image:repository:grafana/grafanatag:7.3.0sha:\"\"pullPolicy:IfNotPresentservice:type:NodePort## Administrator credentials when not using an existing SecretadminUser:adminadminPassword:admin## Pass the plugins you want installed as a list.##plugins:- grafana-simple-json-datasource- briangann-datatable-panel- grafana-piechart-panel## Configure grafana datasources## ref: http://docs.grafana.org/administration/provisioning/#datasources##datasources:datasources.yaml:apiVersion:1datasources:- name:Karavi-Topologytype:grafana-simple-json-datasourceaccess:proxyurl:'https://karavi-topology:8443'isDefault:nullversion:1editable:truejsonData:tlsAuthWithCACert:truesecureJsonData:tlsCaCert:| -----BEGIN CERTIFICATE-----RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe-----ENDCERTIFICATE----- - name: Prometheustype:prometheusaccess:proxyurl:'http://prometheus:9090'isDefault:nullversion:1editable:truetestFramework:enabled:falsesidecar:datasources:enabled:truedashboards:enabled:true## Additional grafana server ConfigMap mounts## Defines additional mounts with ConfigMap. CofigMap must be manually created in the namespace.extraConfigmapMounts:[]   Add certificate to an existing Grafana instance - This only happens if you configure jsonData to not skip tls verification. If this is the case, you'll need to re-deploy grafana as shown above or, form Grafana UI, edit Karavi-Topology datasource to use the certificate. To do the latter:  Visit your Grafana UI on a browser Navigate to setting and go to Data Sources Click on Karavi-Topology Ensure that Skip TLS Verify is already off Switch on With CA Cert Copy the above certificate into the TLS Auth Details text box that appears Click Save \u0026 Test and validate that everything is working fine when a green bar showing Data source is working appears   How can I diagnose an issue with CSM for Observability? Once you have attempted to install CSM for Observability to your Kubernetes or OpenShift cluster, the first step in troubleshooting is locating the problem.\nGet information on the state of your Pods.\nkubectl get pods -n $namespace Get verbose output of the current state of a Pod.\nkubectl describe pod -n $namespace $pod How can I view logs? View pod container logs. Output logs to a file for further debugging.\nkubectl logs -n $namespace $pod $container kubectl logs -n $namespace $pod $container \u003e $logFileName More information for viewing logs can be found here.\nHow can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? The ServiceMonitor allows us to define how a set of services should be monitored by Prometheus. Please see our prometheus documentation for creating a ServiceMonitor.\nHow can I debug and troubleshoot issues with Kubernetes?   To debug your application that may not be behaving correctly, please reference Kubernetes troubleshooting applications guide.\n  For tips on debugging your cluster, please see this troubleshooting guide.\n  How can I troubleshoot latency problems with CSM for Observability? CSM for Observability is instrumented to report trace data to Zipkin. Please see Tracing for more information on enabling tracing for CSM for Observability.\nWhy does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Check the pods in the CSM for Observability namespace. If the pod starting with ‘karavi-observability-cert-manager-cainjector-*’ is in ‘CrashLoopBackOff’ or ‘Error” stage with a number of restarts, check if the logs for that pod show the below error:\nkubectl logs -n $namespace $cert-manager-cainjector-podname error registering secret controller: no matches for kind \"MutatingWebhookConfiguration\" in version \"admissionregistration.k8s.io/v1beta1\" If the Kubernetes cluster version is 1.22.2 (or higher), this error is due to an incompatible cert-manager version. Please upgrade to the latest CSM for Observability release (v1.0.1 or higher).\nWhy do I see FailedMount warnings when describing pods in my cluster? The warning can arise when a self-signed certificate for otel-collector is issued. It takes a few minutes or less for the signed certificate to generate and be consumed in the namespace. Once the certificate is consumed, the FailedMount warnings are resolved and the containers start properly.\n[root@:~]$ kubectl describe pod -n $namespace $pod MountVolume.SetUp failed for volume \"tls-secret\" : secret \"otel-collector-tls\" not found Unable to attach or mount volumes: unmounted volumes=[tls-secret], unattached volumes=[vxflexos-config-params vxflexos-config tls-secret karavi-metrics-powerflex-configmap kube-api-access-4fqgl karavi-authorization-config proxy-server-root-certificate]: timed out waiting for the condition ","excerpt":"Frequently Asked Questions  Why do I see a certificate problem when …","ref":"/csm-docs/docs/observability/troubleshooting/","title":"Troubleshooting"},{"body":"Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate troubleshooting. If you experience a problem with CSM for Resiliency it is important you provide us with as much information as possible so that we can diagnose the issue and improve CSM for Resiliency. Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate sending us the logs and other information needed to diagnose a problem.\nMonitoring Protected Pods and Node Status There are two tools for monitoring the status of protected pods and nodes.\nThe mon.sh script displays the following information every 5 seconds:\n The date and time. A list of the nodes and their status. A list of the taints applied to each node. A list of the leases in the CSI driver’s namespace. (Edit the script to change the CSI driver namespace if necessary. It defaults to vxflexos as the driver namespace.) A list of the CSI driver pods and their status (defaults to vxflexos namespace.) A list of the protected pods and their status. (Edit the script if you do not use the default podmon label key.)  For systems with many protected pods, the monx.sh may provide a more usable output format. It displays the following fields every 5 seconds:\n The date and time. A list of the nodes and their status. A list of the taints applied to each node. A summary for each node hosting protected pods of the number of pods in various states such as the Running, Creating, and Error states. (Edit the script if you do not use the default podmon label key.) A list of the protected pods not in the Running state.  Collecting Logs If you have a problem with CSM for Resiliency it’s best to collect the logs to help with diagnosis. This tool can also be used to collect logs to submit as part of an issue to help us diagnose. Please use the collect_logs.sh. Type “collect_logs.sh –help” for help on the arguments.\nThe script collects the following information:\n A list of the driver pods. A list of the protected pods. The podmon container logs for each of the driver pods. The driver container logs for each of the driver pods. For each namespace containing protected pods, the recent events logged in that namespace.  After successful execution of the script, it will deposit a file similar to driver.logs.20210319_1407.tgz in the current directory. Please submit that file with any issues.\nActions to take during failure to clean pod resources completely The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint.\n","excerpt":"Some tools have been provided in the tools directory that will help …","ref":"/csm-docs/docs/resiliency/troubleshooting/","title":"Troubleshooting"},{"body":"Frequently Asked Questions  Why does the installation fail due to an invalid cipherKey value? Why does the cluster-init pod show the error “cluster has already been initialized”? Why does the precheck fail when creating an application? How can I view detailed logs for the CSM Installer? After deleting an application, why can’t I re-create the same application?  Why does the installation fail due to an invalid cipherKey value? The cipherKey value used during deployment of the CSM Installer must be exactly 32 characters in length and contained within quotes.\nWhy does the cluster-init pod show the error “cluster has already been initialized”? During the initial start-up of the CSM Installer, the database will be initialized by the cluster-init job. If the CSM Installer is uninstalled and then re-installed on the same cluster, this error may be shown due to the Persistent Volume for the database already containing an initialized database. The CSM Installer will function as normal and the cluster-init job can be ignored.\nIf a clean installation of the CSM Installer is required, the dbVolumeDirectory (default location /var/lib/cockroachdb) must be deleted from the worker node which is hosting the Persistent Volume. After this directory is deleted, the CSM Installer can be re-installed.\nCaution: Deleting the dbVolumeDirectory location will remove any data persisted by the CSM Installer including clusters, storage systems, and installed applications.\nWhy does the precheck fail when creating an application? Each CSI Driver and CSM Module has required software or CRDs that must be installed before the application can be deployed in the cluster. These prechecks are verified when the csm create application command is executed. If the error message “create application failed” is displayed, review the CSM Installer logs to view details about the failed prechecks.\nIf the precheck fails due to required software (e.g. iSCSI, NFS, SDC) not installed on the cluster nodes, follow these steps to address the issue:\n Delete the cluster from the CSM Installer using the csm delete cluster command. Update the nodes in the cluster by installing required software. Add the cluster to the CSM Installer using the csm add cluster command.  How can I view detailed logs for the CSM Installer? Detailed logs of the CSM Installer can be displayed using the following command:\nkubectl logs -f -n \u003cnamespace\u003e deploy/dell-csm-installer After deleting an application, why can’t I re-create the same application? After deleting an application using the csm delete application command, the namespace and other non-application resources including Secrets are not deleted from the cluster. This is to prevent removing any resources that may not have been created by the CSM Installer. The namespace must be manually deleted before attempting to re-create the same application using the CSM Installer.\n","excerpt":"Frequently Asked Questions  Why does the installation fail due to an …","ref":"/csm-docs/v1/deployment/troubleshooting/","title":"Troubleshooting"},{"body":"Frequently Asked Questions  Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? How can I diagnose an issue with Container Storage Modules (CSM) for Observability? How can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? How can I debug and troubleshoot issues with Kubernetes? How can I troubleshoot latency problems with CSM for Observability? Why does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Why do I see FailedMount warnings when describing pods in my cluster?  Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? This issue can arise when the topology service manifest is updated to expose the service as NodePort and a client makes a request to the service. Karavi-toplogy is configured with a self-signed or custom certificate and when a client does not recognize a server’s certificate, it shows an error and pings the server(karavi-topology) with the error. You would see the issue when accessing the service through a browser or curl:\nBrowser experience A user who tries to connect to karavi-topology on any browser may receive an error/warning message about the certificate. The message may vary depending on the browser. For instance, in Internet Explorer, you’ll see:\nThere is a problem with this website's security certificate. The security certificate presented by this website was not issued by a trusted certificate authority While this certificate problem may indicate an attempt to fool you or intercept data you send to the server, see resolution on how to fix it\nCurl experience A user who tries to connect to karavi-topology by using curl may receive the following warning or error message:\n[root@:~]$ curl -v https://\u003ckaravi-topology-cluster-IP\u003e:\u003cport?/query * Trying ***********... * TCP_NODELAY set * Connected to *********** (***********) port 31433 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/certs/ca-certificates.crt CApath: /etc/ssl/certs * TLSv1.3 (OUT), TLS handshake, Client hello (1): * TLSv1.3 (IN), TLS handshake, Server hello (2): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Unknown (8): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Certificate (11): * TLSv1.3 (OUT), TLS alert, Server hello (2): * SSL certificate problem: unable to get local issuer certificate * stopped the pause stream! * Closing connection 0 curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.haxx.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. Kubernetes Admin experience Due to the error above, the client pings the topology server with a TLS handshake error which is logged in karavi-topology pod. For instance,\n[root@:~]$ kubectl logs -n powerflex karavi-topology-5d4669d6dd-trzxw 2021/04/27 09:38:28 Set DriverNames to [csi-vxflexos.dellemc.com] 2021/04/28 07:15:05 http: TLS handshake error from 10.42.0.0:58450: local error: tls: bad record MAC 2021/04/28 07:16:14 http: TLS handshake error from 10.42.0.0:55311: local error: tls: bad record MAC Resolution To resolve this issue, we need to configure the client to be aware of the karavi-topology certificate (this includes all custom SSL certificate that are not issued from a trusted Certificate Authority (CA))\nGet a copy of the certificate used by karavi-topology If we supplied a custom certificate during installing karavi-topology, we can simply open the .crt and copy the text. However, if it was assigned by cert-manager, you can get a copy of the certificate by running the following kubectl command on the clusters.\n[root@:~]$ kubectl -n \u003cnamespace\u003e get secret karavi-topology-tls -o jsonpath='{.data.tls\\.crt}' | base64 -d -----BEGIN CERTIFICATE----- RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe -----END CERTIFICATE----- Configure your client to accept the above certificate A workaround on most browsers is to accept the karavi-topology certificate by clicking Continue to this website (not recommended). This will make all other successive communication to not cause any certificate error. Anyhow, you will need to read the documentation for your specific client to configure the above certificate. For Grafana, here are two ways to configure the karavi-topology datasource to use the above certificate:\n Deploy certificate with new Grafana instance Please follow the steps in Sample Grafana Deployment but attach the certificate to your `grafana-values.yaml` before deploying. The file should look like: # grafana-values.yaml image:repository:grafana/grafanatag:7.3.0sha:\"\"pullPolicy:IfNotPresentservice:type:NodePort## Administrator credentials when not using an existing SecretadminUser:adminadminPassword:admin## Pass the plugins you want installed as a list.##plugins:- grafana-simple-json-datasource- briangann-datatable-panel- grafana-piechart-panel## Configure grafana datasources## ref: http://docs.grafana.org/administration/provisioning/#datasources##datasources:datasources.yaml:apiVersion:1datasources:- name:Karavi-Topologytype:grafana-simple-json-datasourceaccess:proxyurl:'https://karavi-topology:8443'isDefault:nullversion:1editable:truejsonData:tlsAuthWithCACert:truesecureJsonData:tlsCaCert:| -----BEGIN CERTIFICATE-----RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe-----ENDCERTIFICATE----- - name: Prometheustype:prometheusaccess:proxyurl:'http://prometheus:9090'isDefault:nullversion:1editable:truetestFramework:enabled:falsesidecar:datasources:enabled:truedashboards:enabled:true## Additional grafana server ConfigMap mounts## Defines additional mounts with ConfigMap. CofigMap must be manually created in the namespace.extraConfigmapMounts:[]   Add certificate to an existing Grafana instance - This only happens if you configure jsonData to not skip tls verification. If this is the case, you'll need to re-deploy grafana as shown above or, form Grafana UI, edit Karavi-Topology datasource to use the certificate. To do the latter:  Visit your Grafana UI on a browser Navigate to setting and go to Data Sources Click on Karavi-Topology Ensure that Skip TLS Verify is already off Switch on With CA Cert Copy the above certificate into the TLS Auth Details text box that appears Click Save \u0026 Test and validate that everything is working fine when a green bar showing Data source is working appears   How can I diagnose an issue with CSM for Observability? Once you have attempted to install CSM for Observability to your Kubernetes or OpenShift cluster, the first step in troubleshooting is locating the problem.\nGet information on the state of your Pods.\nkubectl get pods -n $namespace Get verbose output of the current state of a Pod.\nkubectl describe pod -n $namespace $pod How can I view logs? View pod container logs. Output logs to a file for further debugging.\nkubectl logs -n $namespace $pod $container kubectl logs -n $namespace $pod $container \u003e $logFileName More information for viewing logs can be found here.\nHow can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? The ServiceMonitor allows us to define how a set of services should be monitored by Prometheus. Please see our prometheus documentation for creating a ServiceMonitor.\nHow can I debug and troubleshoot issues with Kubernetes?   To debug your application that may not be behaving correctly, please reference Kubernetes troubleshooting applications guide.\n  For tips on debugging your cluster, please see this troubleshooting guide.\n  How can I troubleshoot latency problems with CSM for Observability? CSM for Observability is instrumented to report trace data to Zipkin. Please see Tracing for more information on enabling tracing for CSM for Observability.\nWhy does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Check the pods in the CSM for Observability namespace. If the pod starting with ‘karavi-observability-cert-manager-cainjector-*’ is in ‘CrashLoopBackOff’ or ‘Error” stage with a number of restarts, check if the logs for that pod show the below error:\nkubectl logs -n $namespace $cert-manager-cainjector-podname error registering secret controller: no matches for kind \"MutatingWebhookConfiguration\" in version \"admissionregistration.k8s.io/v1beta1\" If the Kubernetes cluster version is 1.22.2 (or higher), this error is due to an incompatible cert-manager version. Please upgrade to the latest CSM for Observability release (v1.0.1 or higher).\nWhy do I see FailedMount warnings when describing pods in my cluster? The warning can arise when a self-signed certificate for otel-collector is issued. It takes a few minutes or less for the signed certificate to generate and be consumed in the namespace. Once the certificate is consumed, the FailedMount warnings are resolved and the containers start properly.\n[root@:~]$ kubectl describe pod -n $namespace $pod MountVolume.SetUp failed for volume \"tls-secret\" : secret \"otel-collector-tls\" not found Unable to attach or mount volumes: unmounted volumes=[tls-secret], unattached volumes=[vxflexos-config-params vxflexos-config tls-secret karavi-metrics-powerflex-configmap kube-api-access-4fqgl karavi-authorization-config proxy-server-root-certificate]: timed out waiting for the condition ","excerpt":"Frequently Asked Questions  Why do I see a certificate problem when …","ref":"/csm-docs/v1/observability/troubleshooting/","title":"Troubleshooting"},{"body":"Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate troubleshooting. If you experience a problem with CSM for Resiliency it is important you provide us with as much information as possible so that we can diagnose the issue and improve CSM for Resiliency. Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate sending us the logs and other information needed to diagnose a problem.\nMonitoring Protected Pods and Node Status There are two tools for monitoring the status of protected pods and nodes.\nThe mon.sh script displays the following information every 5 seconds:\n The date and time. A list of the nodes and their status. A list of the taints applied to each node. A list of the leases in the CSI driver’s namespace. (Edit the script to change the CSI driver namespace if necessary. It defaults to vxflexos as the driver namespace.) A list of the CSI driver pods and their status (defaults to vxflexos namespace.) A list of the protected pods and their status. (Edit the script if you do not use the default podmon label key.)  For systems with many protected pods, the monx.sh may provide a more usable output format. It displays the following fields every 5 seconds:\n The date and time. A list of the nodes and their status. A list of the taints applied to each node. A summary for each node hosting protected pods of the number of pods in various states such as the Running, Creating, and Error states. (Edit the script if you do not use the default podmon label key.) A list of the protected pods not in the Running state.  Collecting Logs If you have a problem with CSM for Resiliency it’s best to collect the logs to help with diagnosis. This tool can also be used to collect logs to submit as part of an issue to help us diagnose. Please use the collect_logs.sh. Type “collect_logs.sh –help” for help on the arguments.\nThe script collects the following information:\n A list of the driver pods. A list of the protected pods. The podmon container logs for each of the driver pods. The driver container logs for each of the driver pods. For each namespace containing protected pods, the recent events logged in that namespace.  After successful execution of the script, it will deposit a file similar to driver.logs.20210319_1407.tgz in the current directory. Please submit that file with any issues.\nActions to take during failure to clean pod resources completely The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint.\n","excerpt":"Some tools have been provided in the tools directory that will help …","ref":"/csm-docs/v1/resiliency/troubleshooting/","title":"Troubleshooting"},{"body":"Frequently Asked Questions  Why does the installation fail due to an invalid cipherKey value? Why does the cluster-init pod show the error “cluster has already been initialized”? Why does the precheck fail when creating an application? How can I view detailed logs for the CSM Installer? After deleting an application, why can’t I re-create the same application?  Why does the installation fail due to an invalid cipherKey value? The cipherKey value used during deployment of the CSM Installer must be exactly 32 characters in length and contained within quotes.\nWhy does the cluster-init pod show the error “cluster has already been initialized”? During the initial start-up of the CSM Installer, the database will be initialized by the cluster-init job. If the CSM Installer is uninstalled and then re-installed on the same cluster, this error may be shown due to the Persistent Volume for the database already containing an initialized database. The CSM Installer will function as normal and the cluster-init job can be ignored.\nIf a clean installation of the CSM Installer is required, the dbVolumeDirectory (default location /var/lib/cockroachdb) must be deleted from the worker node which is hosting the Persistent Volume. After this directory is deleted, the CSM Installer can be re-installed.\nCaution: Deleting the dbVolumeDirectory location will remove any data persisted by the CSM Installer including clusters, storage systems, and installed applications.\nWhy does the precheck fail when creating an application? Each CSI Driver and CSM Module has required software or CRDs that must be installed before the application can be deployed in the cluster. These prechecks are verified when the csm create application command is executed. If the error message “create application failed” is displayed, review the CSM Installer logs to view details about the failed prechecks.\nIf the precheck fails due to required software (e.g. iSCSI, NFS, SDC) not installed on the cluster nodes, follow these steps to address the issue:\n Delete the cluster from the CSM Installer using the csm delete cluster command. Update the nodes in the cluster by installing required software. Add the cluster to the CSM Installer using the csm add cluster command.  How can I view detailed logs for the CSM Installer? Detailed logs of the CSM Installer can be displayed using the following command:\nkubectl logs -f -n \u003cnamespace\u003e deploy/dell-csm-installer After deleting an application, why can’t I re-create the same application? After deleting an application using the csm delete application command, the namespace and other non-application resources including Secrets are not deleted from the cluster. This is to prevent removing any resources that may not have been created by the CSM Installer. The namespace must be manually deleted before attempting to re-create the same application using the CSM Installer.\n","excerpt":"Frequently Asked Questions  Why does the installation fail due to an …","ref":"/csm-docs/v2/deployment/troubleshooting/","title":"Troubleshooting"},{"body":"Frequently Asked Questions  Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? How can I diagnose an issue with Container Storage Modules (CSM) for Observability? How can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? How can I debug and troubleshoot issues with Kubernetes? How can I troubleshoot latency problems with CSM for Observability? Why does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Why do I see FailedMount warnings when describing pods in my cluster?  Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? This issue can arise when the topology service manifest is updated to expose the service as NodePort and a client makes a request to the service. Karavi-toplogy is configured with a self-signed or custom certificate and when a client does not recognize a server’s certificate, it shows an error and pings the server(karavi-topology) with the error. You would see the issue when accessing the service through a browser or curl:\nBrowser experience A user who tries to connect to karavi-topology on any browser may receive an error/warning message about the certificate. The message may vary depending on the browser. For instance, in Internet Explorer, you’ll see:\nThere is a problem with this website's security certificate. The security certificate presented by this website was not issued by a trusted certificate authority While this certificate problem may indicate an attempt to fool you or intercept data you send to the server, see resolution on how to fix it\nCurl experience A user who tries to connect to karavi-topology by using curl may receive the following warning or error message:\n[root@:~]$ curl -v https://\u003ckaravi-topology-cluster-IP\u003e:\u003cport?/query * Trying ***********... * TCP_NODELAY set * Connected to *********** (***********) port 31433 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/certs/ca-certificates.crt CApath: /etc/ssl/certs * TLSv1.3 (OUT), TLS handshake, Client hello (1): * TLSv1.3 (IN), TLS handshake, Server hello (2): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Unknown (8): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Certificate (11): * TLSv1.3 (OUT), TLS alert, Server hello (2): * SSL certificate problem: unable to get local issuer certificate * stopped the pause stream! * Closing connection 0 curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.haxx.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. Kubernetes Admin experience Due to the error above, the client pings the topology server with a TLS handshake error which is logged in karavi-topology pod. For instance,\n[root@:~]$ kubectl logs -n powerflex karavi-topology-5d4669d6dd-trzxw 2021/04/27 09:38:28 Set DriverNames to [csi-vxflexos.dellemc.com] 2021/04/28 07:15:05 http: TLS handshake error from 10.42.0.0:58450: local error: tls: bad record MAC 2021/04/28 07:16:14 http: TLS handshake error from 10.42.0.0:55311: local error: tls: bad record MAC Resolution To resolve this issue, we need to configure the client to be aware of the karavi-topology certificate (this includes all custom SSL certificate that are not issued from a trusted Certificate Authority (CA))\nGet a copy of the certificate used by karavi-topology If we supplied a custom certificate during installing karavi-topology, we can simply open the .crt and copy the text. However, if it was assigned by cert-manager, you can get a copy of the certificate by running the following kubectl command on the clusters.\n[root@:~]$ kubectl -n \u003cnamespace\u003e get secret karavi-topology-tls -o jsonpath='{.data.tls\\.crt}' | base64 -d -----BEGIN CERTIFICATE----- RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe -----END CERTIFICATE----- Configure your client to accept the above certificate A workaround on most browsers is to accept the karavi-topology certificate by clicking Continue to this website (not recommended). This will make all other successive communication to not cause any certificate error. Anyhow, you will need to read the documentation for your specific client to configure the above certificate. For Grafana, here are two ways to configure the karavi-topology datasource to use the above certificate:\n Deploy certificate with new Grafana instance Please follow the steps in Sample Grafana Deployment but attach the certificate to your `grafana-values.yaml` before deploying. The file should look like: # grafana-values.yaml image:repository:grafana/grafanatag:7.3.0sha:\"\"pullPolicy:IfNotPresentservice:type:NodePort## Administrator credentials when not using an existing SecretadminUser:adminadminPassword:admin## Pass the plugins you want installed as a list.##plugins:- grafana-simple-json-datasource- briangann-datatable-panel- grafana-piechart-panel## Configure grafana datasources## ref: http://docs.grafana.org/administration/provisioning/#datasources##datasources:datasources.yaml:apiVersion:1datasources:- name:Karavi-Topologytype:grafana-simple-json-datasourceaccess:proxyurl:'https://karavi-topology:8443'isDefault:nullversion:1editable:truejsonData:tlsAuthWithCACert:truesecureJsonData:tlsCaCert:| -----BEGIN CERTIFICATE-----RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe-----ENDCERTIFICATE----- - name: Prometheustype:prometheusaccess:proxyurl:'http://prometheus:9090'isDefault:nullversion:1editable:truetestFramework:enabled:falsesidecar:datasources:enabled:truedashboards:enabled:true## Additional grafana server ConfigMap mounts## Defines additional mounts with ConfigMap. CofigMap must be manually created in the namespace.extraConfigmapMounts:[]   Add certificate to an existing Grafana instance - This only happens if you configure jsonData to not skip tls verification. If this is the case, you'll need to re-deploy grafana as shown above or, form Grafana UI, edit Karavi-Topology datasource to use the certificate. To do the latter:  Visit your Grafana UI on a browser Navigate to setting and go to Data Sources Click on Karavi-Topology Ensure that Skip TLS Verify is already off Switch on With CA Cert Copy the above certificate into the TLS Auth Details text box that appears Click Save \u0026 Test and validate that everything is working fine when a green bar showing Data source is working appears   How can I diagnose an issue with CSM for Observability? Once you have attempted to install CSM for Observability to your Kubernetes or OpenShift cluster, the first step in troubleshooting is locating the problem.\nGet information on the state of your Pods.\nkubectl get pods -n $namespace Get verbose output of the current state of a Pod.\nkubectl describe pod -n $namespace $pod How can I view logs? View pod container logs. Output logs to a file for further debugging.\nkubectl logs -n $namespace $pod $container kubectl logs -n $namespace $pod $container \u003e $logFileName More information for viewing logs can be found here.\nHow can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? The ServiceMonitor allows us to define how a set of services should be monitored by Prometheus. Please see our prometheus documentation for creating a ServiceMonitor.\nHow can I debug and troubleshoot issues with Kubernetes?   To debug your application that may not be behaving correctly, please reference Kubernetes troubleshooting applications guide.\n  For tips on debugging your cluster, please see this troubleshooting guide.\n  How can I troubleshoot latency problems with CSM for Observability? CSM for Observability is instrumented to report trace data to Zipkin. Please see Tracing for more information on enabling tracing for CSM for Observability.\nWhy does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Check the pods in the CSM for Observability namespace. If the pod starting with ‘karavi-observability-cert-manager-cainjector-*’ is in ‘CrashLoopBackOff’ or ‘Error” stage with a number of restarts, check if the logs for that pod show the below error:\nkubectl logs -n $namespace $cert-manager-cainjector-podname error registering secret controller: no matches for kind \"MutatingWebhookConfiguration\" in version \"admissionregistration.k8s.io/v1beta1\" If the Kubernetes cluster version is 1.22.2 (or higher), this error is due to an incompatible cert-manager version. Please upgrade to the latest CSM for Observability release (v1.0.1 or higher).\nWhy do I see FailedMount warnings when describing pods in my cluster? The warning can arise when a self-signed certificate for otel-collector is issued. It takes a few minutes or less for the signed certificate to generate and be consumed in the namespace. Once the certificate is consumed, the FailedMount warnings are resolved and the containers start properly.\n[root@:~]$ kubectl describe pod -n $namespace $pod MountVolume.SetUp failed for volume \"tls-secret\" : secret \"otel-collector-tls\" not found Unable to attach or mount volumes: unmounted volumes=[tls-secret], unattached volumes=[vxflexos-config-params vxflexos-config tls-secret karavi-metrics-powerflex-configmap kube-api-access-4fqgl karavi-authorization-config proxy-server-root-certificate]: timed out waiting for the condition ","excerpt":"Frequently Asked Questions  Why do I see a certificate problem when …","ref":"/csm-docs/v2/observability/troubleshooting/","title":"Troubleshooting"},{"body":"Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate troubleshooting. If you experience a problem with CSM for Resiliency it is important you provide us with as much information as possible so that we can diagnose the issue and improve CSM for Resiliency. Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate sending us the logs and other information needed to diagnose a problem.\nMonitoring Protected Pods and Node Status There are two tools for monitoring the status of protected pods and nodes.\nThe mon.sh script displays the following information every 5 seconds:\n The date and time. A list of the nodes and their status. A list of the taints applied to each node. A list of the leases in the CSI driver’s namespace. (Edit the script to change the CSI driver namespace if necessary. It defaults to vxflexos as the driver namespace.) A list of the CSI driver pods and their status (defaults to vxflexos namespace.) A list of the protected pods and their status. (Edit the script if you do not use the default podmon label key.)  For systems with many protected pods, the monx.sh may provide a more usable output format. It displays the following fields every 5 seconds:\n The date and time. A list of the nodes and their status. A list of the taints applied to each node. A summary for each node hosting protected pods of the number of pods in various states such as the Running, Creating, and Error states. (Edit the script if you do not use the default podmon label key.) A list of the protected pods not in the Running state.  Collecting Logs If you have a problem with CSM for Resiliency it’s best to collect the logs to help with diagnosis. This tool can also be used to collect logs to submit as part of an issue to help us diagnose. Please use the collect_logs.sh. Type “collect_logs.sh –help” for help on the arguments.\nThe script collects the following information:\n A list of the driver pods. A list of the protected pods. The podmon container logs for each of the driver pods. The driver container logs for each of the driver pods. For each namespace containing protected pods, the recent events logged in that namespace.  After successful execution of the script, it will deposit a file similar to driver.logs.20210319_1407.tgz in the current directory. Please submit that file with any issues.\nActions to take during failure to clean pod resources completely The node-podmon cleanup algorithm purposefully will not remove the node taint until all the protected volumes have been cleaned up from the node. This works well if the node fault lasts long enough that controller-podmon can evacuate all the protected pods from the node. However, if the failure is short-lived, and controller-podmon does not clean up all the protected pods on the node, or if for some reason node-podmon cannot clean a pod completely, the taint is left on the node, and manual intervention is required. The required intervention is for the operator to reboot the node, which will ensure that no zombie pods survive. Upon seeing the reboot, node-podmon will then remove the taint.\n","excerpt":"Some tools have been provided in the tools directory that will help …","ref":"/csm-docs/v2/resiliency/troubleshooting/","title":"Troubleshooting"},{"body":"Frequently Asked Questions  Why does the installation fail due to an invalid cipherKey value? Why does the cluster-init pod show the error “cluster has already been initialized”? Why does the precheck fail when creating an application? How can I view detailed logs for the CSM Installer? After deleting an application, why can’t I re-create the same application?  Why does the installation fail due to an invalid cipherKey value? The cipherKey value used during deployment of the CSM Installer must be exactly 32 characters in length and contained within quotes.\nWhy does the cluster-init pod show the error “cluster has already been initialized”? During the initial start-up of the CSM Installer, the database will be initialized by the cluster-init job. If the CSM Installer is uninstalled and then re-installed on the same cluster, this error may be shown due to the Persistent Volume for the database already containing an initialized database. The CSM Installer will function as normal and the cluster-init job can be ignored.\nIf a clean installation of the CSM Installer is required, the dbVolumeDirectory (default location /var/lib/cockroachdb) must be deleted from the worker node which is hosting the Persistent Volume. After this directory is deleted, the CSM Installer can be re-installed.\nCaution: Deleting the dbVolumeDirectory location will remove any data persisted by the CSM Installer including clusters, storage systems, and installed applications.\nWhy does the precheck fail when creating an application? Each CSI Driver and CSM Module has required software or CRDs that must be installed before the application can be deployed in the cluster. These prechecks are verified when the csm create application command is executed. If the error message “create application failed” is displayed, review the CSM Installer logs to view details about the failed prechecks.\nIf the precheck fails due to required software (e.g. iSCSI, NFS, SDC) not installed on the cluster nodes, follow these steps to address the issue:\n Delete the cluster from the CSM Installer using the csm delete cluster command. Update the nodes in the cluster by installing required software. Add the cluster to the CSM Installer using the csm add cluster command.  How can I view detailed logs for the CSM Installer? Detailed logs of the CSM Installer can be displayed using the following command:\nkubectl logs -f -n \u003cnamespace\u003e deploy/dell-csm-installer After deleting an application, why can’t I re-create the same application? After deleting an application using the csm delete application command, the namespace and other non-application resources including Secrets are not deleted from the cluster. This is to prevent removing any resources that may not have been created by the CSM Installer. The namespace must be manually deleted before attempting to re-create the same application using the CSM Installer.\n","excerpt":"Frequently Asked Questions  Why does the installation fail due to an …","ref":"/csm-docs/v3/deployment/troubleshooting/","title":"Troubleshooting"},{"body":"Frequently Asked Questions  Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? How can I diagnose an issue with Container Storage Modules (CSM) for Observability? How can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? How can I debug and troubleshoot issues with Kubernetes? How can I troubleshoot latency problems with CSM for Observability? Why does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage?  Why do I see a certificate problem when accessing the topology service outside of my Kubernetes cluster? This issue can arise when the topology service manifest is updated to expose the service as NodePort and a client makes a request to the service. Karavi-toplogy is configured with a self-signed or custom certificate and when a client does not recognize a server’s certificate, it shows an error and pings the server(karavi-topology) with the error. You would see the issue when accessing the service through a browser or curl:\nBrowser experience A user who tries to connect to karavi-topology on any browser may receive an error/warning message about the certificate. The message may vary depending on the browser. For instance, in Internet Explorer, you’ll see:\nThere is a problem with this website's security certificate. The security certificate presented by this website was not issued by a trusted certificate authority While this certificate problem may indicate an attempt to fool you or intercept data you send to the server, see resolution on how to fix it\nCurl experience A user who tries to connect to karavi-topology by using curl may receive the following warning or error message:\n[root@:~]$ curl -v https://\u003ckaravi-topology-cluster-IP\u003e:\u003cport?/query * Trying ***********... * TCP_NODELAY set * Connected to *********** (***********) port 31433 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/certs/ca-certificates.crt CApath: /etc/ssl/certs * TLSv1.3 (OUT), TLS handshake, Client hello (1): * TLSv1.3 (IN), TLS handshake, Server hello (2): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Unknown (8): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Certificate (11): * TLSv1.3 (OUT), TLS alert, Server hello (2): * SSL certificate problem: unable to get local issuer certificate * stopped the pause stream! * Closing connection 0 curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.haxx.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. Kubernetes Admin experience Due to the error above, the client pings the topology server with a TLS handshake error which is logged in karavi-topology pod. For instance,\n[root@:~]$ kubectl logs -n powerflex karavi-topology-5d4669d6dd-trzxw 2021/04/27 09:38:28 Set DriverNames to [csi-vxflexos.dellemc.com] 2021/04/28 07:15:05 http: TLS handshake error from 10.42.0.0:58450: local error: tls: bad record MAC 2021/04/28 07:16:14 http: TLS handshake error from 10.42.0.0:55311: local error: tls: bad record MAC Resolution To resolve this issue, we need to configure the client to be aware of the karavi-topology certificate (this includes all custom SSL certificate that are not issued from a trusted Certificate Authority (CA))\nGet a copy of the certificate used by karavi-topology If we supplied a custom certificate during installing karavi-topology, we can simply open the .crt and copy the text. However, if it was assigned by cert-manager, you can get a copy of the certificate by running the following kubectl command on the clusters.\n[root@:~]$ kubectl -n \u003cnamespace\u003e get secret karavi-topology-tls -o jsonpath='{.data.tls\\.crt}' | base64 -d -----BEGIN CERTIFICATE----- RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe RaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe -----END CERTIFICATE----- Configure your client to accept the above certificate A workaround on most browsers is to accept the karavi-topology certificate by clicking Continue to this website (not recommended). This will make all other successive communication to not cause any certificate error. Anyhow, you will need to read the documentation for your specific client to configure the above certificate. For Grafana, here are two ways to configure the karavi-topology datasource to use the above certificate:\n Deploy certificate with new Grafana instance Please follow the steps in Sample Grafana Deployment but attach the certificate to your `grafana-values.yaml` before deploying. The file should look like: # grafana-values.yaml image:repository:grafana/grafanatag:7.3.0sha:\"\"pullPolicy:IfNotPresentservice:type:NodePort## Administrator credentials when not using an existing SecretadminUser:adminadminPassword:admin## Pass the plugins you want installed as a list.##plugins:- grafana-simple-json-datasource- briangann-datatable-panel- grafana-piechart-panel## Configure grafana datasources## ref: http://docs.grafana.org/administration/provisioning/#datasources##datasources:datasources.yaml:apiVersion:1datasources:- name:Karavi-Topologytype:grafana-simple-json-datasourceaccess:proxyurl:'https://karavi-topology:8443'isDefault:nullversion:1editable:truejsonData:tlsAuthWithCACert:truesecureJsonData:tlsCaCert:| -----BEGIN CERTIFICATE-----RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATeRaNDOMcErTifCATe..RaNDOMcErTifCATe-----ENDCERTIFICATE----- - name: Prometheustype:prometheusaccess:proxyurl:'http://prometheus:9090'isDefault:nullversion:1editable:truetestFramework:enabled:falsesidecar:datasources:enabled:truedashboards:enabled:true## Additional grafana server ConfigMap mounts## Defines additional mounts with ConfigMap. CofigMap must be manually created in the namespace.extraConfigmapMounts:[]   Add certificate to an existing Grafana instance - This only happens if you configure jsonData to not skip tls verification. If this is the case, you'll need to re-deploy grafana as shown above or, form Grafana UI, edit Karavi-Topology datasource to use the certificate. To do the latter:  Visit your Grafana UI on a browser Navigate to setting and go to Data Sources Click on Karavi-Topology Ensure that Skip TLS Verify is already off Switch on With CA Cert Copy the above certificate into the TLS Auth Details text box that appears Click Save \u0026 Test and validate that everything is working fine when a green bar showing Data source is working appears   How can I diagnose an issue with CSM for Observability? Once you have attempted to install CSM for Observability to your Kubernetes or OpenShift cluster, the first step in troubleshooting is locating the problem.\nGet information on the state of your Pods.\nkubectl get pods -n $namespace Get verbose output of the current state of a Pod.\nkubectl describe pod -n $namespace $pod How can I view logs? View pod container logs. Output logs to a file for further debugging.\nkubectl logs -n $namespace $pod $container kubectl logs -n $namespace $pod $container \u003e $logFileName More information for viewing logs can be found here.\nHow can I create a ServiceMonitor object for Prometheus if I’m using Rancher monitoring stack? The ServiceMonitor allows us to define how a set of services should be monitored by Prometheus. Please see our prometheus documentation for creating a ServiceMonitor.\nHow can I debug and troubleshoot issues with Kubernetes?   To debug your application that may not be behaving correctly, please reference Kubernetes troubleshooting applications guide.\n  For tips on debugging your cluster, please see this troubleshooting guide.\n  How can I troubleshoot latency problems with CSM for Observability? CSM for Observability is instrumented to report trace data to Zipkin. Please see Tracing for more information on enabling tracing for CSM for Observability.\nWhy does the Observability installation timeout with pods stuck in ‘ContainerCreating’/‘CrashLoopBackOff’/‘Error’ stage? Check the pods in the CSM for Observability namespace. If the pod starting with ‘karavi-observability-cert-manager-cainjector-*’ is in ‘CrashLoopBackOff’ or ‘Error” stage with a number of restarts, check if the logs for that pod show the below error:\nkubectl logs -n $namespace $cert-manager-cainjector-podname error registering secret controller: no matches for kind \"MutatingWebhookConfiguration\" in version \"admissionregistration.k8s.io/v1beta1\" If the Kubernetes cluster version is 1.22.2 (or higher), this error is due to an incompatible cert-manager version. Please upgrade to the latest CSM for Observability release (v1.0.1 or higher).\n","excerpt":"Frequently Asked Questions  Why do I see a certificate problem when …","ref":"/csm-docs/v3/observability/troubleshooting/","title":"Troubleshooting"},{"body":"Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate troubleshooting. If you experience a problem with CSM for Resiliency it is important you provide us with as much information as possible so that we can diagnose the issue and improve CSM for Resiliency. Some tools have been provided in the tools directory that will help you understand the system’s state and facilitate sending us the logs and other information needed to diagnose a problem.\nMonitoring Protected Pods and Node Status There are two tools for monitoring the status of protected pods and nodes.\nThe mon.sh script displays the following information every 5 seconds:\n The date and time. A list of the nodes and their status. A list of the taints applied to each node. A list of the leases in the CSI driver’s namespace. (Edit the script to change the CSI driver namespace if necessary. It defaults to vxflexos as the driver namespace.) A list of the CSI driver pods and their status (defaults to vxflexos namespace.) A list of the protected pods and their status. (Edit the script if you do not use the default podmon label key.)  For systems with many protected pods, the monx.sh may provide a more usable output format. It displays the following fields every 5 seconds:\n The date and time. A list of the nodes and their status. A list of the taints applied to each node. A summary for each node hosting protected pods of the number of pods in various states such as the Running, Creating, and Error states. (Edit the script if you do not use the default podmon label key.) A list of the protected pods not in the Running state.  Collecting Logs If you have a problem with CSM for Resiliency it’s best to collect the logs to help with diagnosis. This tool can also be used to collect logs to submit as part of an issue to help us diagnose. Please use the collect_logs.sh. Type “collect_logs.sh –help” for help on the arguments.\nThe script collects the following information:\n A list of the driver pods. A list of the protected pods. The podmon container logs for each of the driver pods. The driver container logs for each of the driver pods. For each namespace containing protected pods, the recent events logged in that namespace.  After successful execution of the script, it will deposit a file similar to driver.logs.20210319_1407.tgz in the current directory. Please submit that file with any issues.\n","excerpt":"Some tools have been provided in the tools directory that will help …","ref":"/csm-docs/v3/resiliency/troubleshooting/","title":"Troubleshooting"},{"body":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using the OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\nPrerequisites Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here:v5.0.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v5.0.x\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v5.0.1   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller.  Installation Dell CSI Operator has been tested and qualified with\n Upstream Kubernetes or OpenShift (see supported versions)  Before you begin If you have installed an old version of the dell-csi-operator which was available with the name CSI Operator, please refer to this section before continuing.\nFull list of CSI Drivers and versions supported by the Dell CSI Operator    CSI Driver Version ConfigVersion Kubernetes Version OpenShift Version     CSI PowerMax 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI PowerMax 2.2.0 v2.2.0 1.21, 1.22, 1.23 4.8, 4.8 EUS, 4.9   CSI PowerMax 2.3.0 v2.3.0 1.22, 1.23, 1.24 4.9, 4.10, 4.10 EUS   CSI PowerFlex 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI PowerFlex 2.2.0 v2.2.0 1.21, 1.22, 1.23 4.8, 4.8 EUS, 4.9   CSI PowerFlex 2.3.0 v2.3.0 1.22, 1.23, 1.24 4.9, 4.10, 4.10 EUS   CSI PowerScale 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI PowerScale 2.2.0 v2.2.0 1.21, 1.22, 1.23 4.8, 4.8 EUS, 4.9   CSI PowerScale 2.3.0 v2.3.0 1.22, 1.23, 1.24 4.9, 4.10, 4.10 EUS   CSI Unity XT 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI Unity XT 2.2.0 v2.2.0 1.21, 1.22, 1.23 4.8, 4.8 EUS, 4.9   CSI Unity XT 2.3.0 v2.3.0 1.22, 1.23, 1.24 4.9, 4.10, 4.10 EUS   CSI PowerStore 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI PowerStore 2.2.0 v2.2.0 1.21, 1.22, 1.23 4.8, 4.8 EUS, 4.9   CSI PowerStore 2.3.0 v2.3.0 1.22, 1.23, 1.24 4.9, 4.10, 4.10 EUS     Dell CSI Operator can be installed via OLM (Operator Lifecycle Manager) and manual installation.\nInstallation Using Operator Lifecycle Manager dell-csi-operator can be installed using Operator Lifecycle Manager (OLM) on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the Operator to -\n Automatic - If you want the Operator to be automatically installed or upgraded (once an upgrade becomes available) Manual - If you want a Cluster Administrator to manually review and approve the InstallPlan for installation/upgrades  NOTE: The recommended version of OLM for upstream Kubernetes is v0.18.2.\nPre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.\n$ git clone https://github.com/dell/dell-csi-operator.git $ cd dell-csi-operator $ tar -czf config.tar.gz driverconfig/ # Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM $ kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e Upstream Kubernetes  For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page.  Red Hat OpenShift Clusters  For installing via OpenShift with the Operator, go to the OpenShift page.  Manual Installation Steps  Skip step 1 for “offline bundle installation” and continue using the workspace created by untar of dell-csi-operator-bundle.tar.gz.\n  Clone and checkout the required dell-csi-operator version using git clone -b  https://github.com/dell/dell-csi-operator.git. cd dell-csi-operator Run bash scripts/install.sh to install the operator.   NOTE: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default. Any existing installations of Dell CSI Operator (v1.2.0 or later) installed using install.sh to the ‘default’ or ‘dell-csi-operator’ namespace can be upgraded to the new version by running install.sh --upgrade.\n  Run the command oc get pods -n dell-csi-operator to validate the installation. If completed successfully, you should be able to see the operator-related pod in the ‘dell-csi-operator’ namespace.   Custom Resource Definitions As part of the Dell CSI Operator installation, a CRD representing each driver installation is also installed.\nList of CRDs which are installed in API Group storage.dell.com\n csipowermax csiunity csivxflexos csiisilon csipowerstore csipowermaxrevproxy  For installation of the supported drivers, a CustomResource has to be created in your cluster.\nPre-Requisites for installation of the CSI Drivers Pre-requisites for upstream Kubernetes Clusters On upstream Kubernetes clusters, make sure to install\n VolumeSnapshot CRDs  On clusters running v1.22,v1.23 \u0026 v1.24, make sure to install v1 VolumeSnapshot CRDs   External Volume Snapshot Controller with the correct version  Pre-requisites for Red Hat OpenShift Clusters iSCSI If you are installing a CSI driver which is going to use iSCSI as the transport protocol, please follow the following instructions.\nIn Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:99-iscsidlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:3.2.0systemd:units:- name:\"iscsid.service\"enabled:trueOnce the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of the iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid\nThe service should be up and running (i.e. should be active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\n Login to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn’t exist or it doesn’t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026 restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running Red Hat CoreOS, make sure that automatic ISCSI login at boot is configured. Please contact RedHat for more details.  MultiPath If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\necho 'defaults { user_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0\nUse the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:workers-multipath-conf-defaultlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:3.2.0storage:files:- contents:source:data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0Kverification:{}filesystem:rootmode:400path:/etc/multipath.confAfter deploying thisMachineConfig object, CoreOS will start multipath service automatically.\nAlternatively, you can check the status of the multipath service by entering the following command in each worker nodes.\nsudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\nInstalling CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object in your cluster.\nSample manifest files for each driver CustomResourceDefintion have been provided in the samples folder to help with the installation of the drivers. These files follow the naming convention\n{driver name}_{driver version}_k8s_{k8 version}.yaml  Or\n{driver name}_{driver version}_ops_{OpenShift version}.yaml  For e.g.\n samples/powermax_v220_k8s_123.yaml* \u003c- To install CSI PowerMax driver v2.2.0 on a Kubernetes 1.23 cluster samples/powermax_v220_ops_49.yaml* \u003c- To install CSI PowerMax driver v2.2.0 on an OpenShift 4.9 cluster  Copy the correct sample file and edit the mandatory \u0026 any optional parameters specific to your driver installation by following the instructions here\n NOTE: A detailed explanation of the various mandatory and optional fields in the CustomResource is available here. Please make sure to read through and understand the various fields.\n Run the following command to install the CSI driver.\nkubectl create -f \u003cdriver-manifest.yaml\u003e Note: If you are using an OLM based installation, the example manifests are available in the OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the installation Once the driver Custom Resource has been created, you can verify the installation\n  Check if Driver CR got created successfully\nFor e.g. – If you installed the PowerMax driver\n$ kubectl get csipowermax -n \u003cdriver-namespace\u003e   Check the status of the Custom Resource to verify if the driver installation was successful\n  If the driver-namespace was set to test-powermax, and the name of the driver is powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the details of the Custom Resource.\nNote: If the state of the CustomResource is Running then all the driver pods have been successfully installed. If the state is SuccessFul, then it means the driver deployment was successful but some driver pods may not be in a Running state. Please refer to the Troubleshooting section here if you encounter any issues during installation.\nChanges in installation for latest CSI drivers If you are installing the latest versions of the CSI drivers, the driver controller will be installed as a Kubernetes Deployment instead of a Statefulset. These installations can also run multiple replicas for the driver controller pods(not supported for StatefulSets) to support High Availability for the Controller.\nUpdate CSI Drivers The CSI Drivers installed by the Dell CSI Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include –\n Modifying the installation directly via kubectl edit For example - If the name of the installed Unity XT driver is unity, then run # Replace driver-namespace with the namespace where the Unity XT driver is installed $ kubectl edit csiunity/unity -n \u003cdriver-namespace\u003e and modify the installation. The usual fields to edit are the version of drivers and sidecars and the env variables.\n Modify the API object in place via kubectl patch command.  To create patch file or edit deployments, refer here for driver version \u0026 env variables and here for version of side-cars. The latest versions of drivers could have additional env variables or sidecars.\nThe below notes explain some of the general items to take care of.\nNOTES:\n If you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required. driver:configVersion:v2.3.0 Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, we will have to modify the below block while upgrading the driver.To get the volume health state add external-health-monitor sidecar in the sidecar section and valueunder controller set to true and the value under node set to true as shown below: i. Add controller and node section as below: controller:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"dnsPolicy:ClusterFirstWithHostNetnode:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"ii. Update the sidecar versions and add external-health-monitor sidecar if you want to enable health monitor of CSI volumes from Controller plugin:\nsideCars:- args:- --volume-name-prefix=csiunity- --default-fstype=ext4image:k8s.gcr.io/sig-storage/csi-provisioner:v3.1.0imagePullPolicy:IfNotPresentname:provisioner- args:- --snapshot-name-prefix=csiunitysnapimage:k8s.gcr.io/sig-storage/csi-snapshotter:v5.0.1imagePullPolicy:IfNotPresentname:snapshotter- args:- --monitor-interval=60simage:gcr.io/k8s-staging-sig-storage/csi-external-health-monitor-controller:v0.5.0imagePullPolicy:IfNotPresentname:external-health-monitor- image:k8s.gcr.io/sig-storage/csi-attacher:v3.4.0imagePullPolicy:IfNotPresentname:attacher- image:k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.1imagePullPolicy:IfNotPresentname:registrar- image:k8s.gcr.io/sig-storage/csi-resizer:v1.4.0imagePullPolicy:IfNotPresentname:resizer Configmap needs to be created with command kubectl create -f configmap.yaml using following yaml file.  kind:ConfigMapmetadata:name:unity-config-paramsnamespace:test-unitydata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"info\"ALLOW_RWO_MULTIPOD_ACCESS:\"false\"MAX_UNITY_VOLUMES_PER_NODE:\"0\"SYNC_NODE_INFO_TIME_INTERVAL:\"15\"TENANT_NAME:\"\"NOTE: Replicas in the driver CR file should not be greater than or equal to the number of worker nodes when upgrading the driver. If the Replicas count is not less than the worker node count, some of the driver controller pods would land in a pending state, and upgrade will not be successful. Driver controller pods go in a pending state because they have anti-affinity to each other and cannot be scheduled on nodes where there is a driver controller pod already running. Refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity for more details.\nNOTE: Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.\nNOTE: From v1.4.0 onwards, Dell CSI Operator does not support the creation of StorageClass and VolumeSnapshotClass objects. Although these fields are still present in the various driver CustomResourceDefinitions, they would be ignored by the operator. These fields will be removed from the CustomResourceDefinitions in a future release. If StorageClass and VolumeSnapshotClass need to be retained, you should upgrade the driver as per the recommended way noted above. StorageClass and VolumeSnapshotClass would not be retained on driver uninstallation.\nSupported modifications  Changing environment variable values for driver Adding (supported) environment variables Updating the image of the driver  Limitations  The Dell CSI Operator can’t manage any existing driver installed using Helm charts. If you already have installed one of the Dell CSI drivers in your cluster and want to use the operator based deployment, uninstall the driver and then redeploy the driver following the installation procedure described. The Dell CSI Operator is not fully compliant with the OperatorHub React UI elements and some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSI Operator  Custom Resource Specification Each CSI Driver installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.\nBelow is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - Refer full list of supported driver for finding out the appropriate config version here replicas - Number of replicas for controller plugin - Must be set to 1 for all drivers\ndnsPolicy - Determines the dnsPolicy for the node daemonset. Accepted values are Default, ClusterFirst, ClusterFirstWithHostNet, None common\nThis field is mandatory and is used to specify common properties for both controller and the node plugin\n image - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values  Optional fields controller - List of environment variables and values which are applicable only for controller\nnode - List of environment variables and values which are applicable only for node\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver\nforceUpdate\nBoolean value which can be set to true in order to force update the status of the CSI Driver\ntolerations List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet\nIt should be set separately in the controller and node sections if you want separate set of tolerations for them\nnodeSelector Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet\nfsGroupPolicy Defines which FS Group policy mode to be used, Supported modes: None, File and ReadWriteOnceWithFSType\nHere is a sample specification annotated with comments to explain each field\napiVersion:storage.dell.com/v1kind:CSIPowerMax# Type of the drivermetadata:name:test-powermax# Name of the drivernamespace:test-powermax# Namespace where driver is installedspec:driver:# Used to specify configuration versionconfigVersion:v3# Refer the table containing the full list of supported drivers to find the appropriate config versionreplicas:1forceUpdate:false# Set to true in case you want to force an update of driver statuscommon:# All common specificationimage:\"dellemc/csi-powermax:v1.4.0.000R\"#driver image for a particular releaseimagePullPolicy:IfNotPresentenvs:- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"You can set the field replicas to a higher number than 1 for the latest driver versions.\nNote - The image field should point to the correct image tag for version of the driver you are installing.\nFor e.g. - If you wish to install v1.4 of the CSI PowerMax driver, use the image tag dellemc/csi-powermax:v1.4.0.000R\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. The only exception to this is modifications requested by the documentation, for example, filling in blank IPs or other such system-specific data. Any modifications not specifically requested by the documentation should be only done after consulting with Dell support.\nModify the driver specification  Choose the correct configVersion. Refer the table containing the full list of supported drivers and versions. Provide the namespace (in metadata section) where you want to install the driver. Provide a name (in metadata section) for the driver. This will be the name of the Custom Resource. Edit the values for mandatory configuration parameters specific to your installation. Edit/Add any values for optional configuration parameters to customize your installation. If you are installing the latest versions of the CSI drivers, the default number of replicas is set to 2. You can increase/decrease this value.  StorageClass and VolumeSnapshotClass New Installations You should not provide any StorageClass or VolumeSnapshotClass details during driver installation. The sample files for all the drivers have been updated to reflect this change. Even if these details are there in the sample files, StorageClass or VolumeSnapshotClass will not be created.\nWhat happens to my existing StorageClass \u0026 VolumeSnapshotClass objects  In case you are upgrading an existing driver installation by using kubectl edit or by patching the object in place, any existing objects will remain as is. If you added more objects as part of the upgrade, then this request will be ignored by the Operator. If you uninstall the older driver, then any StorageClass or VolumeSnapshotClass objects will be deleted. An uninstall and followed by an install of the driver would also result in StorageClass and VolumeSnapshotClass getting deleted and not getting created again.  NOTE: For more information on pre-requisites and parameters, please refer to the sub-pages below for each driver.\nNOTE: Storage Classes and Volume Snapshot Classes would no longer be created during the installation of the driver via an operator from v1.4.0 and higher.\n","excerpt":"The Dell CSI Operator is a Kubernetes Operator, which can be used to …","ref":"/csm-docs/docs/csidriver/installation/operator/","title":"CSI Driver installation using Dell CSI Operator"},{"body":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using the OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\nPrerequisites Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here:v5.0.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v5.0.x\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v5.0.1   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller.  Installation Dell CSI Operator has been tested and qualified with\n Upstream Kubernetes or OpenShift (see supported versions)  Before you begin If you have installed an old version of the dell-csi-operator which was available with the name CSI Operator, please refer to this section before continuing.\nFull list of CSI Drivers and versions supported by the Dell CSI Operator    CSI Driver Version ConfigVersion Kubernetes Version OpenShift Version     CSI PowerMax 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI PowerMax 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI PowerMax 2.2.0 v2.2.0 1.21, 1.22, 1.23 4.8, 4.8 EUS, 4.9   CSI PowerFlex 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI PowerFlex 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI PowerFlex 2.2.0 v2.2.0 1.21, 1.22, 1.23 4.8, 4.8 EUS, 4.9   CSI PowerScale 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI PowerScale 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI PowerScale 2.2.0 v2.2.0 1.21, 1.22, 1.23 4.8, 4.8 EUS, 4.9   CSI Unity 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI Unity 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI Unity 2.2.0 v2.2.0 1.21, 1.22, 1.23 4.8, 4.8 EUS, 4.9   CSI PowerStore 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI PowerStore 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI PowerStore 2.2.0 v2.2.0 1.21, 1.22, 1.23 4.8, 4.8 EUS, 4.9     Dell CSI Operator can be installed via OLM (Operator Lifecycle Manager) and manual installation.\nInstallation Using Operator Lifecycle Manager dell-csi-operator can be installed using Operator Lifecycle Manager (OLM) on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the Operator to -\n Automatic - If you want the Operator to be automatically installed or upgraded (once an upgrade becomes available) Manual - If you want a Cluster Administrator to manually review and approve the InstallPlan for installation/upgrades  NOTE: The recommended version of OLM for upstream Kubernetes is v0.18.2.\nPre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.\n$ git clone https://github.com/dell/dell-csi-operator.git $ cd dell-csi-operator $ tar -czf config.tar.gz driverconfig/ # Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM $ kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e Upstream Kubernetes  For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page.  Red Hat OpenShift Clusters  For installing via OpenShift with the Operator, go to the OpenShift page.  Manual Installation Steps  Skip step 1 for “offline bundle installation” and continue using the workspace created by untar of dell-csi-operator-bundle.tar.gz.\n  Clone and checkout the required dell-csi-operator version using git clone -b  https://github.com/dell/dell-csi-operator.git. cd dell-csi-operator Run bash scripts/install.sh to install the operator.   NOTE: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default. Any existing installations of Dell CSI Operator (v1.2.0 or later) installed using install.sh to the ‘default’ or ‘dell-csi-operator’ namespace can be upgraded to the new version by running install.sh --upgrade.\n  Run the command oc get pods -n dell-csi-operator to validate the installation. If completed successfully, you should be able to see the operator-related pod in the ‘dell-csi-operator’ namespace.   Custom Resource Definitions As part of the Dell CSI Operator installation, a CRD representing each driver installation is also installed.\nList of CRDs which are installed in API Group storage.dell.com\n csipowermax csiunity csivxflexos csiisilon csipowerstore csipowermaxrevproxy  For installation of the supported drivers, a CustomResource has to be created in your cluster.\nPre-Requisites for installation of the CSI Drivers Pre-requisites for upstream Kubernetes Clusters On upstream Kubernetes clusters, make sure to install\n VolumeSnapshot CRDs  On clusters running v1.21,v1.22 \u0026 v1.23, make sure to install v1 VolumeSnapshot CRDs   External Volume Snapshot Controller with the correct version  Pre-requisites for Red Hat OpenShift Clusters iSCSI If you are installing a CSI driver which is going to use iSCSI as the transport protocol, please follow the following instructions.\nIn Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:99-iscsidlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0systemd:units:- name:\"iscsid.service\"enabled:trueOnce the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of the iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid\nThe service should be up and running (i.e. should be active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\n Login to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn’t exist or it doesn’t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026 restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running Red Hat CoreOS, make sure that automatic ISCSI login at boot is configured. Please contact RedHat for more details.  MultiPath If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\necho 'defaults { user_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0\nUse the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:workers-multipath-conf-defaultlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0storage:files:- contents:source:data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0Kverification:{}filesystem:rootmode:400path:/etc/multipath.confAfter deploying thisMachineConfig object, CoreOS will start multipath service automatically.\nAlternatively, you can check the status of the multipath service by entering the following command in each worker nodes.\nsudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\nInstalling CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object in your cluster.\nSample manifest files for each driver CustomResourceDefintion have been provided in the samples folder to help with the installation of the drivers. These files follow the naming convention\n{driver name}_{driver version}_k8s_{k8 version}.yaml  Or\n{driver name}_{driver version}_ops_{OpenShift version}.yaml  For e.g.\n samples/powermax_v220_k8s_123.yaml* \u003c- To install CSI PowerMax driver v2.2.0 on a Kubernetes 1.23 cluster samples/powermax_v220_ops_49.yaml* \u003c- To install CSI PowerMax driver v2.2.0 on an OpenShift 4.9 cluster  Copy the correct sample file and edit the mandatory \u0026 any optional parameters specific to your driver installation by following the instructions here\n NOTE: A detailed explanation of the various mandatory and optional fields in the CustomResource is available here. Please make sure to read through and understand the various fields.\n Run the following command to install the CSI driver.\nkubectl create -f \u003cdriver-manifest.yaml\u003e Note: If you are using an OLM based installation, the example manifests are available in the OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the installation Once the driver Custom Resource has been created, you can verify the installation\n  Check if Driver CR got created successfully\nFor e.g. – If you installed the PowerMax driver\n$ kubectl get csipowermax -n \u003cdriver-namespace\u003e   Check the status of the Custom Resource to verify if the driver installation was successful\n  If the driver-namespace was set to test-powermax, and the name of the driver is powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the details of the Custom Resource.\nNote: If the state of the CustomResource is Running then all the driver pods have been successfully installed. If the state is SuccessFul, then it means the driver deployment was successful but some driver pods may not be in a Running state. Please refer to the Troubleshooting section here if you encounter any issues during installation.\nChanges in installation for latest CSI drivers If you are installing the latest versions of the CSI drivers, the driver controller will be installed as a Kubernetes Deployment instead of a Statefulset. These installations can also run multiple replicas for the driver controller pods(not supported for StatefulSets) to support High Availability for the Controller.\nUpdate CSI Drivers The CSI Drivers installed by the Dell CSI Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include –\n Modifying the installation directly via kubectl edit For e.g. - If the name of the installed unity driver is unity, then run # Replace driver-namespace with the namespace where the Unity driver is installed $ kubectl edit csiunity/unity -n \u003cdriver-namespace\u003e and modify the installation. The usual fields to edit are the version of drivers and sidecars and the env variables.\n Modify the API object in place via kubectl patch command.  To create patch file or edit deployments, refer here for driver version \u0026 env variables and here for version of side-cars. The latest versions of drivers could have additional env variables or sidecars.\nThe below notes explain some of the general items to take care of.\nNOTES:\n If you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required. driver:configVersion:v2.2.0 Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, we will have to modify the below block while upgrading the driver.To get the volume health state add external-health-monitor sidecar in the sidecar section and valueunder controller set to true and the value under node set to true as shown below: i. Add controller and node section as below: controller:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"dnsPolicy:ClusterFirstWithHostNetnode:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"ii. Update the sidecar versions and add external-health-monitor sidecar if you want to enable health monitor of CSI volumes from Controller plugin:\nsideCars:- args:- --volume-name-prefix=csiunity- --default-fstype=ext4image:k8s.gcr.io/sig-storage/csi-provisioner:v3.1.0imagePullPolicy:IfNotPresentname:provisioner- args:- --snapshot-name-prefix=csiunitysnapimage:k8s.gcr.io/sig-storage/csi-snapshotter:v5.0.1imagePullPolicy:IfNotPresentname:snapshotter- args:- --monitor-interval=60simage:gcr.io/k8s-staging-sig-storage/csi-external-health-monitor-controller:v0.4.0imagePullPolicy:IfNotPresentname:external-health-monitor- image:k8s.gcr.io/sig-storage/csi-attacher:v3.4.0imagePullPolicy:IfNotPresentname:attacher- image:k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.0imagePullPolicy:IfNotPresentname:registrar- image:k8s.gcr.io/sig-storage/csi-resizer:v1.4.0imagePullPolicy:IfNotPresentname:resizer Configmap needs to be created with command kubectl create -f configmap.yaml using following yaml file.  kind:ConfigMapmetadata:name:unity-config-paramsnamespace:test-unitydata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"info\"ALLOW_RWO_MULTIPOD_ACCESS:\"false\"MAX_UNITY_VOLUMES_PER_NODE:\"0\"SYNC_NODE_INFO_TIME_INTERVAL:\"15\"TENANT_NAME:\"\"NOTE: Replicas in the driver CR file should not be greater than or equal to the number of worker nodes when upgrading the driver. If the Replicas count is not less than the worker node count, some of the driver controller pods would land in a pending state, and upgrade will not be successful. Driver controller pods go in a pending state because they have anti-affinity to each other and cannot be scheduled on nodes where there is a driver controller pod already running. Refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity for more details.\nNOTE: Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.\nNOTE: From v1.4.0 onwards, Dell CSI Operator does not support the creation of StorageClass and VolumeSnapshotClass objects. Although these fields are still present in the various driver CustomResourceDefinitions, they would be ignored by the operator. These fields will be removed from the CustomResourceDefinitions in a future release. If StorageClass and VolumeSnapshotClass need to be retained, you should upgrade the driver as per the recommended way noted above. StorageClass and VolumeSnapshotClass would not be retained on driver uninstallation.\nSupported modifications  Changing environment variable values for driver Adding (supported) environment variables Updating the image of the driver  Limitations  The Dell CSI Operator can’t manage any existing driver installed using Helm charts. If you already have installed one of the DellEMC CSI driver in your cluster and want to use the operator based deployment, uninstall the driver and then redeploy the driver following the installation procedure described above The Dell CSI Operator is not fully compliant with the OperatorHub React UI elements and some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSI Operator  Custom Resource Specification Each CSI Driver installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.\nBelow is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - Refer full list of supported driver for finding out the appropriate config version here replicas - Number of replicas for controller plugin - Must be set to 1 for all drivers\ndnsPolicy - Determines the dnsPolicy for the node daemonset. Accepted values are Default, ClusterFirst, ClusterFirstWithHostNet, None common\nThis field is mandatory and is used to specify common properties for both controller and the node plugin\n image - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values  Optional fields controller - List of environment variables and values which are applicable only for controller\nnode - List of environment variables and values which are applicable only for node\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver\nforceUpdate\nBoolean value which can be set to true in order to force update the status of the CSI Driver\ntolerations List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet\nIt should be set separately in the controller and node sections if you want separate set of tolerations for them\nnodeSelector Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet\nfsGroupPolicy Defines which FS Group policy mode to be used, Supported modes: None, File and ReadWriteOnceWithFSType\nHere is a sample specification annotated with comments to explain each field\napiVersion:storage.dell.com/v1kind:CSIPowerMax# Type of the drivermetadata:name:test-powermax# Name of the drivernamespace:test-powermax# Namespace where driver is installedspec:driver:# Used to specify configuration versionconfigVersion:v3# Refer the table containing the full list of supported drivers to find the appropriate config versionreplicas:1forceUpdate:false# Set to true in case you want to force an update of driver statuscommon:# All common specificationimage:\"dellemc/csi-powermax:v1.4.0.000R\"#driver image for a particular releaseimagePullPolicy:IfNotPresentenvs:- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"You can set the field replicas to a higher number than 1 for the latest driver versions.\nNote - The image field should point to the correct image tag for version of the driver you are installing.\nFor e.g. - If you wish to install v1.4 of the CSI PowerMax driver, use the image tag dellemc/csi-powermax:v1.4.0.000R\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. The only exception to this is modifications requested by the documentation, for example, filling in blank IPs or other such system-specific data. Any modifications not specifically requested by the documentation should be only done after consulting with Dell support.\nModify the driver specification  Choose the correct configVersion. Refer the table containing the full list of supported drivers and versions. Provide the namespace (in metadata section) where you want to install the driver. Provide a name (in metadata section) for the driver. This will be the name of the Custom Resource. Edit the values for mandatory configuration parameters specific to your installation. Edit/Add any values for optional configuration parameters to customize your installation. If you are installing the latest versions of the CSI drivers, the default number of replicas is set to 2. You can increase/decrease this value.  StorageClass and VolumeSnapshotClass New Installations You should not provide any StorageClass or VolumeSnapshotClass details during driver installation. The sample files for all the drivers have been updated to reflect this change. Even if these details are there in the sample files, StorageClass or VolumeSnapshotClass will not be created.\nWhat happens to my existing StorageClass \u0026 VolumeSnapshotClass objects  In case you are upgrading an existing driver installation by using kubectl edit or by patching the object in place, any existing objects will remain as is. If you added more objects as part of the upgrade, then this request will be ignored by the Operator. If you uninstall the older driver, then any StorageClass or VolumeSnapshotClass objects will be deleted. An uninstall and followed by an install of the driver would also result in StorageClass and VolumeSnapshotClass getting deleted and not getting created again.  NOTE: For more information on pre-requisites and parameters, please refer to the sub-pages below for each driver.\nNOTE: Storage Classes and Volume Snapshot Classes would no longer be created during the installation of the driver via an operator from v1.4.0 and higher.\n","excerpt":"The Dell CSI Operator is a Kubernetes Operator, which can be used to …","ref":"/csm-docs/v1/csidriver/installation/operator/","title":"CSI Driver installation using Dell CSI Operator"},{"body":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using the OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\nPrerequisites Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here:v5.0.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v5.0.x\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v5.0.1   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller.  Installation Dell CSI Operator has been tested and qualified with\n Upstream Kubernetes or OpenShift (see supported versions)  Before you begin If you have installed an old version of the dell-csi-operator which was available with the name CSI Operator, please refer to this section before continuing.\nFull list of CSI Drivers and versions supported by the Dell CSI Operator    CSI Driver Version ConfigVersion Kubernetes Version OpenShift Version     CSI PowerMax 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI PowerMax 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI PowerMax 2.2.0 v2.2.0 1.21, 1.22, 1.23 4.8, 4.8 EUS, 4.9   CSI PowerFlex 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI PowerFlex 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI PowerFlex 2.2.0 v2.2.0 1.21, 1.22, 1.23 4.8, 4.8 EUS, 4.9   CSI PowerScale 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI PowerScale 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI PowerScale 2.2.0 v2.2.0 1.21, 1.22, 1.23 4.8, 4.8 EUS, 4.9   CSI Unity 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI Unity 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI Unity 2.2.0 v2.2.0 1.21, 1.22, 1.23 4.8, 4.8 EUS, 4.9   CSI PowerStore 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI PowerStore 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI PowerStore 2.2.0 v2.2.0 1.21, 1.22, 1.23 4.8, 4.8 EUS, 4.9     Dell CSI Operator can be installed via OLM (Operator Lifecycle Manager) and manual installation.\nInstallation Using Operator Lifecycle Manager dell-csi-operator can be installed using Operator Lifecycle Manager (OLM) on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the Operator to -\n Automatic - If you want the Operator to be automatically installed or upgraded (once an upgrade becomes available) Manual - If you want a Cluster Administrator to manually review and approve the InstallPlan for installation/upgrades  NOTE: The recommended version of OLM for upstream Kubernetes is v0.18.2.\nPre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.\n$ git clone https://github.com/dell/dell-csi-operator.git $ cd dell-csi-operator $ tar -czf config.tar.gz driverconfig/ # Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM $ kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e Upstream Kubernetes  For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page.  Red Hat OpenShift Clusters  For installing via OpenShift with the Operator, go to the OpenShift page.  Manual Installation Steps  Skip step 1 for “offline bundle installation” and continue using the workspace created by untar of dell-csi-operator-bundle.tar.gz.\n  Clone the Dell CSI Operator repository. cd dell-csi-operator git checkout dell-csi-operator-`your-version’ Run bash scripts/install.sh to install the operator.   NOTE: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default. Any existing installations of Dell CSI Operator (v1.2.0 or later) installed using install.sh to the ‘default’ or ‘dell-csi-operator’ namespace can be upgraded to the new version by running install.sh --upgrade.\n  Run the command oc get pods -n dell-csi-operator to validate the installation. If completed successfully, you should be able to see the operator-related pod in the ‘dell-csi-operator’ namespace.   Custom Resource Definitions As part of the Dell CSI Operator installation, a CRD representing each driver installation is also installed.\nList of CRDs which are installed in API Group storage.dell.com\n csipowermax csiunity csivxflexos csiisilon csipowerstore csipowermaxrevproxy  For installation of the supported drivers, a CustomResource has to be created in your cluster.\nPre-Requisites for installation of the CSI Drivers Pre-requisites for upstream Kubernetes Clusters On upstream Kubernetes clusters, make sure to install\n VolumeSnapshot CRDs  On clusters running v1.21,v1.22 \u0026 v1.23, make sure to install v1 VolumeSnapshot CRDs   External Volume Snapshot Controller with the correct version  Pre-requisites for Red Hat OpenShift Clusters iSCSI If you are installing a CSI driver which is going to use iSCSI as the transport protocol, please follow the following instructions.\nIn Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:99-iscsidlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0systemd:units:- name:\"iscsid.service\"enabled:trueOnce the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of the iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid\nThe service should be up and running (i.e. should be active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\n Login to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn’t exist or it doesn’t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026 restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running Red Hat CoreOS, make sure that automatic ISCSI login at boot is configured. Please contact RedHat for more details.  MultiPath If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\necho 'defaults { user_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0\nUse the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:workers-multipath-conf-defaultlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0storage:files:- contents:source:data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0Kverification:{}filesystem:rootmode:400path:/etc/multipath.confAfter deploying thisMachineConfig object, CoreOS will start multipath service automatically.\nAlternatively, you can check the status of the multipath service by entering the following command in each worker nodes.\nsudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\nInstalling CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object in your cluster.\nSample manifest files for each driver CustomResourceDefintion have been provided in the samples folder to help with the installation of the drivers. These files follow the naming convention\n{driver name}_{driver version}_k8s_{k8 version}.yaml  Or\n{driver name}_{driver version}_ops_{OpenShift version}.yaml  For e.g.\n samples/powermax_v220_k8s_123.yaml* \u003c- To install CSI PowerMax driver v2.2.0 on a Kubernetes 1.23 cluster samples/powermax_v220_ops_49.yaml* \u003c- To install CSI PowerMax driver v2.2.0 on an OpenShift 4.9 cluster  Copy the correct sample file and edit the mandatory \u0026 any optional parameters specific to your driver installation by following the instructions here\n NOTE: A detailed explanation of the various mandatory and optional fields in the CustomResource is available here. Please make sure to read through and understand the various fields.\n Run the following command to install the CSI driver.\nkubectl create -f \u003cdriver-manifest.yaml\u003e Note: If you are using an OLM based installation, the example manifests are available in the OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the installation Once the driver Custom Resource has been created, you can verify the installation\n  Check if Driver CR got created successfully\nFor e.g. – If you installed the PowerMax driver\n$ kubectl get csipowermax -n \u003cdriver-namespace\u003e   Check the status of the Custom Resource to verify if the driver installation was successful\n  If the driver-namespace was set to test-powermax, and the name of the driver is powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the details of the Custom Resource.\nNote: If the state of the CustomResource is Running then all the driver pods have been successfully installed. If the state is SuccessFul, then it means the driver deployment was successful but some driver pods may not be in a Running state. Please refer to the Troubleshooting section here if you encounter any issues during installation.\nChanges in installation for latest CSI drivers If you are installing the latest versions of the CSI drivers, the driver controller will be installed as a Kubernetes Deployment instead of a Statefulset. These installations can also run multiple replicas for the driver controller pods(not supported for StatefulSets) to support High Availability for the Controller.\nUpdate CSI Drivers The CSI Drivers installed by the Dell CSI Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include –\n Modifying the installation directly via kubectl edit For e.g. - If the name of the installed unity driver is unity, then run # Replace driver-namespace with the namespace where the Unity driver is installed $ kubectl edit csiunity/unity -n \u003cdriver-namespace\u003e and modify the installation. The usual fields to edit are the version of drivers and sidecars and the env variables.\n Modify the API object in place via kubectl patch command.  To create patch file or edit deployments, refer here for driver version \u0026 env variables and here for version of side-cars. The latest versions of drivers could have additional env variables or sidecars.\nThe below notes explain some of the general items to take care of.\nNOTES:\n If you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required. driver:configVersion:v2.2.0 Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, we will have to modify the below block while upgrading the driver.To get the volume health state add external-health-monitor sidecar in the sidecar section and valueunder controller set to true and the value under node set to true as shown below: i. Add controller and node section as below: controller:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"dnsPolicy:ClusterFirstWithHostNetnode:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"ii. Update the sidecar versions and add external-health-monitor sidecar if you want to enable health monitor of CSI volumes from Controller plugin:\nsideCars:- args:- --volume-name-prefix=csiunity- --default-fstype=ext4image:k8s.gcr.io/sig-storage/csi-provisioner:v3.1.0imagePullPolicy:IfNotPresentname:provisioner- args:- --snapshot-name-prefix=csiunitysnapimage:k8s.gcr.io/sig-storage/csi-snapshotter:v5.0.1imagePullPolicy:IfNotPresentname:snapshotter- args:- --monitor-interval=60simage:gcr.io/k8s-staging-sig-storage/csi-external-health-monitor-controller:v0.4.0imagePullPolicy:IfNotPresentname:external-health-monitor- image:k8s.gcr.io/sig-storage/csi-attacher:v3.4.0imagePullPolicy:IfNotPresentname:attacher- image:k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.0imagePullPolicy:IfNotPresentname:registrar- image:k8s.gcr.io/sig-storage/csi-resizer:v1.4.0imagePullPolicy:IfNotPresentname:resizer Configmap needs to be created with command kubectl create -f configmap.yaml using following yaml file.  kind:ConfigMapmetadata:name:unity-config-paramsnamespace:test-unitydata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"info\"ALLOW_RWO_MULTIPOD_ACCESS:\"false\"MAX_UNITY_VOLUMES_PER_NODE:\"0\"SYNC_NODE_INFO_TIME_INTERVAL:\"15\"TENANT_NAME:\"\"NOTE: Replicas in the driver CR file should not be greater than or equal to the number of worker nodes when upgrading the driver. If the Replicas count is not less than the worker node count, some of the driver controller pods would land in a pending state, and upgrade will not be successful. Driver controller pods go in a pending state because they have anti-affinity to each other and cannot be scheduled on nodes where there is a driver controller pod already running. Refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity for more details.\nNOTE: Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.\nNOTE: From v1.4.0 onwards, Dell CSI Operator does not support the creation of StorageClass and VolumeSnapshotClass objects. Although these fields are still present in the various driver CustomResourceDefinitions, they would be ignored by the operator. These fields will be removed from the CustomResourceDefinitions in a future release. If StorageClass and VolumeSnapshotClass need to be retained, you should upgrade the driver as per the recommended way noted above. StorageClass and VolumeSnapshotClass would not be retained on driver uninstallation.\nSupported modifications  Changing environment variable values for driver Adding (supported) environment variables Updating the image of the driver  Limitations  The Dell CSI Operator can’t manage any existing driver installed using Helm charts. If you already have installed one of the DellEMC CSI driver in your cluster and want to use the operator based deployment, uninstall the driver and then redeploy the driver following the installation procedure described above The Dell CSI Operator is not fully compliant with the OperatorHub React UI elements and some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSI Operator  Custom Resource Specification Each CSI Driver installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.\nBelow is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - Refer full list of supported driver for finding out the appropriate config version here replicas - Number of replicas for controller plugin - Must be set to 1 for all drivers\ndnsPolicy - Determines the dnsPolicy for the node daemonset. Accepted values are Default, ClusterFirst, ClusterFirstWithHostNet, None common\nThis field is mandatory and is used to specify common properties for both controller and the node plugin\n image - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values  Optional fields controller - List of environment variables and values which are applicable only for controller\nnode - List of environment variables and values which are applicable only for node\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver\nforceUpdate\nBoolean value which can be set to true in order to force update the status of the CSI Driver\ntolerations List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet\nIt should be set separately in the controller and node sections if you want separate set of tolerations for them\nnodeSelector Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet\nfsGroupPolicy Defines which FS Group policy mode to be used, Supported modes: None, File and ReadWriteOnceWithFSType\nHere is a sample specification annotated with comments to explain each field\napiVersion:storage.dell.com/v1kind:CSIPowerMax# Type of the drivermetadata:name:test-powermax# Name of the drivernamespace:test-powermax# Namespace where driver is installedspec:driver:# Used to specify configuration versionconfigVersion:v3# Refer the table containing the full list of supported drivers to find the appropriate config versionreplicas:1forceUpdate:false# Set to true in case you want to force an update of driver statuscommon:# All common specificationimage:\"dellemc/csi-powermax:v1.4.0.000R\"#driver image for a particular releaseimagePullPolicy:IfNotPresentenvs:- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"You can set the field replicas to a higher number than 1 for the latest driver versions.\nNote - The image field should point to the correct image tag for version of the driver you are installing.\nFor e.g. - If you wish to install v1.4 of the CSI PowerMax driver, use the image tag dellemc/csi-powermax:v1.4.0.000R\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. The only exception to this is modifications requested by the documentation, for example, filling in blank IPs or other such system-specific data. Any modifications not specifically requested by the documentation should be only done after consulting with Dell support.\nModify the driver specification  Choose the correct configVersion. Refer the table containing the full list of supported drivers and versions. Provide the namespace (in metadata section) where you want to install the driver. Provide a name (in metadata section) for the driver. This will be the name of the Custom Resource. Edit the values for mandatory configuration parameters specific to your installation. Edit/Add any values for optional configuration parameters to customize your installation. If you are installing the latest versions of the CSI drivers, the default number of replicas is set to 2. You can increase/decrease this value.  StorageClass and VolumeSnapshotClass New Installations You should not provide any StorageClass or VolumeSnapshotClass details during driver installation. The sample files for all the drivers have been updated to reflect this change. Even if these details are there in the sample files, StorageClass or VolumeSnapshotClass will not be created.\nWhat happens to my existing StorageClass \u0026 VolumeSnapshotClass objects  In case you are upgrading an existing driver installation by using kubectl edit or by patching the object in place, any existing objects will remain as is. If you added more objects as part of the upgrade, then this request will be ignored by the Operator. If you uninstall the older driver, then any StorageClass or VolumeSnapshotClass objects will be deleted. An uninstall and followed by an install of the driver would also result in StorageClass and VolumeSnapshotClass getting deleted and not getting created again.  NOTE: For more information on pre-requisites and parameters, please refer to the sub-pages below for each driver.\nNOTE: Storage Classes and Volume Snapshot Classes would no longer be created during the installation of the driver via an operator from v1.4.0 and higher.\n","excerpt":"The Dell CSI Operator is a Kubernetes Operator, which can be used to …","ref":"/csm-docs/v2/csidriver/installation/operator/","title":"CSI Driver installation using Dell CSI Operator"},{"body":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell EMC for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using the OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\nPrerequisites Volume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here:v4.2.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v4.2.x\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 4.2.x version of snapshotter/snapshot-controller.  Installation Dell CSI Operator has been tested and qualified with\n Upstream Kubernetes or OpenShift (see supported versions)  Before you begin If you have installed an old version of the dell-csi-operator which was available with the name CSI Operator, please refer to this section before continuing.\nFull list of CSI Drivers and versions supported by the Dell CSI Operator    CSI Driver Version ConfigVersion Kubernetes Version OpenShift Version     CSI PowerMax 1.7 v6 1.19, 1.20, 1.21 4.6, 4.7   CSI PowerMax 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI PowerMax 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI PowerFlex 1.5 v5 1.19, 1.20, 1.21 4.6, 4.7   CSI PowerFlex 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI PowerFlex 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI PowerScale 1.6 v6 1.19, 1.20, 1.21 4.6, 4.7   CSI PowerScale 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI PowerScale 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI Unity 1.6 v5 1.19, 1.20, 1.21 4.6, 4.7   CSI Unity 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI Unity 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9   CSI PowerStore 1.4 v4 1.19, 1.20, 1.21 4.6, 4.7   CSI PowerStore 2.0.0 v2.0.0 1.20, 1.21, 1.22 4.6 EUS, 4.7, 4.8   CSI PowerStore 2.1.0 v2.1.0 1.20, 1.21, 1.22 4.8, 4.8 EUS, 4.9     Dell CSI Operator can be installed via OLM (Operator Lifecycle Manager) and manual installation.\nInstallation Using Operator Lifecycle Manager dell-csi-operator can be installed using Operator Lifecycle Manager (OLM) on upstream Kubernetes clusters \u0026 Red Hat OpenShift Clusters.\nThe installation process involves the creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the Operator to -\n Automatic - If you want the Operator to be automatically installed or upgraded (once an upgrade becomes available) Manual - If you want a Cluster Administrator to manually review and approve the InstallPlan for installation/upgrades  NOTE: The recommended version of OLM for upstream Kubernetes is v0.18.3.\nPre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.\n$ git clone https://github.com/dell/dell-csi-operator.git $ cd dell-csi-operator $ tar -czf config.tar.gz driverconfig/ # Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM $ kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u003coperator-namespace\u003e Upstream Kubernetes  For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page.  Red Hat OpenShift Clusters  For installing via OpenShift with the Operator, go to the OpenShift page.  Manual Installation Steps  Skip step 1 for “offline bundle installation” and continue using the workspace created by untar of dell-csi-operator-bundle.tar.gz.\n  Clone the Dell CSI Operator repository. git checkout dell-csi-operator- Run bash scripts/install.sh to install the operator.   NOTE: Dell CSI Operator version 1.4.0 and higher would install to the ‘dell-csi-operator’ namespace by default. Any existing installations of Dell CSI Operator (v1.2.0 or later) installed using install.sh to the ‘default’ or ‘dell-csi-operator’ namespace can be upgraded to the new version by running install.sh --upgrade.\n  Run the command oc get pods -n dell-csi-operator to validate the installation. If completed successfully, you should be able to see the operator-related pod in the ‘dell-csi-operator’ namespace.   Custom Resource Definitions As part of the Dell CSI Operator installation, a CRD representing each driver installation is also installed.\nList of CRDs which are installed in API Group storage.dell.com\n csipowermax csiunity csivxflexos csiisilon csipowerstore csipowermaxrevproxy  For installation of the supported drivers, a CustomResource has to be created in your cluster.\nPre-Requisites for installation of the CSI Drivers Pre-requisites for upstream Kubernetes Clusters On upstream Kubernetes clusters, make sure to install\n VolumeSnapshot CRDs  On clusters running v1.20,v1.21 \u0026 v1.22, make sure to install v1 VolumeSnapshot CRDs On clusters running v1.19, make sure to install v1beta1 VolumeSnapshot CRDs   External Volume Snapshot Controller with the correct version  Pre-requisites for Red Hat OpenShift Clusters iSCSI If you are installing a CSI driver which is going to use iSCSI as the transport protocol, please follow the following instructions.\nIn Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:99-iscsidlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0systemd:units:- name:\"iscsid.service\"enabled:trueOnce the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of the iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid\nThe service should be up and running (i.e. should be active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\n Login to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn’t exist or it doesn’t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026 restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running Red Hat CoreOS, make sure that automatic ISCSI login at boot is configured. Please contact RedHat for more details.  MultiPath If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\necho 'defaults { user_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0\nUse the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:workers-multipath-conf-defaultlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0storage:files:- contents:source:data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0Kverification:{}filesystem:rootmode:400path:/etc/multipath.confAfter deploying thisMachineConfig object, CoreOS will start multipath service automatically.\nAlternatively, you can check the status of the multipath service by entering the following command in each worker nodes.\nsudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally, you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer to official documentation of the multipath configuration.\nReplacing CSI Operator with Dell CSI Operator Dell CSI Operator was previously available, with the name CSI Operator, for both manual and OLM installation.\nCSI Operator has been discontinued and has been renamed to Dell CSI Operator. This is just a name change and as a result, the Kubernetes resources created as part of the Operator deployment will use the name dell-csi-operator instead of csi-operator.\nBefore proceeding with the installation of the new Dell CSI Operator, any existing CSI Operator installation has to be completely removed from the cluster.\nNote - This doesn’t impact any of the CSI Drivers which have been installed in the cluster\nIf the old CSI Operator was installed manually, then run the following command from the root of the repository which was used originally for installation\nbash scripts/undeploy.sh  If you don’t have the original repository available, then run the following commands\ngit clone https://github.com/dell/dell-csi-operator.git cd dell-csi-operator git checkout csi-operator-v1.0.0 bash scripts/undeploy.sh  Note - Once you have removed the old CSI Operator, then for installing the new Dell CSI Operator, you will need to pull/checkout the latest code\nIf you had installed the old CSI Operator using OLM, then please follow the uninstallation instructions provided by OperatorHub. This will mostly involve:\n* Deleting the CSI Operator Subscription * Deleting the CSI Operator CSV  Installing CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object in your cluster.\nSample manifest files for each driver CustomResourceDefintion have been provided in the samples folder to help with the installation of the drivers. These files follow the naming convention\n{driver name}_{driver version}_k8s_{k8 version}.yaml  Or\n{driver name}_{driver version}_ops_{OpenShift version}.yaml  For e.g.\n sample/powermax_v140_k8s_117.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on a Kubernetes 1.17 cluster sample/powermax_v140_ops_46.yaml* \u003c- To install CSI PowerMax driver v1.4.0 on an OpenShift 4.6 cluster  Copy the correct sample file and edit the mandatory \u0026 any optional parameters specific to your driver installation by following the instructions here\n NOTE: A detailed explanation of the various mandatory and optional fields in the CustomResource is available here. Please make sure to read through and understand the various fields.\n Run the following command to install the CSI driver.\nkubectl create -f \u003cdriver-manifest.yaml\u003e Note: If you are using an OLM based installation, the example manifests are available in the OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the installation Once the driver Custom Resource has been created, you can verify the installation\n  Check if Driver CR got created successfully\nFor e.g. – If you installed the PowerMax driver\n$ kubectl get csipowermax -n \u003cdriver-namespace\u003e   Check the status of the Custom Resource to verify if the driver installation was successful\n  If the driver-namespace was set to test-powermax, and the name of the driver is powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the details of the Custom Resource.\nNote: If the state of the CustomResource is Running then all the driver pods have been successfully installed. If the state is SuccessFul, then it means the driver deployment was successful but some driver pods may not be in a Running state. Please refer to the Troubleshooting section here if you encounter any issues during installation.\nChanges in installation for latest CSI drivers If you are installing the latest versions of the CSI drivers, the driver controller will be installed as a Kubernetes Deployment instead of a Statefulset. These installations can also run multiple replicas for the driver controller pods(not supported for StatefulSets) to support High Availability for the Controller.\nUpdate CSI Drivers The CSI Drivers installed by the Dell CSI Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include –\n Modifying the installation directly via kubectl edit For e.g. - If the name of the installed unity driver is unity, then run # Replace driver-namespace with the namespace where the Unity driver is installed $ kubectl edit csiunity/unity -n \u003cdriver-namespace\u003e and modify the installation\n Modify the API object in-place via kubectl patch  NOTES:\n If you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required. driver:configVersion:v2.1.0 Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, we will have to modify the below block while upgrading the driver.To get the volume health state add external-health-monitor sidecar in the sidecar section and valueunder controller set to true and the value under node set to true as shown below: i. Add controller and node section as below: controller:envs:- name:X_CSI_ENABLE_VOL_HEALTH_MONITORvalue:\"true\"dnsPolicy:ClusterFirstWithHostNetnode:envs:- name:X_CSI_ENABLE_VOL_HEALTH_MONITORvalue:\"true\"ii. Update the sidecar versions and add external-health-monitor sidecar if you want to enable health monitor of CSI volumes from Controller plugin:\nsideCars:- args:- --volume-name-prefix=csiunity- --default-fstype=ext4image:k8s.gcr.io/sig-storage/csi-provisioner:v3.0.0imagePullPolicy:IfNotPresentname:provisioner- args:- --snapshot-name-prefix=csiunitysnapimage:k8s.gcr.io/sig-storage/csi-snapshotter:v4.2.1imagePullPolicy:IfNotPresentname:snapshotter- args:- --monitor-interval=60simage:gcr.io/k8s-staging-sig-storage/csi-external-health-monitor-controller:v0.4.0imagePullPolicy:IfNotPresentname:external-health-monitor- image:k8s.gcr.io/sig-storage/csi-attacher:v3.3.0imagePullPolicy:IfNotPresentname:attacher- image:k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.3.0imagePullPolicy:IfNotPresentname:registrar- image:k8s.gcr.io/sig-storage/csi-resizer:v1.3.0imagePullPolicy:IfNotPresentname:resizer Configmap needs to be created with command kubectl create -f configmap.yaml using following yaml file.  kind:ConfigMapmetadata:name:unity-config-paramsnamespace:test-unitydata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"info\"ALLOW_RWO_MULTIPOD_ACCESS:\"false\"MAX_UNITY_VOLUMES_PER_NODE:\"0\"SYNC_NODE_INFO_TIME_INTERVAL:\"0\"TENANT_NAME:\"\"NOTE: Replicas in the driver CR file should not be greater than or equal to the number of worker nodes when upgrading the driver. If the Replicas count is not less than the worker node count, some of the driver controller pods would land in a pending state, and upgrade will not be successful. Driver controller pods go in a pending state because they have anti-affinity to each other and cannot be scheduled on nodes where there is a driver controller pod already running. Refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity for more details.\nNOTE: Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.\nNOTE: From v1.4.0 onwards, Dell CSI Operator does not support the creation of StorageClass and VolumeSnapshotClass objects. Although these fields are still present in the various driver CustomResourceDefinitions, they would be ignored by the operator. These fields will be removed from the CustomResourceDefinitions in a future release. If StorageClass and VolumeSnapshotClass need to be retained, you should upgrade the driver as per the recommended way noted above. StorageClass and VolumeSnapshotClass would not be retained on driver uninstallation.\nSupported modifications  Changing environment variable values for driver Adding (supported) environment variables Updating the image of the driver  Limitations  The Dell CSI Operator can’t manage any existing driver installed using Helm charts. If you already have installed one of the DellEMC CSI driver in your cluster and want to use the operator based deployment, uninstall the driver and then redeploy the driver following the installation procedure described above The Dell CSI Operator is not fully compliant with the OperatorHub React UI elements and some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSI Operator  Custom Resource Specification Each CSI Driver installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.\nBelow is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - Refer full list of supported driver for finding out the appropriate config version here replicas - Number of replicas for controller plugin - Must be set to 1 for all drivers\ndnsPolicy - Determines the dnsPolicy for the node daemonset. Accepted values are Default, ClusterFirst, ClusterFirstWithHostNet, None common\nThis field is mandatory and is used to specify common properties for both controller and the node plugin\n image - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values  Optional fields controller - List of environment variables and values which are applicable only for controller\nnode - List of environment variables and values which are applicable only for node\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver\nforceUpdate\nBoolean value which can be set to true in order to force update the status of the CSI Driver\ntolerations List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet\nIt should be set separately in the controller and node sections if you want separate set of tolerations for them\nnodeSelector Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet\nHere is a sample specification annotated with comments to explain each field\napiVersion:storage.dell.com/v1kind:CSIPowerMax# Type of the drivermetadata:name:test-powermax# Name of the drivernamespace:test-powermax# Namespace where driver is installedspec:driver:# Used to specify configuration versionconfigVersion:v3# Refer the table containing the full list of supported drivers to find the appropriate config versionreplicas:1forceUpdate:false# Set to true in case you want to force an update of driver statuscommon:# All common specificationimage:\"dellemc/csi-powermax:v1.4.0.000R\"#driver image for a particular releaseimagePullPolicy:IfNotPresentenvs:- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"You can set the field replicas to a higher number than 1 for the latest driver versions.\nNote - The image field should point to the correct image tag for version of the driver you are installing.\nFor e.g. - If you wish to install v1.4 of the CSI PowerMax driver, use the image tag dellemc/csi-powermax:v1.4.0.000R\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. The only exception to this is modifications requested by the documentation, for example, filling in blank IPs or other such system-specific data. Any modifications not specifically requested by the documentation should be only done after consulting with Dell EMC support.\nModify the driver specification  Choose the correct configVersion. Refer the table containing the full list of supported drivers and versions. Provide the namespace (in metadata section) where you want to install the driver. Provide a name (in metadata section) for the driver. This will be the name of the Custom Resource. Edit the values for mandatory configuration parameters specific to your installation. Edit/Add any values for optional configuration parameters to customize your installation. If you are installing the latest versions of the CSI drivers, the default number of replicas is set to 2. You can increase/decrease this value.  StorageClass and VolumeSnapshotClass New Installations You should not provide any StorageClass or VolumeSnapshotClass details during driver installation. The sample files for all the drivers have been updated to reflect this change. Even if these details are there in the sample files, StorageClass or VolumeSnapshotClass will not be created.\nWhat happens to my existing StorageClass \u0026 VolumeSnapshotClass objects  In case you are upgrading an existing driver installation by using kubectl edit or by patching the object in place, any existing objects will remain as is. If you added more objects as part of the upgrade, then this request will be ignored by the Operator. If you uninstall the older driver, then any StorageClass or VolumeSnapshotClass objects will be deleted. An uninstall and followed by an install of the driver would also result in StorageClass and VolumeSnapshotClass getting deleted and not getting created again.  NOTE: For more information on pre-requisites and parameters, please refer to the sub-pages below for each driver.\nNOTE: Storage Classes and Volume Snapshot Classes would no longer be created during the installation of the driver via an operator from v1.4.0 and higher.\n","excerpt":"The Dell CSI Operator is a Kubernetes Operator, which can be used to …","ref":"/csm-docs/v3/csidriver/installation/operator/","title":"Dell CSI Operator Installation Process"},{"body":"One of the goals of high availability is to eliminate single points of failure in a storage system. In Kubernetes, this can mean that a single PV represents multiple read/write enabled volumes on different arrays, located at reasonable distances with both the volumes in sync with each other. If one of the volumes goes down, there will still be another volume available for read and write. This kind of high availability can be achieved by using SRDF Metro replication mode supported only by Powermax arrays.\nSRDF Metro Architecture In SRDF metro configurations:\n R2 devices are Read/Write accessible to application hosts. Application host can write to both the R1 and R2 sides of the device pair. R2 devices assume the same external device identity(geometry, device WWN) as the R1 devices. All the above characteristic makes SRDF metro best suited for the scenarios in which high availability of data is desired.  With respect to Kubernetes, the SRDF metro mode works in single cluster scenarios. In the metro, both the arrays—arrays with SRDF metro link setup between them—involved in the replication are managed by the same csi-powermax driver. The replication is triggered by creating a volume using a StorageClass with metro-related parameters. The driver on receiving the metro-related parameters in the CreateVolume call creates a metro replicated volume and the details about both the volumes are returned in the volume context to the Kubernetes cluster. So, the PV created in the process represents a pair of metro replicated volumes. When a PV, representing a pair of metro replicated volumes, is claimed by a pod, the host treats each of the volumes represented by the single PV as a separate data path. The switching between the paths, to read and write the data, is managed by the multipath driver. The switching happens automatically, as configured by the user—in round-robin fashion or otherwise—or it can happen if one of the paths goes down. For details on Linux multipath driver setup, click here.\nThe creation of volumes in SRDF metro mode doesn’t involve the replication sidecar or the common controller, nor does it cause the creation of any replication related custom resources; it just needs a csi-powermax driver that implements the CreateVolume grpc endpoint with SRDF metro capability for it to work.\nUsage The metro replicated volumes are created just like the normal volumes, but the StorageClass contains some extra parameters related to metro replication. A StorageClass to create metro replicated volumes may look as follows:\nkind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:storage-class-metroprovisioner:driver.dellemc.comparameters:SRP:'SRP_1'SYMID:'000000000001'ServiceLevel:'Bronze'replication.storage.dell.com/IsReplicationEnabled:'true'replication.storage.dell.com/RdfGroup:'7'replication.storage.dell.com/RdfMode:'METRO'replication.storage.dell.com/RemoteRDFGroup:'7'replication.storage.dell.com/RemoteSYMID:'000000000002'replication.storage.dell.com/RemoteServiceLevel:'Bronze'reclaimPolicy:DeletevolumeBindingMode:Immediate Note: Different namespaces can share the same RDF group for creating volumes.\n Snapshots on SRDF Metro volumes A snapshot can be created on either of the volumes in the metro volume pair depending on the parameters in the VolumeSnapshotClass. The snapshots are by default created on the volumes on the R1 side of the SRDF metro pair, but if a Symmetrix id is specified in the VolumeSnapshotClass parameters, the driver creates the snapshot on the specified array; the specified array can either be the R1 or the R2 array. A VolumeSnapshotClass with symmetrix id specified in parameters may look as follows:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:sample-snapclassdriver:driver.dellemc.comdeletionPolicy:Deleteparameters:SYMID:'000000000001'","excerpt":"One of the goals of high availability is to eliminate single points of …","ref":"/csm-docs/docs/replication/high-availability/","title":"High Availability"},{"body":"One of the goals of high availability is to eliminate single points of failure in a storage system. In Kubernetes, this can mean that a single PV represents multiple read/write enabled volumes on different arrays, located at reasonable distances with both the volumes in sync with each other. If one of the volumes goes down, there will still be another volume available for read and write. This kind of high availability can be achieved by using SRDF Metro replication mode supported only by Powermax arrays.\nSRDF Metro Architecture In SRDF metro configurations:\n R2 devices are Read/Write accessible to application hosts. Application host can write to both the R1 and R2 sides of the device pair. R2 devices assume the same external device identity(geometry, device WWN) as the R1 devices. All the above characteristic makes SRDF metro best suited for the scenarios in which high availability of data is desired.  With respect to Kubernetes, the SRDF metro mode works in single cluster scenarios. In the metro, both the arrays—arrays with SRDF metro link setup between them—involved in the replication are managed by the same csi-powermax driver. The replication is triggered by creating a volume using a StorageClass with metro-related parameters. The driver on receiving the metro-related parameters in the CreateVolume call creates a metro replicated volume and the details about both the volumes are returned in the volume context to the Kubernetes cluster. So, the PV created in the process represents a pair of metro replicated volumes. When a PV, representing a pair of metro replicated volumes, is claimed by a pod, the host treats each of the volumes represented by the single PV as a separate data path. The switching between the paths, to read and write the data, is managed by the multipath driver. The switching happens automatically, as configured by the user—in round-robin fashion or otherwise—or it can happen if one of the paths goes down. For details on Linux multipath driver setup, click here.\nThe creation of volumes in SRDF metro mode doesn’t involve the replication sidecar or the common controller, nor does it cause the creation of any replication related custom resources; it just needs a csi-powermax driver that implements the CreateVolume grpc endpoint with SRDF metro capability for it to work.\nUsage The metro replicated volumes are created just like the normal volumes, but the StorageClass contains some extra parameters related to metro replication. A StorageClass to create metro replicated volumes may look as follows:\nkind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:storage-class-metroprovisioner:driver.dellemc.comparameters:SRP:'SRP_1'SYMID:'000000000001'ServiceLevel:'Bronze'replication.storage.dell.com/IsReplicationEnabled:'true'replication.storage.dell.com/RdfGroup:'7'replication.storage.dell.com/RdfMode:'METRO'replication.storage.dell.com/RemoteRDFGroup:'7'replication.storage.dell.com/RemoteSYMID:'000000000002'replication.storage.dell.com/RemoteServiceLevel:'Bronze'reclaimPolicy:DeletevolumeBindingMode:ImmediateSnapshots on SRDF Metro volumes A snapshot can be created on either of the volumes in the metro volume pair depending on the parameters in the VolumeSnapshotClass. The snapshots are by default created on the volumes on the R1 side of the SRDF metro pair, but if a Symmetrix id is specified in the VolumeSnapshotClass parameters, the driver creates the snapshot on the specified array; the specified array can either be the R1 or the R2 array. A VolumeSnapshotClass with symmetrix id specified in parameters may look as follows:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:sample-snapclassdriver:driver.dellemc.comdeletionPolicy:Deleteparameters:SYMID:'000000000001'","excerpt":"One of the goals of high availability is to eliminate single points of …","ref":"/csm-docs/v1/replication/high-availability/","title":"High Availability"},{"body":"One of the goals of high availability is to eliminate single points of failure in a storage system. In Kubernetes, this can mean that a single PV represents multiple read/write enabled volumes on different arrays, located at reasonable distances with both the volumes in sync with each other. If one of the volumes goes down, there will still be another volume available for read and write. This kind of high availability can be achieved by using SRDF Metro replication mode supported only by Powermax arrays.\nSRDF Metro Architecture In SRDF metro configurations:\n R2 devices are Read/Write accessible to application hosts. Application host can write to both the R1 and R2 sides of the device pair. R2 devices assume the same external device identity(geometry, device WWN) as the R1 devices. All the above characteristic makes SRDF metro best suited for the scenarios in which high availability of data is desired.  With respect to Kubernetes, the SRDF metro mode works in single cluster scenarios. In the metro, both the arrays—arrays with SRDF metro link setup between them—involved in the replication are managed by the same csi-powermax driver. The replication is triggered by creating a volume using a StorageClass with metro-related parameters. The driver on receiving the metro-related parameters in the CreateVolume call creates a metro replicated volume and the details about both the volumes are returned in the volume context to the Kubernetes cluster. So, the PV created in the process represents a pair of metro replicated volumes. When a PV, representing a pair of metro replicated volumes, is claimed by a pod, the host treats each of the volumes represented by the single PV as a separate data path. The switching between the paths, to read and write the data, is managed by the multipath driver. The switching happens automatically, as configured by the user—in round-robin fashion or otherwise—or it can happen if one of the paths goes down. For details on Linux multipath driver setup, click here.\nThe creation of volumes in SRDF metro mode doesn’t involve the replication sidecar or the common controller, nor does it cause the creation of any replication related custom resources; it just needs a csi-powermax driver that implements the CreateVolume grpc endpoint with SRDF metro capability for it to work.\nUsage The metro replicated volumes are created just like the normal volumes, but the StorageClass contains some extra parameters related to metro replication. A StorageClass to create metro replicated volumes may look as follows:\nkind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:storage-class-metroprovisioner:driver.dellemc.comparameters:SRP:'SRP_1'SYMID:'000000000001'ServiceLevel:'Bronze'replication.storage.dell.com/IsReplicationEnabled:'true'replication.storage.dell.com/RdfGroup:'7'replication.storage.dell.com/RdfMode:'METRO'replication.storage.dell.com/RemoteRDFGroup:'7'replication.storage.dell.com/RemoteSYMID:'000000000002'replication.storage.dell.com/RemoteServiceLevel:'Bronze'reclaimPolicy:DeletevolumeBindingMode:ImmediateSnapshots on SRDF Metro volumes A snapshot can be created on either of the volumes in the metro volume pair depending on the parameters in the VolumeSnapshotClass. The snapshots are by default created on the volumes on the R1 side of the SRDF metro pair, but if a Symmetrix id is specified in the VolumeSnapshotClass parameters, the driver creates the snapshot on the specified array; the specified array can either be the R1 or the R2 array. A VolumeSnapshotClass with symmetrix id specified in parameters may look as follows:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:sample-snapclassdriver:driver.dellemc.comdeletionPolicy:Deleteparameters:SYMID:'000000000001'","excerpt":"One of the goals of high availability is to eliminate single points of …","ref":"/csm-docs/v2/replication/high-availability/","title":"High Availability"},{"body":"One of the goals of high availability is to eliminate single points of failure in a storage system. In Kubernetes, this can mean that a single PV represents multiple read/write enabled volumes on different arrays, located at reasonable distances with both the volumes in sync with each other. If one of the volumes goes down, there will still be another volume available for read and write. This kind of high availability can be achieved by using SRDF Metro replication mode supported only by Powermax arrays.\nSRDF Metro Architecture In SRDF metro configurations:\n R2 devices are Read/Write accessible to application hosts. Application host can write to both the R1 and R2 sides of the device pair. R2 devices assume the same external device identity(geometry, device WWN) as the R1 devices. All the above characteristic makes SRDF metro best suited for the scenarios in which high availability of data is desired.  With respect to Kubernetes, the SRDF metro mode works in single cluster scenarios. In the metro, both the arrays—arrays with SRDF metro link setup between them—involved in the replication are managed by the same csi-powermax driver. The replication is triggered by creating a volume using a StorageClass with metro-related parameters. The driver on receiving the metro-related parameters in the CreateVolume call creates a metro replicated volume and the details about both the volumes are returned in the volume context to the Kubernetes cluster. So, the PV created in the process represents a pair of metro replicated volumes. When a PV, representing a pair of metro replicated volumes, is claimed by a pod, the host treats each of the volumes represented by the single PV as a separate data path. The switching between the paths, to read and write the data, is managed by the multipath driver. The switching happens automatically, as configured by the user—in round-robin fashion or otherwise—or it can happen if one of the paths goes down. For details on Linux multipath driver setup, click here.\nThe creation of volumes in SRDF metro mode doesn’t involve the replication sidecar or the common controller, nor does it cause the creation of any replication related custom resources; it just needs a csi-powermax driver that implements the CreateVolume grpc endpoint with SRDF metro capability for it to work.\nUsage The metro replicated volumes are created just like the normal volumes, but the StorageClass contains some extra parameters related to metro replication. A StorageClass to create metro replicated volumes may look as follows:\nkind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:storage-class-metroprovisioner:driver.dellemc.comparameters:SRP:'SRP_1'SYMID:'000000000001'ServiceLevel:'Bronze'replication.storage.dell.com/IsReplicationEnabled:'true'replication.storage.dell.com/RdfGroup:'7'replication.storage.dell.com/RdfMode:'METRO'replication.storage.dell.com/RemoteRDFGroup:'7'replication.storage.dell.com/RemoteSYMID:'000000000002'replication.storage.dell.com/RemoteServiceLevel:'Bronze'reclaimPolicy:DeletevolumeBindingMode:ImmediateSnapshots on SRDF Metro volumes A snapshot can be created on either of the volumes in the metro volume pair depending on the parameters in the VolumeSnapshotClass. The snapshots are by default created on the volumes on the R1 side of the SRDF metro pair, but if a Symmetrix id is specified in the VolumeSnapshotClass parameters, the driver creates the snapshot on the specified array; the specified array can either be the R1 or the R2 array. A VolumeSnapshotClass with symmetrix id specified in parameters may look as follows:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:sample-snapclassdriver:driver.dellemc.comdeletionPolicy:Deleteparameters:SYMID:'000000000001'","excerpt":"One of the goals of high availability is to eliminate single points of …","ref":"/csm-docs/v3/replication/high-availability/","title":"High Availability"},{"body":"Container Storage Modules (CSM) for Observability is part of the open-source suite of Kubernetes storage enablers for Dell products.\nIt is an OpenTelemetry agent that collects array-level metrics for Dell storage so they can be scraped into a Prometheus database. With CSM for Observability, you will gain visibility not only on the capacity of the volumes/file shares you manage with Dell CSM CSI (Container Storage Interface) drivers but also their performance in terms of bandwidth, IOPS, and response time.\nThanks to pre-packaged Grafana dashboards, you will be able to go through these metrics history and see the topology between a Kubernetes PV (Persistent Volume) and its translation as a LUN or file share in the backend array. This module also allows Kubernetes admins to collect array level metrics to check the overall capacity and performance directly from the Prometheus/Grafana tools rather than interfacing directly with the storage system itself.\nMetrics data is collected and pushed to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. SSL certificates for TLS between nodes are handled by cert-manager.\nCSM for Observability is composed of several services, each living in its own GitHub repository, that can be installed following one of the three deployments we support here. Contributions can be made to this repository or any of the CSM for Observability repositories listed below.\n  Name Repository Description     Performance Metrics for PowerFlex CSM Metrics for PowerFlex Performance Metrics for PowerFlex captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerFlex. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics so they can be visualized in Grafana. Please visit the repository for more information.   Performance Metrics for PowerStore CSM Metrics for PowerStore Performance Metrics for PowerStore captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerStore. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics so they can be visualized in Grafana. Please visit the repository for more information.   Volume Topology CSM Topology Topology provides Kubernetes administrators with the topology data related to containerized storage that is provisioned by a CSI (Container Storage Interface) Driver for Dell storage products. The Topology service is enabled by default as part of the CSM for Observability Helm Chart values file. Please visit the repository for more information.    CSM for Observability Capabilities CSM for Observability provides the following capabilities:\n  Capability PowerMax PowerFlex Unity XT PowerScale PowerStore     Collect and expose Volume Metrics via the OpenTelemetry Collector no yes no no yes   Collect and expose File System Metrics via the OpenTelemetry Collector no no no no yes   Collect and expose export (k8s) node metrics via the OpenTelemetry Collector no yes no no no   Collect and expose filesystem capacity metrics via the OpenTelemetry Collector no no no no yes   Collect and expose block storage capacity metrics via the OpenTelemetry Collector no yes no no yes   Non-disruptive config changes no yes no no yes   Non-disruptive log level changes no yes no no yes   Grafana Dashboards for displaying metrics and topology data no yes no no yes    Supported Operating Systems/Container Orchestrator Platforms   COP/OS Supported Versions     Kubernetes 1.22, 1.23, 1.24   Red Hat OpenShift 4.9, 4.10   Rancher Kubernetes Engine yes   RHEL 7.x, 8.x   CentOS 7.8, 7.9    Supported Storage Platforms    PowerFlex PowerStore     Storage Array 3.5.x, 3.6.x 1.0.x, 2.0.x, 2.1.x    Supported CSI Drivers CSM for Observability supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell PowerFlex csi-powerflex v2.0 +   CSI Driver for Dell PowerStore csi-powerstore v2.0 +    Topology Data CSM for Observability provides Kubernetes administrators with the topology data related to containerized storage. This topology data is visualized using Grafana:   Field Description     Namespace The namespace associated with the persistent volume claim   Persistent Volume The name of the persistent volume   Status The status of the persistent volume. “Released” indicates the persistent volume does not have a claim. “Bound” indicates the persistent volume has a claim   Persistent Volume Claim The name of the persistent volume claim associated with the persistent volume   CSI Driver The name of the CSI driver that was responsible for provisioning the volume on the storage system   Created The date the persistent volume was created   Provisioned Size The provisioned size of the persistent volume   Storage Class The storage class associated with the persistent volume   Storage System Volume Name The name of the volume on the storage system that is associated with the persistent volume   Storage Pool The storage pool name the volume/storage class is associated with   Storage System The storage system ID or IP address the volume is associated with   Protocol The storage system protocol type the volume/storage class is associated with    TLS Encryption CSM for Observability deployment relies on cert-manager to manage SSL certificates that are used to encrypt communication between various components. When deploying CSM for Observability, cert-manager is installed and configured automatically. The cert-manager components listed below will be installed alongside CSM for Observability.\n  Component     cert-manager   cert-manager-cainjector   cert-manager-webhook    If desired you may provide your own certificate key pair to be used inside the cluster by providing the path to the certificate and key in the Helm chart config. If you do not provide a certificate, one will be generated for you on installation.\n NOTE: The certificate provided must be a CA certificate. This is to facilitate automated certificate rotation.\n Viewing Logs Logs can be viewed by using the kubectl logs CLI command to output logs for a specific Pod or Deployment.\nFor example, the following script will capture logs of all Pods in the CSM namespace and save the output to one file per Pod.\n#!/bin/bash namespace=[CSM_NAMESPACE] for pod in $(kubectl get pods -n $namespace -o name); do logFileName=$(echo $pod | tr / -).txt kubectl logs -n $namespace $pod --all-containers \u003e $logFileName done ","excerpt":"Container Storage Modules (CSM) for Observability is part of the …","ref":"/csm-docs/docs/observability/","title":"Observability"},{"body":"Container Storage Modules (CSM) for Observability is part of the open-source suite of Kubernetes storage enablers for Dell products.\nIt is an OpenTelemetry agent that collects array-level metrics for Dell storage so they can be scraped into a Prometheus database. With CSM for Observability, you will gain visibility not only on the capacity of the volumes/file shares you manage with Dell CSM CSI (Container Storage Interface) drivers but also their performance in terms of bandwidth, IOPS, and response time.\nThanks to pre-packaged Grafana dashboards, you will be able to go through these metrics history and see the topology between a Kubernetes PV (Persistent Volume) and its translation as a LUN or file share in the backend array. This module also allows Kubernetes admins to collect array level metrics to check the overall capacity and performance directly from the Prometheus/Grafana tools rather than interfacing directly with the storage system itself.\nMetrics data is collected and pushed to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. SSL certificates for TLS between nodes are handled by cert-manager.\nCSM for Observability is composed of several services, each living in its own GitHub repository, that can be installed following one of the three deployments we support here. Contributions can be made to this repository or any of the CSM for Observability repositories listed below.\n  Name Repository Description     Performance Metrics for PowerFlex CSM Metrics for PowerFlex Performance Metrics for PowerFlex captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerFlex. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics so they can be visualized in Grafana. Please visit the repository for more information.   Performance Metrics for PowerStore CSM Metrics for PowerStore Performance Metrics for PowerStore captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerStore. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics so they can be visualized in Grafana. Please visit the repository for more information.   Volume Topology CSM Topology Topology provides Kubernetes administrators with the topology data related to containerized storage that is provisioned by a CSI (Container Storage Interface) Driver for Dell storage products. The Topology service is enabled by default as part of the CSM for Observability Helm Chart values file. Please visit the repository for more information.    CSM for Observability Capabilities CSM for Observability provides the following capabilities:\n  Capability PowerMax PowerFlex Unity PowerScale PowerStore     Collect and expose Volume Metrics via the OpenTelemetry Collector no yes no no yes   Collect and expose File System Metrics via the OpenTelemetry Collector no no no no yes   Collect and expose export (k8s) node metrics via the OpenTelemetry Collector no yes no no no   Collect and expose filesystem capacity metrics via the OpenTelemetry Collector no no no no yes   Collect and expose block storage capacity metrics via the OpenTelemetry Collector no yes no no yes   Non-disruptive config changes no yes no no yes   Non-disruptive log level changes no yes no no yes   Grafana Dashboards for displaying metrics and topology data no yes no no yes    Supported Operating Systems/Container Orchestrator Platforms   COP/OS Supported Versions     Kubernetes 1.21, 1.22, 1.23   Red Hat OpenShift 4.8, 4.9   Rancher Kubernetes Engine yes   RHEL 7.x, 8.x   CentOS 7.8, 7.9    Supported Storage Platforms    PowerFlex PowerStore     Storage Array 3.5.x, 3.6.x 1.0.x, 2.0.x, 2.1.x    Supported CSI Drivers CSM for Observability supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell PowerFlex csi-powerflex v2.0, v2.1, v2.2   CSI Driver for Dell PowerStore csi-powerstore v2.0, v2.1, v2.2    Topology Data CSM for Observability provides Kubernetes administrators with the topology data related to containerized storage. This topology data is visualized using Grafana:   Field Description     Namespace The namespace associated with the persistent volume claim   Persistent Volume The name of the persistent volume   Status The status of the persistent volume. “Released” indicating the persistent volume has a claim. “Bound” indicating the persistent volume has a claim   Persistent Volume Claim The name of the persistent volume claim associated with the persistent volume   CSI Driver The name of the CSI driver that was responsible for provisioning the volume on the storage system   Created The date the persistent volume was created   Provisioned Size The provisioned size of the persistent volume   Storage Class The storage class associated with the persistent volume   Storage System Volume Name The name of the volume on the storage system that is associated with the persistent volume   Storage Pool The storage pool name the volume/storage class is associated with   Storage System The storage system ID or IP address the volume is associated with   Protocol The storage system protocol type the volume/storage class is associated with    TLS Encryption CSM for Observability deployment relies on cert-manager to manage SSL certificates that are used to encrypt communication between various components. When deploying CSM for Observability, cert-manager is installed and configured automatically. The cert-manager components listed below will be installed alongside CSM for Observability.\n  Component     cert-manager   cert-manager-cainjector   cert-manager-webhook    If desired you may provide your own certificate key pair to be used inside the cluster by providing the path to the certificate and key in the Helm chart config. If you do not provide a certificate, one will be generated for you on installation.\n NOTE: The certificate provided must be a CA certificate. This is to facilitate automated certificate rotation.\n Viewing Logs Logs can be viewed by using the kubectl logs CLI command to output logs for a specific Pod or Deployment.\nFor example, the following script will capture logs of all Pods in the CSM namespace and save the output to one file per Pod.\n#!/bin/bash namespace=[CSM_NAMESPACE] for pod in $(kubectl get pods -n $namespace -o name); do logFileName=$(echo $pod | tr / -).txt kubectl logs -n $namespace $pod --all-containers \u003e $logFileName done ","excerpt":"Container Storage Modules (CSM) for Observability is part of the …","ref":"/csm-docs/v1/observability/","title":"Observability"},{"body":"Container Storage Modules (CSM) for Observability is part of the open-source suite of Kubernetes storage enablers for Dell products.\nIt is an OpenTelemetry agent that collects array-level metrics for Dell storage so they can be scraped into a Prometheus database. With CSM for Observability, you will gain visibility not only on the capacity of the volumes/file shares you manage with Dell CSM CSI (Container Storage Interface) drivers but also their performance in terms of bandwidth, IOPS, and response time.\nThanks to pre-packaged Grafana dashboards, you will be able to go through these metrics history and see the topology between a Kubernetes PV (Persistent Volume) and its translation as a LUN or file share in the backend array. This module also allows Kubernetes admins to collect array level metrics to check the overall capacity and performance directly from the Prometheus/Grafana tools rather than interfacing directly with the storage system itself.\nMetrics data is collected and pushed to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. SSL certificates for TLS between nodes are handled by cert-manager.\nCSM for Observability is composed of several services, each living in its own GitHub repository, that can be installed following one of the three deployments we support here. Contributions can be made to this repository or any of the CSM for Observability repositories listed below.\n  Name Repository Description     Performance Metrics for PowerFlex CSM Metrics for PowerFlex Performance Metrics for PowerFlex captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerFlex. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics so they can be visualized in Grafana. Please visit the repository for more information.   Performance Metrics for PowerStore CSM Metrics for PowerStore Performance Metrics for PowerStore captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell PowerStore. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics so they can be visualized in Grafana. Please visit the repository for more information.   Volume Topology CSM Topology Topology provides Kubernetes administrators with the topology data related to containerized storage that is provisioned by a CSI (Container Storage Interface) Driver for Dell storage products. The Topology service is enabled by default as part of the CSM for Observability Helm Chart values file. Please visit the repository for more information.    CSM for Observability Capabilities CSM for Observability provides the following capabilities:\n  Capability PowerMax PowerFlex Unity PowerScale PowerStore     Collect and expose Volume Metrics via the OpenTelemetry Collector no yes no no yes   Collect and expose File System Metrics via the OpenTelemetry Collector no no no no yes   Collect and expose export (k8s) node metrics via the OpenTelemetry Collector no yes no no no   Collect and expose filesystem capacity metrics via the OpenTelemetry Collector no no no no yes   Collect and expose block storage capacity metrics via the OpenTelemetry Collector no yes no no yes   Non-disruptive config changes no yes no no yes   Non-disruptive log level changes no yes no no yes   Grafana Dashboards for displaying metrics and topology data no yes no no yes    Supported Operating Systems/Container Orchestrator Platforms   COP/OS Supported Versions     Kubernetes 1.21, 1.22, 1.23   Red Hat OpenShift 4.8, 4.9   Rancher Kubernetes Engine yes   RHEL 7.x, 8.x   CentOS 7.8, 7.9    Supported Storage Platforms    PowerFlex PowerStore     Storage Array 3.5.x, 3.6.x 1.0.x, 2.0.x, 2.1.x    Supported CSI Drivers CSM for Observability supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell PowerFlex csi-powerflex v2.0, v2.1, v2.2   CSI Driver for Dell PowerStore csi-powerstore v2.0, v2.1, v2.2    Topology Data CSM for Observability provides Kubernetes administrators with the topology data related to containerized storage. This topology data is visualized using Grafana:   Field Description     Namespace The namespace associated with the persistent volume claim   Persistent Volume The name of the persistent volume   Status The status of the persistent volume. “Released” indicating the persistent volume has a claim. “Bound” indicating the persistent volume has a claim   Persistent Volume Claim The name of the persistent volume claim associated with the persistent volume   CSI Driver The name of the CSI driver that was responsible for provisioning the volume on the storage system   Created The date the persistent volume was created   Provisioned Size The provisioned size of the persistent volume   Storage Class The storage class associated with the persistent volume   Storage System Volume Name The name of the volume on the storage system that is associated with the persistent volume   Storage Pool The storage pool name the volume/storage class is associated with   Storage System The storage system ID or IP address the volume is associated with   Protocol The storage system protocol type the volume/storage class is associated with    TLS Encryption CSM for Observability deployment relies on cert-manager to manage SSL certificates that are used to encrypt communication between various components. When deploying CSM for Observability, cert-manager is installed and configured automatically. The cert-manager components listed below will be installed alongside CSM for Observability.\n  Component     cert-manager   cert-manager-cainjector   cert-manager-webhook    If desired you may provide your own certificate key pair to be used inside the cluster by providing the path to the certificate and key in the Helm chart config. If you do not provide a certificate, one will be generated for you on installation.\n NOTE: The certificate provided must be a CA certificate. This is to facilitate automated certificate rotation.\n Viewing Logs Logs can be viewed by using the kubectl logs CLI command to output logs for a specific Pod or Deployment.\nFor example, the following script will capture logs of all Pods in the CSM namespace and save the output to one file per Pod.\n#!/bin/bash namespace=[CSM_NAMESPACE] for pod in $(kubectl get pods -n $namespace -o name); do logFileName=$(echo $pod | tr / -).txt kubectl logs -n $namespace $pod --all-containers \u003e $logFileName done ","excerpt":"Container Storage Modules (CSM) for Observability is part of the …","ref":"/csm-docs/v2/observability/","title":"Observability"},{"body":"Container Storage Modules (CSM) for Observability is part of the open-source suite of Kubernetes storage enablers for Dell EMC products.\nIt is an OpenTelemetry agent that collects array-level metrics for Dell EMC storage so they can be scraped into a Prometheus database. With CSM for Observability, you will gain visibility not only on the capacity of the volumes/file shares you manage with Dell CSM CSI (Container Storage Interface) drivers but also their performance in terms of bandwidth, IOPS, and response time.\nThanks to pre-packaged Grafana dashboards, you will be able to go through these metrics history and see the topology between a Kubernetes PV (Persistent Volume) and its translation as a LUN or file share in the backend array. This module also allows Kubernetes admins to collect array level metrics to check the overall capacity and performance directly from the Prometheus/Grafana tools rather than interfacing directly with the storage system itself.\nMetrics data is collected and pushed to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. SSL certificates for TLS between nodes are handled by cert-manager.\nCSM for Observability is composed of several services, each living in its own GitHub repository. Contributions can be made to this repository or any of the CSM for Observability repositories listed below.\n  Name Repository Description     Performance Metrics for PowerFlex CSM Metrics for PowerFlex Performance Metrics for PowerFlex captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell EMC PowerFlex. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics so they can be visualized in Grafana. Please visit the repository for more information.   Performance Metrics for PowerStore CSM Metrics for PowerStore Performance Metrics for PowerStore captures telemetry data about Kubernetes storage usage and performance obtained through the CSI (Container Storage Interface) Driver for Dell EMC PowerStore. The metrics service pushes it to the OpenTelemetry Collector, so it can be processed, and exported in a format consumable by Prometheus. Prometheus can then be configured to scrape the OpenTelemetry Collector exporter endpoint to provide metrics so they can be visualized in Grafana. Please visit the repository for more information.   Volume Topology CSM Topology Topology provides Kubernetes administrators with the topology data related to containerized storage that is provisioned by a CSI (Container Storage Interface) Driver for Dell EMC storage products. Please visit the repository for more information.    CSM for Observability Capabilities CSM for Observability provides the following capabilities:\n  Capability PowerMax PowerFlex Unity PowerScale PowerStore     Collect and expose Volume Metrics via the OpenTelemetry Collector no yes no no yes   Collect and expose File System Metrics via the OpenTelemetry Collector no no no no yes   Collect and expose export (k8s) node metrics via the OpenTelemetry Collector no yes no no no   Collect and expose filesystem capacity metrics via the OpenTelemetry Collector no no no no yes   Collect and expose block storage capacity metrics via the OpenTelemetry Collector no yes no no yes   Non-disruptive config changes no yes no no yes   Non-disruptive log level changes no yes no no yes   Grafana Dashboards for displaying metrics and topology data no yes no no yes    Supported Operating Systems/Container Orchestrator Platforms   COP/OS Supported Versions     Kubernetes 1.20, 1.21, 1.22   Red Hat OpenShift 4.8, 4.9   Rancher Kubernetes Engine yes   RHEL 7.x, 8.x   CentOS 7.8, 7.9    Supported Storage Platforms    PowerFlex PowerStore     Storage Array 3.5.x, 3.6.x 1.0.x, 2.0.x    Supported CSI Drivers CSM for Observability supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell EMC PowerFlex csi-powerflex v2.0,v2.1   CSI Driver for Dell EMC PowerStore csi-powerstore v2.0,v2.1    Topology Data CSM for Observability provides Kubernetes administrators with the topology data related to containerized storage. This topology data is visualized using Grafana:   Field Description     Namespace The namespace associated with the persistent volume claim   Persistent Volume The name of the persistent volume   Status The status of the persistent volume. “Released” indicating the persistent volume has a claim. “Bound” indicating the persistent volume has a claim   Persistent Volume Claim The name of the persistent volume claim associated with the persistent volume   CSI Driver The name of the CSI driver that was responsible for provisioning the volume on the storage system   Created The date the persistent volume was created   Provisioned Size The provisioned size of the persistent volume   Storage Class The storage class associated with the persistent volume   Storage System Volume Name The name of the volume on the storage system that is associated with the persistent volume   Storage Pool The storage pool name the volume/storage class is associated with   Storage System The storage system ID or IP address the volume is associated with   Protocol The storage system protocol type the volume/storage class is associated with    TLS Encryption CSM for Observability deployment relies on cert-manager to manage SSL certificates that are used to encrypt communication between various components. When deploying CSM for Observability, cert-manager is installed and configured automatically. The cert-manager components listed below will be installed alongside CSM for Observability.\n  Component     cert-manager   cert-manager-cainjector   cert-manager-webhook    If desired you may provide your own certificate key pair to be used inside the cluster by providing the path to the certificate and key in the Helm chart config. If you do not provide a certificate, one will be generated for you on installation.\n NOTE: The certificate provided must be a CA certificate. This is to facilitate automated certificate rotation.\n Viewing Logs Logs can be viewed by using the kubectl logs CLI command to output logs for a specific Pod or Deployment.\nFor example, the following script will capture logs of all Pods in the CSM namespace and save the output to one file per Pod.\n#!/bin/bash namespace=[CSM_NAMESPACE] for pod in $(kubectl get pods -n $namespace -o name); do logFileName=$(echo $pod | tr / -).txt kubectl logs -n $namespace $pod --all-containers \u003e $logFileName done ","excerpt":"Container Storage Modules (CSM) for Observability is part of the …","ref":"/csm-docs/v3/observability/","title":"Observability"},{"body":"Release Notes - CSM Observability 1.2.0 New Features/Changes Fixed Issues  PowerStore Grafana dashboard does not populate correctly  Grafana installation script - prometheus address is incorrect prometheus-values.yaml error in json  Known Issues ","excerpt":"Release Notes - CSM Observability 1.2.0 New Features/Changes Fixed …","ref":"/csm-docs/docs/observability/release/","title":"Release notes"},{"body":"Replication Enabled Storage Classes In order to create replicated volumes \u0026 volume groups, you need to add some extra parameters to your storage class definition. These extra parameters generally carry the prefix replication.storage.dell.com to differentiate them from other provisioning parameters.\nReplication enabled storage classes are always created in pairs within/across clusters and are generally mirrors of each other. Before provisioning replicated volumes, make sure that these pairs of storage classes are created properly.\nCommon Parameters There are 3 mandatory key/value pairs which should always be present in the storage class parameters -\nreplication.storage.dell.com/isReplicationEnabled:'true'replication.storage.dell.com/remoteClusterID:\u003cRemoteClusterId\u003ereplication.storage.dell.com/remoteStorageClassName:\u003cRemoteScName\u003eremoteClusterID This should contain the Cluster ID of the remote cluster where the replicated volume is going to be created. In case of a single stretched cluster, it should be always set to self\nremoteStorageClassName This should contain the name of the storage class on the remote cluster which is used to create the remote PersistentVolume.\n Note: You still need to create a pair of storage classes even while using a single stretched cluster\n Driver specific parameters Please refer to the driver specific sections for PowerMax, PowerStore or PowerScale for a detailed list of parameters.\nPV sync Deletion The dell-csm-replicator supports ‘sync deletion’ of replicated PV resources i.e when a replication enabled PV is deleted its corresponding source or target PV can also be deleted.\nThe decision to whether or not sync delete the corresponding PV depends on a Storage Class parameter which can be configured by the user.\nreplication.storage.dell.com/remotePVRetentionPolicy: 'delete' | 'retain' If the remotePVRetentionPolicy is set to ‘delete’, the corresponding PV would be deleted.\nIf the remotePVRetentionPolicy is set to ‘retain’, the corresponding PV would be retained.\nBy default, if the remotePVRetentionPolicy is not specified in the Storage Class, replicated PV resources are retained.\nRG sync Deletion The dell-csm-replicator supports ‘sync deletion’ of RG (DellCSIReplicationGroup) resources i.e when an RG is deleted its corresponding source or target RG can also be deleted.\nThe decision to whether or not sync delete the corresponding RG depends on a Storage Class parameter which can be configured by the user.\nreplication.storage.dell.com/remoteRGRetentionPolicy: 'delete' | 'retain' If the remoteRGRetentionPolicy is set to ‘delete’, the corresponding RG would be deleted.\nIf the remoteRGRetentionPolicy is set to ‘retain’, the corresponding RG would be retained.\nBy default, if the remoteRGRetentionPolicy is not specified in the Storage Class, replicated RG resources are retained.\nExample If you are setting up replication between two clusters with ClusterID set to Cluster A \u0026 Cluster B, then the storage class definitions in both the clusters would look like -\nCluster A apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:rep-srcparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteClusterID:ClusterBreplication.storage.dell.com/remoteStorageClassName:rep-tgt# Some driver specific replication \u0026 non-replication related paramsprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:ImmediateCluster B apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:rep-tgtparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteClusterID:ClusterAreplication.storage.dell.com/remoteStorageClassName:rep-src# Some driver specific replication \u0026 non-replication related paramsprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediate","excerpt":"Replication Enabled Storage Classes In order to create replicated …","ref":"/csm-docs/docs/replication/deployment/storageclasses/","title":"Storage Class"},{"body":"Replication Enabled Storage Classes In order to create replicated volumes \u0026 volume groups, you need to add some extra parameters to your storage class definition. These extra parameters generally carry the prefix replication.storage.dell.com to differentiate them from other provisioning parameters.\nReplication enabled storage classes are always created in pairs within/across clusters and are generally mirrors of each other. Before provisioning replicated volumes, make sure that these pairs of storage classes are created properly.\nCommon Parameters There are 3 mandatory key/value pairs which should always be present in the storage class parameters -\nreplication.storage.dell.com/isReplicationEnabled:'true'replication.storage.dell.com/remoteClusterID:\u003cRemoteClusterId\u003ereplication.storage.dell.com/remoteStorageClassName:\u003cRemoteScName\u003eremoteClusterID This should contain the Cluster ID of the remote cluster where the replicated volume is going to be created. In case of a single stretched cluster, it should be always set to self\nremoteStorageClassName This should contain the name of the storage class on the remote cluster which is used to create the remote PersistentVolume.\n Note: You still need to create a pair of storage classes even while using a single stretched cluster\n Driver specific parameters Please refer to the driver specific sections for PowerMax, PowerStore or PowerScale for a detailed list of parameters.\nPV sync Deletion The dell-csm-replicator supports ‘sync deletion’ of replicated PV resources i.e when a replication enabled PV is deleted its corresponding source or target PV can also be deleted.\nThe decision to whether or not sync delete the corresponding PV depends on a Storage Class parameter which can be configured by the user.\nreplication.storage.dell.com/remotePVRetentionPolicy: 'delete' | 'retain' If the remotePVRetentionPolicy is set to ‘delete’, the corresponding PV would be deleted.\nIf the remotePVRetentionPolicy is set to ‘retain’, the corresponding PV would be retained.\nBy default, if the remotePVRetentionPolicy is not specified in the Storage Class, replicated PV resources are retained.\nRG sync Deletion The dell-csm-replicator supports ‘sync deletion’ of RG (DellCSIReplicationGroup) resources i.e when an RG is deleted its corresponding source or target RG can also be deleted.\nThe decision to whether or not sync delete the corresponding RG depends on a Storage Class parameter which can be configured by the user.\nreplication.storage.dell.com/remoteRGRetentionPolicy: 'delete' | 'retain' If the remoteRGRetentionPolicy is set to ‘delete’, the corresponding RG would be deleted.\nIf the remoteRGRetentionPolicy is set to ‘retain’, the corresponding RG would be retained.\nBy default, if the remoteRGRetentionPolicy is not specified in the Storage Class, replicated RG resources are retained.\nExample If you are setting up replication between two clusters with ClusterID set to Cluster A \u0026 Cluster B, then the storage class definitions in both the clusters would look like -\nCluster A apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:rep-srcparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteClusterID:ClusterBreplication.storage.dell.com/remoteStorageClassName:rep-tgt# Some driver specific replication \u0026 non-replication related paramsprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:ImmediateCluster B apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:rep-tgtparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteClusterID:ClusterAreplication.storage.dell.com/remoteStorageClassName:rep-src# Some driver specific replication \u0026 non-replication related paramsprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediate","excerpt":"Replication Enabled Storage Classes In order to create replicated …","ref":"/csm-docs/v1/replication/deployment/storageclasses/","title":"Storage Class"},{"body":"Replication Enabled Storage Classes In order to create replicated volumes \u0026 volume groups, you need to add some extra parameters to your storage class definition. These extra parameters generally carry the prefix replication.storage.dell.com to differentiate them from other provisioning parameters.\nReplication enabled storage classes are always created in pairs within/across clusters and are generally mirrors of each other. Before provisioning replicated volumes, make sure that these pairs of storage classes are created properly.\nCommon Parameters There are 3 mandatory key/value pairs which should always be present in the storage class parameters -\nreplication.storage.dell.com/isReplicationEnabled:'true'replication.storage.dell.com/remoteClusterID:\u003cRemoteClusterId\u003ereplication.storage.dell.com/remoteStorageClassName:\u003cRemoteScName\u003eremoteClusterID This should contain the Cluster ID of the remote cluster where the replicated volume is going to be created. In case of a single stretched cluster, it should be always set to self\nremoteStorageClassName This should contain the name of the storage class on the remote cluster which is used to create the remote PersistentVolume.\n Note: You still need to create a pair of storage classes even while using a single stretched cluster\n Driver specific parameters Please refer to the driver specific sections for PowerMax, PowerStore or PowerScale for a detailed list of parameters.\nPV sync Deletion The dell-csm-replicator supports ‘sync deletion’ of replicated PV resources i.e when a replication enabled PV is deleted its corresponding source or target PV can also be deleted.\nThe decision to whether or not sync delete the corresponding PV depends on a Storage Class parameter which can be configured by the user.\nreplication.storage.dell.com/remotePVRetentionPolicy: 'delete' | 'retain' If the remotePVRetentionPolicy is set to ‘delete’, the corresponding PV would be deleted.\nIf the remotePVRetentionPolicy is set to ‘retain’, the corresponding PV would be retained.\nBy default, if the remotePVRetentionPolicy is not specified in the Storage Class, replicated PV resources are retained.\nRG sync Deletion The dell-csm-replicator supports ‘sync deletion’ of RG (DellCSIReplicationGroup) resources i.e when an RG is deleted its corresponding source or target RG can also be deleted.\nThe decision to whether or not sync delete the corresponding RG depends on a Storage Class parameter which can be configured by the user.\nreplication.storage.dell.com/remoteRGRetentionPolicy: 'delete' | 'retain' If the remoteRGRetentionPolicy is set to ‘delete’, the corresponding RG would be deleted.\nIf the remoteRGRetentionPolicy is set to ‘retain’, the corresponding RG would be retained.\nBy default, if the remoteRGRetentionPolicy is not specified in the Storage Class, replicated RG resources are retained.\nExample If you are setting up replication between two clusters with ClusterID set to Cluster A \u0026 Cluster B, then the storage class definitions in both the clusters would look like -\nCluster A apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:rep-srcparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteClusterID:ClusterBreplication.storage.dell.com/remoteStorageClassName:rep-tgt# Some driver specific replication \u0026 non-replication related paramsprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:ImmediateCluster B apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:rep-tgtparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteClusterID:ClusterAreplication.storage.dell.com/remoteStorageClassName:rep-src# Some driver specific replication \u0026 non-replication related paramsprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediate","excerpt":"Replication Enabled Storage Classes In order to create replicated …","ref":"/csm-docs/v2/replication/deployment/storageclasses/","title":"Storage Class"},{"body":"Replication Enabled Storage Classes In order to create replicated volumes \u0026 volume groups, you need to add some extra parameters to your storage class definition. These extra parameters generally carry the prefix replication.storage.dell.com to differentiate them from other provisioning parameters.\nReplication enabled storage classes are always created in pairs within/across clusters and are generally mirrors of each other. Before provisioning replicated volumes, make sure that these pairs of storage classes are created properly.\nCommon Parameters There are 3 mandatory key/value pairs which should always be present in the storage class parameters -\nreplication.storage.dell.com/isReplicationEnabled:'true'replication.storage.dell.com/remoteClusterID:\u003cRemoteClusterId\u003ereplication.storage.dell.com/remoteStorageClassName:\u003cRemoteScName\u003eremoteClusterID This should contain the Cluster ID of the remote cluster where the replicated volume is going to be created. In case of a single stretched cluster, it should be always set to self\nremoteStorageClassName This should contain the name of the storage class on the remote cluster which is used to create the remote PersistentVolume.\n Note: You still need to create a pair of storage classes even while using a single stretched cluster\n Driver specific parameters Please refer to the driver specific sections for PowerMax \u0026 PowerStore for a detailed list of parameters.\nPV sync Deletion The dell-csm-replicator supports ‘sync deletion’ of replicated PV resources i.e when a replication enabled PV is deleted its corresponding source or target PV can also be deleted.\nThe decision to whether or not sync delete the corresponding PV depends on a Storage Class parameter which can be configured by the user.\nreplication.storage.dell.com/remotePVRetentionPolicy: 'delete' | 'retain' If the remotePVRetentionPolicy is set to ‘delete’, the corresponding PV would be deleted.\nIf the remotePVRetentionPolicy is set to ‘retain’, the corresponding PV would be retained.\nBy default, if the remotePVRetentionPolicy is not specified in the Storage Class, replicated PV resources are retained.\nRG sync Deletion The dell-csm-replicator supports ‘sync deletion’ of RG (DellCSIReplicationGroup) resources i.e when an RG is deleted its corresponding source or target RG can also be deleted.\nThe decision to whether or not sync delete the corresponding RG depends on a Storage Class parameter which can be configured by the user.\nreplication.storage.dell.com/remoteRGRetentionPolicy: 'delete' | 'retain' If the remoteRGRetentionPolicy is set to ‘delete’, the corresponding RG would be deleted.\nIf the remoteRGRetentionPolicy is set to ‘retain’, the corresponding RG would be retained.\nBy default, if the remoteRGRetentionPolicy is not specified in the Storage Class, replicated RG resources are retained.\nExample If you are setting up replication between two clusters with ClusterID set to Cluster A \u0026 Cluster B, then the storage class definitions in both the clusters would look like -\nCluster A apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:rep-srcparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteClusterID:ClusterBreplication.storage.dell.com/remoteStorageClassName:rep-tgt# Some driver specific replication \u0026 non-replication related paramsprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:ImmediateCluster B apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:rep-tgtparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteClusterID:ClusterAreplication.storage.dell.com/remoteStorageClassName:rep-src# Some driver specific replication \u0026 non-replication related paramsprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediate","excerpt":"Replication Enabled Storage Classes In order to create replicated …","ref":"/csm-docs/v3/replication/deployment/storageclasses/","title":"Storage Class"},{"body":" Running karavictl tenant commands result in an HTTP 504 error Installation fails to install policies After installation, the create-pvc Pod is in an Error state   Retrieve CSM Authorization Server Logs To retrieve logs from services on the CSM Authorization Server, run the following command (e.g proxy-server logs):\n$ k3s kubectl logs deploy/proxy-server -n karavi -c proxy-server For OPA related logs, run:\n$ k3s kubectl logs deploy/proxy-server -n karavi -c opa Running “karavictl tenant” commands result in an HTTP 504 error This situation may occur if there are Iptables or other firewall rules preventing communication with the provided \u003cgrpc-address\u003e:\n$ karavictl tenant list --addr \u003cgrpc-address\u003e { \"ErrorMsg\": \"rpc error: code = Unavailable desc = Gateway Timeout: HTTP status code 504; transport: received the unexpected content-type \\\"text/plain; charset=utf-8\\\"\" } Resolution\nConsult with your system administrator or Iptables/firewall documentation. If there are rules in place to prevent communication with the \u003cgrpc-address\u003e, either new rules must be created or existing rules must be updated.\nInstallation fails to install policies If SELinux is enabled, the policies may fail to install:\nerror: failed to install policies (see /tmp/policy-install-for-karavi3163047435): exit status 1 Resolution\nView the contents /tmp/policy-install-for-karavi* file listed in the error message. If there is a Permission denied error while running the policy-install.sh script, manually run the script to install policies.\n$ cat /tmp/policy-install-for-karavi3163047435 # find the location of the policy-install.sh script located in the file and manually run the script $ /tmp/karavi-installer-2908017483/policy-install.sh After installation, the create-pvc Pod is in an Error state If SELinux is enabled, the create-pvc Pod may be in an Error state:\nkube-system create-pvc-44a763c7-e70f-4e32-a114-e94615041042 0/1 Error 0 102s Resolution\nRun the following commands to allow the PVC to be created:\n$ semanage fcontext -a -t container_file_t \"/var/lib/rancher/k3s/storage(/.*)?\" $ restorecon -R /var/lib/rancher/k3s/storage/ ","excerpt":" Running karavictl tenant commands result in an HTTP 504 error …","ref":"/csm-docs/docs/authorization/troubleshooting/","title":"Troubleshooting"},{"body":"","excerpt":"","ref":"/csm-docs/docs/csidriver/troubleshooting/","title":"Troubleshooting"},{"body":" Running karavictl tenant commands result in an HTTP 504 error   Retrieve CSM Authorization Server Logs To retrieve logs from services on the CSM Authorization Server, run the following command (e.g proxy-server logs):\n$ k3s kubectl logs deploy/proxy-server -n karavi -c proxy-server For OPA related logs, run:\n$ k3s kubectl logs deploy/proxy-server -n karavi -c opa Running “karavictl tenant” commands result in an HTTP 504 error This situation may occur if there are Iptables or other firewall rules preventing communication with the provided \u003cgrpc-address\u003e:\n$ karavictl tenant list --addr \u003cgrpc-address\u003e { \"ErrorMsg\": \"rpc error: code = Unavailable desc = Gateway Timeout: HTTP status code 504; transport: received the unexpected content-type \\\"text/plain; charset=utf-8\\\"\" } Resolution\nConsult with your system administrator or Iptables/firewall documentation. If there are rules in place to prevent communication with the \u003cgrpc-address\u003e, either new rules must be created or existing rules must be updated.\n","excerpt":" Running karavictl tenant commands result in an HTTP 504 error …","ref":"/csm-docs/v1/authorization/troubleshooting/","title":"Troubleshooting"},{"body":"","excerpt":"","ref":"/csm-docs/v1/csidriver/troubleshooting/","title":"Troubleshooting"},{"body":" Running karavictl tenant commands result in an HTTP 504 error   Retrieve CSM Authorization Server Logs To retrieve logs from services on the CSM Authorization Server, run the following command (e.g proxy-server logs):\n$ k3s kubectl logs deploy/proxy-server -n karavi -c proxy-server For OPA related logs, run:\n$ k3s kubectl logs deploy/proxy-server -n karavi -c opa Running “karavictl tenant” commands result in an HTTP 504 error This situation may occur if there are Iptables or other firewall rules preventing communication with the provided \u003cgrpc-address\u003e:\n$ karavictl tenant list --addr \u003cgrpc-address\u003e { \"ErrorMsg\": \"rpc error: code = Unavailable desc = Gateway Timeout: HTTP status code 504; transport: received the unexpected content-type \\\"text/plain; charset=utf-8\\\"\" } Resolution\nConsult with your system administrator or Iptables/firewall documentation. If there are rules in place to prevent communication with the \u003cgrpc-address\u003e, either new rules must be created or existing rules must be updated.\n","excerpt":" Running karavictl tenant commands result in an HTTP 504 error …","ref":"/csm-docs/v2/authorization/troubleshooting/","title":"Troubleshooting"},{"body":"","excerpt":"","ref":"/csm-docs/v2/csidriver/troubleshooting/","title":"Troubleshooting"},{"body":" Running karavictl inject leaves the vxflexos-controller in a Pending state Running karavictl inject leaves the powermax-controller in a Pending state Running karavictl inject leaves the isilon-controller in a Pending state Running karavictl tenant commands result in an HTTP 504 error   Retrieve CSM Authorization Server Logs To retrieve logs from services on the CSM Authorization Server, run the following command (e.g proxy-server logs):\n$ k3s kubectl logs deploy/proxy-server -n karavi -c proxy-server For OPA related logs, run:\n$ k3s kubectl logs deploy/proxy-server -n karavi -c opa Running “karavictl inject” leaves the vxflexos-controller in a “Pending” state This situation may occur when the number of vxflexos-controller pods that are deployed is equal to the number of schedulable nodes.\n$ kubectl get pods -n vxflexos NAME READY STATUS RESTARTS AGE vxflexos-controller-696cc5945f-4t94d 0/6 Pending 0 3m2s vxflexos-controller-75cdcbc5db-k25zx 5/5 Running 0 3m41s vxflexos-controller-75cdcbc5db-nkxqh 5/5 Running 0 3m42s vxflexos-node-mjc74 3/3 Running 0 2m44s vxflexos-node-zgswp 3/3 Running 0 2m44s Resolution\nTo resolve this issue, we need to temporarily reduce the number of replicas that the driver deployment is using.\n  Edit the deployment\n$ kubectl edit -n vxflexos deploy/vxflexos-controller   Find replicas under the spec section of the deployment manifest.\n  Reduce the number of replicas by 1\n  Save the file\n  Confirm that the updated controller pods have been deployed\n$ kubectl get pods -n vxflexos NAME READY STATUS RESTARTS AGE vxflexos-controller-696cc5945f-4t94d 6/6 Running 0 4m41s vxflexos-node-mjc74 3/3 Running 0 3m44s vxflexos-node-zgswp 3/3 Running 0 3m44s   Edit the deployment again\n  Find replicas under the spec section of the deployment manifest.\n  Increase the number of replicas by 1\n  Save the file\n  Confirm that the updated controller pods have been deployed\n$ kubectl get pods -n vxflexos NAME READY STATUS RESTARTS AGE vxflexos-controller-696cc5945f-4t94d 6/6 Running 0 5m41s vxflexos-controller-696cc5945f-6xxhb 6/6 Running 0 5m41s vxflexos-node-mjc74 3/3 Running 0 4m44s vxflexos-node-zgswp 3/3 Running 0 4m44s   Running “karavictl inject” leaves the powermax-controller in a “Pending” state This situation may occur when the number of powermax-controller pods that are deployed is equal to the number of schedulable nodes.\n$ kubectl get pods -n powermax NAME READY STATUS RESTARTS AGE powermax-controller-58d8779f5d-v7t56 0/6 Pending 0 25s powermax-controller-78f749847-jqphx 5/5 Running 0 10m powermax-controller-78f749847-w6vp5 5/5 Running 0 10m powermax-node-gx5pk 3/3 Running 0 21s powermax-node-k5gwc 3/3 Running 0 17s Resolution\nTo resolve this issue, we need to temporarily reduce the number of replicas that the driver deployment is using.\n  Edit the deployment\n$ kubectl edit -n powermax deploy/powermax-controller   Find replicas under the spec section of the deployment manifest.\n  Reduce the number of replicas by 1\n  Save the file\n  Confirm that the updated controller pods have been deployed\n$ kubectl get pods -n powermax NAME READY STATUS RESTARTS AGE powermax-controller-58d8779f5d-cqx8d 6/6 Running 0 22s powermax-node-gx5pk 3/3 Running 3 8m3s powermax-node-k5gwc 3/3 Running 3 7m59s   Edit the deployment again\n  Find replicas under the spec section of the deployment manifest.\n  Increase the number of replicas by 1\n  Save the file\n  Confirm that the updated controller pods have been deployed\n$ kubectl get pods -n powermax NAME READY STATUS RESTARTS AGE powermax-controller-58d8779f5d-cqx8d 6/6 Running 0 22s powermax-controller-58d8779f5d-v7t56 6/6 Running 22 8m7s powermax-node-gx5pk 3/3 Running 3 8m3s powermax-node-k5gwc 3/3 Running 3 7m59s   Running “karavictl inject” leaves the isilon-controller in a “Pending” state This situation may occur when the number of Isilon controller pods that are deployed is equal to the number of schedulable nodes.\n$ kubectl get pods -n isilon NAME READY STATUS RESTARTS AGE isilon-controller-58d8779f5d-v7t56 0/6 Pending 0 25s isilon-controller-78f749847-jqphx 5/5 Running 0 10m isilon-controller-78f749847-w6vp5 5/5 Running 0 10m isilon-node-gx5pk 3/3 Running 0 21s isilon-node-k5gwc 3/3 Running 0 17s Resolution\nTo resolve this issue, we need to temporarily reduce the number of replicas that the driver deployment is using.\n  Edit the deployment\n$ kubectl edit -n \u003cnamespace\u003e deploy/isilon-controller   Find replicas under the spec section of the deployment manifest.\n  Reduce the number of replicas by 1\n  Save the file\n  Confirm that the updated controller pods have been deployed\n$ kubectl get pods -n isilon NAME READY STATUS RESTARTS AGE isilon-controller-696cc5945f-4t94d 6/6 Running 0 4m41s isilon-node-mjc74 3/3 Running 0 3m44s isilon-node-zgswp 3/3 Running 0 3m44s   Edit the deployment again\n  Find replicas under the spec section of the deployment manifest.\n  Increase the number of replicas by 1\n  Save the file\n  Confirm that the updated controller pods have been deployed\n$ kubectl get pods -n isilon NAME READY STATUS RESTARTS AGE isilon-controller-58d8779f5d-cqx8d 6/6 Running 0 22s isilon-controller-58d8779f5d-v7t56 6/6 Running 22 8m7s isilon-node-gx5pk 3/3 Running 3 8m3s isilon-node-k5gwc 3/3 Running 3 7m59s   Running “karavictl tenant” commands result in an HTTP 504 error This situation may occur if there are Iptables or other firewall rules preventing communication with the provided \u003cgrpc-address\u003e:\n$ karavictl tenant list --addr \u003cgrpc-address\u003e { \"ErrorMsg\": \"rpc error: code = Unavailable desc = Gateway Timeout: HTTP status code 504; transport: received the unexpected content-type \\\"text/plain; charset=utf-8\\\"\" } Resolution\nConsult with your system administrator or Iptables/firewall documentation. If there are rules in place to prevent communication with the \u003cgrpc-address\u003e, either new rules must be created or existing rules must be updated.\n","excerpt":" Running karavictl inject leaves the vxflexos-controller in a Pending …","ref":"/csm-docs/v3/authorization/troubleshooting/","title":"Troubleshooting"},{"body":"","excerpt":"","ref":"/csm-docs/v3/csidriver/troubleshooting/","title":"Troubleshooting"},{"body":"You can migrate existing pre-provisioned volumes to another storage class by using volume migration feature.\nAs of CSM 1.3 two versions of migration are supported:\n To replicated storage class from NON replicated one To NON replicated storage class from replicated one  Prerequisites  Original volume is from the one of currently supported CSI drivers (see Support Matrix) Migrated sidecar is installed alongside with the driver, you can enable it in your myvalues.yaml file  migration:enabled:trueSupport Matrix    Migration Type PowerMax PowerStore PowerScale PowerFlex Unity     NON_REPL_TO_REPL Yes No No No No   REPL_TO_NON_REPL Yes No No No No    Basic Usage To trigger migration procedure, you need to patch existing PersistentVolume with migration annotation (by default migration.storage.dell.com/migrate-to) and in value of said annotation specify StorageClass name you want to migrate to.\nFor example, if we have PV named test-pv already provisioned and we want to migrate it to replicated storage class named powermax-replication we can run:\nkubectl patch pv test-pv -p '{\"metadata\": {\"annotations\":{\"migration.storage.dell.com/migrate-to\":\"powermax-replication\"}}}' Patching PV resource will trigger migration sidecar that will call VolumeMigrate call from the CSI driver. After migration is finished new PersistentVolume will be created in cluster with name of original PV plus -to-\u003csc-name\u003e appended to it.\nIn our example, we will see this when running kubectl get pv:\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE test-pv 1Gi RWO Retain Bound default/test-pvc powermax 5m test-pv-to-powermax-replication 1Gi RWO Retain Available powermax-replication 10s When Volume Migration is finished source PV will be updated with EVENT that denotes that this has taken place.\nNewly created PV (test-pv-to-powermax-replication in our example) is available for consumption via static provisioning by any PVC that will request it.\nNamespace Considerations For Replication Replication Groups in CSM Replication can be made namespaced, meaning that one SC will generate one Replication Group per namespace. This is also important when migrating volumes from/to replcation storage class.\nWhen just setting one annotation migration.storage.dell.com/migrate-to migrated volume is assumed to be used in same namespace as original PV and it’s PVC. In the case of being migrated to replication enabled storage class will be inserted in namespaced Replication Group inside PVC namespace.\nHowever, you can define in which namespace migrated volume must be used after migration by setting migration.storage.dell.com/namespace. You can use the same annotation in a scenario where you only have a statically provisioned PV, and you don’t have it bound to any PVC, and you want to migrate it to another storage class.\nNon Disruptive Migration You can migrate your PVs without disrupting workflows if you use StatefulSet with multiple replicas to deploy application.\nInstruction (you can also use repctl for convenience):\n Find every PV for your StatefulSet and patch it with migration.storage.dell.com/migrate-to annotation that points to new storage class  kubectl patch pv \u003cpv-name\u003e -p '{\"metadata\": {\"annotations\":{\"migration.storage.dell.com/migrate-to\":\"powermax-replication\"}}}' Ensure you have a copy of StatefulSet manifest somewhere ready, we will need it later. If you don’t have it, you can get it from cluster  kubectl get sts \u003csts-name\u003e -n \u003cns-name\u003e -o yaml \u003e sts-manifest.yaml To not disrupt any workflows we will need to delete StatefulSet without deleting any pods, to do so you can use --cascade flag  kubectl delete sts \u003csts-name\u003e -n \u003cns-name\u003e --cascade=orphan Change StorageClass in your manifest of StatefulSet to point to a new storage class, then apply it to the cluster  kubectl apply -f sts-manifest.yaml Find a PVC and pod of one replica of StatefulSet delete PVCs first and Pod after it  kubectl delete pvc \u003cpvc-name\u003e -n \u003cns-name\u003e kubectl delete pod \u003cpod-name\u003e -n \u003cns-name\u003e Wait for new pod to be created by StatefulSet, it should create new PVC that will use migrated PV.\nRepeat step 5 until all replicas use new PVCs  Using repctl You can use repctl CLI tool to help you simplify running migration specific commands.\nSingle PV In most its basic form repctl can do the same as kubectl, for example, migrating single PV from our example will look like:\n./repctl migrate pv test-pv --to-sc powermax-replication repctl will go and patch the resource for you. You can also provide --wait flag for it to wait until migrated PV is created in cluster. repctl also can set migration.storage.dell.com/namespace for you if you provide --target-ns flag.\nAside from just migrating single PVs repctl can migrate PVCs and StatefulSets.\nPVC repctl can find PV for any given PVC for you and patch it. This could be done with similar command to single PV migration:\n./repctl migrate pvc test-pvc --to-sc powermax-replication -n default Notice that we provide original namespace (default in our example) for this command because PVCs are namespaced resource and we need namespace to be able to find it.\nStatefulSet repctl can help you migrate entire StatefulSet by automating migration process.\nYou can use this command to do so:\n./repctl migrate sts test-sts --to-sc powermax-replication -n default By default, it will find every Pod, PVC and PV for provided StatefulSet and patch every PV with annotation.\nYou can also optionally provide --ndu flag, with this flag provided repctl will do steps provided in Non Disruptive Migration section automatically.\n","excerpt":"You can migrate existing pre-provisioned volumes to another storage …","ref":"/csm-docs/docs/replication/migrating-volumes/","title":"Migrating Volumes"},{"body":"The dell-csm-replicator supports monitoring of DellCSIReplicationGroup Custom Resources (CRs).\nEach RG is polled at a pre-defined interval and for each RG, a gRPC call is made to the driver which returns the status of the protection group on the array.\nIf an RG doesn’t have any PVs associated with it, the driver will not receive any monitoring request for that RG.\nThis status can be obtained from the RG as under:\nNAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 51d Ready SUSPENDED 2021-09-10T10:48:09Z ","excerpt":"The dell-csm-replicator supports monitoring of DellCSIReplicationGroup …","ref":"/csm-docs/docs/replication/monitoring/","title":"Monitoring"},{"body":"The dell-csm-replicator supports monitoring of DellCSIReplicationGroup Custom Resources (CRs).\nEach RG is polled at a pre-defined interval and for each RG, a gRPC call is made to the driver which returns the status of the protection group on the array.\nIf an RG doesn’t have any PVs associated with it, the driver will not receive any monitoring request for that RG.\nThis status can be obtained from the RG as under:\nNAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 51d Ready SUSPENDED 2021-09-10T10:48:09Z ","excerpt":"The dell-csm-replicator supports monitoring of DellCSIReplicationGroup …","ref":"/csm-docs/v1/replication/monitoring/","title":"Monitoring"},{"body":"The dell-csm-replicator supports monitoring of DellCSIReplicationGroup Custom Resources (CRs).\nEach RG is polled at a pre-defined interval and for each RG, a gRPC call is made to the driver which returns the status of the protection group on the array.\nIf an RG doesn’t have any PVs associated with it, the driver will not receive any monitoring request for that RG.\nThis status can be obtained from the RG as under:\nNAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 51d Ready SUSPENDED 2021-09-10T10:48:09Z ","excerpt":"The dell-csm-replicator supports monitoring of DellCSIReplicationGroup …","ref":"/csm-docs/v2/replication/monitoring/","title":"Monitoring"},{"body":"The dell-csm-replicator supports monitoring of DellCSIReplicationGroup Custom Resources (CRs).\nEach RG is polled at a pre-defined interval and for each RG, a gRPC call is made to the driver which returns the status of the protection group on the array.\nIf an RG doesn’t have any PVs associated with it, the driver will not receive any monitoring request for that RG.\nThis status can be obtained from the RG as under:\nNAME AGE STATE LINK STATE LAST LINKSTATE UPDATE replicated-rg-240721b0-12fb-4151-8dd8-94794ae2493e 51d Ready SUSPENDED 2021-09-10T10:48:09Z ","excerpt":"The dell-csm-replicator supports monitoring of DellCSIReplicationGroup …","ref":"/csm-docs/v3/replication/monitoring/","title":"Monitoring"},{"body":"Enabling Replication In CSI PowerMax Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerMax supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Configure SRDF connection between multiple PowerMax instances. Follow instructions by PowerMax storage for creating the SRDF Groups between a set of arrays.\nYou can ensure that you configured remote arrays by navigating to the Data Protection tab and choosing SRDF Groups on the managing Unisphere of your array. You should see a list of remote systems with the SRDF Group number that is configured and the Online field set to a green tick.\nWhile using any SRDF groups, ensure that they are for exclusive use by the CSI PowerMax driver -\n Any SRDF group which will be used by the driver is not in use by any other application If an SRDF group is already in use by a CSI driver, don’t use it for provisioning replicated volumes outside CSI provisioning workflows.  There are some important limitations that apply to how CSI PowerMax driver uses SRDF groups -\n One replicated storage group always contains volumes provisioned from a single namespace While using SRDF mode Async/Metro, a single SRDF group can be used to provision volumes within a single namespace. You can still create multiple storage classes using the same SRDF group for different Service Levels. But all these storage classes will be restricted to provisioning volumes within a single namespace. When using SRDF mode Sync, a single SRDF group can be used to provision volumes from multiple namespaces.  In Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\n Check controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING\n Check that controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs.\n  If you don’t have something installed or something out-of-place please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set helm parameter replication.enabled in your copy of example values.yaml file (usually called my-powermax-settings.yaml, myvalues.yaml etc.).\nHere is an example of how that would look like\n...# Set this to true to enable replicationreplication:enabled:trueimage:dellemc/dell-csi-replicator:v1.0.0replicationContextPrefix:\"powermax\"replicationPrefix:\"replication.storage.dell.com\"...You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerMax following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\n NOTE: you need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\n Creating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nPair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-srdfprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\u003cSRPName\u003e SYMID: \u003cSYMID\u003eServiceLevel:\u003cServiceLevel\u003e replication.storage.dell.com/RemoteSYMID: \u003cRemoteSYMID\u003ereplication.storage.dell.com/RemoteSRP:\u003cRemoteSRP\u003e replication.storage.dell.com/isReplicationEnabled: \"true\"replication.storage.dell.com/RemoteServiceLevel:\u003cRemoteServiceLevel\u003e replication.storage.dell.com/RdfMode: \u003cRdfMode\u003ereplication.storage.dell.com/Bias:\"false\"replication.storage.dell.com/RdfGroup:\u003cRdfGroup\u003e replication.storage.dell.com/RemoteRDFGroup: \u003cRemoteRDFGroup\u003ereplication.storage.dell.com/remoteStorageClassName:\u003cRemoteStorageClassName\u003e replication.storage.dell.com/remoteClusterID: \u003cRemoteClusterID\u003eLet’s go through each parameter and what it means:\n replication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/RemoteStorageClassName points to the name of the remote storage class, if you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/RemoteClusterID represents the ID of a remote cluster, it is the same id you put in the replication controller config map. replication.storage.dell.com/RemoteSYMID is the Symmetrix id of the remote array. replication.storage.dell.com/RemoteSRP is the storage pool of the remote array. replication.storage.dell.com/RemoteServiceLevel is the service level that will be assigned to remote volumes. replication.storage.dell.com/RdfMode points to the RDF mode you want to use. It should be one out of “ASYNC”, “METRO” and “SYNC”. If mode is set to METRO, driver does not need RemoteStorageClassName and RemoteClusterID as it supports METRO with single cluster configuration. replication.storage.dell.com/Bias when the RdfMode is set to METRO, this parameter is required to indicate driver to use Bias or Witness. If set to true, the driver will configure METRO with Bias, if set to false, the driver will configure METRO with Witness. replication.storage.dell.com/RdfGroup is the local SRDF group number, as configured. replication.storage.dell.com/RemoteRDFGroup is the remote SRDF group number, as configured.  Let’s follow up that with an example, let’s assume we have two Kubernetes clusters and two PowerMax storage arrays:\n Clusters have IDs of cluster-1 and cluster-2 There are two arrays local Symmetrix array: 000000000001 and remote Symmetrix array: 000000000002 Storage arrays are connected to each other via RdfGroup 1 and RemoteRDFGroup 2 Cluster cluster-1 connected to array 000000000001 Cluster cluster-2 connected to array 000000000002 RDF Mode is ASYNC  And this how would our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-srdfprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\"SRP\"SYMID:\"000000000001\"ServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSYMID:\"000000000002\"replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/RemoteSRP:\"SRP\"replication.storage.dell.com/RemoteServiceLevel:\"Optimized\"replication.storage.dell.com/RdfMode:\"ASYNC\"replication.storage.dell.com/Bias:\"false\"replication.storage.dell.com/RdfGroup:\"1\"replication.storage.dell.com/RemoteRDFGroup:\"2\"replication.storage.dell.com/remoteStorageClassName:\"powermax-srdf\"replication.storage.dell.com/remoteClusterID:\"cluster-2\"StorageClass to be created in cluster-2:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-srdfprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\"SRP\"SYMID:\"000000000002\"ServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSYMID:\"000000000001\"replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/RemoteServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSRP:\"SRP\"replication.storage.dell.com/Bias:\"false\"replication.storage.dell.com/RdfMode:\"ASYNC\"replication.storage.dell.com/RdfGroup:\"2\"replication.storage.dell.com/RemoteRDFGroup:\"1\"replication.storage.dell.com/remoteStorageClassName:\"powermax-srdf\"replication.storage.dell.com/remoteClusterID:\"cluster-1\"After figuring out how storage classes would look like you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example here, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from manual installation and see how config would look like\nsourceClusterID:\"cluster-1\"targetClusterID:\"cluster-2\"name:\"powermax-replication\"driver:\"powermax\"reclaimPolicy:\"Retain\"replicationPrefix:\"replication.storage.dell.com\"parameters:rdfMode:\"ASYNC\"srp:source:\"SRP_1\"target:\"SRP_1\"symId:source:\"000000000001\"target:\"000000000002\"serviceLevel:source:\"Optimized\"target:\"Optimized\"rdfGroup:source:\"1\"target:\"2\"After preparing the config you can apply it to both clusters with repctl, just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes would be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nProvisioning Metro Volumes Here is an example of a storage class configured for Metro mode,\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-metroprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\"SRP\"SYMID:\"000000000001\"ServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSYMID:\"000000000002\"replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/RemoteSRP:\"SRP\"replication.storage.dell.com/RemoteServiceLevel:\"Optimized\"replication.storage.dell.com/RdfMode:\"Metro\"replication.storage.dell.com/Bias:\"true\"replication.storage.dell.com/RdfGroup:\"3\"replication.storage.dell.com/RemoteRDFGroup:\"3\"After installing the driver and creating a storage class with Metro config (as shown above) we can create volumes. On your cluster, create a PersistentVolumeClaim using this storage class. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nSupported Replication Actions The CSI PowerMax driver supports the following list of replication actions:\nBasic Site Specific Actions -  FAILOVER_LOCAL FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL UNPLANNED_FAILOVER_REMOTE REPROTECT_LOCAL REPROTECT_REMOTE  Advanced Site Specific Actions - In this section we are going to refer to “Site A” as the original source site \u0026 “Site B” as the original target site. Any action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\n FAILOVER_WITHOUT_SWAP_LOCAL  You can use this action to do a failover when you are at Site B, and don’t want to swap the replication direction. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_LOCAL. After receiving this request the CSI driver will attempxt to Fail over to Site B which is the local site.   FAILOVER_WITHOUT_SWAP_REMOTE  You can use this action to do a failover when you are at Site A, and don’t want to swap the replication direction. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_REMOTE. After receiving this request the CSI driver will attempt to Fail over to Site B which is the remote site.   FAILBACK_LOCAL  You can use this action to do a failback, and when you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_LOCAL. After receiving this request the CSI driver will attempt to Fail back from Site B to Site A which is the local site.   FAILBACK_REMOTE  You can use this action to do a failback, and when you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_REMOTE. After receiving this request the CSI driver will attempt to Fail back to Site A from Site B which is the local site.   SWAP_LOCAL  You can use this action to swap the replication direction, and you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_LOCAL. After receiving this request the CSI driver will attempt to do SWAP at Site A which is the local site.   SWAP_REMOTE  You can use this action to swap the replication direction, and you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_REMOTE. After receiving this request the CSI driver will attempt to do SWAP at Site B which is the remote site.    Maintenance Actions -  SUSPEND RESUME ESTABLISH SYNC  Deletion of DellCSIReplicationGroup The deletion of DellCSIReplicationGroup custom resource triggers the DeleteStorageProtectionGroup call on the driver. The storage protection group on the array can be deleted only if it has no volumes associated with it. If the deletion is triggered on the storage protection group with volumes, the deletion will fail and the dell-csi-driver will return a final error to the dell-csm-replication sidecar.\n","excerpt":"Enabling Replication In CSI PowerMax Container Storage Modules (CSM) …","ref":"/csm-docs/docs/replication/deployment/powermax/","title":"PowerMax"},{"body":"Enabling Replication In CSI PowerMax Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerMax supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Configure SRDF connection between multiple PowerMax instances. Follow instructions by PowerMax storage for creating the SRDF Groups between a set of arrays.\nYou can ensure that you configured remote arrays by navigating to the Data Protection tab and choosing SRDF Groups on the managing Unisphere of your array. You should see a list of remote systems with the SRDF Group number that is configured and the Online field set to a green tick.\nWhile using any SRDF groups, ensure that they are for exclusive use by the CSI PowerMax driver -\n Any SRDF group which will be used by the driver is not in use by any other application If an SRDF group is already in use by a CSI driver, don’t use it for provisioning replicated volumes outside CSI provisioning workflows.  There are some important limitations that apply to how CSI PowerMax driver uses SRDF groups -\n One replicated storage group always contains volumes provisioned from a single namespace While using SRDF mode Async/Metro, a single SRDF group can be used to provision volumes within a single namespace. You can still create multiple storage classes using the same SRDF group for different Service Levels. But all these storage classes will be restricted to provisioning volumes within a single namespace. When using SRDF mode Sync, a single SRDF group can be used to provision volumes from multiple namespaces.  In Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\n Check controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING\n Check that controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs.\n  If you don’t have something installed or something out-of-place please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set helm parameter replication.enabled in your copy of example values.yaml file (usually called my-powermax-settings.yaml, myvalues.yaml etc.).\nHere is an example of how that would look like\n...# Set this to true to enable replicationreplication:enabled:trueimage:dellemc/dell-csi-replicator:v1.0.0replicationContextPrefix:\"powermax\"replicationPrefix:\"replication.storage.dell.com\"...You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerMax following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\n NOTE: you need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\n Creating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nPair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-srdfprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\u003cSRPName\u003e SYMID: \u003cSYMID\u003eServiceLevel:\u003cServiceLevel\u003e replication.storage.dell.com/RemoteSYMID: \u003cRemoteSYMID\u003ereplication.storage.dell.com/RemoteSRP:\u003cRemoteSRP\u003e replication.storage.dell.com/isReplicationEnabled: \"true\"replication.storage.dell.com/RemoteServiceLevel:\u003cRemoteServiceLevel\u003e replication.storage.dell.com/RdfMode: \u003cRdfMode\u003ereplication.storage.dell.com/Bias:\"false\"replication.storage.dell.com/RdfGroup:\u003cRdfGroup\u003e replication.storage.dell.com/RemoteRDFGroup: \u003cRemoteRDFGroup\u003ereplication.storage.dell.com/remoteStorageClassName:\u003cRemoteStorageClassName\u003e replication.storage.dell.com/remoteClusterID: \u003cRemoteClusterID\u003eLet’s go through each parameter and what it means:\n replication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/RemoteStorageClassName points to the name of the remote storage class, if you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/RemoteClusterID represents the ID of a remote cluster, it is the same id you put in the replication controller config map. replication.storage.dell.com/RemoteSYMID is the Symmetrix id of the remote array. replication.storage.dell.com/RemoteSRP is the storage pool of the remote array. replication.storage.dell.com/RemoteServiceLevel is the service level that will be assigned to remote volumes. replication.storage.dell.com/RdfMode points to the RDF mode you want to use. It should be one out of “ASYNC”, “METRO” and “SYNC”. If mode is set to METRO, driver does not need RemoteStorageClassName and RemoteClusterID as it supports METRO with single cluster configuration. replication.storage.dell.com/Bias when the RdfMode is set to METRO, this parameter is required to indicate driver to use Bias or Witness. If set to true, the driver will configure METRO with Bias, if set to false, the driver will configure METRO with Witness. replication.storage.dell.com/RdfGroup is the local SRDF group number, as configured. replication.storage.dell.com/RemoteRDFGroup is the remote SRDF group number, as configured.  Let’s follow up that with an example, let’s assume we have two Kubernetes clusters and two PowerMax storage arrays:\n Clusters have IDs of cluster-1 and cluster-2 There are two arrays local Symmetrix array: 000000000001 and remote Symmetrix array: 000000000002 Storage arrays are connected to each other via RdfGroup 1 and RemoteRDFGroup 2 Cluster cluster-1 connected to array 000000000001 Cluster cluster-2 connected to array 000000000002 RDF Mode is ASYNC  And this how would our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-srdfprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\"SRP\"SYMID:\"000000000001\"ServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSYMID:\"000000000002\"replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/RemoteSRP:\"SRP\"replication.storage.dell.com/RemoteServiceLevel:\"Optimized\"replication.storage.dell.com/RdfMode:\"ASYNC\"replication.storage.dell.com/Bias:\"false\"replication.storage.dell.com/RdfGroup:\"1\"replication.storage.dell.com/RemoteRDFGroup:\"2\"replication.storage.dell.com/remoteStorageClassName:\"powermax-srdf\"replication.storage.dell.com/remoteClusterID:\"cluster-2\"StorageClass to be created in cluster-2:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-srdfprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\"SRP\"SYMID:\"000000000002\"ServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSYMID:\"000000000001\"replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/RemoteServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSRP:\"SRP\"replication.storage.dell.com/Bias:\"false\"replication.storage.dell.com/RdfMode:\"ASYNC\"replication.storage.dell.com/RdfGroup:\"2\"replication.storage.dell.com/RemoteRDFGroup:\"1\"replication.storage.dell.com/remoteStorageClassName:\"powermax-srdf\"replication.storage.dell.com/remoteClusterID:\"cluster-1\"After figuring out how storage classes would look like you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example here, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from manual installation and see how config would look like\nsourceClusterID:\"cluster-1\"targetClusterID:\"cluster-2\"name:\"powermax-replication\"driver:\"powermax\"reclaimPolicy:\"Retain\"replicationPrefix:\"replication.storage.dell.com\"parameters:rdfMode:\"ASYNC\"srp:source:\"SRP_1\"target:\"SRP_1\"symId:source:\"000000000001\"target:\"000000000002\"serviceLevel:source:\"Optimized\"target:\"Optimized\"rdfGroup:source:\"1\"target:\"2\"After preparing the config you can apply it to both clusters with repctl, just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes would be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nProvisioning Metro Volumes Here is an example of a storage class configured for Metro mode,\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-metroprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\"SRP\"SYMID:\"000000000001\"ServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSYMID:\"000000000002\"replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/RemoteSRP:\"SRP\"replication.storage.dell.com/RemoteServiceLevel:\"Optimized\"replication.storage.dell.com/RdfMode:\"Metro\"replication.storage.dell.com/Bias:\"true\"replication.storage.dell.com/RdfGroup:\"3\"replication.storage.dell.com/RemoteRDFGroup:\"3\"After installing the driver and creating a storage class with Metro config (as shown above) we can create volumes. On your cluster, create a PersistentVolumeClaim using this storage class. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nSupported Replication Actions The CSI PowerMax driver supports the following list of replication actions:\nBasic Site Specific Actions -  FAILOVER_LOCAL FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL UNPLANNED_FAILOVER_REMOTE REPROTECT_LOCAL REPROTECT_REMOTE  Advanced Site Specific Actions - In this section we are going to refer to “Site A” as the original source site \u0026 “Site B” as the original target site. Any action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\n FAILOVER_WITHOUT_SWAP_LOCAL  You can use this action to do a failover when you are at Site B, and don’t want to swap the replication direction. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_LOCAL. After receiving this request the CSI driver will attempxt to Fail over to Site B which is the local site.   FAILOVER_WITHOUT_SWAP_REMOTE  You can use this action to do a failover when you are at Site A, and don’t want to swap the replication direction. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_REMOTE. After receiving this request the CSI driver will attempt to Fail over to Site B which is the remote site.   FAILBACK_LOCAL  You can use this action to do a failback, and when you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_LOCAL. After receiving this request the CSI driver will attempt to Fail back from Site B to Site A which is the local site.   FAILBACK_REMOTE  You can use this action to do a failback, and when you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_REMOTE. After receiving this request the CSI driver will attempt to Fail back to Site A from Site B which is the local site.   SWAP_LOCAL  You can use this action to swap the replication direction, and you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_LOCAL. After receiving this request the CSI driver will attempt to do SWAP at Site A which is the local site.   SWAP_REMOTE  You can use this action to swap the replication direction, and you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_REMOTE. After receiving this request the CSI driver will attempt to do SWAP at Site B which is the remote site.    Maintenance Actions -  SUSPEND RESUME ESTABLISH SYNC  Deletion of DellCSIReplicationGroup The deletion of DellCSIReplicationGroup custom resource triggers the DeleteStorageProtectionGroup call on the driver. The storage protection group on the array can be deleted only if it has no volumes associated with it. If the deletion is triggered on the storage protection group with volumes, the deletion will fail and the dell-csi-driver will return a final error to the dell-csm-replication sidecar.\n","excerpt":"Enabling Replication In CSI PowerMax Container Storage Modules (CSM) …","ref":"/csm-docs/v1/replication/deployment/powermax/","title":"PowerMax"},{"body":"Enabling Replication In CSI PowerMax Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerMax supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Configure SRDF connection between multiple PowerMax instances. Follow instructions by PowerMax storage for creating the SRDF Groups between a set of arrays.\nYou can ensure that you configured remote arrays by navigating to the Data Protection tab and choosing SRDF Groups on the managing Unisphere of your array. You should see a list of remote systems with the SRDF Group number that is configured and the Online field set to a green tick.\nWhile using any SRDF groups, ensure that they are for exclusive use by the CSI PowerMax driver -\n Any SRDF group which will be used by the driver is not in use by any other application If an SRDF group is already in use by a CSI driver, don’t use it for provisioning replicated volumes outside CSI provisioning workflows.  There are some important limitations that apply to how CSI PowerMax driver uses SRDF groups -\n One replicated storage group always contains volumes provisioned from a single namespace While using SRDF mode Async/Metro, a single SRDF group can be used to provision volumes within a single namespace. You can still create multiple storage classes using the same SRDF group for different Service Levels. But all these storage classes will be restricted to provisioning volumes within a single namespace. When using SRDF mode Sync, a single SRDF group can be used to provision volumes from multiple namespaces.  In Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\n Check controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING\n Check that controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs.\n  If you don’t have something installed or something out-of-place please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set helm parameter replication.enabled in your copy of example values.yaml file (usually called my-powermax-settings.yaml, myvalues.yaml etc.).\nHere is an example of how that would look like\n...# Set this to true to enable replicationreplication:enabled:trueimage:dellemc/dell-csi-replicator:v1.0.0replicationContextPrefix:\"powermax\"replicationPrefix:\"replication.storage.dell.com\"...You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerMax following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\n NOTE: you need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\n Creating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nPair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-srdfprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\u003cSRPName\u003e SYMID: \u003cSYMID\u003eServiceLevel:\u003cServiceLevel\u003e replication.storage.dell.com/RemoteSYMID: \u003cRemoteSYMID\u003ereplication.storage.dell.com/RemoteSRP:\u003cRemoteSRP\u003e replication.storage.dell.com/isReplicationEnabled: \"true\"replication.storage.dell.com/RemoteServiceLevel:\u003cRemoteServiceLevel\u003e replication.storage.dell.com/RdfMode: \u003cRdfMode\u003ereplication.storage.dell.com/Bias:\"false\"replication.storage.dell.com/RdfGroup:\u003cRdfGroup\u003e replication.storage.dell.com/RemoteRDFGroup: \u003cRemoteRDFGroup\u003ereplication.storage.dell.com/remoteStorageClassName:\u003cRemoteStorageClassName\u003e replication.storage.dell.com/remoteClusterID: \u003cRemoteClusterID\u003eLet’s go through each parameter and what it means:\n replication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/RemoteStorageClassName points to the name of the remote storage class, if you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/RemoteClusterID represents the ID of a remote cluster, it is the same id you put in the replication controller config map. replication.storage.dell.com/RemoteSYMID is the Symmetrix id of the remote array. replication.storage.dell.com/RemoteSRP is the storage pool of the remote array. replication.storage.dell.com/RemoteServiceLevel is the service level that will be assigned to remote volumes. replication.storage.dell.com/RdfMode points to the RDF mode you want to use. It should be one out of “ASYNC”, “METRO” and “SYNC”. If mode is set to METRO, driver does not need RemoteStorageClassName and RemoteClusterID as it supports METRO with single cluster configuration. replication.storage.dell.com/Bias when the RdfMode is set to METRO, this parameter is required to indicate driver to use Bias or Witness. If set to true, the driver will configure METRO with Bias, if set to false, the driver will configure METRO with Witness. replication.storage.dell.com/RdfGroup is the local SRDF group number, as configured. replication.storage.dell.com/RemoteRDFGroup is the remote SRDF group number, as configured.  Let’s follow up that with an example, let’s assume we have two Kubernetes clusters and two PowerMax storage arrays:\n Clusters have IDs of cluster-1 and cluster-2 There are two arrays local Symmetrix array: 000000000001 and remote Symmetrix array: 000000000002 Storage arrays are connected to each other via RdfGroup 1 and RemoteRDFGroup 2 Cluster cluster-1 connected to array 000000000001 Cluster cluster-2 connected to array 000000000002 RDF Mode is ASYNC  And this how would our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-srdfprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\"SRP\"SYMID:\"000000000001\"ServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSYMID:\"000000000002\"replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/RemoteSRP:\"SRP\"replication.storage.dell.com/RemoteServiceLevel:\"Optimized\"replication.storage.dell.com/RdfMode:\"ASYNC\"replication.storage.dell.com/Bias:\"false\"replication.storage.dell.com/RdfGroup:\"1\"replication.storage.dell.com/RemoteRDFGroup:\"2\"replication.storage.dell.com/remoteStorageClassName:\"powermax-srdf\"replication.storage.dell.com/remoteClusterID:\"cluster-2\"StorageClass to be created in cluster-2:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-srdfprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\"SRP\"SYMID:\"000000000002\"ServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSYMID:\"000000000001\"replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/RemoteServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSRP:\"SRP\"replication.storage.dell.com/Bias:\"false\"replication.storage.dell.com/RdfMode:\"ASYNC\"replication.storage.dell.com/RdfGroup:\"2\"replication.storage.dell.com/RemoteRDFGroup:\"1\"replication.storage.dell.com/remoteStorageClassName:\"powermax-srdf\"replication.storage.dell.com/remoteClusterID:\"cluster-1\"After figuring out how storage classes would look like you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example here, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from manual installation and see how config would look like\nsourceClusterID:\"cluster-1\"targetClusterID:\"cluster-2\"name:\"powermax-replication\"driver:\"powermax\"reclaimPolicy:\"Retain\"replicationPrefix:\"replication.storage.dell.com\"parameters:rdfMode:\"ASYNC\"srp:source:\"SRP_1\"target:\"SRP_1\"symId:source:\"000000000001\"target:\"000000000002\"serviceLevel:source:\"Optimized\"target:\"Optimized\"rdfGroup:source:\"1\"target:\"2\"After preparing the config you can apply it to both clusters with repctl, just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes would be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nProvisioning Metro Volumes Here is an example of a storage class configured for Metro mode,\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-metroprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\"SRP\"SYMID:\"000000000001\"ServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSYMID:\"000000000002\"replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/RemoteSRP:\"SRP\"replication.storage.dell.com/RemoteServiceLevel:\"Optimized\"replication.storage.dell.com/RdfMode:\"Metro\"replication.storage.dell.com/Bias:\"true\"replication.storage.dell.com/RdfGroup:\"3\"replication.storage.dell.com/RemoteRDFGroup:\"3\"After installing the driver and creating a storage class with Metro config (as shown above) we can create volumes. On your cluster, create a PersistentVolumeClaim using this storage class. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nSupported Replication Actions The CSI PowerMax driver supports the following list of replication actions:\nBasic Site Specific Actions -  FAILOVER_LOCAL FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL UNPLANNED_FAILOVER_REMOTE REPROTECT_LOCAL REPROTECT_REMOTE  Advanced Site Specific Actions - In this section we are going to refer to “Site A” as the original source site \u0026 “Site B” as the original target site. Any action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\n FAILOVER_WITHOUT_SWAP_LOCAL  You can use this action to do a failover when you are at Site B, and don’t want to swap the replication direction. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_LOCAL. After receiving this request the CSI driver will attempxt to Fail over to Site B which is the local site.   FAILOVER_WITHOUT_SWAP_REMOTE  You can use this action to do a failover when you are at Site A, and don’t want to swap the replication direction. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_REMOTE. After receiving this request the CSI driver will attempt to Fail over to Site B which is the remote site.   FAILBACK_LOCAL  You can use this action to do a failback, and when you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_LOCAL. After receiving this request the CSI driver will attempt to Fail back from Site B to Site A which is the local site.   FAILBACK_REMOTE  You can use this action to do a failback, and when you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_REMOTE. After receiving this request the CSI driver will attempt to Fail back to Site A from Site B which is the local site.   SWAP_LOCAL  You can use this action to swap the replication direction, and you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_LOCAL. After receiving this request the CSI driver will attempt to do SWAP at Site A which is the local site.   SWAP_REMOTE  You can use this action to swap the replication direction, and you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_REMOTE. After receiving this request the CSI driver will attempt to do SWAP at Site B which is the remote site.    Maintenance Actions -  SUSPEND RESUME ESTABLISH SYNC  Deletion of DellCSIReplicationGroup The deletion of DellCSIReplicationGroup custom resource triggers the DeleteStorageProtectionGroup call on the driver. The storage protection group on the array can be deleted only if it has no volumes associated with it. If the deletion is triggered on the storage protection group with volumes, the deletion will fail and the dell-csi-driver will return a final error to the dell-csm-replication sidecar.\n","excerpt":"Enabling Replication In CSI PowerMax Container Storage Modules (CSM) …","ref":"/csm-docs/v2/replication/deployment/powermax/","title":"PowerMax"},{"body":"Enabling Replication In CSI PowerMax Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell EMC PowerMax supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Configure SRDF connection between multiple PowerMax instances. Follow instructions by PowerMax storage for creating the SRDF Groups between a set of arrays.\nYou can ensure that you configured remote arrays by navigating to the Data Protection tab and choosing SRDF Groups on the managing Unisphere of your array. You should see a list of remote systems with the SRDF Group number that is configured and the Online field set to a green tick.\nWhile using any SRDF groups, ensure that they are for exclusive use by the CSI PowerMax driver -\n Any SRDF group which will be used by the driver is not in use by any other application If an SRDF group is already in use by a CSI driver, don’t use it for provisioning replicated volumes outside CSI provisioning workflows.  There are some important limitations that apply to how CSI PowerMax driver uses SRDF groups -\n One replicated storage group always contains volumes provisioned from a single namespace While using SRDF mode Async/Metro, a single SRDF group can be used to provision volumes within a single namespace. You can still create multiple storage classes using the same SRDF group for different Service Levels. But all these storage classes will be restricted to provisioning volumes within a single namespace. When using SRDF mode Sync, a single SRDF group can be used to provision volumes from multiple namespaces.  In Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\n Check controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING\n Check that controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs.\n  If you don’t have something installed or something out-of-place please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set helm parameter replication.enabled in your copy of example values.yaml file (usually called my-powermax-settings.yaml, myvalues.yaml etc.).\nHere is an example of how that would look like\n...# Set this to true to enable replicationreplication:enabled:trueimage:dellemc/dell-csi-replicator:v1.0.0replicationContextPrefix:\"powermax\"replicationPrefix:\"replication.storage.dell.com\"...You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerMax following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\n NOTE: you need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\n Creating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nPair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository at ./samples/storageclass/powermax_srdf.yaml.\nIt will look like this:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-srdfprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\u003cSRPName\u003e SYMID: \u003cSYMID\u003eServiceLevel:\u003cServiceLevel\u003e replication.storage.dell.com/RemoteSYMID: \u003cRemoteSYMID\u003ereplication.storage.dell.com/RemoteSRP:\u003cRemoteSRP\u003e replication.storage.dell.com/isReplicationEnabled: \"true\"replication.storage.dell.com/RemoteServiceLevel:\u003cRemoteServiceLevel\u003e replication.storage.dell.com/RdfMode: \u003cRdfMode\u003ereplication.storage.dell.com/Bias:\"false\"replication.storage.dell.com/RdfGroup:\u003cRdfGroup\u003e replication.storage.dell.com/RemoteRDFGroup: \u003cRemoteRDFGroup\u003ereplication.storage.dell.com/remoteStorageClassName:\u003cRemoteStorageClassName\u003e replication.storage.dell.com/remoteClusterID: \u003cRemoteClusterID\u003eLet’s go through each parameter and what it means:\n replication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/RemoteStorageClassName points to the name of the remote storage class, if you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/RemoteClusterID represents the ID of a remote cluster, it is the same id you put in the replication controller config map. replication.storage.dell.com/RemoteSYMID is the Symmetrix id of the remote array. replication.storage.dell.com/RemoteSRP is the storage pool of the remote array. replication.storage.dell.com/RemoteServiceLevel is the service level that will be assigned to remote volumes. replication.storage.dell.com/RdfMode points to the RDF mode you want to use. It should be one out of “ASYNC”, “METRO” and “SYNC”. If mode is set to METRO, driver does not need RemoteStorageClassName and RemoteClusterID as it supports METRO with single cluster configuration. replication.storage.dell.com/Bias when the RdfMode is set to METRO, this parameter is required to indicate driver to use Bias or Witness. If set to true, the driver will configure METRO with Bias, if set to false, the driver will configure METRO with Witness. replication.storage.dell.com/RdfGroup is the local SRDF group number, as configured. replication.storage.dell.com/RemoteRDFGroup is the remote SRDF group number, as configured.  Let’s follow up that with an example, let’s assume we have two Kubernetes clusters and two PowerMax storage arrays:\n Clusters have IDs of cluster-1 and cluster-2 There are two arrays local Symmetrix array: 000000000001 and remote Symmetrix array: 000000000002 Storage arrays are connected to each other via RdfGroup 1 and RemoteRDFGroup 2 Cluster cluster-1 connected to array 000000000001 Cluster cluster-2 connected to array 000000000002 RDF Mode is ASYNC  And this how would our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-srdfprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\"SRP\"SYMID:\"000000000001\"ServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSYMID:\"000000000002\"replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/RemoteSRP:\"SRP\"replication.storage.dell.com/RemoteServiceLevel:\"Optimized\"replication.storage.dell.com/RdfMode:\"ASYNC\"replication.storage.dell.com/Bias:\"false\"replication.storage.dell.com/RdfGroup:\"1\"replication.storage.dell.com/RemoteRDFGroup:\"2\"replication.storage.dell.com/remoteStorageClassName:\"powermax-srdf\"replication.storage.dell.com/remoteClusterID:\"cluster-2\"StorageClass to be created in cluster-2:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-srdfprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\"SRP\"SYMID:\"000000000002\"ServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSYMID:\"000000000001\"replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/RemoteServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSRP:\"SRP\"replication.storage.dell.com/Bias:\"false\"replication.storage.dell.com/RdfMode:\"ASYNC\"replication.storage.dell.com/RdfGroup:\"2\"replication.storage.dell.com/RemoteRDFGroup:\"1\"replication.storage.dell.com/remoteStorageClassName:\"powermax-srdf\"replication.storage.dell.com/remoteClusterID:\"cluster-1\"After figuring out how storage classes would look like you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example in repctl/examples/powermax_example_values.yaml, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from manual installation and see how config would look like\nsourceClusterID:\"cluster-1\"targetClusterID:\"cluster-2\"name:\"powermax-replication\"driver:\"powermax\"reclaimPolicy:\"Retain\"replicationPrefix:\"replication.storage.dell.com\"parameters:rdfMode:\"ASYNC\"srp:source:\"SRP_1\"target:\"SRP_1\"symId:source:\"000000000001\"target:\"000000000002\"serviceLevel:source:\"Optimized\"target:\"Optimized\"rdfGroup:source:\"1\"target:\"2\"After preparing the config you can apply it to both clusters with repctl, just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes would be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl list storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nProvisioning Metro Volumes Here is an example of a storage class configured for Metro mode,\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-metroprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:Immediateparameters:SRP:\"SRP\"SYMID:\"000000000001\"ServiceLevel:\"Optimized\"replication.storage.dell.com/RemoteSYMID:\"000000000002\"replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/RemoteSRP:\"SRP\"replication.storage.dell.com/RemoteServiceLevel:\"Optimized\"replication.storage.dell.com/RdfMode:\"Metro\"replication.storage.dell.com/Bias:\"true\"replication.storage.dell.com/RdfGroup:\"3\"replication.storage.dell.com/RemoteRDFGroup:\"3\"After installing the driver and creating a storage class with Metro config (as shown above) we can create volumes. On your cluster, create a PersistentVolumeClaim using this storage class. The CSI PowerMax driver will create a volume on the array, add it to a StorageProtectionGroup and configure replication using the parameters provided in the replication-enabled Storage Class.\nSupported Replication Actions The CSI PowerMax driver supports the following list of replication actions:\nBasic Site Specific Actions -  FAILOVER_LOCAL FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL UNPLANNED_FAILOVER_REMOTE REPROTECT_LOCAL REPROTECT_REMOTE  Advanced Site Specific Actions - In this section we are going to refer to “Site A” as the original source site \u0026 “Site B” as the original target site. Any action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\n FAILOVER_WITHOUT_SWAP_LOCAL  You can use this action to do a failover when you are at Site B, and don’t want to swap the replication direction. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_LOCAL. After receiving this request the CSI driver will attempxt to Fail over to Site B which is the local site.   FAILOVER_WITHOUT_SWAP_REMOTE  You can use this action to do a failover when you are at Site A, and don’t want to swap the replication direction. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILOVER_WITHOUT_SWAP_REMOTE. After receiving this request the CSI driver will attempt to Fail over to Site B which is the remote site.   FAILBACK_LOCAL  You can use this action to do a failback, and when you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_LOCAL. After receiving this request the CSI driver will attempt to Fail back from Site B to Site A which is the local site.   FAILBACK_REMOTE  You can use this action to do a failback, and when you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with FAILBACK_REMOTE. After receiving this request the CSI driver will attempt to Fail back to Site A from Site B which is the local site.   SWAP_LOCAL  You can use this action to swap the replication direction, and you are at Site A. On Site A, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_LOCAL. After receiving this request the CSI driver will attempt to do SWAP at Site A which is the local site.   SWAP_REMOTE  You can use this action to swap the replication direction, and you are at Site B. On Site B, run kubectl edit rg \u003crg-name\u003e and edit the ‘action’ in spec with SWAP_REMOTE. After receiving this request the CSI driver will attempt to do SWAP at Site B which is the remote site.    Maintenance Actions -  SUSPEND RESUME ESTABLISH SYNC  Deletion of DellCSIReplicationGroup The deletion of DellCSIReplicationGroup custom resource triggers the DeleteStorageProtectionGroup call on the driver. The storage protection group on the array can be deleted only if it has no volumes associated with it. If the deletion is triggered on the storage protection group with volumes, the deletion will fail and the dell-csi-driver will return a final error to the dell-csm-replication sidecar.\n","excerpt":"Enabling Replication In CSI PowerMax Container Storage Modules (CSM) …","ref":"/csm-docs/v3/replication/deployment/powermax/","title":"PowerMax"},{"body":"","excerpt":"","ref":"/csm-docs/docs/csidriver/release/","title":"Release Notes"},{"body":"","excerpt":"","ref":"/csm-docs/v1/csidriver/release/","title":"Release Notes"},{"body":"","excerpt":"","ref":"/csm-docs/v2/csidriver/release/","title":"Release Notes"},{"body":"","excerpt":"","ref":"/csm-docs/v3/csidriver/release/","title":"Release Notes"},{"body":"Release Notes - CSM Authorization 1.3.0 New Features/Changes  CSM-Authorization can deployed with helm  Fixed Issues  Authorization proxy server install fails due to missing container-selinux Permissions on karavictl and k3s binaries are incorrect  Known Issues  Authorization NGINX Ingress Controller fails to install on OpenShift  ","excerpt":"Release Notes - CSM Authorization 1.3.0 New Features/Changes …","ref":"/csm-docs/docs/authorization/release/","title":"Release notes"},{"body":"Container Storage Modules (CSM) for Replication is part of the open-source suite of Kubernetes storage enablers for Dell products.\nCSM for Replication project aims to bring Replication \u0026 Disaster Recovery capabilities of Dell Storage Arrays to Kubernetes clusters. It helps you replicate groups of volumes using the native replication technology available on the storage array and can provide you a way to restart applications in case of both planned and unplanned migration.\nCSM for Replication Capabilities CSM for Replication provides the following capabilities:\n  Capability PowerMax PowerStore PowerScale PowerFlex Unity     Replicate data using native storage array based replication yes yes yes no no   Create PersistentVolume objects in the cluster representing the replicated volume yes yes yes no no   Create DellCSIReplicationGroup objects in the cluster yes yes yes no no   Failover \u0026 Reprotect applications using the replicated volumes yes yes yes no no   Provides a command line utility - repctl for configuring \u0026 managing replication related resources across multiple clusters yes yes yes no no    Supported Operating Systems/Container Orchestrator Platforms   COP/OS PowerMax PowerStore PowerScale     Kubernetes 1.22, 1.23, 1.24 1.22, 1.23, 1.24 1.22, 1.23, 1.24   Red Hat OpenShift 4.9, 4.10 4.9, 4.10 4.9, 4.10   RHEL 7.x, 8.x 7.x, 8.x 7.x, 8.x   CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9   Ubuntu 20.04 20.04 20.04   SLES 15SP2 15SP2 15SP2    Supported Storage Platforms    PowerMax PowerStore PowerScale     Storage Array 5978.479.479, 5978.711.711, Unisphere 9.2 1.0.x, 2.0.x, 2.1.x OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3    Supported CSI Drivers CSM for Replication supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell PowerMax csi-powermax v2.0 +   CSI Driver for Dell PowerStore csi-powerstore v2.0 +   CSI Driver for Dell PowerScale csi-powerscale v2.2 +    Details As on the storage arrays, all replication related Kubernetes entities are required/created in pairs -\n Pair of Kubernetes Clusters Pair of replication enabled Storage classes Pair of PersistentVolumes representing the replicated pair on the storage array Pair of DellCSIReplicationGroup objects representing the replicated protection groups on the storage array  You can also use a single stretched Kubernetes cluster for protecting your applications. Even in this topology, rest of the objects still exist in pairs.\nWhat it does not do  Replicate application manifests within/across clusters. Stop applications before the planned/unplanned migration. Start applications after the migration. Replicate PersistentVolumeClaim objects within/across clusters. Replication with METRO mode does not need Replicator sidecar and common controller. Different namespaces cannot share the same RDF group for creating volumes with ASYNC mode for PowerMax. Same RDF group cannot be shared across different replication modes for PowerMax.  CSM for Replication Module Capabilities CSM for Replication provides the following capabilities:\n  Capability PowerMax PowerStore PowerScale PowerFlex Unity     Asynchronous replication of PVs accross or single K8s clusters yes yes (block) yes no no   Synchronous replication of PVs accross or single K8s clusters yes no no no no   Metro replication single (stretched) cluster yes no no no no   Replication actions (failover, reprotect) yes yes yes no no    Supported Platforms The following matrix provides a list of all supported versions for each Dell Storage product.\n   Platforms PowerMax PowerStore PowerScale     Kubernetes 1.22, 1.23, 1.24 1.22, 1.23, 1.24 1.22, 1.23, 1.24   RedHat Openshift 4.9, 4.10 4.9, 4.10 4.9, 4.10   CSI Driver 2.x(k8s), 2.2+(OpenShift) 2.x 2.2+    For compatibility with storage arrays please refer to corresponding CSI drivers\nQuickStart  Install all required components:   Enable replication during CSI driver installation Install CSM Replication Controller \u0026 repctl  Create replication enabled storage classes Create PersistentVolumeClaim using the replication enabled storage class  How it works At a high level, the following happens when you create a PersistentVolumeClaim object using a replication enabled storage class -\n CSI driver creates protection group on the storage array (if required) CSI driver creates the volume and adds it to the protection group. There will be a corresponding group and pair on the remote storage array A DellCSIReplicationGroup object is created in the cluster representing the protection group on the storage array A replica of the PersistentVolume \u0026 DellCSIReplicationGroup is created  You can refer this page for more details about the architecture.\nOnce the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), you can exercise the general Disaster Recovery workflows -\n Planned Migration to the target cluster/array Unplanned Migration to the target cluster/array Reprotect volumes at the target cluster/array Maintenance activities like - Suspend, Resume, Establish replication  ","excerpt":"Container Storage Modules (CSM) for Replication is part of the …","ref":"/csm-docs/docs/replication/","title":"Replication"},{"body":"Container Storage Modules (CSM) for Replication is part of the open-source suite of Kubernetes storage enablers for Dell products.\nCSM for Replication project aims to bring Replication \u0026 Disaster Recovery capabilities of Dell Storage Arrays to Kubernetes clusters. It helps you replicate groups of volumes using the native replication technology available on the storage array and can provide you a way to restart applications in case of both planned and unplanned migration.\nCSM for Replication Capabilities CSM for Replication provides the following capabilities:\n  Capability PowerMax PowerStore PowerScale PowerFlex Unity     Replicate data using native storage array based replication yes yes yes no no   Create PersistentVolume objects in the cluster representing the replicated volume yes yes yes no no   Create DellCSIReplicationGroup objects in the cluster yes yes yes no no   Failover \u0026 Reprotect applications using the replicated volumes yes yes yes no no   Provides a command line utility - repctl for configuring \u0026 managing replication related resources across multiple clusters yes yes yes no no    Supported Operating Systems/Container Orchestrator Platforms   COP/OS PowerMax PowerStore PowerScale     Kubernetes 1.21, 1.22, 1.23 1.21, 1.22, 1.23 1.21, 1.22, 1.23   Red Hat OpenShift 4.8, 4.9 4.8, 4.9 4.8, 4.9   RHEL 7.x, 8.x 7.x, 8.x 7.x, 8.x   CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9   Ubuntu 20.04 20.04 20.04   SLES 15SP2 15SP2 15SP2    Supported Storage Platforms    PowerMax PowerStore PowerScale     Storage Array 5978.479.479, 5978.711.711, Unisphere 9.2 1.0.x, 2.0.x, 2.1.x OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3    Supported CSI Drivers CSM for Replication supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell PowerMax csi-powermax v2.0, v2.1, v2.2   CSI Driver for Dell PowerStore csi-powerstore v2.0, v2.1, v2.2   CSI Driver for Dell PowerScale csi-powerscale v2.2    Details As on the storage arrays, all replication related Kubernetes entities are required/created in pairs -\n Pair of Kubernetes Clusters Pair of replication enabled Storage classes Pair of PersistentVolumes representing the replicated pair on the storage array Pair of DellCSIReplicationGroup objects representing the replicated protection groups on the storage array  You can also use a single stretched Kubernetes cluster for protecting your applications. Even in this topology, rest of the objects still exist in pairs.\nWhat it does not do  Replicate application manifests within/across clusters. Stop applications before the planned/unplanned migration. Start applications after the migration. Replicate PersistentVolumeClaim objects within/across clusters. Replication with METRO mode does not need Replicator sidecar and common controller.  CSM for Replication Module Capabilities CSM for Replication provides the following capabilities:\n  Capability PowerMax PowerStore PowerScale PowerFlex Unity     Asynchronous replication of PVs accross or single K8s clusters yes yes (block) yes no no   Synchronous replication of PVs accross or single K8s clusters yes no no no no   Metro replication single (stretched) cluster yes no no no no   Replication actions (failover, reprotect) yes yes yes no no    Supported Platforms The following matrix provides a list of all supported versions for each Dell Storage product.\n   Platforms PowerMax PowerStore PowerScale     Kubernetes 1.21, 1.22, 1.23 1.21, 1.22, 1.23 1.21, 1.22, 1.23   RedHat Openshift 4.8, 4.9 4.8, 4.9 4.8, 4.9   CSI Driver 2.x 2.x 2.2+    For compatibility with storage arrays please refer to corresponding CSI drivers\nQuickStart  Install all required components:   Enable replication during CSI driver installation Install CSM Replication Controller \u0026 repctl  Create replication enabled storage classes Create PersistentVolumeClaim using the replication enabled storage class  How it works At a high level, the following happens when you create a PersistentVolumeClaim object using a replication enabled storage class -\n CSI driver creates protection group on the storage array (if required) CSI driver creates the volume and adds it to the protection group. There will be a corresponding group and pair on the remote storage array A DellCSIReplicationGroup object is created in the cluster representing the protection group on the storage array A replica of the PersistentVolume \u0026 DellCSIReplicationGroup is created  You can refer this page for more details about the architecture.\nOnce the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), you can exercise the general Disaster Recovery workflows -\n Planned Migration to the target cluster/array Unplanned Migration to the target cluster/array Reprotect volumes at the target cluster/array Maintenance activities like - Suspend, Resume, Establish replication  ","excerpt":"Container Storage Modules (CSM) for Replication is part of the …","ref":"/csm-docs/v1/replication/","title":"Replication"},{"body":"Container Storage Modules (CSM) for Replication is part of the open-source suite of Kubernetes storage enablers for Dell products.\nCSM for Replication project aims to bring Replication \u0026 Disaster Recovery capabilities of Dell Storage Arrays to Kubernetes clusters. It helps you replicate groups of volumes using the native replication technology available on the storage array and can provide you a way to restart applications in case of both planned and unplanned migration.\nCSM for Replication Capabilities CSM for Replication provides the following capabilities:\n  Capability PowerScale Unity PowerStore PowerFlex PowerMax     Replicate data using native storage array based replication yes no yes no yes   Create PersistentVolume objects in the cluster representing the replicated volume yes no yes no yes   Create DellCSIReplicationGroup objects in the cluster yes no yes no yes   Failover \u0026 Reprotect applications using the replicated volumes yes no yes no yes   Provides a command line utility - repctl for configuring \u0026 managing replication related resources across multiple clusters yes no yes no yes    Supported Operating Systems/Container Orchestrator Platforms   COP/OS PowerMax PowerStore PowerScale     Kubernetes 1.21, 1.22, 1.23 1.21, 1.22, 1.23 1.21, 1.22, 1.23   Red Hat OpenShift 4.8, 4.9 4.8, 4.9 4.8, 4.9   RHEL 7.x, 8.x 7.x, 8.x 7.x, 8.x   CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9   Ubuntu 20.04 20.04 20.04   SLES 15SP2 15SP2 15SP2    Supported Storage Platforms    PowerMax PowerStore PowerScale     Storage Array 5978.479.479, 5978.711.711, Unisphere 9.2 1.0.x, 2.0.x, 2.1.x OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3    Supported CSI Drivers CSM for Replication supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell PowerMax csi-powermax v2.0, v2.1, v2.2   CSI Driver for Dell PowerStore csi-powerstore v2.0, v2.1, v2.2   CSI Driver for Dell PowerScale csi-powerscale v2.2    Details As on the storage arrays, all replication related Kubernetes entities are required/created in pairs -\n Pair of Kubernetes Clusters Pair of replication enabled Storage classes Pair of PersistentVolumes representing the replicated pair on the storage array Pair of DellCSIReplicationGroup objects representing the replicated protection groups on the storage array  You can also use a single stretched Kubernetes cluster for protecting your applications. Even in this topology, rest of the objects still exist in pairs.\nWhat it does not do  Replicate application manifests within/across clusters. Stop applications before the planned/unplanned migration. Start applications after the migration. Replicate PersistentVolumeClaim objects within/across clusters. Replication with METRO mode does not need Replicator sidecar and common controller.  CSM for Replication Module Capabilities CSM for Replication provides the following capabilities:\n  Capability PowerMax PowerStore PowerScale PowerFlex Unity     Asynchronous replication of PVs accross K8s clusters yes yes yes no no   Synchronous replication of PVs accross K8s clusters yes no no no no   Single cluster (stretched) mode replication yes yes yes no no   Replication actions (failover, reprotect) yes yes yes no no    Supported Platforms The following matrix provides a list of all supported versions for each Dell Storage product.\n   Platforms PowerMax PowerStore PowerScale     Kubernetes 1.21, 1.22, 1.23 1.21, 1.22, 1.23 1.21, 1.22, 1.23   CSI Driver 2.x 2.x 2.2+       Platforms PowerMax PowerStore PowerScale     RedHat Openshift 4.8, 4.9 4.8, 4.9 4.8, 4.9   CSI Driver 2.2+ 2.x 2.2+    For compatibility with storage arrays please refer to corresponding CSI drivers\nQuickStart  Install all required components:   Enable replication during CSI driver installation Install CSM Replication Controller \u0026 repctl  Create replication enabled storage classes Create PersistentVolumeClaim using the replication enabled storage class  How it works At a high level, the following happens when you create a PersistentVolumeClaim object using a replication enabled storage class -\n CSI driver creates protection group on the storage array (if required) CSI driver creates the volume and adds it to the protection group. There will be a corresponding group and pair on the remote storage array A DellCSIReplicationGroup object is created in the cluster representing the protection group on the storage array A replica of the PersistentVolume \u0026 DellCSIReplicationGroup is created  You can refer this page for more details about the architecture.\nOnce the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), you can exercise the general Disaster Recovery workflows -\n Planned Migration to the target cluster/array Unplanned Migration to the target cluster/array Reprotect volumes at the target cluster/array Maintenance activities like - Suspend, Resume, Establish replication  ","excerpt":"Container Storage Modules (CSM) for Replication is part of the …","ref":"/csm-docs/v2/replication/","title":"Replication"},{"body":"Container Storage Modules (CSM) for Replication is part of the open-source suite of Kubernetes storage enablers for Dell EMC products.\nCSM for Replication project aims to bring Replication \u0026 Disaster Recovery capabilities of Dell EMC Storage Arrays to Kubernetes clusters. It helps you replicate groups of volumes using the native replication technology available on the storage array and can provide you a way to restart applications in case of both planned and unplanned migration.\nCSM for Replication Capabilities CSM for Replication provides the following capabilities:\n  Capability PowerScale Unity PowerStore PowerFlex PowerMax     Replicate data using native storage array based replication no no yes no yes   Create PersistentVolume objects in the cluster representing the replicated volume no no yes no yes   Create DellCSIReplicationGroup objects in the cluster no no yes no yes   Failover \u0026 Reprotect applications using the replicated volumes no no yes no yes   Provides a command line utility - repctl for configuring \u0026 managing replication related resources across multiple clusters no no yes no yes    Supported Operating Systems/Container Orchestrator Platforms   COP/OS PowerMax PowerStore     Kubernetes 1.20, 1.21, 1.22 1.20, 1.21, 1.22   Red Hat OpenShift X 4.8, 4.9   RHEL 7.x, 8.x 7.x, 8.x   CentOS 7.8, 7.9 7.8, 7.9   Ubuntu 20.04 20.04   SLES 15SP2 15SP2    Supported Storage Platforms    PowerMax PowerStore     Storage Array 5978.479.479, 5978.669.669, 5978.711.711, Unisphere 9.2 1.0.x, 2.0.x    Supported CSI Drivers CSM for Replication supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell EMC PowerMax csi-powermax v2.0, v2.1   CSI Driver for Dell EMC PowerStore csi-powerstore v2.0, v2.1    Details As on the storage arrays, all replication related Kubernetes entities are required/created in pairs -\n Pair of Kubernetes Clusters Pair of replication enabled Storage classes Pair of PersistentVolumes representing the replicated pair on the storage array Pair of DellCSIReplicationGroup objects representing the replicated protection groups on the storage array  You can also use a single stretched Kubernetes cluster for protecting your applications. Even in this topology, rest of the objects still exist in pairs.\nWhat it does not do  Replicate application manifests within/across clusters Stop applications before the planned/unplanned migration Start applications after the migration Replicate PersistentVolumeClaim objects within/across clusters  CSM for Replication Module Capabilities CSM for Replication provides the following capabilities:\n   Capability PowerMax PowerStore PowerScale PowerFlex Unity     Asynchronous replication of PVs accross K8s clusters yes yes no no no   Synchronous replication of PVs accross K8s clusters yes no no no no   Single cluster (stretched) mode replication yes yes no no no   Replication actions (failover, reprotect) yes yes no no no    Supported Platforms The following matrix provides a list of all supported versions for each Dell EMC Storage product.\n   Platforms PowerMax PowerStore     Kubernetes 1.20, 1.21, 1.22 1.20, 1.21, 1.22   CSI Driver 2.x 2.x    For compatibility with storage arrays please refer to corresponding CSI drivers\nQuickStart  Install all required components:   Enable replication during CSI driver installation Install CSM Replication Controller \u0026 repctl  Create replication enabled storage classes Create PersistentVolumeClaim using the replication enabled storage class  How it works At a high level, the following happens when you create a PersistentVolumeClaim object using a replication enabled storage class -\n CSI driver creates protection group on the storage array (if required) CSI driver creates the volume and adds it to the protection group. There will be a corresponding group and pair on the remote storage array A DellCSIReplicationGroup object is created in the cluster representing the protection group on the storage array A replica of the PersistentVolume \u0026 DellCSIReplicationGroup is created  You can refer this page for more details about the architecture.\nOnce the DellCSIReplicationGroup \u0026 PersistentVolume objects have been replicated across clusters (or within the same cluster), you can exercise the general Disaster Recovery workflows -\n Planned Migration to the target cluster/array Unplanned Migration to the target cluster/array Reprotect volumes at the target cluster/array Maintenance activities like - Suspend, Resume, Establish replication  ","excerpt":"Container Storage Modules (CSM) for Replication is part of the …","ref":"/csm-docs/v3/replication/","title":"Replication"},{"body":"You can exercise native replication control operations from Dell storage arrays by performing “Actions” on the replicated group of volumes using the DellCSIReplicationGroup object.\nYou can patch the DellCSIReplicationGroup Custom Resource and set the action field in the spec to one of the allowed values (refer to tables in this document).\nWhen you set the action field in the Custom Resource object, the following happens:\n State of the RG CR is set to action_in_progress. For e.g. if you set the action field to SYNC, then the state will change to SYNC_IN_PROGRESS, action field will reset to empty dell-csi-replicator sidecar issues the command to the CSI driver to perform the appropriate action Once the CSI driver has completed the operation, State of the RG CR goes back to Ready  While the action is in progress, you shouldn’t update the action field. Any attempt to change the action field will be rejected and it will be reset to empty. There are certain pre-requisites that have to be fulfilled before any action can be done on the RG CR. For e.g. - you can’t perform a Reprotect without doing a Failover first. There are some “Workflows” defined in Section 2 of this document which provide a sequence of operations for some common use-cases. An important exception to these rules is the action UNPLANNED_FAILOVER which can be run at any time.\n Note - Throughout this document, we are going to refer to “Hopkinton” as the original source site \u0026 “Durham” as the original target site.\n Site Specific Actions These actions can be run at any site, but they have some site-specific context included.\nAny action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\nFor e.g. -\n If the CR at Hopkinton is patched with action FAILOVER_REMOTE, it means that the driver will attempt to Fail Over to Durham which is the remote site. If the CR at Durham is patched with action FAILOVER_LOCAL, it means that the driver will attempt to Fail Over to Durham which is the local site. If the CR at Durham is patched with REPROTECT_LOCAL, it means that the driver will Re-protect the volumes at Durham which is the local site.  The following table lists details of what actions should be used in different Disaster Recovery workflows \u0026 the equivalent operation done on the storage array:\n  Workflow Actions PowerMax PowerStore PowerScale     Planned Migration FAILOVER_LOCAL\nFAILOVER_REMOTE symrdf failover -swap FAILOVER (no REPROTECT after FAILOVER) allow_writes on target, disable local policy   Reprotect REPROTECT_LOCAL\nREPROTECT_REMOTE symrdf resume/est REPROTECT enable local policy, disallow_writes on remote   Unplanned Migration UNPLANNED_FAILOVER_LOCAL\nUNPLANNED_FAILOVER_REMOTE symrdf failover -force FAILOVER (at target site) break association on target    Maintenance Actions These actions can be run at any site and are used to change the replication link state for maintenance activities. The following table lists the supported maintenance actions and the equivalent operation done on the storage arrays\n  Action Description PowerMax PowerStore PowerScale     SUSPEND Temporarily suspend replication symrdf suspend PAUSE disable local policy   RESUME Resume replication symrdf resume RESUME enable local policy   SYNC Synchronize all changes from source to target symrdf establish SYNCHRONIZE NOW start syncIQ job    How to perform actions We strongly recommend using repctl to perform any actions on DellCSIReplicationGroup objects. You can find detailed steps here\nIf you wish to use kubectl to perform actions, then use kubectl edit/patch operations and set the action field in the Custom Resource. While performing site-specific actions, please consult each driver’s documentation to get an exhaustive list of all the supported actions.\nFor a brief guide on using actions for various DR workflows, please refer to this document\n","excerpt":"You can exercise native replication control operations from Dell …","ref":"/csm-docs/docs/replication/replication-actions/","title":"Replication Actions"},{"body":"You can exercise native replication control operations from Dell storage arrays by performing “Actions” on the replicated group of volumes using the DellCSIReplicationGroup object.\nYou can patch the DellCSIReplicationGroup Custom Resource and set the action field in the spec to one of the allowed values (refer to tables in this document).\nWhen you set the action field in the Custom Resource object, the following happens:\n State of the RG CR is set to action_in_progress. For e.g. if you set the action field to SYNC, then the state will change to SYNC_IN_PROGRESS, action field will reset to empty dell-csi-replicator sidecar issues the command to the CSI driver to perform the appropriate action Once the CSI driver has completed the operation, State of the RG CR goes back to Ready  While the action is in progress, you shouldn’t update the action field. Any attempt to change the action field will be rejected and it will be reset to empty. There are certain pre-requisites that have to be fulfilled before any action can be done on the RG CR. For e.g. - you can’t perform a Reprotect without doing a Failover first. There are some “Workflows” defined in Section 2 of this document which provide a sequence of operations for some common use-cases. An important exception to these rules is the action UNPLANNED_FAILOVER which can be run at any time.\n Note - Throughout this document, we are going to refer to “Hopkinton” as the original source site \u0026 “Durham” as the original target site.\n Site Specific Actions These actions can be run at any site, but they have some site-specific context included.\nAny action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\nFor e.g. -\n If the CR at Hopkinton is patched with action FAILOVER_REMOTE, it means that the driver will attempt to Fail Over to Durham which is the remote site. If the CR at Durham is patched with action FAILOVER_LOCAL, it means that the driver will attempt to Fail Over to Durham which is the local site. If the CR at Durham is patched with REPROTECT_LOCAL, it means that the driver will Re-protect the volumes at Durham which is the local site.  The following table lists details of what actions should be used in different Disaster Recovery workflows \u0026 the equivalent operation done on the storage array:\n  Workflow Actions PowerMax PowerStore PowerScale     Planned Migration FAILOVER_LOCAL\nFAILOVER_REMOTE symrdf failover -swap FAILOVER (no REPROTECT after FAILOVER) allow_writes on target, disable local policy   Reprotect REPROTECT_LOCAL\nREPROTECT_REMOTE symrdf resume/est REPROTECT enable local policy, disallow_writes on remote   Unplanned Migration UNPLANNED_FAILOVER_LOCAL\nUNPLANNED_FAILOVER_REMOTE symrdf failover -force FAILOVER (at target site) break association on target    Maintenance Actions These actions can be run at any site and are used to change the replication link state for maintenance activities. The following table lists the supported maintenance actions and the equivalent operation done on the storage arrays\n  Action Description PowerMax PowerStore PowerScale     SUSPEND Temporarily suspend replication symrdf suspend PAUSE disable local policy   RESUME Resume replication symrdf resume RESUME enable local policy   SYNC Synchronize all changes from source to target symrdf establish SYNCHRONIZE NOW start syncIQ job    How to perform actions We strongly recommend using repctl to perform any actions on DellCSIReplicationGroup objects. You can find detailed steps here\nIf you wish to use kubectl to perform actions, then use kubectl edit/patch operations and set the action field in the Custom Resource. While performing site-specific actions, please consult each driver’s documentation to get an exhaustive list of all the supported actions.\nFor a brief guide on using actions for various DR workflows, please refer to this document\n","excerpt":"You can exercise native replication control operations from Dell …","ref":"/csm-docs/v1/replication/replication-actions/","title":"Replication Actions"},{"body":"You can exercise native replication control operations from Dell storage arrays by performing “Actions” on the replicated group of volumes using the DellCSIReplicationGroup object.\nYou can patch the DellCSIReplicationGroup Custom Resource and set the action field in the spec to one of the allowed values (refer to tables in this document).\nWhen you set the action field in the Custom Resource object, the following happens:\n State of the RG CR is set to action_in_progress. For e.g. if you set the action field to SYNC, then the state will change to SYNC_IN_PROGRESS, action field will reset to empty dell-csi-replicator sidecar issues the command to the CSI driver to perform the appropriate action Once the CSI driver has completed the operation, State of the RG CR goes back to Ready  While the action is in progress, you shouldn’t update the action field. Any attempt to change the action field will be rejected and it will be reset to empty. There are certain pre-requisites that have to be fulfilled before any action can be done on the RG CR. For e.g. - you can’t perform a Reprotect without doing a Failover first. There are some “Workflows” defined in Section 2 of this document which provide a sequence of operations for some common use-cases. An important exception to these rules is the action UNPLANNED_FAILOVER which can be run at any time.\n Note - Throughout this document, we are going to refer to “Hopkinton” as the original source site \u0026 “Durham” as the original target site.\n Site Specific Actions These actions can be run at any site, but they have some site-specific context included.\nAny action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\nFor e.g. -\n If the CR at Hopkinton is patched with action FAILOVER_REMOTE, it means that the driver will attempt to Fail Over to Durham which is the remote site. If the CR at Durham is patched with action FAILOVER_LOCAL, it means that the driver will attempt to Fail Over to Durham which is the local site. If the CR at Durham is patched with REPROTECT_LOCAL, it means that the driver will Re-protect the volumes at Durham which is the local site.  The following table lists details of what actions should be used in different Disaster Recovery workflows \u0026 the equivalent operation done on the storage array:\n  Workflow Actions PowerMax PowerStore PowerScale     Planned Migration FAILOVER_LOCAL\nFAILOVER_REMOTE symrdf failover -swap FAILOVER (no REPROTECT after FAILOVER) allow_writes on target, disable local policy   Reprotect REPROTECT_LOCAL\nREPROTECT_REMOTE symrdf resume/est REPROTECT enable local policy, disallow_writes on remote   Unplanned Migration UNPLANNED_FAILOVER_LOCAL\nUNPLANNED_FAILOVER_REMOTE symrdf failover -force FAILOVER (at target site) break association on target    Maintenance Actions These actions can be run at any site and are used to change the replication link state for maintenance activities. The following table lists the supported maintenance actions and the equivalent operation done on the storage arrays\n  Action Description PowerMax PowerStore PowerScale     SUSPEND Temporarily suspend replication symrdf suspend PAUSE disable local policy   RESUME Resume replication symrdf resume RESUME enable local policy   SYNC Synchronize all changes from source to target symrdf establish SYNCHRONIZE NOW start syncIQ job    How to perform actions We strongly recommend using repctl to perform any actions on DellCSIReplicationGroup objects. You can find detailed steps here\nIf you wish to use kubectl to perform actions, then use kubectl edit/patch operations and set the action field in the Custom Resource. While performing site-specific actions, please consult each driver’s documentation to get an exhaustive list of all the supported actions.\nFor a brief guide on using actions for various DR workflows, please refer to this document\n","excerpt":"You can exercise native replication control operations from Dell …","ref":"/csm-docs/v2/replication/replication-actions/","title":"Replication Actions"},{"body":"You can exercise native replication control operations from Dell EMC storage arrays by performing “Actions” on the replicated group of volumes using the DellCSIReplicationGroup object.\nYou can patch the DellCSIReplicationGroup Custom Resource and set the action field in the spec to one of the allowed values (refer to tables in this document).\nWhen you set the action field in the Custom Resource object, the following happens:\n State of the RG CR is set to action_in_progress. For e.g. if you set the action field to SYNC, then the state will change to SYNC_IN_PROGRESS, action field will reset to empty dell-csi-replicator sidecar issues the command to the CSI driver to perform the appropriate action Once the CSI driver has completed the operation, State of the RG CR goes back to Ready  While the action is in progress, you shouldn’t update the action field. Any attempt to change the action field will be rejected and it will be reset to empty. There are certain pre-requisites that have to be fulfilled before any action can be done on the RG CR. For e.g. - you can’t perform a Reprotect without doing a Failover first. There are some “Workflows” defined in Section 2 of this document which provide a sequence of operations for some common use-cases. An important exception to these rules is the action UNPLANNED_FAILOVER which can be run at any time.\n Note - Throughout this document, we are going to refer to “Hopkinton” as the original source site \u0026 “Durham” as the original target site.\n Site Specific Actions These actions can be run at any site, but they have some site-specific context included.\nAny action with the LOCAL suffix means, do this action for the local site. Any action with the REMOTE suffix means do this action for the remote site.\nFor e.g. -\n If the CR at Hopkinton is patched with action FAILOVER_REMOTE, it means that the driver will attempt to Fail Over to Durham which is the remote site. If the CR at Durham is patched with action FAILOVER_LOCAL, it means that the driver will attempt to Fail over to Durham which is the local site. If the CR at Durham is patched with REPROTECT_LOCAL, it means that the driver will Re-protect the volumes at Durham which is the local site.  The following table lists details of what actions should be used in different Disaster Recovery workflows \u0026 the equivalent operation done on the storage array:\n   Workflow Actions PowerMax PowerStore     Planned Migration FAILOVER_LOCAL\nFAILOVER_REMOTE symrdf failover -swap FAILOVER (no REPROTECT after FAILOVER)   Reprotect REPROTECT_LOCAL REPROTECT_REMOTE symrdf resume/est REPROTECT   Unplanned Migration UNPLANNED_FAILOVER_LOCAL UNPLANNED_FAILOVER_REMOTE symrdf failover -force FAILOVER (at target site)    Maintenance Actions These actions can be run at any site and are used to change the replication link state for maintenance activities. The following table lists the supported maintenance actions and the equivalent operation done on the storage arrays\n  Action Description PowerMax PowerStore     SUSPEND Temporarily suspend replication symrdf suspend PAUSE   RESUME Resume replication symrdf resume RESUME   SYNC Synchronize all changes from source to target symrdf establish SYNCHRONIZE NOW    How to perform actions We strongly recommend using repctl to perform any actions on DellCSIReplicationGroup objects. You can find detailed steps here\nIf you wish to use kubectl to perform actions, then use kubectl edit/patch operations and set the action field in the Custom Resource. While performing site-specific actions, please consult each driver’s documentation to get an exhaustive list of all the supported actions.\nFor a brief guide on using actions for various DR workflows, please refer to this document\n","excerpt":"You can exercise native replication control operations from Dell EMC …","ref":"/csm-docs/v3/replication/replication-actions/","title":"Replication Actions"},{"body":"Container Storage Modules (CSM) for Resiliency is part of the open-source suite of Kubernetes storage enablers for Dell products.\nUser applications can have problems if you want their Pods to be resilient to node failure. This is especially true of those deployed with StatefulSets that use PersistentVolumeClaims. Kubernetes guarantees that there will never be two copies of the same StatefulSet Pod running at the same time and accessing storage. Therefore, it does not clean up StatefulSet Pods if the node executing them fails.\nFor the complete discussion and rationale, you can read the pod-safety design proposal.\nFor more background on the forced deletion of Pods in a StatefulSet, please visit Force Delete StatefulSet Pods.\nCSM for Resiliency High-Level Description CSM for Resiliency is designed to make Kubernetes Applications, including those that utilize persistent storage, more resilient to various failures. The first component of the Resiliency module is a pod monitor that is specifically designed to protect stateful applications from various failures. It is not a standalone application, but rather is deployed as a sidecar to CSI (Container Storage Interface) drivers, in both the driver’s controller pods and the driver’s node pods. Deploying CSM for Resiliency as a sidecar allows it to make direct requests to the driver through the Unix domain socket that Kubernetes sidecars use to make CSI requests.\nSome of the methods CSM for Resiliency invokes in the driver are standard CSI methods, such as NodeUnpublishVolume, NodeUnstageVolume, and ControllerUnpublishVolume. CSM for Resiliency also uses proprietary calls that are not part of the standard CSI specification. Currently, there is only one, ValidateVolumeHostConnectivity that returns information on whether a host is connected to the storage system and/or whether any I/O activity has happened in the recent past from a list of specified volumes. This allows CSM for Resiliency to make more accurate determinations about the state of the system and its persistent volumes. CSM for Resiliency is designed to adhere to pod affinity settings of pods.\nAccordingly, CSM for Resiliency is adapted to and qualified with each CSI driver it is to be used with. Different storage systems have different nuances and characteristics that CSM for Resiliency must take into account.\nCSM for Resiliency Capabilities CSM for Resiliency provides the following capabilities:\n  Capability PowerScale Unity XT PowerStore PowerFlex PowerMax     Detect pod failures when: Node failure, K8S Control Plane Network failure, K8S Control Plane failure, Array I/O Network failure yes yes no yes no   Cleanup pod artifacts from failed nodes yes yes no yes no   Revoke PV access from failed nodes yes yes no yes no    Supported Operating Systems/Container Orchestrator Platforms   COP/OS Supported Versions     Kubernetes 1.22, 1.23, 1.24   Red Hat OpenShift 4.9, 4.10   RHEL 7.x, 8.x   CentOS 7.8, 7.9    Supported Storage Platforms    PowerFlex Unity XT PowerScale     Storage Array 3.5.x, 3.6.x 5.0.5, 5.0.6, 5.0.7, 5.1.0, 5.1.2 OneFS 8.1, 8.2, 9.0, 9.1, 9.2, 9.3, 9.4    Supported CSI Drivers CSM for Resiliency supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell PowerFlex csi-powerflex v2.0.0 +   CSI Driver for Dell Unity XT csi-unity v2.0.0 +   CSI Driver for Dell PowerScale csi-powerscale v2.3.0 +    PowerFlex Support PowerFlex is a highly scalable array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerFlex leverages these PowerFlex features:\n Very quick detection of Array I/O Network Connectivity status changes (generally takes 1-2 seconds for the array to detect changes) A robust mechanism if Nodes are doing I/O to volumes (sampled over a 5-second period). Low latency REST API supports fast CSI provisioning and de-provisioning operations. A proprietary network protocol provided by the SDC component that can run over the same IP interface as the K8S control plane or over a separate IP interface for Array I/O.  Unity XT Support Dell Unity XT is targeted for midsized deployments, remote or branch offices, and cost-sensitive mixed workloads. Unity XT systems are designed to deliver the best value in the market. They support all-Flash, and are available in purpose-built (all Flash or hybrid Flash), converged deployment options (through VxBlock), and software-defined virtual edition.\n Unity XT (purpose-built): A modern midrange storage solution, engineered from the groundup to meet market demands for Flash, affordability and incredible simplicity. The Unity XT Family is available in 12 All Flash models and 12 Hybrid models. VxBlock (converged): Unity XT storage options are also available in Dell VxBlock System 1000. UnityVSA (virtual): The Unity XT Virtual Storage Appliance (VSA) allows the advanced unified storage and data management features of the Unity XT family to be easily deployed on VMware ESXi servers. This allows for a ‘software defined’ approach. UnityVSA is available in two editions:  Community Edition is a free downloadable 4 TB solution recommended for nonproduction use. Professional Edition is a licensed subscription-based offering available at capacity levels of 10 TB, 25 TB, and 50 TB. The subscription includes access to online support resources, EMC Secure Remote Services (ESRS), and on-call software- and systems-related support.    All three deployment options, Unity XT, UnityVSA, and Unity-based VxBlock, enjoy one architecture, one interface with consistent features and rich data services.\nPowerScale Support PowerScale is a highly scalable NFS array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerScale leverages the following PowerScale features:\n Detection of Array I/O Network Connectivity status changes. A robust mechanism to detect if Nodes are actively doing I/O to volumes. Low latency REST API supports fast CSI provisioning and de-provisioning operations.  Limitations and Exclusions This file contains information on Limitations and Exclusions that users should be aware of. Additionally, there are driver specific limitations and exclusions that may be called out in the Deploying CSM for Resiliency page.\nSupported and Tested Operating Modes The following provisioning types are supported and have been tested:\n Dynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “FileSystem”. Dynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “Block”. Use of the above volumes with Pods created by StatefulSets. Up to 12 or so protected pods on a given node. Failing up to 3 nodes at a time in 9 worker node clusters, or failing 1 node at a time in smaller clusters. Application recovery times are dependent on the number of pods that need to be moved as a result of the failure. See the section on “Testing and Performance” for some of the details. Multi-array are supported. In case of CSI Driver for PowerScale and CSI Driver for Unity, if any one of the array is not connected, the array connectivity will be false. CSI Driver for Powerflex connectivity will be determined by connection to default array.  Not Tested But Assumed to Work  Deployments with the above volume types, provided two pods from the same deployment do not reside on the same node. At the current time anti-affinity rules should be used to guarantee no two pods accessing the same volumes are scheduled to the same node.  Not Yet Tested or Supported   Pods that use persistent volumes from multiple CSI drivers. This cannot be supported because multiple controller-podmons (one for each driver type) would be trying to manage the failover with conflicting actions.\n  ReadWriteMany volumes. This may have issues if a node has multiple pods accessing the same volumes. In any case once pod cleanup fences the volumes on a node, they will no longer be available to any pods using those volumes on that node. We will endeavor to support this in the future.\n  Multiple instances of the same driver type (for example two CSI driver for Dell PowerFlex deployments.)\n  Deploying and Managing Applications Protected by CSM for Resiliency The first thing to remember about CSM for Resiliency is that it only takes action on pods configured with the designated label. Both the key and the value have to match what is in the podmon helm configuration. CSM for Resiliency emits a log message at startup with the label key and value it is using to monitor pods:\nlabelSelector: {map[podmon.dellemc.com/driver:csi-vxflexos] The above message indicates the key is: podmon.dellemc.com/driver and the label value is csi-vxflexos. To search for the pods that would be monitored, try this:\n[root@lglbx209 podmontest]# kubectl get pods -A -l podmon.dellemc.com/driver=csi-vxflexos NAMESPACE NAME READY STATUS RESTARTS AGE pmtu1 podmontest-0 1/1 Running 0 3m7s pmtu2 podmontest-0 1/1 Running 0 3m8s pmtu3 podmontest-0 1/1 Running 0 3m6s If CSM for Resiliency detects a problem with a pod caused by a node or other failure that it can initiate remediation for, it will add an event to that pod’s events:\nkubectl get events -n pmtu1 ... 61s Warning NodeFailure pod/podmontest-0 podmon cleaning pod [7520ba2a-cec5-4dff-8537-20c9bdafbe26 node.example.com] with force delete ... CSM for Resiliency may also generate events if it is unable to cleanup a pod for some reason. For example, it may not clean up a pod because the pod is still doing I/O to the array.\nImportant Before putting an application into production that relies on CSM for Resiliency monitoring, it is important to do a few test failovers first. To do this take the node that is running the pod offline for at least 2-3 minutes. Verify that there is an event message similar to the one above is logged, and that the pod recovers and restarts normally with no loss of data. (Note that if the node is running many CSM for Resiliency protected pods, the node may need to be down longer for CSM for Resiliency to have time to evacuate all the protected pods.)\nApplication Recommendations   It is recommended that pods that will be monitored by CSM for Resiliency be configured to exit if they receive any I/O errors. That should help achieve the recovery as quickly as possible.\n  CSM for Resiliency does not directly monitor application health. However, if standard Kubernetes health checks are configured, that may help reduce pod recovery time in the event of node failure, as CSM for Resiliency should receive an event that the application is Not Ready. Note that a Not Ready pod is not sufficient to trigger CSM for Resiliency action unless there is also some condition indicating a Node failure or problem, such as the Node is tainted, or the array has lost connectivity to the node.\n  As noted previously in the Limitations and Exclusions section, CSM for Resiliency has not yet been verified to work with ReadWriteMany or ReadOnlyMany volumes. Also, it has not been verified to work with pod controllers other than StatefulSet.\n  Recovering From Failures Normally CSM for Resiliency should be able to move pods that have been impacted by Node Failures to a healthy node. After the failed nodes have come back online, CSM for Resiliency cleans them up (especially any potential zombie pods) and then automatically removes the CSM for Resiliency node taint that prevents pods from being scheduled to the failed node(s). There are a few cases where this cannot be fully automated and operator intervention is required, including:\n  CSM for Resiliency expects that when a node failure occurs, all CSM for Resiliency labeled pods are evacuated and rescheduled on other nodes. This process may not complete however if the node comes back online before CSM for Resiliency has had time to evacuate all the labeled pods. The remaining pods may not restart correctly, going to “Error” or “CrashLoopBackoff”. We are considering some possible remediation for this condition but have not implemented them yet.\nIf this happens, try deleting the pod with “kubectl delete pod …”. In our experience this normally will cause the pod to be restarted and transition to the “Running” state.\n  Podmon-node is responsible for cleaning up failed nodes after the nodes’ communication has been restored. The algorithm checks to see that all the monitored pods have terminated and their volumes and mounts have been cleaned up.\nIf some of the monitored pods are still executing, node-podmon will emit the following log message at the end of a cleanup cycle (and retry the cleanup after a delay):\npods skipped for cleanup because still present: \u003cpod-list\u003e If this happens, DO NOT manually remove the CSM for Resiliency node taint. Doing so could possibly cause data corruption if volumes were not cleaned up, and a pod using those volumes was subsequently scheduled to that node.\nThe correct course of action in this case is to reboot the failed node(s) that have not removed their taints in a reasonable time (5-10 minutes after the node is online again.) The operator can delay executing this reboot until it is convenient, but new pods will not be scheduled to it in the interim. This reboot will cancel any potential zombie pods. After the reboot, node-podmon should automatically remove the node taint after a short time.\n  Testing Methodology and Results A three tier testing methodology is used for CSM for Resiliency:\n Unit testing with high coverage (\u003e90% statement) tests the program logic and is especially used to test the error paths by injecting faults. An integration test describes test scenarios in Gherkin that sets up specific testing scenarios executed against a Kubernetes test cluster. The tests use ranges for many of the parameters to add an element of “chaos testing”. Script based testing supports longevity testing in a Kubernetes cluster. For example, one test repeatedly fails three different lists of nodes in succession and is used to fail 1/3 of the cluster’s worker nodes on a cyclic basis and repeat indefinitely. This test collect statistics on length of time for pod evacuation, pod recovery, and node cleanup.  ","excerpt":"Container Storage Modules (CSM) for Resiliency is part of the …","ref":"/csm-docs/docs/resiliency/","title":"Resiliency"},{"body":"Container Storage Modules (CSM) for Resiliency is part of the open-source suite of Kubernetes storage enablers for Dell products.\nUser applications can have problems if you want their Pods to be resilient to node failure. This is especially true of those deployed with StatefulSets that use PersistentVolumeClaims. Kubernetes guarantees that there will never be two copies of the same StatefulSet Pod running at the same time and accessing storage. Therefore, it does not clean up StatefulSet Pods if the node executing them fails.\nFor the complete discussion and rationale, you can read the pod-safety design proposal.\nFor more background on the forced deletion of Pods in a StatefulSet, please visit Force Delete StatefulSet Pods.\nCSM for Resiliency High-Level Description CSM for Resiliency is designed to make Kubernetes Applications, including those that utilize persistent storage, more resilient to various failures. The first component of the Resiliency module is a pod monitor that is specifically designed to protect stateful applications from various failures. It is not a standalone application, but rather is deployed as a sidecar to CSI (Container Storage Interface) drivers, in both the driver’s controller pods and the driver’s node pods. Deploying CSM for Resiliency as a sidecar allows it to make direct requests to the driver through the Unix domain socket that Kubernetes sidecars use to make CSI requests.\nSome of the methods CSM for Resiliency invokes in the driver are standard CSI methods, such as NodeUnpublishVolume, NodeUnstageVolume, and ControllerUnpublishVolume. CSM for Resiliency also uses proprietary calls that are not part of the standard CSI specification. Currently, there is only one, ValidateVolumeHostConnectivity that returns information on whether a host is connected to the storage system and/or whether any I/O activity has happened in the recent past from a list of specified volumes. This allows CSM for Resiliency to make more accurate determinations about the state of the system and its persistent volumes. CSM for Resiliency is designed to adhere to pod affinity settings of pods.\nAccordingly, CSM for Resiliency is adapted to and qualified with each CSI driver it is to be used with. Different storage systems have different nuances and characteristics that CSM for Resiliency must take into account.\nCSM for Resiliency Capabilities CSM for Resiliency provides the following capabilities:\n  Capability PowerScale Unity PowerStore PowerFlex PowerMax     Detect pod failures when: Node failure, K8S Control Plane Network failure, K8S Control Plane failure, Array I/O Network failure no yes no yes no   Cleanup pod artifacts from failed nodes no yes no yes no   Revoke PV access from failed nodes no yes no yes no    Supported Operating Systems/Container Orchestrator Platforms   COP/OS Supported Versions     Kubernetes 1.21, 1.22, 1.23   Red Hat OpenShift 4.8, 4.9   RHEL 7.x, 8.x   CentOS 7.8, 7.9    Supported Storage Platforms    PowerFlex Unity     Storage Array 3.5.x, 3.6.x 5.0.5, 5.0.6, 5.0.7, 5.1.0, 5.1.2    Supported CSI Drivers CSM for Resiliency supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell PowerFlex csi-powerflex v2.0, v2.1, v2.2   CSI Driver for Dell Unity csi-unity v2.0, v2.1, v2.2    PowerFlex Support PowerFlex is a highly scalable array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerFlex leverages the following PowerFlex features:\n Very quick detection of Array I/O Network Connectivity status changes (generally takes 1-2 seconds for the array to detect changes) A robust mechanism if Nodes are doing I/O to volumes (sampled over a 5-second period). Low latency REST API supports fast CSI provisioning and de-provisioning operations. A proprietary network protocol provided by the SDC component that can run over the same IP interface as the K8S control plane or over a separate IP interface for Array I/O.  Unity Support Dell Unity is targeted for midsized deployments, remote or branch offices, and cost-sensitive mixed workloads. Unity systems are designed for all-Flash, deliver the best value in the market, and are available in purpose-built (all Flash or hybrid Flash), converged deployment options (through VxBlock), and software-defined virtual edition.\n Unity (purpose-built): A modern midrange storage solution, engineered from the groundup to meet market demands for Flash, affordability and incredible simplicity. The Unity Family is available in 12 All Flash models and 12 Hybrid models. VxBlock (converged): Unity storage options are also available in Dell VxBlock System 1000. UnityVSA (virtual): The Unity Virtual Storage Appliance (VSA) allows the advanced unified storage and data management features of the Unity family to be easily deployed on VMware ESXi servers, for a ‘software defined’ approach. UnityVSA is available in two editions:  Community Edition is a free downloadable 4 TB solution recommended for nonproduction use. Professional Edition is a licensed subscription-based offering available at capacity levels of 10 TB, 25 TB, and 50 TB. The subscription includes access to online support resources, EMC Secure Remote Services (ESRS), and on-call software- and systems-related support.    All three deployment options, i.e. Unity, UnityVSA, and Unity-based VxBlock, enjoy one architecture, one interface with consistent features and rich data services.\nLimitations and Exclusions This file contains information on Limitations and Exclusions that users should be aware of. Additionally, there are driver specific limitations and exclusions that may be called out in the Deploying CSM for Resiliency page.\nSupported and Tested Operating Modes The following provisioning types are supported and have been tested:\n Dynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “FileSystem”. Dynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “Block”. Use of the above volumes with Pods created by StatefulSets. Up to 12 or so protected pods on a given node. Failing up to 3 nodes at a time in 9 worker node clusters, or failing 1 node at a time in smaller clusters. Application recovery times are dependent on the number of pods that need to be moved as a result of the failure. See the section on “Testing and Performance” for some of the details.  Not Tested But Assumed to Work  Deployments with the above volume types, provided two pods from the same deployment do not reside on the same node. At the current time anti-affinity rules should be used to guarantee no two pods accessing the same volumes are scheduled to the same node. Multi-array support  Not Yet Tested or Supported   Pods that use persistent volumes from multiple CSI drivers. This cannot be supported because multiple controller-podmons (one for each driver type) would be trying to manage the failover with conflicting actions.\n  ReadWriteMany volumes. This may have issues if a node has multiple pods accessing the same volumes. In any case once pod cleanup fences the volumes on a node, they will no longer be available to any pods using those volumes on that node. We will endeavor to support this in the future.\n  Multiple instances of the same driver type (for example two CSI driver for Dell PowerFlex deployments.)\n  Deploying and Managing Applications Protected by CSM for Resiliency The first thing to remember about CSM for Resiliency is that it only takes action on pods configured with the designated label. Both the key and the value have to match what is in the podmon helm configuration. CSM for Resiliency emits a log message at startup with the label key and value it is using to monitor pods:\nlabelSelector: {map[podmon.dellemc.com/driver:csi-vxflexos] The above message indicates the key is: podmon.dellemc.com/driver and the label value is csi-vxflexos. To search for the pods that would be monitored, try this:\n[root@lglbx209 podmontest]# kubectl get pods -A -l podmon.dellemc.com/driver=csi-vxflexos NAMESPACE NAME READY STATUS RESTARTS AGE pmtu1 podmontest-0 1/1 Running 0 3m7s pmtu2 podmontest-0 1/1 Running 0 3m8s pmtu3 podmontest-0 1/1 Running 0 3m6s If CSM for Resiliency detects a problem with a pod caused by a node or other failure that it can initiate remediation for, it will add an event to that pod’s events:\nkubectl get events -n pmtu1 ... 61s Warning NodeFailure pod/podmontest-0 podmon cleaning pod [7520ba2a-cec5-4dff-8537-20c9bdafbe26 node.example.com] with force delete ... CSM for Resiliency may also generate events if it is unable to cleanup a pod for some reason. For example, it may not clean up a pod because the pod is still doing I/O to the array.\nImportant Before putting an application into production that relies on CSM for Resiliency monitoring, it is important to do a few test failovers first. To do this take the node that is running the pod offline for at least 2-3 minutes. Verify that there is an event message similar to the one above is logged, and that the pod recovers and restarts normally with no loss of data. (Note that if the node is running many CSM for Resiliency protected pods, the node may need to be down longer for CSM for Resiliency to have time to evacuate all the protected pods.)\nApplication Recommendations   It is recommended that pods that will be monitored by CSM for Resiliency be configured to exit if they receive any I/O errors. That should help achieve the recovery as quickly as possible.\n  CSM for Resiliency does not directly monitor application health. However, if standard Kubernetes health checks are configured, that may help reduce pod recovery time in the event of node failure, as CSM for Resiliency should receive an event that the application is Not Ready. Note that a Not Ready pod is not sufficient to trigger CSM for Resiliency action unless there is also some condition indicating a Node failure or problem, such as the Node is tainted, or the array has lost connectivity to the node.\n  As noted previously in the Limitations and Exclusions section, CSM for Resiliency has not yet been verified to work with ReadWriteMany or ReadOnlyMany volumes. Also, it has not been verified to work with pod controllers other than StatefulSet.\n  Recovering From Failures Normally CSM for Resiliency should be able to move pods that have been impacted by Node Failures to a healthy node. After the failed nodes have come back online, CSM for Resiliency cleans them up (especially any potential zombie pods) and then automatically removes the CSM for Resiliency node taint that prevents pods from being scheduled to the failed node(s). There are a few cases where this cannot be fully automated and operator intervention is required, including:\n  CSM for Resiliency expects that when a node failure occurs, all CSM for Resiliency labeled pods are evacuated and rescheduled on other nodes. This process may not complete however if the node comes back online before CSM for Resiliency has had time to evacuate all the labeled pods. The remaining pods may not restart correctly, going to “Error” or “CrashLoopBackoff”. We are considering some possible remediation for this condition but have not implemented them yet.\nIf this happens, try deleting the pod with “kubectl delete pod …”. In our experience this normally will cause the pod to be restarted and transition to the “Running” state.\n  Podmon-node is responsible for cleaning up failed nodes after the nodes’ communication has been restored. The algorithm checks to see that all the monitored pods have terminated and their volumes and mounts have been cleaned up.\nIf some of the monitored pods are still executing, node-podmon will emit the following log message at the end of a cleanup cycle (and retry the cleanup after a delay):\npods skipped for cleanup because still present: \u003cpod-list\u003e If this happens, DO NOT manually remove the CSM for Resiliency node taint. Doing so could possibly cause data corruption if volumes were not cleaned up, and a pod using those volumes was subsequently scheduled to that node.\nThe correct course of action in this case is to reboot the failed node(s) that have not removed their taints in a reasonable time (5-10 minutes after the node is online again.) The operator can delay executing this reboot until it is convenient, but new pods will not be scheduled to it in the interim. This reboot will cancel any potential zombie pods. After the reboot, node-podmon should automatically remove the node taint after a short time.\n  Testing Methodology and Results A three tier testing methodology is used for CSM for Resiliency:\n Unit testing with high coverage (\u003e90% statement) tests the program logic and is especially used to test the error paths by injecting faults. An integration test describes test scenarios in Gherkin that sets up specific testing scenarios executed against a Kubernetes test cluster. The tests use ranges for many of the parameters to add an element of “chaos testing”. Script based testing supports longevity testing in a Kubernetes cluster. For example, one test repeatedly fails three different lists of nodes in succession and is used to fail 1/3 of the cluster’s worker nodes on a cyclic basis and repeat indefinitely. This test collect statistics on length of time for pod evacuation, pod recovery, and node cleanup.  ","excerpt":"Container Storage Modules (CSM) for Resiliency is part of the …","ref":"/csm-docs/v1/resiliency/","title":"Resiliency"},{"body":"Container Storage Modules (CSM) for Resiliency is part of the open-source suite of Kubernetes storage enablers for Dell products.\nUser applications can have problems if you want their Pods to be resilient to node failure. This is especially true of those deployed with StatefulSets that use PersistentVolumeClaims. Kubernetes guarantees that there will never be two copies of the same StatefulSet Pod running at the same time and accessing storage. Therefore, it does not clean up StatefulSet Pods if the node executing them fails.\nFor the complete discussion and rationale, you can read the pod-safety design proposal.\nFor more background on the forced deletion of Pods in a StatefulSet, please visit Force Delete StatefulSet Pods.\nCSM for Resiliency High-Level Description CSM for Resiliency is designed to make Kubernetes Applications, including those that utilize persistent storage, more resilient to various failures. The first component of the Resiliency module is a pod monitor that is specifically designed to protect stateful applications from various failures. It is not a standalone application, but rather is deployed as a sidecar to CSI (Container Storage Interface) drivers, in both the driver’s controller pods and the driver’s node pods. Deploying CSM for Resiliency as a sidecar allows it to make direct requests to the driver through the Unix domain socket that Kubernetes sidecars use to make CSI requests.\nSome of the methods CSM for Resiliency invokes in the driver are standard CSI methods, such as NodeUnpublishVolume, NodeUnstageVolume, and ControllerUnpublishVolume. CSM for Resiliency also uses proprietary calls that are not part of the standard CSI specification. Currently, there is only one, ValidateVolumeHostConnectivity that returns information on whether a host is connected to the storage system and/or whether any I/O activity has happened in the recent past from a list of specified volumes. This allows CSM for Resiliency to make more accurate determinations about the state of the system and its persistent volumes. CSM for Resiliency is designed to adhere to pod affinity settings of pods.\nAccordingly, CSM for Resiliency is adapted to and qualified with each CSI driver it is to be used with. Different storage systems have different nuances and characteristics that CSM for Resiliency must take into account.\nCSM for Resiliency Capabilities CSM for Resiliency provides the following capabilities:\n  Capability PowerScale Unity PowerStore PowerFlex PowerMax     Detect pod failures when: Node failure, K8S Control Plane Network failure, K8S Control Plane failure, Array I/O Network failure no yes no yes no   Cleanup pod artifacts from failed nodes no yes no yes no   Revoke PV access from failed nodes no yes no yes no    Supported Operating Systems/Container Orchestrator Platforms   COP/OS Supported Versions     Kubernetes 1.21, 1.22, 1.23   Red Hat OpenShift 4.8, 4.9   RHEL 7.x, 8.x   CentOS 7.8, 7.9    Supported Storage Platforms    PowerFlex Unity     Storage Array 3.5.x, 3.6.x 5.0.5, 5.0.6, 5.0.7, 5.1.0, 5.1.2    Supported CSI Drivers CSM for Resiliency supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell PowerFlex csi-powerflex v2.0, v2.1, v2.2   CSI Driver for Dell Unity csi-unity v2.0, v2.1, v2.2    PowerFlex Support PowerFlex is a highly scalable array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerFlex leverages the following PowerFlex features:\n Very quick detection of Array I/O Network Connectivity status changes (generally takes 1-2 seconds for the array to detect changes) A robust mechanism if Nodes are doing I/O to volumes (sampled over a 5-second period). Low latency REST API supports fast CSI provisioning and de-provisioning operations. A proprietary network protocol provided by the SDC component that can run over the same IP interface as the K8S control plane or over a separate IP interface for Array I/O.  Unity Support Dell Unity is targeted for midsized deployments, remote or branch offices, and cost-sensitive mixed workloads. Unity systems are designed for all-Flash, deliver the best value in the market, and are available in purpose-built (all Flash or hybrid Flash), converged deployment options (through VxBlock), and software-defined virtual edition.\n Unity (purpose-built): A modern midrange storage solution, engineered from the groundup to meet market demands for Flash, affordability and incredible simplicity. The Unity Family is available in 12 All Flash models and 12 Hybrid models. VxBlock (converged): Unity storage options are also available in Dell VxBlock System 1000. UnityVSA (virtual): The Unity Virtual Storage Appliance (VSA) allows the advanced unified storage and data management features of the Unity family to be easily deployed on VMware ESXi servers, for a ‘software defined’ approach. UnityVSA is available in two editions:  Community Edition is a free downloadable 4 TB solution recommended for nonproduction use. Professional Edition is a licensed subscription-based offering available at capacity levels of 10 TB, 25 TB, and 50 TB. The subscription includes access to online support resources, EMC Secure Remote Services (ESRS), and on-call software- and systems-related support.    All three deployment options, i.e. Unity, UnityVSA, and Unity-based VxBlock, enjoy one architecture, one interface with consistent features and rich data services.\nLimitations and Exclusions This file contains information on Limitations and Exclusions that users should be aware of. Additionally, there are driver specific limitations and exclusions that may be called out in the Deploying CSM for Resiliency page.\nSupported and Tested Operating Modes The following provisioning types are supported and have been tested:\n Dynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “FileSystem”. Dynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “Block”. Use of the above volumes with Pods created by StatefulSets. Up to 12 or so protected pods on a given node. Failing up to 3 nodes at a time in 9 worker node clusters, or failing 1 node at a time in smaller clusters. Application recovery times are dependent on the number of pods that need to be moved as a result of the failure. See the section on “Testing and Performance” for some of the details.  Not Tested But Assumed to Work  Deployments with the above volume types, provided two pods from the same deployment do not reside on the same node. At the current time anti-affinity rules should be used to guarantee no two pods accessing the same volumes are scheduled to the same node. Multi-array support  Not Yet Tested or Supported   Pods that use persistent volumes from multiple CSI drivers. This cannot be supported because multiple controller-podmons (one for each driver type) would be trying to manage the failover with conflicting actions.\n  ReadWriteMany volumes. This may have issues if a node has multiple pods accessing the same volumes. In any case once pod cleanup fences the volumes on a node, they will no longer be available to any pods using those volumes on that node. We will endeavor to support this in the future.\n  Multiple instances of the same driver type (for example two CSI driver for Dell PowerFlex deployments.)\n  Deploying and Managing Applications Protected by CSM for Resiliency The first thing to remember about CSM for Resiliency is that it only takes action on pods configured with the designated label. Both the key and the value have to match what is in the podmon helm configuration. CSM for Resiliency emits a log message at startup with the label key and value it is using to monitor pods:\nlabelSelector: {map[podmon.dellemc.com/driver:csi-vxflexos] The above message indicates the key is: podmon.dellemc.com/driver and the label value is csi-vxflexos. To search for the pods that would be monitored, try this:\n[root@lglbx209 podmontest]# kubectl get pods -A -l podmon.dellemc.com/driver=csi-vxflexos NAMESPACE NAME READY STATUS RESTARTS AGE pmtu1 podmontest-0 1/1 Running 0 3m7s pmtu2 podmontest-0 1/1 Running 0 3m8s pmtu3 podmontest-0 1/1 Running 0 3m6s If CSM for Resiliency detects a problem with a pod caused by a node or other failure that it can initiate remediation for, it will add an event to that pod’s events:\nkubectl get events -n pmtu1 ... 61s Warning NodeFailure pod/podmontest-0 podmon cleaning pod [7520ba2a-cec5-4dff-8537-20c9bdafbe26 node.example.com] with force delete ... CSM for Resiliency may also generate events if it is unable to cleanup a pod for some reason. For example, it may not clean up a pod because the pod is still doing I/O to the array.\nImportant Before putting an application into production that relies on CSM for Resiliency monitoring, it is important to do a few test failovers first. To do this take the node that is running the pod offline for at least 2-3 minutes. Verify that there is an event message similar to the one above is logged, and that the pod recovers and restarts normally with no loss of data. (Note that if the node is running many CSM for Resiliency protected pods, the node may need to be down longer for CSM for Resiliency to have time to evacuate all the protected pods.)\nApplication Recommendations   It is recommended that pods that will be monitored by CSM for Resiliency be configured to exit if they receive any I/O errors. That should help achieve the recovery as quickly as possible.\n  CSM for Resiliency does not directly monitor application health. However, if standard Kubernetes health checks are configured, that may help reduce pod recovery time in the event of node failure, as CSM for Resiliency should receive an event that the application is Not Ready. Note that a Not Ready pod is not sufficient to trigger CSM for Resiliency action unless there is also some condition indicating a Node failure or problem, such as the Node is tainted, or the array has lost connectivity to the node.\n  As noted previously in the Limitations and Exclusions section, CSM for Resiliency has not yet been verified to work with ReadWriteMany or ReadOnlyMany volumes. Also, it has not been verified to work with pod controllers other than StatefulSet.\n  Recovering From Failures Normally CSM for Resiliency should be able to move pods that have been impacted by Node Failures to a healthy node. After the failed nodes have come back online, CSM for Resiliency cleans them up (especially any potential zombie pods) and then automatically removes the CSM for Resiliency node taint that prevents pods from being scheduled to the failed node(s). There are a few cases where this cannot be fully automated and operator intervention is required, including:\n  CSM for Resiliency expects that when a node failure occurs, all CSM for Resiliency labeled pods are evacuated and rescheduled on other nodes. This process may not complete however if the node comes back online before CSM for Resiliency has had time to evacuate all the labeled pods. The remaining pods may not restart correctly, going to “Error” or “CrashLoopBackoff”. We are considering some possible remediation for this condition but have not implemented them yet.\nIf this happens, try deleting the pod with “kubectl delete pod …”. In our experience this normally will cause the pod to be restarted and transition to the “Running” state.\n  Podmon-node is responsible for cleaning up failed nodes after the nodes’ communication has been restored. The algorithm checks to see that all the monitored pods have terminated and their volumes and mounts have been cleaned up.\nIf some of the monitored pods are still executing, node-podmon will emit the following log message at the end of a cleanup cycle (and retry the cleanup after a delay):\npods skipped for cleanup because still present: \u003cpod-list\u003e If this happens, DO NOT manually remove the CSM for Resiliency node taint. Doing so could possibly cause data corruption if volumes were not cleaned up, and a pod using those volumes was subsequently scheduled to that node.\nThe correct course of action in this case is to reboot the failed node(s) that have not removed their taints in a reasonable time (5-10 minutes after the node is online again.) The operator can delay executing this reboot until it is convenient, but new pods will not be scheduled to it in the interim. This reboot will cancel any potential zombie pods. After the reboot, node-podmon should automatically remove the node taint after a short time.\n  Testing Methodology and Results A three tier testing methodology is used for CSM for Resiliency:\n Unit testing with high coverage (\u003e90% statement) tests the program logic and is especially used to test the error paths by injecting faults. An integration test describes test scenarios in Gherkin that sets up specific testing scenarios executed against a Kubernetes test cluster. The tests use ranges for many of the parameters to add an element of “chaos testing”. Script based testing supports longevity testing in a Kubernetes cluster. For example, one test repeatedly fails three different lists of nodes in succession and is used to fail 1/3 of the cluster’s worker nodes on a cyclic basis and repeat indefinitely. This test collect statistics on length of time for pod evacuation, pod recovery, and node cleanup.  ","excerpt":"Container Storage Modules (CSM) for Resiliency is part of the …","ref":"/csm-docs/v2/resiliency/","title":"Resiliency"},{"body":"Container Storage Modules (CSM) for Resiliency is part of the open-source suite of Kubernetes storage enablers for Dell EMC products.\nUser applications can have problems if you want their Pods to be resilient to node failure. This is especially true of those deployed with StatefulSets that use PersistentVolumeClaims. Kubernetes guarantees that there will never be two copies of the same StatefulSet Pod running at the same time and accessing storage. Therefore, it does not clean up StatefulSet Pods if the node executing them fails.\nFor the complete discussion and rationale, go to https://github.com/kubernetes/community and search for the pod-safety.md file (path: contributors/design-proposals/storage/pod-safety.md). For more background on the forced deletion of Pods in a StatefulSet, please visit Force Delete StatefulSet Pods.\nCSM for Resiliency High-Level Description CSM for Resiliency is designed to make Kubernetes Applications, including those that utilize persistent storage, more resilient to various failures. The first component of the Resiliency module is a pod monitor that is specifically designed to protect stateful applications from various failures. It is not a standalone application, but rather is deployed as a sidecar to CSI (Container Storage Interface) drivers, in both the driver’s controller pods and the driver’s node pods. Deploying CSM for Resiliency as a sidecar allows it to make direct requests to the driver through the Unix domain socket that Kubernetes sidecars use to make CSI requests.\nSome of the methods CSM for Resiliency invokes in the driver are standard CSI methods, such as NodeUnpublishVolume, NodeUnstageVolume, and ControllerUnpublishVolume. CSM for Resiliency also uses proprietary calls that are not part of the standard CSI specification. Currently, there is only one, ValidateVolumeHostConnectivity that returns information on whether a host is connected to the storage system and/or whether any I/O activity has happened in the recent past from a list of specified volumes. This allows CSM for Resiliency to make more accurate determinations about the state of the system and its persistent volumes.\nAccordingly, CSM for Resiliency is adapted to and qualified with each CSI driver it is to be used with. Different storage systems have different nuances and characteristics that CSM for Resiliency must take into account.\nCSM for Resiliency Capabilities CSM for Resiliency provides the following capabilities:\n  Capability PowerScale Unity PowerStore PowerFlex PowerMax     Detect pod failures for the following failure types - Node failure, K8S Control Plane Network failure, Array I/O Network failure no yes no yes no   Cleanup pod artifacts from failed nodes no yes no yes no   Revoke PV access from failed nodes no yes no yes no    Supported Operating Systems/Container Orchestrator Platforms   COP/OS Supported Versions     Kubernetes 1.20, 1.21, 1.22   Red Hat OpenShift 4.8, 4.9   RHEL 7.x, 8.x   CentOS 7.8, 7.9    Supported Storage Platforms    PowerFlex Unity     Storage Array 3.5.x, 3.6.x 5.0.5, 5.0.6, 5.0.7, 5.1.0    Supported CSI Drivers CSM for Authorization supports the following CSI drivers and versions.   Storage Array CSI Driver Supported Versions     CSI Driver for Dell EMC PowerFlex csi-powerflex v2.0,v2.1   CSI Driver for Dell EMC Unity csi-unity v2.0,v2.1    PowerFlex Support PowerFlex is a highly scalable array that is very well suited to Kubernetes deployments. The CSM for Resiliency support for PowerFlex leverages the following PowerFlex features:\n Very quick detection of Array I/O Network Connectivity status changes (generally takes 1-2 seconds for the array to detect changes) A robust mechanism if Nodes are doing I/O to volumes (sampled over a 5-second period). Low latency REST API supports fast CSI provisioning and de-provisioning operations. A proprietary network protocol provided by the SDC component that can run over the same IP interface as the K8S control plane or over a separate IP interface for Array I/O.  Unity Support Dell EMC Unity is targeted for midsized deployments, remote or branch offices, and cost-sensitive mixed workloads. Unity systems are designed for all-Flash, deliver the best value in the market, and are available in purpose-built (all Flash or hybrid Flash), converged deployment options (through VxBlock), and software-defined virtual edition.\n Unity (purpose-built): A modern midrange storage solution, engineered from the groundup to meet market demands for Flash, affordability and incredible simplicity. The Unity Family is available in 12 All Flash models and 12 Hybrid models. VxBlock (converged): Unity storage options are also available in Dell EMC VxBlock System 1000. UnityVSA (virtual): The Unity Virtual Storage Appliance (VSA) allows the advanced unified storage and data management features of the Unity family to be easily deployed on VMware ESXi servers, for a ‘software defined’ approach. UnityVSA is available in two editions:  Community Edition is a free downloadable 4 TB solution recommended for nonproduction use. Professional Edition is a licensed subscription-based offering available at capacity levels of 10 TB, 25 TB, and 50 TB. The subscription includes access to online support resources, EMC Secure Remote Services (ESRS), and on-call software- and systems-related support.    All three deployment options, i.e. Unity, UnityVSA, and Unity-based VxBlock, enjoy one architecture, one interface with consistent features and rich data services.\nLimitations and Exclusions This file contains information on Limitations and Exclusions that users should be aware of. Additionally, there are driver specific limitations and exclusions that may be called out in the Deploying CSM for Resiliency page.\nSupported and Tested Operating Modes The following provisioning types are supported and have been tested:\n Dynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “FileSystem”. Dynamic PVC/PVs of accessModes “ReadWriteOnce” and volumeMode “Block”. Use of the above volumes with Pods created by StatefulSets. Up to 12 or so protected pods on a given node. Failing up to 3 nodes at a time in 9 worker node clusters, or failing 1 node at a time in smaller clusters. Application recovery times are dependent on the number of pods that need to be moved as a result of the failure. See the section on “Testing and Performance” for some of the details.  Not Tested But Assumed to Work  Deployments with the above volume types, provided two pods from the same deployment do not reside on the same node. At the current time anti-affinity rules should be used to guarantee no two pods accessing the same volumes are scheduled to the same node. Multi-array support  Not Yet Tested or Supported   Pods that use persistent volumes from multiple CSI drivers. This cannot be supported because multiple controller-podmons (one for each driver type) would be trying to manage the failover with conflicting actions.\n  ReadWriteMany volumes. This may have issues if a node has multiple pods accessing the same volumes. In any case once pod cleanup fences the volumes on a node, they will no longer be available to any pods using those volumes on that node. We will endeavor to support this in the future.\n  Multiple instances of the same driver type (for example two CSI driver for Dell EMC PowerFlex deployments.)\n  Deploying and Managing Applications Protected by CSM for Resiliency The first thing to remember about CSM for Resiliency is that it only takes action on pods configured with the designated label. Both the key and the value have to match what is in the podmon helm configuration. CSM for Resiliency emits a log message at startup with the label key and value it is using to monitor pods:\nlabelSelector: {map[podmon.dellemc.com/driver:csi-vxflexos] The above message indicates the key is: podmon.dellemc.com/driver and the label value is csi-vxflexos. To search for the pods that would be monitored, try this:\n[root@lglbx209 podmontest]# kubectl get pods -A -l podmon.dellemc.com/driver=csi-vxflexos NAMESPACE NAME READY STATUS RESTARTS AGE pmtu1 podmontest-0 1/1 Running 0 3m7s pmtu2 podmontest-0 1/1 Running 0 3m8s pmtu3 podmontest-0 1/1 Running 0 3m6s If CSM for Resiliency detects a problem with a pod caused by a node or other failure that it can initiate remediation for, it will add an event to that pod’s events:\nkubectl get events -n pmtu1 ... 61s Warning NodeFailure pod/podmontest-0 podmon cleaning pod [7520ba2a-cec5-4dff-8537-20c9bdafbe26 node.example.com] with force delete ... CSM for Resiliency may also generate events if it is unable to cleanup a pod for some reason. For example, it may not clean up a pod because the pod is still doing I/O to the array.\nImportant Before putting an application into production that relies on CSM for Resiliency monitoring, it is important to do a few test failovers first. To do this take the node that is running the pod offline for at least 2-3 minutes. Verify that there is an event message similar to the one above is logged, and that the pod recovers and restarts normally with no loss of data. (Note that if the node is running many CSM for Resiliency protected pods, the node may need to be down longer for CSM for Resiliency to have time to evacuate all the protected pods.)\nApplication Recommendations   It is recommended that pods that will be monitored by CSM for Resiliency be configured to exit if they receive any I/O errors. That should help achieve the recovery as quickly as possible.\n  CSM for Resiliency does not directly monitor application health. However, if standard Kubernetes health checks are configured, that may help reduce pod recovery time in the event of node failure, as CSM for Resiliency should receive an event that the application is Not Ready. Note that a Not Ready pod is not sufficient to trigger CSM for Resiliency action unless there is also some condition indicating a Node failure or problem, such as the Node is tainted, or the array has lost connectivity to the node.\n  As noted previously in the Limitations and Exclusions section, CSM for Resiliency has not yet been verified to work with ReadWriteMany or ReadOnlyMany volumes. Also, it has not been verified to work with pod controllers other than StatefulSet.\n  Recovering From Failures Normally CSM for Resiliency should be able to move pods that have been impacted by Node Failures to a healthy node. After the failed nodes have come back online, CSM for Resiliency cleans them up (especially any potential zombie pods) and then automatically removes the CSM for Resiliency node taint that prevents pods from being scheduled to the failed node(s). There are a few cases where this cannot be fully automated and operator intervention is required, including:\n  CSM for Resiliency expects that when a node failure occurs, all CSM for Resiliency labeled pods are evacuated and rescheduled on other nodes. This process may not complete however if the node comes back online before CSM for Resiliency has had time to evacuate all the labeled pods. The remaining pods may not restart correctly, going to “Error” or “CrashLoopBackoff”. We are considering some possible remediation for this condition but have not implemented them yet.\nIf this happens, try deleting the pod with “kubectl delete pod …”. In our experience this normally will cause the pod to be restarted and transition to the “Running” state.\n  Podmon-node is responsible for cleaning up failed nodes after the nodes’ communication has been restored. The algorithm checks to see that all the monitored pods have terminated and their volumes and mounts have been cleaned up.\nIf some of the monitored pods are still executing, node-podmon will emit the following log message at the end of a cleanup cycle (and retry the cleanup after a delay):\npods skipped for cleanup because still present: \u003cpod-list\u003e If this happens, DO NOT manually remove the CSM for Resiliency node taint. Doing so could possibly cause data corruption if volumes were not cleaned up, and a pod using those volumes was subsequently scheduled to that node.\nThe correct course of action in this case is to reboot the failed node(s) that have not removed their taints in a reasonable time (5-10 minutes after the node is online again.) The operator can delay executing this reboot until it is convenient, but new pods will not be scheduled to it in the interim. This reboot will cancel any potential zombie pods. After the reboot, node-podmon should automatically remove the node taint after a short time.\n  Testing Methodology and Results A three tier testing methodology is used for CSM for Resiliency:\n Unit testing with high coverage (\u003e90% statement) tests the program logic and is especially used to test the error paths by injecting faults. An integration test describes test scenarios in Gherkin that sets up specific testing scenarios executed against a Kubernetes test cluster. The tests use ranges for many of the parameters to add an element of “chaos testing”. Script based testing supports longevity testing in a Kubernetes cluster. For example, one test repeatedly fails three different lists of nodes in succession and is used to fail 1/3 of the cluster’s worker nodes on a cyclic basis and repeat indefinitely. This test collect statistics on length of time for pod evacuation, pod recovery, and node cleanup.  ","excerpt":"Container Storage Modules (CSM) for Resiliency is part of the …","ref":"/csm-docs/v3/resiliency/","title":"Resiliency"},{"body":"Enabling Replication in CSI PowerScale Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerScale supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Ensure that SyncIQ service is enabled on both arrays, you can do that by navigating to SyncIQ section under Data protection tab.\nThe current implementation supports one-to-one replication so you need to ensure that one array can reach another and vice versa.\nSyncIQ encryption If you wish to use SyncIQ encryption you should ensure that you’ve added a server certificate first by navigating to Data protection-\u003eSyncIQ-\u003eSettings.\nAfter adding the certificate, you can choose to use it by checking Encrypt SyncIQ connection from the dropdown.\nAfter that, you can add similar certificates of other arrays in SyncIQ-\u003e Certificates, and ensure you’ve added the certificate of the array you want to replicate to.\nSimilar steps should be done in the reverse direction, so array-1 has the array-2 certificate visible in SyncIQ-\u003e Certificates tab and array-2 has the array-1 certificate visible in its own SyncIQ-\u003eCertificates tab.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\n Check controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING\n Check that controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs.\n  If you don’t have something installed or something is out-of-place, please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled, you need to ensure you have set helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-isilon-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like:\n...# controller: configure controller specific parameterscontroller:...# replication: allows to configure replicationreplication:enabled:trueimage:dellemc/dell-csi-replicator:v1.2.0replicationContextPrefix:\"powerscale\"replicationPrefix:\"replication.storage.dell.com\"...You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module, you can continue to install the CSI driver for PowerScale following the usual installation procedure. Just ensure you’ve added the necessary array connection information to secret.\nSyncIQ encryption If you plan to use encryption, you need to set replicationCertificateID in the array connection secret. To check the ID of the certificate for the cluster, you can navigate to Data protection-\u003eSyncIQ-\u003eSettings, find your certificate in the Server Certificates section and then push the View/Edit button. It will open a dialog that should contain the Id field. Use the value of that field to set replicationCertificateID.\n NOTE: you need to install your driver on ALL clusters where you want to use replication. Both arrays must be accessible from each cluster.\n Creating Storage Classes To provision replicated volumes, you need to create adequately configured storage classes on both the source and target clusters.\nA pair of storage classes on the source, and target clusters would be essentially mirrored copies of one another. You can create them manually or with the help of repctl.\nManual Storage Class Creation You can find a sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-replicationprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"isilon-replication\"replication.storage.dell.com/remoteClusterID:\"target\"replication.storage.dell.com/remoteSystem:\"cluster-2\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"AccessZone:SystemIsiPath:/ifs/data/csiRootClientEnabled:\"false\"ClusterName:cluster-1Let’s go through each parameter and what it means:\n replication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents the ID of a remote cluster. It is the same id you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system that should match whatever clusterName you called it in isilon-creds secret. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure.   NOTE: Available RPO values “Five_Minutes”, “Fifteen_Minutes”, “Thirty_Minutes”, “One_Hour”, “Six_Hours”, “Twelve_Hours”, “One_Day”\n  replication.storage.dell.com/ignoreNamespaces, if set to true PowerScale driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. Accesszone is the name of the access zone a volume can be created in IsiPath is the base path for the volumes to be created on the PowerScale cluster RootClientEnabled determines whether the driver should enable root squashing or not ClusterName name of PowerScale cluster, where PV will be provisioned, specified as it was listed in isilon-creds secret.  After figuring out how storage classes would look, you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class creation with repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example here, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from manual installation and see what config would look like:\nsourceClusterID:\"source\"targetClusterID:\"target\"name:\"isilon-replication\"driver:\"isilon\"reclaimPolicy:\"Delete\"replicationPrefix:\"replication.storage.dell.com\"parameters:rpo:\"Five_Minutes\"ignoreNamespaces:\"false\"volumeGroupPrefix:\"csi\"accessZone:\"System\"isiPath:\"/ifs/data/csi\"rootClientEnabled:\"false\"clusterName:source:\"cluster-1\"target:\"cluster-2\" NOTE: both storage classes expected to use access zone with same name\n After preparing the config, you can apply it to both clusters with repctl. Before you do this, ensure you’ve added your clusters to repctl via the add command.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes would be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes, you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication-enabled Storage Classes. The CSI PowerScale driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerScale driver supports the following list of replication actions:\n FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC  ","excerpt":"Enabling Replication in CSI PowerScale Container Storage Modules (CSM) …","ref":"/csm-docs/docs/replication/deployment/powerscale/","title":"PowerScale"},{"body":"Enabling Replication in CSI PowerScale Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerScale supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Ensure that SyncIQ service is enabled on both arrays, you can do that by navigating to SyncIQ section under Data protection tab.\nThe current implementation supports one-to-one replication so you need to ensure that one array can reach another and vice versa.\nSyncIQ encryption If you wish to use SyncIQ encryption you should ensure that you’ve added a server certificate first by navigating to Data protection-\u003eSyncIQ-\u003eSettings.\nAfter adding the certificate, you can choose to use it by checking Encrypt SyncIQ connection from the dropdown.\nAfter that, you can add similar certificates of other arrays in SyncIQ-\u003e Certificates, and ensure you’ve added the certificate of the array you want to replicate to.\nSimilar steps should be done in the reverse direction, so array-1 has the array-2 certificate visible in SyncIQ-\u003e Certificates tab and array-2 has the array-1 certificate visible in its own SyncIQ-\u003eCertificates tab.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\n Check controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING\n Check that controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs.\n  If you don’t have something installed or something is out-of-place, please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled, you need to ensure you have set helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-isilon-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like:\n...# controller: configure controller specific parameterscontroller:...# replication: allows to configure replicationreplication:enabled:trueimage:dellemc/dell-csi-replicator:v1.2.0replicationContextPrefix:\"powerscale\"replicationPrefix:\"replication.storage.dell.com\"...You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module, you can continue to install the CSI driver for PowerScale following the usual installation procedure. Just ensure you’ve added the necessary array connection information to secret.\nSyncIQ encryption If you plan to use encryption, you need to set replicationCertificateID in the array connection secret. To check the ID of the certificate for the cluster, you can navigate to Data protection-\u003eSyncIQ-\u003eSettings, find your certificate in the Server Certificates section and then push the View/Edit button. It will open a dialog that should contain the Id field. Use the value of that field to set replicationCertificateID.\n NOTE: you need to install your driver on ALL clusters where you want to use replication. Both arrays must be accessible from each cluster.\n Creating Storage Classes To provision replicated volumes, you need to create adequately configured storage classes on both the source and target clusters.\nA pair of storage classes on the source, and target clusters would be essentially mirrored copies of one another. You can create them manually or with the help of repctl.\nManual Storage Class Creation You can find a sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-replicationprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"isilon-replication\"replication.storage.dell.com/remoteClusterID:\"target\"replication.storage.dell.com/remoteSystem:\"cluster-2\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"AccessZone:SystemIsiPath:/ifs/data/csiRootClientEnabled:\"false\"ClusterName:cluster-1Let’s go through each parameter and what it means:\n replication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents the ID of a remote cluster. It is the same id you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system that should match whatever clusterName you called it in isilon-creds secret. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure.   NOTE: Available RPO values “Five_Minutes”, “Fifteen_Minutes”, “Thirty_Minutes”, “One_Hour”, “Six_Hours”, “Twelve_Hours”, “One_Day”\n  replication.storage.dell.com/ignoreNamespaces, if set to true PowerScale driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. Accesszone is the name of the access zone a volume can be created in IsiPath is the base path for the volumes to be created on the PowerScale cluster RootClientEnabled determines whether the driver should enable root squashing or not ClusterName name of PowerScale cluster, where PV will be provisioned, specified as it was listed in isilon-creds secret.  After figuring out how storage classes would look, you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class creation with repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example here, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from manual installation and see what config would look like:\nsourceClusterID:\"source\"targetClusterID:\"target\"name:\"isilon-replication\"driver:\"isilon\"reclaimPolicy:\"Delete\"replicationPrefix:\"replication.storage.dell.com\"parameters:rpo:\"Five_Minutes\"ignoreNamespaces:\"false\"volumeGroupPrefix:\"csi\"accessZone:\"System\"isiPath:\"/ifs/data/csi\"rootClientEnabled:\"false\"clusterName:source:\"cluster-1\"target:\"cluster-2\" NOTE: both storage classes expected to use access zone with same name\n After preparing the config, you can apply it to both clusters with repctl. Before you do this, ensure you’ve added your clusters to repctl via the add command.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes would be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes, you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication-enabled Storage Classes. The CSI PowerScale driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerScale driver supports the following list of replication actions:\n FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC  ","excerpt":"Enabling Replication in CSI PowerScale Container Storage Modules (CSM) …","ref":"/csm-docs/v1/replication/deployment/powerscale/","title":"PowerScale"},{"body":"Enabling Replication in CSI PowerScale Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerScale supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Ensure that SyncIQ service is enabled on both arrays, you can do that by navigating to SyncIQ section under Data protection tab.\nThe current implementation supports one-to-one replication so you need to ensure that one array can reach another and vice versa.\nSyncIQ encryption If you wish to use SyncIQ encryption you should ensure that you’ve added a server certificate first by navigating to Data protection-\u003eSyncIQ-\u003eSettings.\nAfter adding the certificate, you can choose to use it by checking Encrypt SyncIQ connection from the dropdown.\nAfter that, you can add similar certificates of other arrays in SyncIQ-\u003e Certificates, and ensure you’ve added the certificate of the array you want to replicate to.\nSimilar steps should be done in the reverse direction, so array-1 has the array-2 certificate visible in SyncIQ-\u003e Certificates tab and array-2 has the array-1 certificate visible in its own SyncIQ-\u003eCertificates tab.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\n Check controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING\n Check that controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster-id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by a list of target clusters IDs.\n  If you don’t have something installed or something is out-of-place, please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled, you need to ensure you have set helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-isilon-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like:\n...# controller: configure controller specific parameterscontroller:...# replication: allows to configure replicationreplication:enabled:trueimage:dellemc/dell-csi-replicator:v1.2.0replicationContextPrefix:\"powerscale\"replicationPrefix:\"replication.storage.dell.com\"...You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module, you can continue to install the CSI driver for PowerScale following the usual installation procedure. Just ensure you’ve added the necessary array connection information to secret.\nSyncIQ encryption If you plan to use encryption, you need to set replicationCertificateID in the array connection secret. To check the ID of the certificate for the cluster, you can navigate to Data protection-\u003eSyncIQ-\u003eSettings, find your certificate in the Server Certificates section and then push the View/Edit button. It will open a dialog that should contain the Id field. Use the value of that field to set replicationCertificateID.\n NOTE: you need to install your driver on ALL clusters where you want to use replication. Both arrays must be accessible from each cluster.\n Creating Storage Classes To provision replicated volumes, you need to create adequately configured storage classes on both the source and target clusters.\nA pair of storage classes on the source, and target clusters would be essentially mirrored copies of one another. You can create them manually or with the help of repctl.\nManual Storage Class Creation You can find a sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-replicationprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"isilon-replication\"replication.storage.dell.com/remoteClusterID:\"target\"replication.storage.dell.com/remoteSystem:\"cluster-2\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"AccessZone:SystemIsiPath:/ifs/data/csiRootClientEnabled:\"false\"ClusterName:cluster-1Let’s go through each parameter and what it means:\n replication.storage.dell.com/isReplicationEnabled if set to true, will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents the ID of a remote cluster. It is the same id you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system that should match whatever clusterName you called it in isilon-creds secret. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure.   NOTE: Available RPO values “Five_Minutes”, “Fifteen_Minutes”, “Thirty_Minutes”, “One_Hour”, “Six_Hours”, “Twelve_Hours”, “One_Day”\n  replication.storage.dell.com/ignoreNamespaces, if set to true PowerScale driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. Accesszone is the name of the access zone a volume can be created in IsiPath is the base path for the volumes to be created on the PowerScale cluster RootClientEnabled determines whether the driver should enable root squashing or not ClusterName name of PowerScale cluster, where PV will be provisioned, specified as it was listed in isilon-creds secret.  After figuring out how storage classes would look, you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class creation with repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example here, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from manual installation and see what config would look like:\nsourceClusterID:\"source\"targetClusterID:\"target\"name:\"isilon-replication\"driver:\"isilon\"reclaimPolicy:\"Delete\"replicationPrefix:\"replication.storage.dell.com\"parameters:rpo:\"Five_Minutes\"ignoreNamespaces:\"false\"volumeGroupPrefix:\"csi\"accessZone:\"System\"isiPath:\"/ifs/data/csi\"rootClientEnabled:\"false\"clusterName:source:\"cluster-1\"target:\"cluster-2\" NOTE: both storage classes expected to use access zone with same name\n After preparing the config, you can apply it to both clusters with repctl. Before you do this, ensure you’ve added your clusters to repctl via the add command.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes would be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes, you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication-enabled Storage Classes. The CSI PowerScale driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerScale driver supports the following list of replication actions:\n FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC  ","excerpt":"Enabling Replication in CSI PowerScale Container Storage Modules (CSM) …","ref":"/csm-docs/v2/replication/deployment/powerscale/","title":"PowerScale"},{"body":"Enabling Replication In CSI PowerStore Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerStore supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Be sure to configure replication between multiple PowerStore instances using instructions provided by PowerStore storage.\nYou can ensure that you configured remote systems by navigating to the Protection tab and choosing Remote System in UI of your PowerStore instance.\nYou should see a list of remote systems with both Management State and Data Connection fields set to OK.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\n Check controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING\n Check that controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by list of target clusters IDs.\n  If you don’t have something installed or something is out-of-place, please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-powerstore-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like\n...# controller: configure controller specific parameterscontroller:...# replication: allows to configure replicationreplication:enabled:trueimage:dellemc/dell-csi-replicator:v1.0.0replicationContextPrefix:\"powerstore\"replicationPrefix:\"replication.storage.dell.com\"...You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerStore following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\n NOTE: you need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\n Creating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nA pair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:\"powerstore-replication\"provisioner:\"csi-powerstore.dellemc.com\"reclaimPolicy:RetainvolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"powerstore-replication\"replication.storage.dell.com/remoteClusterID:\"tgt-cluster-id\"replication.storage.dell.com/remoteSystem:\"RT-0000\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"arrayID:\"Unique\"Let’s go through each parameter and what it means:\n replication.storage.dell.com/isReplicationEnabled if set to true will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents ID of a remote cluster. It is the same id you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system as seen from the current PowerStore instance. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. replication.storage.dell.com/ignoreNamespaces, if set to true PowerStore driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. arrayID is a unique identifier of the storage array you specified in array connection secret.  Let’s follow up that with an example. Let’s assume you have two Kubernetes clusters and two PowerStore storage arrays:\n Clusters have IDs of cluster-1 and cluster-2 Storage arrays connected between each other and show up as remote systems with names RT-0001 and RT-0002 Cluster cluster-1 connected to array RT-0001 Cluster cluster-2 connected to array RT-0002 Storage array RT-0001 has a unique ID of PS000000001 Storage array RT-0002 has a unique ID of PS000000002  And this is what our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:\"powerstore-replication\"provisioner:\"csi-powerstore.dellemc.com\"reclaimPolicy:RetainvolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"powerstore-replication\"replication.storage.dell.com/remoteClusterID:\"cluster-2\"replication.storage.dell.com/remoteSystem:\"RT-0002\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"arrayID:\"PS000000001\"StorageClass to be created in cluster-2:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:\"powerstore-replication\"provisioner:\"csi-powerstore.dellemc.com\"reclaimPolicy:RetainvolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"powerstore-replication\"replication.storage.dell.com/remoteClusterID:\"cluster-1\"replication.storage.dell.com/remoteSystem:\"RT-0001\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"arrayID:\"PS000000002\"After figuring out how storage classes would look, you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example in here, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from manual installation and see how config would look like\nsourceClusterID:\"cluster-1\"targetClusterID:\"cluster-2\"name:\"powerstore-replication\"driver:\"powerstore\"reclaimPolicy:\"Retain\"replicationPrefix:\"replication.storage.dell.com\"parameters:arrayID:source:\"PS000000001\"target:\"PS000000002\"remoteSystem:source:\"RT-0002\"target:\"RT-0001\"rpo:\"Five_Minutes\"ignoreNamespaces:\"false\"volumeGroupPrefix:\"csi\"After preparing the config you can apply it to both clusters with repctl. Just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes would be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerStore driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerStore driver supports the following list of replication actions:\n FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC  ","excerpt":"Enabling Replication In CSI PowerStore Container Storage Modules (CSM) …","ref":"/csm-docs/docs/replication/deployment/powerstore/","title":"PowerStore"},{"body":"Enabling Replication In CSI PowerStore Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerStore supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Be sure to configure replication between multiple PowerStore instances using instructions provided by PowerStore storage.\nYou can ensure that you configured remote systems by navigating to the Protection tab and choosing Remote System in UI of your PowerStore instance.\nYou should see a list of remote systems with both Management State and Data Connection fields set to OK.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\n Check controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING\n Check that controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by list of target clusters IDs.\n  If you don’t have something installed or something is out-of-place, please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-powerstore-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like\n...# controller: configure controller specific parameterscontroller:...# replication: allows to configure replicationreplication:enabled:trueimage:dellemc/dell-csi-replicator:v1.0.0replicationContextPrefix:\"powerstore\"replicationPrefix:\"replication.storage.dell.com\"...You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerStore following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\n NOTE: you need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\n Creating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nA pair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:\"powerstore-replication\"provisioner:\"csi-powerstore.dellemc.com\"reclaimPolicy:RetainvolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"powerstore-replication\"replication.storage.dell.com/remoteClusterID:\"tgt-cluster-id\"replication.storage.dell.com/remoteSystem:\"RT-0000\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"arrayID:\"Unique\"Let’s go through each parameter and what it means:\n replication.storage.dell.com/isReplicationEnabled if set to true will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents ID of a remote cluster. It is the same id you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system as seen from the current PowerStore instance. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. replication.storage.dell.com/ignoreNamespaces, if set to true PowerStore driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. arrayID is a unique identifier of the storage array you specified in array connection secret.  Let’s follow up that with an example. Let’s assume you have two Kubernetes clusters and two PowerStore storage arrays:\n Clusters have IDs of cluster-1 and cluster-2 Storage arrays connected between each other and show up as remote systems with names RT-0001 and RT-0002 Cluster cluster-1 connected to array RT-0001 Cluster cluster-2 connected to array RT-0002 Storage array RT-0001 has a unique ID of PS000000001 Storage array RT-0002 has a unique ID of PS000000002  And this is what our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:\"powerstore-replication\"provisioner:\"csi-powerstore.dellemc.com\"reclaimPolicy:RetainvolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"powerstore-replication\"replication.storage.dell.com/remoteClusterID:\"cluster-2\"replication.storage.dell.com/remoteSystem:\"RT-0002\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"arrayID:\"PS000000001\"StorageClass to be created in cluster-2:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:\"powerstore-replication\"provisioner:\"csi-powerstore.dellemc.com\"reclaimPolicy:RetainvolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"powerstore-replication\"replication.storage.dell.com/remoteClusterID:\"cluster-1\"replication.storage.dell.com/remoteSystem:\"RT-0001\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"arrayID:\"PS000000002\"After figuring out how storage classes would look, you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example in here, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from manual installation and see how config would look like\nsourceClusterID:\"cluster-1\"targetClusterID:\"cluster-2\"name:\"powerstore-replication\"driver:\"powerstore\"reclaimPolicy:\"Retain\"replicationPrefix:\"replication.storage.dell.com\"parameters:arrayID:source:\"PS000000001\"target:\"PS000000002\"remoteSystem:source:\"RT-0002\"target:\"RT-0001\"rpo:\"Five_Minutes\"ignoreNamespaces:\"false\"volumeGroupPrefix:\"csi\"After preparing the config you can apply it to both clusters with repctl. Just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes would be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerStore driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerStore driver supports the following list of replication actions:\n FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC  ","excerpt":"Enabling Replication In CSI PowerStore Container Storage Modules (CSM) …","ref":"/csm-docs/v1/replication/deployment/powerstore/","title":"PowerStore"},{"body":"Enabling Replication In CSI PowerStore Container Storage Modules (CSM) Replication sidecar is a helper container that is installed alongside a CSI driver to facilitate replication functionality. Such CSI drivers must implement dell-csi-extensions calls.\nCSI driver for Dell PowerStore supports necessary extension calls from dell-csi-extensions. To be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Be sure to configure replication between multiple PowerStore instances using instructions provided by PowerStore storage.\nYou can ensure that you configured remote systems by navigating to the Protection tab and choosing Remote System in UI of your PowerStore instance.\nYou should see a list of remote systems with both Management State and Data Connection fields set to OK.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\n Check controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING\n Check that controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by list of target clusters IDs.\n  If you don’t have something installed or something is out-of-place, please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-powerstore-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like\n...# controller: configure controller specific parameterscontroller:...# replication: allows to configure replicationreplication:enabled:trueimage:dellemc/dell-csi-replicator:v1.0.0replicationContextPrefix:\"powerstore\"replicationPrefix:\"replication.storage.dell.com\"...You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerStore following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\n NOTE: you need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\n Creating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nA pair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository here.\nIt will look like this:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:\"powerstore-replication\"provisioner:\"csi-powerstore.dellemc.com\"reclaimPolicy:RetainvolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"powerstore-replication\"replication.storage.dell.com/remoteClusterID:\"tgt-cluster-id\"replication.storage.dell.com/remoteSystem:\"RT-0000\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"arrayID:\"Unique\"Let’s go through each parameter and what it means:\n replication.storage.dell.com/isReplicationEnabled if set to true will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents ID of a remote cluster. It is the same id you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system as seen from the current PowerStore instance. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. replication.storage.dell.com/ignoreNamespaces, if set to true PowerStore driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. arrayID is a unique identifier of the storage array you specified in array connection secret.  Let’s follow up that with an example. Let’s assume you have two Kubernetes clusters and two PowerStore storage arrays:\n Clusters have IDs of cluster-1 and cluster-2 Storage arrays connected between each other and show up as remote systems with names RT-0001 and RT-0002 Cluster cluster-1 connected to array RT-0001 Cluster cluster-2 connected to array RT-0002 Storage array RT-0001 has a unique ID of PS000000001 Storage array RT-0002 has a unique ID of PS000000002  And this is what our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:\"powerstore-replication\"provisioner:\"csi-powerstore.dellemc.com\"reclaimPolicy:RetainvolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"powerstore-replication\"replication.storage.dell.com/remoteClusterID:\"cluster-2\"replication.storage.dell.com/remoteSystem:\"RT-0002\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"arrayID:\"PS000000001\"StorageClass to be created in cluster-2:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:\"powerstore-replication\"provisioner:\"csi-powerstore.dellemc.com\"reclaimPolicy:RetainvolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"powerstore-replication\"replication.storage.dell.com/remoteClusterID:\"cluster-1\"replication.storage.dell.com/remoteSystem:\"RT-0001\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"arrayID:\"PS000000002\"After figuring out how storage classes would look, you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example in here, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from manual installation and see how config would look like\nsourceClusterID:\"cluster-1\"targetClusterID:\"cluster-2\"name:\"powerstore-replication\"driver:\"powerstore\"reclaimPolicy:\"Retain\"replicationPrefix:\"replication.storage.dell.com\"parameters:arrayID:source:\"PS000000001\"target:\"PS000000002\"remoteSystem:source:\"RT-0002\"target:\"RT-0001\"rpo:\"Five_Minutes\"ignoreNamespaces:\"false\"volumeGroupPrefix:\"csi\"After preparing the config you can apply it to both clusters with repctl. Just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes would be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl get storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerStore driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerStore driver supports the following list of replication actions:\n FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC  ","excerpt":"Enabling Replication In CSI PowerStore Container Storage Modules (CSM) …","ref":"/csm-docs/v2/replication/deployment/powerstore/","title":"PowerStore"},{"body":"Enabling Replication In CSI PowerStore For the Container Storage Modules (CSM) for Replication sidecar container to work properly it needs to be installed alongside CSI driver that supports replication dell-csi-extensions calls.\nCSI driver for Dell EMC PowerStore supports necessary extension calls from dell-csi-extensions and to be able to provision replicated volumes you would need to do the steps described in the following sections.\nBefore Installation On Storage Array Be sure to configure replication between multiple PowerStore instances using instructions provided by PowerStore storage.\nYou can ensure that you configured remote systems by navigating to the Protection tab and choosing Remote System in UI of your PowerStore instance.\nYou should see a list of remote systems with both Management State and Data Connection fields set to OK.\nIn Kubernetes Ensure you installed CRDs and replication controller in your clusters.\nTo verify you have everything in order you can execute the following commands:\n Check controller pods kubectl get pods -n dell-replication-controller Pods should be READY and RUNNING\n Check that controller config map is properly populated kubectl get cm -n dell-replication-controller dell-replication-controller-config -o yaml data field should be properly populated with cluster id of your choosing and, if using multi-cluster installation, your targets: parameter should be populated by list of target clusters IDs.\n  If you don’t have something installed or something is out-of-place, please refer to installation instructions in installation-repctl or installation.\nInstalling Driver With Replication Module To install the driver with replication enabled you need to ensure you have set helm parameter controller.replication.enabled in your copy of example values.yaml file (usually called my-powerstore-settings.yaml, myvalues.yaml etc.).\nHere is an example of what that would look like\n...# controller: configure controller specific parameterscontroller:...# replication: allows to configure replicationreplication:enabled:trueimage:dellemc/dell-csi-replicator:v1.0.0replicationContextPrefix:\"powerstore\"replicationPrefix:\"replication.storage.dell.com\"...You can leave other parameters like image, replicationContextPrefix, and replicationPrefix as they are.\nAfter enabling the replication module you can continue to install the CSI driver for PowerStore following usual installation procedure, just ensure you’ve added necessary array connection information to secret.\n NOTE: you need to install your driver at least on the source cluster, but it is recommended to install drivers on all clusters you will use for replication.\n Creating Storage Classes To be able to provision replicated volumes you need to create properly configured storage classes on both source and target clusters.\nA pair of storage classes on the source and target clusters would be essentially mirrored copies of one another. You can create them manually or with help from repctl.\nManual Storage Class Creation You can find sample replication enabled storage class in the driver repository at ./samples/storageclass/powerstore-replication.yaml.\nIt will look like this:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:\"powerstore-replication\"provisioner:\"csi-powerstore.dellemc.com\"reclaimPolicy:RetainvolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"powerstore-replication\"replication.storage.dell.com/remoteClusterID:\"tgt-cluster-id\"replication.storage.dell.com/remoteSystem:\"RT-0000\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"arrayID:\"Unique\"Let’s go through each parameter and what it means:\n replication.storage.dell.com/isReplicationEnabled if set to true will mark this storage class as replication enabled, just leave it as true. replication.storage.dell.com/remoteStorageClassName points to the name of the remote storage class. If you are using replication with the multi-cluster configuration you can make it the same as the current storage class name. replication.storage.dell.com/remoteClusterID represents ID of a remote cluster. It is the same id you put in the replication controller config map. replication.storage.dell.com/remoteSystem is the name of the remote system as seen from the current PowerStore instance. replication.storage.dell.com/rpo is an acceptable amount of data, which is measured in units of time, that may be lost due to a failure. replication.storage.dell.com/ignoreNamespaces, if set to true PowerStore driver, it will ignore in what namespace volumes are created and put every volume created using this storage class into a single volume group. replication.storage.dell.com/volumeGroupPrefix represents what string would be appended to the volume group name to differentiate them. arrayID is a unique identifier of the storage array you specified in array connection secret.  Let’s follow up that with an example. Let’s assume you have two Kubernetes clusters and two PowerStore storage arrays:\n Clusters have IDs of cluster-1 and cluster-2 Storage arrays connected between each other and show up as remote systems with names RT-0001 and RT-0002 Cluster cluster-1 connected to array RT-0001 Cluster cluster-2 connected to array RT-0002 Storage array RT-0001 has a unique ID of PS000000001 Storage array RT-0002 has a unique ID of PS000000002  And this is what our pair of storage classes would look like:\nStorageClass to be created in cluster-1:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:\"powerstore-replication\"provisioner:\"csi-powerstore.dellemc.com\"reclaimPolicy:RetainvolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"powerstore-replication\"replication.storage.dell.com/remoteClusterID:\"cluster-2\"replication.storage.dell.com/remoteSystem:\"RT-0002\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"arrayID:\"PS000000001\"StorageClass to be created in cluster-2:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:\"powerstore-replication\"provisioner:\"csi-powerstore.dellemc.com\"reclaimPolicy:RetainvolumeBindingMode:Immediateparameters:replication.storage.dell.com/isReplicationEnabled:\"true\"replication.storage.dell.com/remoteStorageClassName:\"powerstore-replication\"replication.storage.dell.com/remoteClusterID:\"cluster-1\"replication.storage.dell.com/remoteSystem:\"RT-0001\"replication.storage.dell.com/rpo:Five_Minutesreplication.storage.dell.com/ignoreNamespaces:\"false\"replication.storage.dell.com/volumeGroupPrefix:\"csi\"arrayID:\"PS000000002\"After figuring out how storage classes would look, you just need to go and apply them to your Kubernetes clusters with kubectl.\nStorage Class Creation With repctl repctl can simplify storage class creation by creating a pair of mirrored storage classes in both clusters (using a single storage class configuration) in one command.\nTo create storage classes with repctl you need to fill up the config with necessary information. You can find an example in repctl/examples/powerstore_example_values.yaml, copy it, and modify it to your needs.\nIf you open this example you can see a lot of similar fields and parameters you can modify in the storage class.\nLet’s use the same example from manual installation and see how config would look like\nsourceClusterID:\"cluster-1\"targetClusterID:\"cluster-2\"name:\"powerstore-replication\"driver:\"powerstore\"reclaimPolicy:\"Retain\"replicationPrefix:\"replication.storage.dell.com\"parameters:arrayID:source:\"PS000000001\"target:\"PS000000002\"remoteSystem:source:\"RT-0002\"target:\"RT-0001\"rpo:\"Five_Minutes\"ignoreNamespaces:\"false\"volumeGroupPrefix:\"csi\"After preparing the config you can apply it to both clusters with repctl. Just make sure you’ve added your clusters to repctl via the add command before.\nTo create storage classes just run ./repctl create sc --from-config \u003cconfig-file\u003e and storage classes would be applied to both clusters.\nAfter creating storage classes you can make sure they are in place by using ./repctl list storageclasses command.\nProvisioning Replicated Volumes After installing the driver and creating storage classes you are good to create volumes using newly created storage classes.\nOn your source cluster, create a PersistentVolumeClaim using one of the replication enabled Storage Classes. The CSI PowerStore driver will create a volume on the array, add it to a VolumeGroup and configure replication using the parameters provided in the replication enabled Storage Class.\nSupported Replication Actions The CSI PowerStore driver supports the following list of replication actions:\n FAILOVER_REMOTE UNPLANNED_FAILOVER_LOCAL REPROTECT_LOCAL SUSPEND RESUME SYNC  ","excerpt":"Enabling Replication In CSI PowerStore For the Container Storage …","ref":"/csm-docs/v3/replication/deployment/powerstore/","title":"PowerStore"},{"body":"repctl repctl is a command-line client for configuring replication and managing replicated resources between multiple Kubernetes clusters.\nUsage Managing Clusters To begin managing replication with repctl you need to add your Kubernetes clusters, you can do that using cluster add command\n./repctl cluster add -f \u003cconfig-file\u003e -n \u003cname\u003e You can view clusters that are currently being managed by repctl by running cluster get command\n./repctl cluster get Or, alternatively, using get cluster command\n./repctl get cluster Also, you can inject information about all of your current clusters as config maps into the same clusters, so it can be used by dell-csi-replicator\n./repctl cluster inject You can also generate kubeconfigs from existing replication service accounts and inject them in config maps by providing --use-sa flag\n./repctl cluster inject --use-sa Querying Resources After adding clusters you want to manage with repctl you can query resources from multiple clusters at once using get command.\nFor example, this command will list all storage classes in all clusters that currently are being managed by repctl\n./repctl get storageclasses --all If you want to query some particular clusters you can do that by specifying clusters flag\n./repctl get pv --clusters cluster-1,cluster-3 All other different flags for querying resources you can check using included into the tool help flag -h.\nCreating Resources Generic Generic create command allows you to apply provided config file into multiple clusters at once\n/repctl create -f \u003cpath-to-file\u003e PersistentVolumeClaims You can use repctl to create PVCs from Replication Group’s PVs on the target cluster\n./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false  By default, ‘create pvc’ will do a ‘dry-run’ while creating PVCs. If you don’t encounter any issues in the dry-run, then you can re-run the command by turning off the dry-run flag to false.\n Storage Classes repctl can create special replication enabled storage classes from provided config, you can find example configs in examples folder\n./repctl create sc --from-config \u003cconfig-file\u003e` Single Cluster Replication repctl supports working with replication within a single Kubernetes cluster.\nJust add cluster you want to use with cluster add command, and you can list, filter, and create resources.\nVolumes and ReplicationGroups created as “target” resources would be prefixed with replicated- so you can easily differentiate them.\nYou can also differentiate between single cluster replication configured StorageClasses and ReplicationGroups and multi-cluster ones by checking remoteClusterID field, for a single cluster the field would be set to self.\nTo create replication enabled storage classes for single cluster replication using create sc command be sure to set both sourceClusterID and targetClusterID to the same clusterID and continue as usual with executing the command. Name of StorageClass resource that created as “target” will be appended with -tgt.\nExecuting Actions repctl can be used to execute various replication actions on ReplicationGroups.\nFailover This command will perform a planned failover to a cluster or an RG.\nWhen working with multiple clusters, you can perform failover by specifying the target cluster ID. To do that use --target \u003ctargetClusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failover by specifying the target replication group ID. To do that use --target \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-rg-id\u003e In both scenarios repctl will patch the CR at the source site with action FAILOVER_REMOTE.\nYou can also provide --unplanned parameter, then repctl will perform an unplanned failover to a given cluster or an RG, instead of FAILOVER_REMOTE repctl will patch CR at target cluster with action UNPLANNED_FAILOVER_LOCAL.\nReprotect This command will perform a reprotect at the specified cluster or the RG.\nWhen working with multiple clusters, you can perform reprotect by specifying the cluster ID. To do that use --at \u003cclusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e reprotect --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform reprotect by specifying the replication group ID. To do that use --rg \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e reprotect In both scenarios repctl will patch the CR at the source site with action REPROTECT_LOCAL.\nFailback This command will perform a planned failback to a cluster or an RG.\nWhen working with multiple clusters, you can perform failback by specifying the cluster ID, to do that use --target \u003cclusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failback by specifying the replication group ID. To do that use --target \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-rg-id\u003e In both scenarios repctl will patch the CR at the source site with action FAILBACK_LOCAL.\nYou can also provide --discard parameter, then repctl will perform a failback but discard any writes at target, instead of FAILBACK_LOCAL repctl will patch CR at target cluster with action ACTION_FAILBACK_DISCARD_CHANGES_LOCAL.\nSwap This command will perform a swap at a specified cluster or an RG.\nWhen working with multiple clusters, you can perform swap by specifying the cluster ID. To do that use --at \u003cclusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e swap --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform swap by specifying the replication group ID. To do that use --rg \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e swap repctl will patch CR at the source cluster with action SWAP_LOCAL.\nWait For Completion When executing actions you can provide --wait argument to make repctl wait for completion of specified action.\nFor example when executing failover:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e --wait Maintenance Actions You can also use exec command to execute maintenance actions such as suspend, resume, and sync.\nFor single or multi-cluster config:\n./repctl --rg \u003crg-id\u003e exec -a \u003cACTION\u003e Where \u003cACTION\u003e can be one of the following:\n suspend will suspend replication, changes will no longer be synced between replication sites resume will resume replication, canceling the effect of suspend action sync will force synchronization of change between replication sites  ","excerpt":"repctl repctl is a command-line client for configuring replication and …","ref":"/csm-docs/docs/replication/tools/","title":"Tools"},{"body":"repctl repctl is a command-line client for configuring replication and managing replicated resources between multiple Kubernetes clusters.\nUsage Managing Clusters To begin managing replication with repctl you need to add your Kubernetes clusters, you can do that using cluster add command\n./repctl cluster add -f \u003cconfig-file\u003e -n \u003cname\u003e You can view clusters that are currently being managed by repctl by running cluster get command\n./repctl cluster get Or, alternatively, using get cluster command\n./repctl get cluster Also, you can inject information about all of your current clusters as config maps into the same clusters, so it can be used by dell-csi-replicator\n./repctl cluster inject You can also generate kubeconfigs from existing replication service accounts and inject them in config maps by providing --use-sa flag\n./repctl cluster inject --use-sa Querying Resources After adding clusters you want to manage with repctl you can query resources from multiple clusters at once using get command.\nFor example, this command will list all storage classes in all clusters that currently are being managed by repctl\n./repctl get storageclasses --all If you want to query some particular clusters you can do that by specifying clusters flag\n./repctl get pv --clusters cluster-1,cluster-3 All other different flags for querying resources you can check using included into the tool help flag -h.\nCreating Resources Generic Generic create command allows you to apply provided config file into multiple clusters at once\n/repctl create -f \u003cpath-to-file\u003e PersistentVolumeClaims You can use repctl to create PVCs from Replication Group’s PVs on the target cluster\n./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false  By default, ‘create pvc’ will do a ‘dry-run’ while creating PVCs. If you don’t encounter any issues in the dry-run, then you can re-run the command by turning off the dry-run flag to false.\n Storage Classes repctl can create special replication enabled storage classes from provided config, you can find example configs in examples folder\n./repctl create sc --from-config \u003cconfig-file\u003e` Single Cluster Replication repctl supports working with replication within a single Kubernetes cluster.\nJust add cluster you want to use with cluster add command, and you can list, filter, and create resources.\nVolumes and ReplicationGroups created as “target” resources would be prefixed with replicated- so you can easily differentiate them.\nYou can also differentiate between single cluster replication configured StorageClasses and ReplicationGroups and multi-cluster ones by checking remoteClusterID field, for a single cluster the field would be set to self.\nTo create replication enabled storage classes for single cluster replication using create sc command be sure to set both sourceClusterID and targetClusterID to the same clusterID and continue as usual with executing the command. Name of StorageClass resource that created as “target” will be appended with -tgt.\nExecuting Actions repctl can be used to execute various replication actions on ReplicationGroups.\nFailover This command will perform a planned failover to a cluster or an RG.\nWhen working with multiple clusters, you can perform failover by specifying the target cluster ID. To do that use --target \u003ctargetClusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failover by specifying the target replication group ID. To do that use --target \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-rg-id\u003e In both scenarios repctl will patch the CR at the source site with action FAILOVER_REMOTE.\nYou can also provide --unplanned parameter, then repctl will perform an unplanned failover to a given cluster or an RG, instead of FAILOVER_REMOTE repctl will patch CR at target cluster with action UNPLANNED_FAILOVER_LOCAL.\nReprotect This command will perform a reprotect at the specified cluster or the RG.\nWhen working with multiple clusters, you can perform reprotect by specifying the cluster ID. To do that use --at \u003cclusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e reprotect --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform reprotect by specifying the replication group ID. To do that use --rg \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e reprotect In both scenarios repctl will patch the CR at the source site with action REPROTECT_LOCAL.\nFailback This command will perform a planned failback to a cluster or an RG.\nWhen working with multiple clusters, you can perform failback by specifying the cluster ID, to do that use --target \u003cclusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failback by specifying the replication group ID. To do that use --target \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-rg-id\u003e In both scenarios repctl will patch the CR at the source site with action FAILBACK_LOCAL.\nYou can also provide --discard parameter, then repctl will perform a failback but discard any writes at target, instead of FAILBACK_LOCAL repctl will patch CR at target cluster with action ACTION_FAILBACK_DISCARD_CHANGES_LOCAL.\nSwap This command will perform a swap at a specified cluster or an RG.\nWhen working with multiple clusters, you can perform swap by specifying the cluster ID. To do that use --at \u003cclusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e swap --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform swap by specifying the replication group ID. To do that use --rg \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e swap repctl will patch CR at the source cluster with action SWAP_LOCAL.\nWait For Completion When executing actions you can provide --wait argument to make repctl wait for completion of specified action.\nFor example when executing failover:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e --wait Maintenance Actions You can also use exec command to execute maintenance actions such as suspend, resume, and sync.\nFor single or multi-cluster config:\n./repctl --rg \u003crg-id\u003e exec -a \u003cACTION\u003e Where \u003cACTION\u003e can be one of the following:\n suspend will suspend replication, changes will no longer be synced between replication sites resume will resume replication, canceling the effect of suspend action sync will force synchronization of change between replication sites  ","excerpt":"repctl repctl is a command-line client for configuring replication and …","ref":"/csm-docs/v1/replication/tools/","title":"Tools"},{"body":"repctl repctl is a command-line client for configuring replication and managing replicated resources between multiple Kubernetes clusters.\nUsage Managing Clusters To begin managing replication with repctl you need to add your Kubernetes clusters, you can do that using cluster add command\n./repctl cluster add -f \u003cconfig-file\u003e -n \u003cname\u003e You can view clusters that are currently being managed by repctl by running cluster get command\n./repctl cluster get Or, alternatively, using get cluster command\n./repctl get cluster Also, you can inject information about all of your current clusters as config maps into the same clusters, so it can be used by dell-csi-replicator\n./repctl cluster inject You can also generate kubeconfigs from existing replication service accounts and inject them in config maps by providing --use-sa flag\n./repctl cluster inject --use-sa Querying Resources After adding clusters you want to manage with repctl you can query resources from multiple clusters at once using get command.\nFor example, this command will list all storage classes in all clusters that currently are being managed by repctl\n./repctl get storageclasses --all If you want to query some particular clusters you can do that by specifying clusters flag\n./repctl get pv --clusters cluster-1,cluster-3 All other different flags for querying resources you can check using included into the tool help flag -h.\nCreating Resources Generic Generic create command allows you to apply provided config file into multiple clusters at once\n/repctl create -f \u003cpath-to-file\u003e PersistentVolumeClaims You can use repctl to create PVCs from Replication Group’s PVs on the target cluster\n./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false  By default, ‘create pvc’ will do a ‘dry-run’ while creating PVCs. If you don’t encounter any issues in the dry-run, then you can re-run the command by turning off the dry-run flag to false.\n Storage Classes repctl can create special replication enabled storage classes from provided config, you can find example configs in examples folder\n./repctl create sc --from-config \u003cconfig-file\u003e` Single Cluster Replication repctl supports working with replication within a single Kubernetes cluster.\nJust add cluster you want to use with cluster add command, and you can list, filter, and create resources.\nVolumes and ReplicationGroups created as “target” resources would be prefixed with replicated- so you can easily differentiate them.\nYou can also differentiate between single cluster replication configured StorageClasses and ReplicationGroups and multi-cluster ones by checking remoteClusterID field, for a single cluster the field would be set to self.\nTo create replication enabled storage classes for single cluster replication using create sc command be sure to set both sourceClusterID and targetClusterID to the same clusterID and continue as usual with executing the command. Name of StorageClass resource that created as “target” will be appended with -tgt.\nExecuting Actions repctl can be used to execute various replication actions on ReplicationGroups.\nFailover This command will perform a planned failover to a cluster or an RG.\nWhen working with multiple clusters, you can perform failover by specifying the target cluster ID. To do that use --target \u003ctargetClusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failover by specifying the target replication group ID. To do that use --target \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-rg-id\u003e In both scenarios repctl will patch the CR at the source site with action FAILOVER_REMOTE.\nYou can also provide --unplanned parameter, then repctl will perform an unplanned failover to a given cluster or an RG, instead of FAILOVER_REMOTE repctl will patch CR at target cluster with action UNPLANNED_FAILOVER_LOCAL.\nReprotect This command will perform a reprotect at the specified cluster or the RG.\nWhen working with multiple clusters, you can perform reprotect by specifying the cluster ID. To do that use --at \u003cclusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e reprotect --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform reprotect by specifying the replication group ID. To do that use --rg \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e reprotect In both scenarios repctl will patch the CR at the source site with action REPROTECT_LOCAL.\nFailback This command will perform a planned failback to a cluster or an RG.\nWhen working with multiple clusters, you can perform failback by specifying the cluster ID, to do that use --target \u003cclusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failback by specifying the replication group ID. To do that use --target \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-rg-id\u003e In both scenarios repctl will patch the CR at the source site with action FAILBACK_LOCAL.\nYou can also provide --discard parameter, then repctl will perform a failback but discard any writes at target, instead of FAILBACK_LOCAL repctl will patch CR at target cluster with action ACTION_FAILBACK_DISCARD_CHANGES_LOCAL.\nSwap This command will perform a swap at a specified cluster or an RG.\nWhen working with multiple clusters, you can perform swap by specifying the cluster ID. To do that use --at \u003cclusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e swap --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform swap by specifying the replication group ID. To do that use --rg \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e swap repctl will patch CR at the source cluster with action SWAP_LOCAL.\nWait For Completion When executing actions you can provide --wait argument to make repctl wait for completion of specified action.\nFor example when executing failover:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e --wait Maintenance Actions You can also use exec command to execute maintenance actions such as suspend, resume, and sync.\nFor single or multi-cluster config:\n./repctl --rg \u003crg-id\u003e exec -a \u003cACTION\u003e Where \u003cACTION\u003e can be one of the following:\n suspend will suspend replication, changes will no longer be synced between replication sites resume will resume replication, canceling the effect of suspend action sync will force synchronization of change between replication sites  ","excerpt":"repctl repctl is a command-line client for configuring replication and …","ref":"/csm-docs/v2/replication/tools/","title":"Tools"},{"body":"repctl repctl is a command-line client for configuring replication and managing replicated resources between multiple Kubernetes clusters.\nUsage Managing Clusters To begin managing replication with repctl you need to add your Kubernetes clusters, you can do that using cluster add command\n./repctl cluster add -f \u003cconfig-file\u003e -n \u003cname\u003e You can view clusters that are currently being managed by repctl by running cluster get command\n./repctl cluster get Or, alternatively, using get cluster command\n./repctl get cluster Also, you can inject information about all of your current clusters as config maps into the same clusters, so it can be used by dell-csi-replicator\n./repctl cluster inject You can also generate kubeconfigs from existing replication service accounts and inject them in config maps by providing --use-sa flag\n./repctl cluster inject --use-sa Querying Resources After adding clusters you want to manage with repctl you can query resources from multiple clusters at once using get command.\nFor example, this command will list all storage classes in all clusters that currently are being managed by repctl\n./repctl get storageclasses --all If you want to query some particular clusters you can do that by specifying clusters flag\n./repctl get pv --clusters cluster-1,cluster-3 All other different flags for querying resources you can check using included into the tool help flag -h.\nCreating Resources Generic Generic create command allows you to apply provided config file into multiple clusters at once\n/repctl create -f \u003cpath-to-file\u003e PersistentVolumeClaims You can use repctl to create PVCs from Replication Group’s PVs on the target cluster\n./repctl create pvc --rg \u003crg-name\u003e -t \u003ctarget-namespace\u003e --dry-run=false  By default, ‘create pvc’ will do a ‘dry-run’ while creating PVCs. If you don’t encounter any issues in the dry-run, then you can re-run the command by turning off the dry-run flag to false.\n Storage Classes repctl can create special replication enabled storage classes from provided config, you can find example configs in examples folder\n./repctl create sc --from-config \u003cconfig-file\u003e` Single Cluster Replication repctl supports working with replication within a single Kubernetes cluster.\nJust add cluster you want to use with cluster add command, and you can list, filter, and create resources.\nVolumes and ReplicationGroups created as “target” resources would be prefixed with replicated- so you can easily differentiate them.\nYou can also differentiate between single cluster replication configured StorageClasses and ReplicationGroups and multi-cluster ones by checking remoteClusterID field, for a single cluster the field would be set to self.\nTo create replication enabled storage classes for single cluster replication using create sc command be sure to set both sourceClusterID and targetClusterID to the same clusterID and continue as usual with executing the command. Name of StorageClass resource that created as “target” will be appended with -tgt.\nExecuting Actions repctl can be used to execute various replication actions on ReplicationGroups.\nFailover This command will perform a planned failover to a cluster or an RG.\nWhen working with multiple clusters, you can perform failover by specifying the target cluster ID. To do that use --target \u003ctargetClusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failover by specifying the target replication group ID. To do that use --target \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-rg-id\u003e In both scenarios repctl will patch the CR at the source site with action FAILOVER_REMOTE.\nYou can also provide --unplanned parameter, then repctl will perform an unplanned failover to a given cluster or an RG, instead of FAILOVER_REMOTE repctl will patch CR at target cluster with action UNPLANNED_FAILOVER_LOCAL.\nReprotect This command will perform a reprotect at the specified cluster or the RG.\nWhen working with multiple clusters, you can perform reprotect by specifying the cluster ID. To do that use --at \u003cclusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e reprotect --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform reprotect by specifying the replication group ID. To do that use --rg \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e reprotect In both scenarios repctl will patch the CR at the source site with action REPROTECT_LOCAL.\nFailback This command will perform a planned failback to a cluster or an RG.\nWhen working with multiple clusters, you can perform failback by specifying the cluster ID, to do that use --target \u003cclusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform failback by specifying the replication group ID. To do that use --target \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e failback --target \u003ctgt-rg-id\u003e In both scenarios repctl will patch the CR at the source site with action FAILBACK_LOCAL.\nYou can also provide --discard parameter, then repctl will perform a failback but discard any writes at target, instead of FAILBACK_LOCAL repctl will patch CR at target cluster with action ACTION_FAILBACK_DISCARD_CHANGES_LOCAL.\nSwap This command will perform a swap at a specified cluster or an RG.\nWhen working with multiple clusters, you can perform swap by specifying the cluster ID. To do that use --at \u003cclusterID\u003e parameter.\n./repctl --rg \u003crg-id\u003e swap --at \u003ctgt-cluster-id\u003e When working with replication within a single cluster, you can perform swap by specifying the replication group ID. To do that use --rg \u003crg-id\u003e parameter.\n./repctl --rg \u003crg-id\u003e swap repctl will patch CR at the source cluster with action SWAP_LOCAL.\nWait For Completion When executing actions you can provide --wait argument to make repctl wait for completion of specified action.\nFor example when executing failover:\n./repctl --rg \u003crg-id\u003e failover --target \u003ctgt-cluster-id\u003e --wait Maintenance Actions You can also use exec command to execute maintenance actions such as suspend, resume, and sync.\nFor single or multi-cluster config:\n./repctl --rg \u003crg-id\u003e exec -a \u003cACTION\u003e Where \u003cACTION\u003e can be one of the following:\n suspend will suspend replication, changes will no longer be synced between replication sites resume will resume replication, canceling the effect of suspend action sync will force synchronization of change between replication sites  ","excerpt":"repctl repctl is a command-line client for configuring replication and …","ref":"/csm-docs/v3/replication/tools/","title":"Tools"},{"body":"Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class   Note: From v1.7, the CSI driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the /samples/volumesnapshotclass folder under respective drivers.\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:csm-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\n Note: VolumeSnapshots can be listed using the command kubectl get volumesnapshot -n \u003cnamespace\u003e\n ","excerpt":"Volume Snapshot Feature In order to use Volume Snapshots, ensure the …","ref":"/csm-docs/docs/snapshots/","title":"Snapshots"},{"body":"Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class   Note: From v1.7, the CSI driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the /samples/volumesnapshotclass folder under respective drivers.\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:csm-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\n Note: VolumeSnapshots can be listed using the command kubectl get volumesnapshot -n \u003cnamespace\u003e\n ","excerpt":"Volume Snapshot Feature In order to use Volume Snapshots, ensure the …","ref":"/csm-docs/v1/snapshots/","title":"Snapshots"},{"body":"Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class   Note: From v1.7, the CSI driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the /samples/volumesnapshotclass folder under respective drivers.\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:csm-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\n Note: VolumeSnapshots can be listed using the command kubectl get volumesnapshot -n \u003cnamespace\u003e\n ","excerpt":"Volume Snapshot Feature In order to use Volume Snapshots, ensure the …","ref":"/csm-docs/v2/snapshots/","title":"Snapshots"},{"body":"Volume Snapshot Feature In order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class   Note: From v1.7, the CSI driver installation process will no longer create VolumeSnapshotClass. If you want to create VolumeSnapshots, then create a VolumeSnapshotClass using the sample provided in the /samples/volumesnapshotclass folder under respective drivers.\n Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:csm-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true, it is available for use.\n Note: VolumeSnapshots can be listed using the command kubectl get volumesnapshot -n \u003cnamespace\u003e\n ","excerpt":"Volume Snapshot Feature In order to use Volume Snapshots, ensure the …","ref":"/csm-docs/v3/snapshots/","title":"Snapshots"},{"body":"   Symptoms Prevention, Resolution or Workaround     Persistent volumes don’t get created on the target cluster. Run kubectl describe on one of the pods of replication controller and see if event says Config update won't be applied because of invalid configmap/secrets. Please fix the invalid configuration. If it does then ensure you correctly populated replication ConfigMap. You can check the current status by running kubectl describe cm -n dell-replication-controller dell-replication-controller-config. If ConfigMap is empty please edit it yourself or use repctl cluster inject command.   Persistent volumes don’t get created on the target cluster. You don’t see any events on the replication-controller pod. Check logs of replication controller by running kubectl logs -n dell-replication-controller dell-replication-controller-manager-\u003cgenerated-symbols\u003e. If you see clusterId - \u003cclusterID\u003e not found errors then be sure to check if you specified the same clusterIDs in both your ConfigMap and replication enabled StorageClass.   You apply replication action by manually editing ReplicationGroup resource field spec.action and don’t see any change of ReplicationGroup state after a while. Check events of the replication-controller pod, if it says Cannot proceed with action \u003cyour-action\u003e. [unsupported action] then check spelling of your action and consult replication-actions page. Alternatively, you can use repctl instead of manually editing ReplicationGroup resources.   You execute failover action using repctl failover command and see failover: error executing failover to source site. This means you tried to failover to a cluster that is already marked source. If you still want to execute failover for RG just choose another cluster.   You’ve created PersistentVolumeClaim using replication enabled StorageClass but don’t see any RGs created in the source cluster. Check annotations of created PersistentVolumeClaim. If it doesn’t have annotations that start with replication.storage.dell.com then please wait for a couple of minutes for them to be added and RG to be created.   When installing common replication controller using helm you see an error that states invalid ownership metadata and missing key \"app.kubernetes.io/managed-by\": must be set to \"Helm\" This means that you haven’t fully deleted the previous release, you can fix it by either deleting entire manifest by using kubectl delete -f deploy/controller.yaml or manually deleting conflicting resources (ClusterRoles, ClusterRoleBinding, etc.)   PV and/or PVCs are not being created at the source/target cluster. If you check the controller’s logs you can see no such host errors Make sure cluster-1’s API is pingable from cluster-2 and vice versa. If one of your clusters is OpenShift located in a private network and needs records in /etc/hosts - exec into controller pod and modify /etc/hosts manually.    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Persistent …","ref":"/csm-docs/docs/replication/troubleshooting/","title":"Troubleshooting"},{"body":"   Symptoms Prevention, Resolution or Workaround     Persistent volumes don’t get created on the target cluster. Run kubectl describe on one of the pods of replication controller and see if event says Config update won't be applied because of invalid configmap/secrets. Please fix the invalid configuration. If it does then ensure you correctly populated replication ConfigMap. You can check the current status by running kubectl describe cm -n dell-replication-controller dell-replication-controller-config. If ConfigMap is empty please edit it yourself or use repctl cluster inject command.   Persistent volumes don’t get created on the target cluster. You don’t see any events on the replication-controller pod. Check logs of replication controller by running kubectl logs -n dell-replication-controller dell-replication-controller-manager-\u003cgenerated-symbols\u003e. If you see clusterId - \u003cclusterID\u003e not found errors then be sure to check if you specified the same clusterIDs in both your ConfigMap and replication enabled StorageClass.   You apply replication action by manually editing ReplicationGroup resource field spec.action and don’t see any change of ReplicationGroup state after a while. Check events of the replication-controller pod, if it says Cannot proceed with action \u003cyour-action\u003e. [unsupported action] then check spelling of your action and consult replication-actions page. Alternatively, you can use repctl instead of manually editing ReplicationGroup resources.   You execute failover action using repctl failover command and see failover: error executing failover to source site. This means you tried to failover to a cluster that is already marked source. If you still want to execute failover for RG just choose another cluster.   You’ve created PersistentVolumeClaim using replication enabled StorageClass but don’t see any RGs created in the source cluster. Check annotations of created PersistentVolumeClaim. If it doesn’t have annotations that start with replication.storage.dell.com then please wait for a couple of minutes for them to be added and RG to be created.   When installing common replication controller using helm you see an error that states invalid ownership metadata and missing key \"app.kubernetes.io/managed-by\": must be set to \"Helm\" This means that you haven’t fully deleted the previous release, you can fix it by either deleting entire manifest by using kubectl delete -f deploy/controller.yaml or manually deleting conflicting resources (ClusterRoles, ClusterRoleBinding, etc.)   PV and/or PVCs are not being created at the source/target cluster. If you check the controller’s logs you can see no such host errors Make sure cluster-1’s API is pingable from cluster-2 and vice versa. If one of your clusters is OpenShift located in a private network and needs records in /etc/hosts - exec into controller pod and modify /etc/hosts manually.    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Persistent …","ref":"/csm-docs/v1/replication/troubleshooting/","title":"Troubleshooting"},{"body":"   Symptoms Prevention, Resolution or Workaround     Persistent volumes don’t get created on the target cluster. Run kubectl describe on one of the pods of replication controller and see if event says Config update won't be applied because of invalid configmap/secrets. Please fix the invalid configuration. If it does then ensure you correctly populated replication ConfigMap. You can check the current status by running kubectl describe cm -n dell-replication-controller dell-replication-controller-config. If ConfigMap is empty please edit it yourself or use repctl cluster inject command.   Persistent volumes don’t get created on the target cluster. You don’t see any events on the replication-controller pod. Check logs of replication controller by running kubectl logs -n dell-replication-controller dell-replication-controller-manager-\u003cgenerated-symbols\u003e. If you see clusterId - \u003cclusterID\u003e not found errors then be sure to check if you specified the same clusterIDs in both your ConfigMap and replication enabled StorageClass.   You apply replication action by manually editing ReplicationGroup resource field spec.action and don’t see any change of ReplicationGroup state after a while. Check events of the replication-controller pod, if it says Cannot proceed with action \u003cyour-action\u003e. [unsupported action] then check spelling of your action and consult replication-actions page. Alternatively, you can use repctl instead of manually editing ReplicationGroup resources.   You execute failover action using repctl failover command and see failover: error executing failover to source site. This means you tried to failover to a cluster that is already marked source. If you still want to execute failover for RG just choose another cluster.   You’ve created PersistentVolumeClaim using replication enabled StorageClass but don’t see any RGs created in the source cluster. Check annotations of created PersistentVolumeClaim. If it doesn’t have annotations that start with replication.storage.dell.com then please wait for a couple of minutes for them to be added and RG to be created.   When installing common replication controller using helm you see an error that states invalid ownership metadata and missing key \"app.kubernetes.io/managed-by\": must be set to \"Helm\" This means that you haven’t fully deleted the previous release, you can fix it by either deleting entire manifest by using kubectl delete -f deploy/controller.yaml or manually deleting conflicting resources (ClusterRoles, ClusterRoleBinding, etc.)   PV and/or PVCs are not being created at the source/target cluster. If you check the controller’s logs you can see no such host errors Make sure cluster-1’s API is pingable from cluster-2 and vice versa. If one of your clusters is OpenShift located in a private network and needs records in /etc/hosts - exec into controller pod and modify /etc/hosts manually.    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Persistent …","ref":"/csm-docs/v2/replication/troubleshooting/","title":"Troubleshooting"},{"body":"   Symptoms Prevention, Resolution or Workaround     Persistent volumes don’t get created on the target cluster. Run kubectl describe on one of the pods of replication controller and see if event says Config update won't be applied because of invalid configmap/secrets. Please fix the invalid configuration. If it does then ensure you correctly populated replication ConfigMap. You can check the current status by running kubectl describe cm -n dell-replication-controller dell-replication-controller-config. If ConfigMap is empty please edit it yourself or use repctl cluster inject command.   Persistent volumes don’t get created on the target cluster. You don’t see any events on the replication-controller pod. Check logs of replication controller by running kubectl logs -n dell-replication-controller dell-replication-controller-manager-\u003cgenerated-symbols\u003e. If you see clusterId - \u003cclusterID\u003e not found errors then be sure to check if you specified the same clusterIDs in both your ConfigMap and replication enabled StorageClass.   You apply replication action by manually editing ReplicationGroup resource field spec.action and don’t see any change of ReplicationGroup state after a while. Check events of the replication-controller pod, if it says Cannot proceed with action \u003cyour-action\u003e. [unsupported action] then check spelling of your action and consult replication-actions page. Alternatively, you can use repctl instead of manually editing ReplicationGroup resources.   You execute failover action using repctl failover command and see failover: error executing failover to source site. This means you tried to failover to a cluster that is already marked source. If you still want to execute failover for RG just choose another cluster.   You’ve created PersistentVolumeClaim using replication enabled StorageClass but don’t see any RGs created in the source cluster. Check annotations of created PersistentVolumeClaim. If it doesn’t have annotations that start with replication.storage.dell.com then please wait for a couple of minutes for them to be added and RG to be created.   When installing common replication controller using helm you see an error that states invalid ownership metadata and missing key \"app.kubernetes.io/managed-by\": must be set to \"Helm\" This means that you haven’t fully deleted the previous release, you can fix it by either deleting entire manifest by using kubectl delete -f deploy/controller.yaml or manually deleting conflicting resources (ClusterRoles, ClusterRoleBinding, etc.)   PV and/or PVCs are not being created at the source/target cluster. If you check the controller’s logs you can see no such host errors Make sure cluster-1’s API is pingable from cluster-2 and vice versa. If one of your clusters is OpenShift located in a private network and needs records in /etc/hosts - exec into controller pod and modify /etc/hosts manually.    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Persistent …","ref":"/csm-docs/v3/replication/troubleshooting/","title":"Troubleshooting"},{"body":"Volume Group Snapshot Feature In order to use Volume Group Snapshots, ensure the volume snapshot module is enabled.\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller Volume Snapshot Class  Creating Volume Group Snapshots This is a sample manifest for creating a Volume Group Snapshot:\napiVersion:volumegroup.storage.dell.com/v1kind:DellCsiVolumeGroupSnapshotmetadata:name:\"vgs-test\"namespace:\"test\"spec:# Add fields heredriverName:\"csi-\u003cdriver-name\u003e.dellemc.com\"# Example: \"csi-powerstore.dellemc.com\"# defines how to process VolumeSnapshot members when volume group snapshot is deleted# \"Retain\" - keep VolumeSnapshot instances# \"Delete\" - delete VolumeSnapshot instancesmemberReclaimPolicy:\"Retain\"volumesnapshotclass:\"\u003csnapshot-class\u003e\"pvcLabel:\"vgs-snap-label\"# pvcList:# - \"pvcName1\"# - \"pvcName2\"The PVC labels field specifies a label that must be present in PVCs that are to be snapshotted. Here is a sample of that portion of a .yaml for a PVC:\nmetadata:name:volume1namespace:testlabels:volume-group:vgs-snap-labelMore details about the installation and use of the VolumeGroup Snapshotter can be found here: dell-csi-volumegroup-snapshotter.\n Note: Volume group cannot be seen from the Kubernetes level as of now only volume group snapshots can be viewed as a CRD\n  Volume Group Snapshots feature is supported with Helm.\n ","excerpt":"Volume Group Snapshot Feature In order to use Volume Group Snapshots, …","ref":"/csm-docs/docs/snapshots/volume-group-snapshots/","title":"Volume Group Snapshots"},{"body":"","excerpt":"","ref":"/csm-docs/docs/csidriver/partners/","title":"Our Ecosystem Partners"},{"body":"","excerpt":"","ref":"/csm-docs/v1/csidriver/partners/","title":"Our Ecosystem Partners"},{"body":"","excerpt":"","ref":"/csm-docs/v2/csidriver/partners/","title":"Our Ecosystem Partners"},{"body":"","excerpt":"","ref":"/csm-docs/v3/csidriver/partners/","title":"Our Ecosystem Partners"},{"body":"Release Notes - CSM Replication 1.3.0 New Features/Changes  Added support for Kubernetes 1.24 Added support for OpenShift 4.10 Added volume upgrade/downgrade functionality for replication volumes  Fixed Issues  Fixed panic occuring when encountering PVC with empty StorageClass PV and RG retention policy checks are no longer case sensitive RG will now display EMPTY link state when no PV found [PowerScale] Running reprotect action on source cluster after failover no longer puts RG into UNKNOWN state [PowerScale] Deleting RG will break replication link before trying to delete group on array  Known Issues There are no known issues in this release.\n","excerpt":"Release Notes - CSM Replication 1.3.0 New Features/Changes  Added …","ref":"/csm-docs/docs/replication/release/","title":"Release notes"},{"body":"CSM Replication module consists of two components:\n CSM Replication sidecar (installed along with the driver) CSM Replication controller  Those two components should be upgraded separately. When upgrading them ensure that you use the same versions for both sidecar and controller, because different versions could be incompatible with each other.\n Note: While upgrading the module via helm, the replicas variable in myvalues.yaml can be at most one less than the number of worker nodes.\n Updating CSM Replication sidecar To upgrade the CSM Replication sidecar that is installed along with the driver, the following steps are required.\n Note: These steps refer to the values file and csi-install.sh script that was used during the initial installation of the Dell CSI driver. Steps\n  Update the controller.replication.image value in the values files to reference the new CSM Replication sidecar image. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace \u003cyour-namespace\u003e --values ./myvalues.yaml --upgrade Run the same command on the second Kubernetes cluster if you use multi-cluster replication topology  Updating CSM Replication controller Upgrading with Helm This option will only work if you have previously installed replication with helm chart available since version 1.1. If you used simple manifest or repctl please use upgrading with repctl\nSteps\n Update the image value in the values files to reference the new CSM Replication sidecar image or use a new version of the csm-replication helm chart Run the install script with the option --upgrade by running: cd ./scripts \u0026\u0026 ./install.sh --values ./myvalues.yaml --upgrade Run the same command on the second Kubernetes cluster if you use multi-cluster replication topology   Note: Upgrade won’t override currently existing ConfigMap, even if you change templated values in myvalues.yaml file. If you want to change the logLevel - edit ConfigMap from within the cluster using kubectl edit cm -n dell-replication-controller dell-replication-controller-config\n Upgrading with repctl  Note: These steps assume that you already have repctl configured to use correct clusters, if you don’t know how to do that please refer to installing with repctl\n Steps\n Find a new version of deployment manifest that can be found in deploy/controller.yaml, with newer image pointing to the version of CSM Replication controller you want to upgrade to Apply said manifest using the usual repctl create command like so ./repctl create -f ./deploy/controller.yaml. The output should have this line Successfully updated existing deployment: dell-replication-controller-manager Check if everything is OK by querying your Kubernetes clusters using kubectl like this kubectl get pods -n dell-replication-controller, your pods should READY and RUNNING  ","excerpt":"CSM Replication module consists of two components:\n CSM Replication …","ref":"/csm-docs/docs/replication/upgrade/","title":"Upgrade"},{"body":"CSM Replication module consists of two components:\n CSM Replication sidecar (installed along with the driver) CSM Replication controller  Those two components should be upgraded separately. When upgrading them ensure that you use the same versions for both sidecar and controller, because different versions could be incompatible with each other.\n Note: While upgrading the module via helm, the replicas variable in myvalues.yaml can be at most one less than the number of worker nodes.\n Updating CSM Replication sidecar To upgrade the CSM Replication sidecar that is installed along with the driver, the following steps are required.\n Note: These steps refer to the values file and csi-install.sh script that was used during the initial installation of the Dell CSI driver. Steps\n  Update the controller.replication.image value in the values files to reference the new CSM Replication sidecar image. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace \u003cyour-namespace\u003e --values ./myvalues.yaml --upgrade Run the same command on the second Kubernetes cluster if you use multi-cluster replication topology  Updating CSM Replication controller Upgrading with Helm This option will only work if you have previously installed replication with helm chart available since version 1.1. If you used simple manifest or repctl please use upgrading with repctl\nSteps\n Update the image value in the values files to reference the new CSM Replication sidecar image or use a new version of the csm-replication helm chart Run the install script with the option --upgrade by running: cd ./scripts \u0026\u0026 ./install.sh --values ./myvalues.yaml --upgrade Run the same command on the second Kubernetes cluster if you use multi-cluster replication topology   Note: Upgrade won’t override currently existing ConfigMap, even if you change templated values in myvalues.yaml file. If you want to change the logLevel - edit ConfigMap from within the cluster using kubectl edit cm -n dell-replication-controller dell-replication-controller-config\n Upgrading with repctl  Note: These steps assume that you already have repctl configured to use correct clusters, if you don’t know how to do that please refer to installing with repctl\n Steps\n Find a new version of deployment manifest that can be found in deploy/controller.yaml, with newer image pointing to the version of CSM Replication controller you want to upgrade to Apply said manifest using the usual repctl create command like so ./repctl create -f ./deploy/controller.yaml. The output should have this line Successfully updated existing deployment: dell-replication-controller-manager Check if everything is OK by querying your Kubernetes clusters using kubectl like this kubectl get pods -n dell-replication-controller, your pods should READY and RUNNING  ","excerpt":"CSM Replication module consists of two components:\n CSM Replication …","ref":"/csm-docs/v1/replication/upgrade/","title":"Upgrade"},{"body":"CSM Replication module consists of two components:\n CSM Replication sidecar (installed along with the driver) CSM Replication controller  Those two components should be upgraded separately. When upgrading them ensure that you use the same versions for both sidecar and controller, because different versions could be incompatible with each other.\n Note: While upgrading the module via helm, the replicas variable in myvalues.yaml can be at most one less than the number of worker nodes.\n Updating CSM Replication sidecar To upgrade the CSM Replication sidecar that is installed along with the driver, the following steps are required.\n Note: These steps refer to the values file and csi-install.sh script that was used during the initial installation of the Dell CSI driver. Steps\n  Update the controller.replication.image value in the values files to reference the new CSM Replication sidecar image. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace \u003cyour-namespace\u003e --values ./myvalues.yaml --upgrade Run the same command on the second Kubernetes cluster if you use multi-cluster replication topology  Updating CSM Replication controller Upgrading with Helm This option will only work if you have previously installed replication with helm chart available since version 1.1. If you used simple manifest or repctl please use upgrading with repctl\nSteps\n Update the image value in the values files to reference the new CSM Replication sidecar image or use a new version of the csm-replication helm chart Run the install script with the option --upgrade by running: cd ./scripts \u0026\u0026 ./install.sh --values ./myvalues.yaml --upgrade Run the same command on the second Kubernetes cluster if you use multi-cluster replication topology   Note: Upgrade won’t override currently existing ConfigMap, even if you change templated values in myvalues.yaml file. If you want to change the logLevel - edit ConfigMap from within the cluster using kubectl edit cm -n dell-replication-controller dell-replication-controller-config\n Upgrading with repctl  Note: These steps assume that you already have repctl configured to use correct clusters, if you don’t know how to do that please refer to installing with repctl\n Steps\n Find a new version of deployment manifest that can be found in deploy/controller.yaml, with newer image pointing to the version of CSM Replication controller you want to upgrade to Apply said manifest using the usual repctl create command like so ./repctl create -f ./deploy/controller.yaml. The output should have this line Successfully updated existing deployment: dell-replication-controller-manager Check if everything is OK by querying your Kubernetes clusters using kubectl like this kubectl get pods -n dell-replication-controller, your pods should READY and RUNNING  ","excerpt":"CSM Replication module consists of two components:\n CSM Replication …","ref":"/csm-docs/v2/replication/upgrade/","title":"Upgrade"},{"body":"CSM Replication module consists of two components:\n CSM Replication sidecar (installed along with the driver) CSM Replication controller  Those two components should be upgraded separately. When upgrading them ensure that you use the same versions for both sidecar and controller, because different versions could be incompatible with each other.\n Note: While upgrading the module via helm, the replicas variable in myvalues.yaml can be at most one less than the number of worker nodes.\n Updating CSM Replication sidecar To upgrade the CSM Replication sidecar that is installed along with the driver, the following steps are required.\n Note: These steps refer to the values file and csi-install.sh script that was used during the initial installation of the Dell CSI driver. Steps\n  Update the controller.replication.image value in the values files to reference the new CSM Replication sidecar image. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace \u003cyour-namespace\u003e --values ./myvalues.yaml --upgrade Run the same command on the second Kubernetes cluster if you use multi-cluster replication topology  Updating CSM Replication controller Upgrading with Helm This option will only work if you have previously installed replication with helm chart available since version 1.1. If you used simple manifest or repctl please use upgrading with repctl\nSteps\n Update the image value in the values files to reference the new CSM Replication sidecar image or use a new version of the csm-replication helm chart Run the install script with the option --upgrade by running: cd ./scripts \u0026\u0026 ./install.sh --values ./myvalues.yaml --upgrade Run the same command on the second Kubernetes cluster if you use multi-cluster replication topology   Note: Upgrade won’t override currently existing ConfigMap, even if you change templated values in myvalues.yaml file. If you want to change the logLevel - edit ConfigMap from within the cluster using kubectl edit cm -n dell-replication-controller dell-replication-controller-config\n Upgrading with repctl  Note: These steps assume that you already have repctl configured to use correct clusters, if you don’t know how to do that please refer to installing with repctl\n Steps\n Find a new version of deployment manifest that can be found in deploy/controller.yaml, with newer image pointing to the version of CSM Replication controller you want to upgrade to Apply said manifest using the usual repctl create command like so ./repctl create -f ./deploy/controller.yaml. The output should have this line Successfully updated existing deployment: dell-replication-controller-manager Check if everything is OK by querying your Kubernetes clusters using kubectl like this kubectl get pods -n dell-replication-controller, your pods should READY and RUNNING  ","excerpt":"CSM Replication module consists of two components:\n CSM Replication …","ref":"/csm-docs/v3/replication/upgrade/","title":"Upgrade"},{"body":"","excerpt":"","ref":"/csm-docs/docs/grasp/","title":"Learn"},{"body":"","excerpt":"","ref":"/csm-docs/v1/grasp/","title":"Learn"},{"body":"","excerpt":"","ref":"/csm-docs/v2/grasp/","title":"Learn"},{"body":"","excerpt":"","ref":"/csm-docs/v3/grasp/","title":"Learn"},{"body":"Release notes for Container Storage Modules:\nCSI Drivers\nCSM for Authorization\nCSM for Observability\nCSM for Replication\nCSM for Resiliency\n","excerpt":"Release notes for Container Storage Modules:\nCSI Drivers\nCSM for …","ref":"/csm-docs/docs/release/","title":"Release notes"},{"body":"Troubleshooting links for Container Storage Modules:\nCSI Drivers\nCSM for Authorization\nCSM for Observability\nCSM for Replication\nCSM for Resiliency\n","excerpt":"Troubleshooting links for Container Storage Modules:\nCSI Drivers\nCSM …","ref":"/csm-docs/docs/troubleshooting/","title":"Troubleshooting"},{"body":"Troubleshooting links for Container Storage Modules:\nCSI Drivers\nCSM for Authorization\nCSM for Observability\nCSM for Replication\nCSM for Resiliency\n","excerpt":"Troubleshooting links for Container Storage Modules:\nCSI Drivers\nCSM …","ref":"/csm-docs/v1/troubleshooting/","title":"Troubleshooting"},{"body":"Troubleshooting links for Container Storage Modules:\nCSI Drivers\nCSM for Authorization\nCSM for Observability\nCSM for Replication\nCSM for Resiliency\n","excerpt":"Troubleshooting links for Container Storage Modules:\nCSI Drivers\nCSM …","ref":"/csm-docs/v2/troubleshooting/","title":"Troubleshooting"},{"body":"Troubleshooting links for Container Storage Modules:\nCSI Drivers\nCSM for Authorization\nCSM for Observability\nCSM for Replication\nCSM for Resiliency\n","excerpt":"Troubleshooting links for Container Storage Modules:\nCSI Drivers\nCSM …","ref":"/csm-docs/v3/troubleshooting/","title":"Troubleshooting"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Replication.\nUninstalling common replication controller To uninstall the common replication controller you can use script uninstall.sh located in scripts folder:\n./uninstall.sh This script will automatically detect how current version is installed (with repctl or with helm) and use the correct method to delete it.\nYou can also manually uninstall replication controller using method that depends on how you installed replication controller.\nIf replication controller was installed using helm use this command:\nhelm delete -n dell-replication-controller replication If you used controller.yaml manifest with either kubectl or repctl use this:\nkubectl delete -f deploy/controller.yaml  NOTE: Be sure to run chosen command on all clusters where you want to uninstall replication controller.\n Uninstalling the replication sidecar To uninstall the replication sidecar you need to uninstall the CSI Driver, please view the uninstall page of the driver.\n","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/docs/replication/uninstall/","title":"Uninstall"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Replication.\nUninstalling common replication controller To uninstall the common replication controller you can use script uninstall.sh located in scripts folder:\n./uninstall.sh This script will automatically detect how current version is installed (with repctl or with helm) and use the correct method to delete it.\nYou can also manually uninstall replication controller using method that depends on how you installed replication controller.\nIf replication controller was installed using helm use this command:\nhelm delete -n dell-replication-controller replication If you used controller.yaml manifest with either kubectl or repctl use this:\nkubectl delete -f deploy/controller.yaml  NOTE: Be sure to run chosen command on all clusters where you want to uninstall replication controller.\n Uninstalling the replication sidecar To uninstall the replication sidecar you need to uninstall the CSI Driver, please view the uninstall page of the driver.\n","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/v1/replication/uninstall/","title":"Uninstall"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Replication.\nUninstalling common replication controller To uninstall the common replication controller you can use script uninstall.sh located in scripts folder:\n./uninstall.sh This script will automatically detect how current version is installed (with repctl or with helm) and use the correct method to delete it.\nYou can also manually uninstall replication controller using method that depends on how you installed replication controller.\nIf replication controller was installed using helm use this command:\nhelm delete -n dell-replication-controller replication If you used controller.yaml manifest with either kubectl or repctl use this:\nkubectl delete -f deploy/controller.yaml  NOTE: Be sure to run chosen command on all clusters where you want to uninstall replication controller.\n Uninstalling the replication sidecar To uninstall the replication sidecar you need to uninstall the CSI Driver, please view the uninstall page of the driver.\n","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/v2/replication/uninstall/","title":"Uninstall"},{"body":"This section outlines the uninstallation steps for Container Storage Modules (CSM) for Replication.\nUninstalling common replication controller To uninstall the common replication controller you can use script uninstall.sh located in scripts folder:\n./uninstall.sh This script will automatically detect how current version is installed (with repctl or with helm) and use the correct method to delete it.\nYou can also manually uninstall replication controller using method that depends on how you installed replication controller.\nIf replication controller was installed using helm use this command:\nhelm delete -n dell-replication-controller replication If you used controller.yaml manifest with either kubectl or repctl use this:\nkubectl delete -f deploy/controller.yaml  NOTE: Be sure to run chosen command on all clusters where you want to uninstall replication controller.\n Uninstalling the replication sidecar To uninstall the replication sidecar you need to uninstall the CSI Driver, please view the uninstall page of the driver.\n","excerpt":"This section outlines the uninstallation steps for Container Storage …","ref":"/csm-docs/v3/replication/uninstall/","title":"Uninstall"},{"body":"For all your support needs or to follow the latest ongoing discussions and updates, join our Slack group. Click Here to request your invite.\nYou can also interact with us on GitHub by creating a GitHub Issue.\n","excerpt":"For all your support needs or to follow the latest ongoing discussions …","ref":"/csm-docs/docs/support/","title":"Support"},{"body":"For all your support needs or to follow the latest ongoing discussions and updates, join our Slack group. Click Here to request your invite.\nYou can also interact with us on GitHub by creating a GitHub Issue.\n","excerpt":"For all your support needs or to follow the latest ongoing discussions …","ref":"/csm-docs/v1/support/","title":"Support"},{"body":"For all your support needs or to follow the latest ongoing discussions and updates, join our Slack group. Click Here to request your invite.\nYou can also interact with us on GitHub by creating a GitHub Issue.\n","excerpt":"For all your support needs or to follow the latest ongoing discussions …","ref":"/csm-docs/v2/support/","title":"Support"},{"body":"For all your support needs or to follow the latest ongoing discussions and updates, join our Slack group. Click Here to request your invite.\nYou can also interact with us on GitHub by creating a GitHub Issue.\n","excerpt":"For all your support needs or to follow the latest ongoing discussions …","ref":"/csm-docs/v3/support/","title":"Support"},{"body":"CSM Docs is an open-source project and we thrive to build a welcoming and open community for anyone who wants to use the project or contribute to it.\nContributing to CSM Docs Become one of the contributors to this project!\nYou can contribute to this project in several ways. Here are some examples:\n Contribute to the CSM documentation. Report an issue. Feature requests.  CSM docs reside in https://github.com/dell/csm-docs.\nCSM project resides in https://github.com/dell/csm.\nDon’t  Break the website view. Commit directly. Compromise backward compatibility. Disrespect your Community Team members. Forget to keep things simple.  Do  Keep it simple. Good work, your best every time. Squash your commits, avoid merges. Keep open communication with other Committers. Ask questions. Test your changes locally and make sure it is not breaking anything.  Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose.\nBranching strategy The CSM documentation portal follows a release branch strategy where a branch is created for each release and all documentation changes made for a release are done on that branch. The release branch is then merged into the main branch at the time of the release. In some situations it may be sufficient to merge a non-release branch to main if it fixes some issue in the documentation for the current released version.\nBranch Naming Convention    Branch Type Example Comment     main main    Release release-1.0 hotfix: release-1.1 patch: release-1.0.1   Feature feature-9-olp-support “9” referring to GitHub issue ID   Bug Fix bugfix-110-remove-docker-compose “110” referring to GitHub issue ID    Steps for working on the main branch  Fork the repository. Create a branch off of the main branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream main branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream main branch. Once your pull request has merged with the required approvals, your branch can be deleted.  Steps for working on a release branch  Fork the repository. Create a branch off of the release branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream release branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream release branch. Once your pull request has merged with the required approvals, your branch can be deleted.  Previewing your changes  Install latest Hugo version extended version.  Note: Please note we have to install an extended version.\n  Create a local copy of the csm-docs repository using git clone. Update docsy submodules inside themes folder using git submodule update --recursive --init Change to the csm-docs folder and run hugo server By default, local changes will be reflected at http://localhost:1313/. Hugo will watch for changes to the content and automatically refreshes the site.\n Note: To bind it to different server address use hugo server --bind 0.0.0.0, default is 127.0.0.1\n  After testing the changes locally, raise a pull request after editing the pages and pushing it to GitHub.  Community guidelines This project follows https://github.com/dell/csm/blob/main/docs/CODE_OF_CONDUCT.md.\nBest Practices Linking the URLs Hardcoded relative links like [troubleshooting observability](../../observability/troubleshooting.md) will behave unexpectedly compared to how they would work on our local file system. To avoid broken links in the portal, use regular relative URLs in links that will be left unchanged by Hugo.\nStyle guide  Use sentence case wherever applicable. Use the numbered lists for items in sequential order and bulletins for the other lists. Check for grammar and spelling. Embed the code within backticks. Use only high-resolution images.  ","excerpt":"CSM Docs is an open-source project and we thrive to build a welcoming …","ref":"/csm-docs/docs/contributionguidelines/","title":"Contribution Guidelines"},{"body":"CSM Docs is an open-source project and we thrive to build a welcoming and open community for anyone who wants to use the project or contribute to it.\nContributing to CSM Docs Become one of the contributors to this project!\nYou can contribute to this project in several ways. Here are some examples:\n Contribute to the CSM documentation. Report an issue. Feature requests.  CSM docs reside in https://github.com/dell/csm-docs.\nCSM project resides in https://github.com/dell/csm.\nDon’t  Break the website view. Commit directly. Compromise backward compatibility. Disrespect your Community Team members. Forget to keep things simple.  Do  Keep it simple. Good work, your best every time. Squash your commits, avoid merges. Keep open communication with other Committers. Ask questions. Test your changes locally and make sure it is not breaking anything.  Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose.\nBranching strategy The CSM documentation portal follows a release branch strategy where a branch is created for each release and all documentation changes made for a release are done on that branch. The release branch is then merged into the main branch at the time of the release. In some situations it may be sufficient to merge a non-release branch to main if it fixes some issue in the documentation for the current released version.\nBranch Naming Convention    Branch Type Example Comment     main main    Release release-1.0 hotfix: release-1.1 patch: release-1.0.1   Feature feature-9-olp-support “9” referring to GitHub issue ID   Bug Fix bugfix-110-remove-docker-compose “110” referring to GitHub issue ID    Steps for working on the main branch  Fork the repository. Create a branch off of the main branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream main branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream main branch. Once your pull request has merged with the required approvals, your branch can be deleted.  Steps for working on a release branch  Fork the repository. Create a branch off of the release branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream release branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream release branch. Once your pull request has merged with the required approvals, your branch can be deleted.  Previewing your changes  Install latest Hugo version extended version.  Note: Please note we have to install an extended version.\n  Create a local copy of the csm-docs repository using git clone. Update docsy submodules inside themes folder using git submodule update --recursive --init Change to the csm-docs folder and run hugo server By default, local changes will be reflected at http://localhost:1313/. Hugo will watch for changes to the content and automatically refreshes the site.\n Note: To bind it to different server address use hugo server --bind 0.0.0.0, default is 127.0.0.1\n  After testing the changes locally, raise a pull request after editing the pages and pushing it to GitHub.  Community guidelines This project follows https://github.com/dell/csm/blob/main/docs/CODE_OF_CONDUCT.md.\nBest Practices Linking the URLs Hardcoded relative links like [troubleshooting observability](../../observability/troubleshooting.md) will behave unexpectedly compared to how they would work on our local file system. To avoid broken links in the portal, use regular relative URLs in links that will be left unchanged by Hugo.\nStyle guide  Use sentence case wherever applicable. Use the numbered lists for items in sequential order and bulletins for the other lists. Check for grammar and spelling. Embed the code within backticks. Use only high-resolution images.  ","excerpt":"CSM Docs is an open-source project and we thrive to build a welcoming …","ref":"/csm-docs/v1/contributionguidelines/","title":"Contribution Guidelines"},{"body":"CSM Docs is an open-source project and we thrive to build a welcoming and open community for anyone who wants to use the project or contribute to it.\nContributing to CSM Docs Become one of the contributors to this project!\nYou can contribute to this project in several ways. Here are some examples:\n Contribute to the CSM documentation. Report an issue. Feature requests.  CSM docs reside in https://github.com/dell/csm-docs.\nCSM project resides in https://github.com/dell/csm.\nDon’t  Break the website view. Commit directly. Compromise backward compatibility. Disrespect your Community Team members. Forget to keep things simple.  Do  Keep it simple. Good work, your best every time. Squash your commits, avoid merges. Keep open communication with other Committers. Ask questions. Test your changes locally and make sure it is not breaking anything.  Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose.\nBranching strategy The CSM documentation portal follows a release branch strategy where a branch is created for each release and all documentation changes made for a release are done on that branch. The release branch is then merged into the main branch at the time of the release. In some situations it may be sufficient to merge a non-release branch to main if it fixes some issue in the documentation for the current released version.\nBranch Naming Convention    Branch Type Example Comment     main main    Release release-1.0 hotfix: release-1.1 patch: release-1.0.1   Feature feature-9-olp-support “9” referring to GitHub issue ID   Bug Fix bugfix-110-remove-docker-compose “110” referring to GitHub issue ID    Steps for working on the main branch  Fork the repository. Create a branch off of the main branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream main branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream main branch. Once your pull request has merged with the required approvals, your branch can be deleted.  Steps for working on a release branch  Fork the repository. Create a branch off of the release branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream release branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream release branch. Once your pull request has merged with the required approvals, your branch can be deleted.  Previewing your changes  Install latest Hugo version extended version.  Note: Please note we have to install an extended version.\n  Create a local copy of the csm-docs repository using git clone. Update docsy submodules inside themes folder using git submodule update --recursive --init Change to the csm-docs folder and run hugo server By default, local changes will be reflected at http://localhost:1313/. Hugo will watch for changes to the content and automatically refreshes the site.\n Note: To bind it to different server address use hugo server --bind 0.0.0.0, default is 127.0.0.1\n  After testing the changes locally, raise a pull request after editing the pages and pushing it to GitHub.  Community guidelines This project follows https://github.com/dell/csm/blob/main/docs/CODE_OF_CONDUCT.md.\nBest Practices Linking the URLs Hardcoded relative links like [troubleshooting observability](../../observability/troubleshooting.md) will behave unexpectedly compared to how they would work on our local file system. To avoid broken links in the portal, use regular relative URLs in links that will be left unchanged by Hugo.\nStyle guide  Use sentence case wherever applicable. Use the numbered lists for items in sequential order and bulletins for the other lists. Check for grammar and spelling. Embed the code within backticks. Use only high-resolution images.  ","excerpt":"CSM Docs is an open-source project and we thrive to build a welcoming …","ref":"/csm-docs/v2/contributionguidelines/","title":"Contribution Guidelines"},{"body":"CSM Docs is an open-source project and we thrive to build a welcoming and open community for anyone who wants to use the project or contribute to it.\nContributing to CSM Docs Become one of the contributors to this project!\nYou can contribute to this project in several ways. Here are some examples:\n Contribute to the CSM documentation. Report an issue. Feature requests.  CSM docs reside in https://github.com/dell/csm-docs.\nCSM project resides in https://github.com/dell/csm.\nDon’t  Break the website view. Commit directly. Compromise backward compatibility. Disrespect your Community Team members. Forget to keep things simple.  Do  Keep it simple. Good work, your best every time. Squash your commits, avoid merges. Keep open communication with other Committers. Ask questions. Test your changes locally and make sure it is not breaking anything.  Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose.\nBranching strategy The CSM documentation portal follows a release branch strategy where a branch is created for each release and all documentation changes made for a release are done on that branch. The release branch is then merged into the main branch at the time of the release. In some situations it may be sufficient to merge a non-release branch to main if it fixes some issue in the documentation for the current released version.\nBranch Naming Convention    Branch Type Example Comment     main main    Release release-1.0 hotfix: release-1.1 patch: release-1.0.1   Feature feature-9-olp-support “9” referring to GitHub issue ID   Bug Fix bugfix-110-remove-docker-compose “110” referring to GitHub issue ID    Steps for working on the main branch  Fork the repository. Create a branch off of the main branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream main branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream main branch. Once your pull request has merged with the required approvals, your branch can be deleted.  Steps for working on a release branch  Fork the repository. Create a branch off of the release branch. The branch name should follow branch naming convention. Make your changes and commit them to your branch. If other code changes have merged into the upstream release branch, perform a rebase of those changes into your branch. Test your changes locally Open a pull request between your branch and the upstream release branch. Once your pull request has merged with the required approvals, your branch can be deleted.  Previewing your changes  Install latest Hugo version extended version.  Note: Please note we have to install an extended version.\n  Create a local copy of the csm-docs repository using git clone. Update docsy submodules inside themes folder using git submodule update --recursive --init Change to the csm-docs folder and run hugo server By default, local changes will be reflected at http://localhost:1313/. Hugo will watch for changes to the content and automatically refreshes the site.\n Note: To bind it to different server address use hugo server --bind 0.0.0.0, default is 127.0.0.1\n  After testing the changes locally, raise a pull request after editing the pages and pushing it to GitHub.  Community guidelines This project follows https://github.com/dell/csm/blob/main/docs/CODE_OF_CONDUCT.md.\nBest Practices Linking the URLs Hardcoded relative links like [troubleshooting observability](../../observability/troubleshooting.md) will behave unexpectedly compared to how they would work on our local file system. To avoid broken links in the portal, use regular relative URLs in links that will be left unchanged by Hugo.\nStyle guide  Use sentence case wherever applicable. Use the numbered lists for items in sequential order and bulletins for the other lists. Check for grammar and spelling. Embed the code within backticks. Use only high-resolution images.  ","excerpt":"CSM Docs is an open-source project and we thrive to build a welcoming …","ref":"/csm-docs/v3/contributionguidelines/","title":"Contribution Guidelines"},{"body":"","excerpt":"","ref":"/csm-docs/docs/policies/","title":"Policies"},{"body":"","excerpt":"","ref":"/csm-docs/v1/policies/","title":"Policies"},{"body":"","excerpt":"","ref":"/csm-docs/v2/policies/","title":"Policies"},{"body":"The Dell Technologies (Dell) Container Storage Modules (CSM) enables simple and consistent integration and automation experiences, extending enterprise storage capabilities to Kubernetes for cloud-native stateful applications. It reduces management complexity so developers can independently consume enterprise storage with ease and automate daily operations such as provisioning, snapshotting, replication, observability, authorization and, resiliency.\nCSM is made up of multiple components including modules (enterprise capabilities), CSI drivers (storage enablement) and, other related applications (deployment, feature controllers, etc).\nCSM Supported Modules and Dell CSI Drivers    Modules/Drivers CSM 1.3 CSM 1.2.1 CSM 1.2 CSM 1.1     Authorization v1.3.0 v1.2.0 v1.2.0 v1.1.0   Observability v1.2.0 v1.1.1 v1.1.0 v1.0.1   Replication v1.3.0 v1.2.0 v1.2.0 v1.1.0   Resiliency v1.2.0 v1.1.0 v1.1.0 v1.0.1   CSI Driver for PowerScale v2.3.0 v2.2.0 v2.2.0 v2.1.0   CSI Driver for Unity XT v2.3.0 v2.2.0 v2.2.0 v2.1.0   CSI Driver for PowerStore v2.3.0 v2.2.0 v2.2.0 v2.1.0   CSI Driver for PowerFlex v2.3.0 v2.2.0 v2.2.0 v2.1.0   CSI Driver for PowerMax v2.3.0 v2.2.0 v2.2.0 v2.1.0    CSM Modules Support Matrix for Dell CSI Drivers    CSM Module CSI PowerFlex v2.3.0 CSI PowerScale v2.3.0 CSI PowerStore v2.3.0 CSI PowerMax v2.3.0 CSI Unity XT v2.3.0     Authorization v1.3 ✔️ ✔️ ❌ ✔️ ❌   Observability v1.2 ✔️ ❌ ✔️ ❌ ❌   Replication v1.3 ❌ ✔️ ✔️ ✔️ ❌   Resiliency v1.2 ✔️ ✔️ ❌ ❌ ✔️    ","excerpt":"The Dell Technologies (Dell) Container Storage Modules (CSM) enables …","ref":"/csm-docs/docs/","title":"Dell Technologies (Dell) Container Storage Modules (CSM)"},{"body":"","excerpt":"","ref":"/csm-docs/blog/news/","title":"News About Docsy"},{"body":"The partnership between Dell Technologies and Google to support Anthos as an on-prem/hybrid Kubernetes platform tightens and expands.\n Anthos 1.5  PowerMax v1.4 PowerStore v1.1   Anthos bare metal  PowerMax PowerStore asdf-bmctl   Go further  Anthos 1.5 First let us talk about Anthos 1.5 that runs on top of VMware hypervisor. Dell is a storage and platform partner since the version 1.1 and it continues !\nBoth drivers (csi-powermax and csi-powerstore are qualified for iSCSI.\nTo ensure the iSCSI daemon is started, you can use the following DaemonSet to take care of it :\nkubectl create -f https://raw.githubusercontent.com/coulof/ds-iscsi/master/ds-iscsi.yaml PowerMax v1.4 As discussed in that post we provide a new installer script for every driver.\nIt never have been easier to install the CSI driver on Anthos. To do so, simply follow the steps of the Product Guide and add --skip-verify for the install command line :\n./csi-install.sh --namespace powermax --values my-powermax-settings.yaml --skip-verify If you come from an existing installation, there is nothing else to do.\nPowerStore v1.1 For the first time, it is my pleasure to announce csi-powerstore qualifies for Anthos v1.5 for iSCSI protocol (NFS will come later).\nPowerStore storage fits particularly well workloads that are on the Edge. Same here the installation on Anthos is the same as what is documented in the product guide with the addition --skip-verify option:\n./csi-install.sh --namespace csi-powerstore --values ./my-powerstoresettings.yaml --skip-verify Anthos bare metal Google recently announced the Anthos for bare metal, which, as its name indicates, brings support for Anthos Kubernetes engine on bare-metal server. This is a great opportunity to leverage specialized hardware or get rid of any kind of constraint on the VM hypervisor.\nPowerMax Thanks to the CSI driver can take full advantage of your Fiber Channel infrastructure and PowerMax end-to-end NVMe capability on Anthos bare metal. That type of architecture fits well with IO intensive workload and business critical application, often tight to transactional data.\nCheckout the installation process in video:\n PowerStore The save level of service is given to PowerStore with a full support for Anthos bare metal.\nCheckout the installation process in video:\n asdf-bmctl During the qualification process I had to juggle with at least 3 different versions of the Anthos bare metal installer.\nBeing sick of doing symlinks anytime I needed to change version, I wrote an asdf plugin to list the available versions, install them, and attach an Anthos bare metal configuration to a specific version.\nYou can :\n install it with asdf plugin-add bmctl git@eos2git.cec.lab.emc.com:coulof/asdf-bmctl.git list the versions with asdf list-all bmctl install them with asdf install bmctl 1.6.0 and then you can set your version locally asdf local bmctl 1.6.0 or globally asdf global bmctl 1.6.0.  Go further If you need a demo or have any question on Dell CSI drivers with Anthos reach out the Dell container community website\n","excerpt":"The partnership between Dell Technologies and Google to support Anthos …","ref":"/csm-docs/blog/2020/10/30/google-anthos-announcements/","title":"Google Anthos announcements"},{"body":"The quaterly update for Dell CSI Driver is there !\n New features  Across portfolio Volume Cloning Volume Expansion online and offline Raw Block Support RedHat CoreOS Docker EE 3.1 Dell CSI Operator CSI Driver for PowerMax CSI Driver for PowerStore CSI Driver for PowerFlex   One more thing ; Ansible for PowerStore v1.1 Useful links  New features Across portfolio This release gives for every driver the :\n Support of OpenShift 4.4 as well as Kubernetes 1.17, 1.18, 1.19 Support for Kubernetes Volume Snapshot Beta API New installer !  With Volume Snapshot’s promotion to beta, one significant change is the CSI external-snapshotter sidecar has been split into two controllers, a common snapshot controller and a CSI external-snapshotter sidecar.\nThe new install script available under dell-csi-helm-installer/csi-install.sh will :\n By default, install of the external-snaphotter for CSI driver. Optionally, install the beta snapshot CRD when the option --snapshot-crd is set during the initial installation.  Most recent Kubernetes distributions like OpenShift or GKE come with the common snapshotter controller installed.\nFor Kubernetes vanilla, you have to deploy the common snapshotter manually. The instructions are available here.\n /!\\ The drivers have validated the external-snapshotter version 1.2 and not the bleeding-edge version\n Volume Cloning Volume cloning is now available for every driver, but PowerFlex (that feature is on the roadmap).\nIt never has been easier [to spin a new environement from the production](({% post_url {% post_url note/dell/2020-05-29-gitlab-powermax %})).\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clone-pvc-0spec:accessModes:- ReadWriteOnceresources:requests:storage:5GistorageClassName:powermaxdataSource:kind:PersistentVolumeClaimname:pvc-0In the PVC definition you must make sure the source of the clone has the same storageClassName, request.storage size, namespace and accessModes.\nVolume Expansion online and offline That feature was already present in csi-powerscale ; it is now available for every Dell CSI driver.\nTo expand a volume, you just have to edit the PV size ; blazing fast example below: {: .size-small}\nRaw Block Support The Raw Block Support was already available with csi-powermax ; it is now available in csi-vxflexos and csi-powerstore.\nThat feature can be used if your application needs a filesystem different from xfs or ext4 or applications that can take advantage of a block device (like HDFS, Oracle ASM, etc.).\nRedHat CoreOS But for PowerFlex, every driver has been qualified with OpenShift 4.3 and 4.4 on CoreOS type of nodes !\nDocker EE 3.1 Docker Enterprise Edition (now part of Mirantis) makes his appearance to the list of officially supported by support.dell.com Kubernetes distributions.\nThe first drivers to qualify Docker EE are : csi-powerscale, csi-unity and csi-vxflexos.\nDell CSI Operator The dell-csi-operator adds support for the installation of the csi-powerstore and the multi-array support for csi-unity.\n At the moment of the publication, the new operator is under the RedHat certification process to get official support. The version 1.1 is not available yet in OperatorHub.io or OpenShift UI. Stay tuned for the update.\n CSI Driver for PowerMax Upon installation, we can enable the CSI PowerMax Reverse Proxy service. The CSI PowerMax Reverse Proxy is a reverse proxy that forwards CSI driver requests to Unisphere servers.\nIt can be used to improve reliability by having redundant Unisphere, or scale-up the number of requests to be sent to Unisphere and the managed PowerMax arrays.\nCSI Driver for PowerStore The csi-powerstore adds NFS to the list of supported protocols. It has all the features that iSCSI and Fiber Channel storage classes have.\nIf you need concurrent filesystem access (i.e. ReadWriteMany access mode) you can use the NFS protocol.\nCSI Driver for PowerFlex The csi-vxflexos is the first driver to bring topology support. It avoids the driver tried to mount a volume when the SDC is not installed (I see you non-CoreOS support ;-))\nOne more thing ; Ansible for PowerStore v1.1 The biggest “hot new feature” is the support for file operation in ansible-powerstore; this means we have access to new modules for:\n File system Snapshot File system NAS server NFS export SMB Share Quota  And of course all the modules conform to Ansible Idempotency requirement.\nUseful links For more details you can check :\n The product guides and release notes in the repositories for csi-powermax, csi-powerscale, csi-powerstore, csi-unity, csi-vxflexos and ansible-powerstore. The FAQs on on Dell container community website:  FAQ for CSI Driver for PowerMax FAQ for CSI Driver for PowerScale FAQ for CSI Driver for PowerStore FAQ for CSI Driver for PowerFlex FAQ for CSI Driver for Unity    ","excerpt":"The quaterly update for Dell CSI Driver is there !\n New features …","ref":"/csm-docs/blog/2020/09/28/csi-drivers-volume-expansion-and-beta-snapshot-support-update/","title":"CSI drivers Volume expansion and beta Snapshot support update !"},{"body":"Every quarter Dell Technologies ships new versions of his CSI Drivers and Ansible modules.\n Dell EMC has anounced new set of CSI Drivers for their storage arrays. Some highliths for these June 2020 releases:\n Qualifications for OpenShift 4.3 and Kubernetes 1.16 for all the drivers Easy upgrade with the CSI Operator for all the drivers Helm 3 support for all the drivers Multi-array support for PowerMax and Unity NFS support for Unity Volume expansion for Isilon Volume cloning for PowerMax CHAP for PowerMax   For the Ansible modules you will have:\n a brand new Ansible module for Unity ! Ansible for Isilon v1.1 brings support for SmartQuotas and is compatible with next OneFS major version.   For more details you can check :\n The product guides and release notes in the repositories for csi-powermax v1.3, csi-isilon v1.2, csi-unity v1.2, csi-vxflexos v1.1.5, ansible-unity v1.0 and ansible-isilon v1.1 The FAQs on on Dell container community website:  FAQ for CSI Driver for PowerMax FAQ for CSI Driver for Unity FAQ for CSI Driver for VxFlexOS FAQ for CSI Driver for Isilon FAQ for Ansible Isilon    ","excerpt":"Every quarter Dell Technologies ships new versions of his CSI Drivers …","ref":"/csm-docs/blog/2020/06/15/june-2020-dell-storage-enablers-big-update/","title":"June 2020 Dell storage enablers big update !"},{"body":"PowerMax is now supported by Google Anthos 1.3\nKeep in mind the snapshot alpha version is not supported in GKE.\nMore details on the support matrix and installation steps, check the official announcement on Dell container community website.\n","excerpt":"PowerMax is now supported by Google Anthos 1.3\nKeep in mind the …","ref":"/csm-docs/blog/2020/05/27/anthos-1.3-qualification-for-powermax/","title":"Anthos 1.3 Qualification for PowerMax"},{"body":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes\n-Product Guide\nPowerMax v1.4 -Release Notes\n-Product Guide\nPowerFlex v1.2 -Release Notes\n-Product Guide\nPowerStore v1.1 -Release Notes\n-Product Guide\nUnity v1.3 -Release Notes\n-Product Guide\n","excerpt":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes …","ref":"/csm-docs/v1/csidriver/archives/","title":"Archives"},{"body":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes\n-Product Guide\nPowerMax v1.4 -Release Notes\n-Product Guide\nPowerFlex v1.2 -Release Notes\n-Product Guide\nPowerStore v1.1 -Release Notes\n-Product Guide\nUnity v1.3 -Release Notes\n-Product Guide\n","excerpt":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes …","ref":"/csm-docs/v2/csidriver/archives/","title":"Archives"},{"body":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes\n-Product Guide\nPowerMax v1.4 -Release Notes\n-Product Guide\nPowerFlex v1.2 -Release Notes\n-Product Guide\nPowerStore v1.1 -Release Notes\n-Product Guide\nUnity v1.3 -Release Notes\n-Product Guide\n","excerpt":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes …","ref":"/csm-docs/v3/csidriver/archives/","title":"Archives"},{"body":"The CSM Authorization module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. Please note, Dell CSM operator currently ONLY supports deploying CSM Authorization sidecar/container.\nPre-requisite Follow the instructions available in CSM Authorization for Configuring a Dell CSI Driver with CSM for Authorization.\n","excerpt":"The CSM Authorization module for supported Dell CSI Drivers can be …","ref":"/csm-docs/docs/deployment/csmoperator/modules/authorization/","title":"Authorization"},{"body":"The CSM Authorization module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. Please note, Dell CSM operator currently ONLY supports deploying CSM Authorization sidecar/container.\nPre-requisite Follow the instructions available in CSM Authorization for Configuring a Dell CSI Driver with CSM for Authorization.\n","excerpt":"The CSM Authorization module for supported Dell CSI Drivers can be …","ref":"/csm-docs/v1/deployment/csmoperator/modules/authorization/","title":"Authorization"},{"body":"Installing Authorization via Dell CSM Operator The Authorization module for supported Dell CSI Drivers can be installed via the Dell CSM Operator.\nTo deploy the Dell CSM Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver along with the module.\nInstall Authorization   Create the required Secrets as documented in the Helm chart procedure.\n  Follow the instructions available here to install the Dell CSI Driver via the CSM Operator. The module section in the ContainerStorageModule CR should be updated to enable Authorization.\n  ","excerpt":"Installing Authorization via Dell CSM Operator The Authorization …","ref":"/csm-docs/v2/deployment/csmoperator/modules/authorization/","title":"Authorization"},{"body":"This is the blog section. It has two categories: News and Releases.\n","excerpt":"This is the blog section. It has two categories: News and Releases.\n","ref":"/csm-docs/blog/","title":"Blog"},{"body":"   Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being created. If some mandatory environment variables are missing or there is a type mismatch, then the Operator will report an error during the reconciliation attempts.\nBecause of this, the status of the Custom Resource will change to “Failed” and the error captured in the “ErrorMessage” field in the status.\nFor example - If the PowerMax driver was installed in the namespace test-powermax and has the name powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the Custom Resource details.\nIf there was an error while installing the driver, then you would see a status like this:\nstatus:status:errorMessage:mandatoryEnv- X_CSI_K8S_CLUSTER_PREFIXnotspecifiedinuserspecstate:FailedThe state of the Custom Resource can also change to Failed because of any other prohibited updates or any failure while installing the driver. In order to recover from this failure, fix the error in the manifest and update/patch the Custom Resource\n  After an update to the driver, the controller pod may not have the latest desired specification.\nThis happens when the controller pod was in a failed state before applying the update. Even though the Dell CSI Operator updates the pod template specification for the StatefulSet, the StatefulSet controller does not apply the update to the pod. This happens because of the unique nature of StatefulSets where the controller tries to retain the last known working state.\nTo get around this problem, the Dell CSI Operator forces an update of the pod specification by deleting the older pod. In case the Dell CSI Operator fails to do so, delete the controller pod to force an update of the controller pod specification\n  The Status of the CSI Driver Custom Resource shows the state of the driver pods after installation. This state will not be updated automatically if there are any changes to the driver pods outside any Operator operations.\nAt times because of inconsistencies in fetching data from the Kubernetes cache, the state of some driver pods may not be updated correctly in the status. To force an update of the state, you can update the Custom Resource forcefully by setting forceUpdate to true. If all the driver pods are in the Available State, then the state of the Custom Resource will be updated as Running\n  ","excerpt":"   Before installing the drivers, Dell CSI Operator tries to validate …","ref":"/csm-docs/docs/csidriver/troubleshooting/operator/","title":"Dell CSI Operator"},{"body":"   Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being created. If some mandatory environment variables are missing or there is a type mismatch, then the Operator will report an error during the reconciliation attempts.\nBecause of this, the status of the Custom Resource will change to “Failed” and the error captured in the “ErrorMessage” field in the status.\nFor example - If the PowerMax driver was installed in the namespace test-powermax and has the name powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the Custom Resource details.\nIf there was an error while installing the driver, then you would see a status like this:\nstatus:status:errorMessage:mandatoryEnv- X_CSI_K8S_CLUSTER_PREFIXnotspecifiedinuserspecstate:FailedThe state of the Custom Resource can also change to Failed because of any other prohibited updates or any failure while installing the driver. In order to recover from this failure, fix the error in the manifest and update/patch the Custom Resource\n  After an update to the driver, the controller pod may not have the latest desired specification.\nThis happens when the controller pod was in a failed state before applying the update. Even though the Dell CSI Operator updates the pod template specification for the StatefulSet, the StatefulSet controller does not apply the update to the pod. This happens because of the unique nature of StatefulSets where the controller tries to retain the last known working state.\nTo get around this problem, the Dell CSI Operator forces an update of the pod specification by deleting the older pod. In case the Dell CSI Operator fails to do so, delete the controller pod to force an update of the controller pod specification\n  The Status of the CSI Driver Custom Resource shows the state of the driver pods after installation. This state will not be updated automatically if there are any changes to the driver pods outside any Operator operations.\nAt times because of inconsistencies in fetching data from the Kubernetes cache, the state of some driver pods may not be updated correctly in the status. To force an update of the state, you can update the Custom Resource forcefully by setting forceUpdate to true. If all the driver pods are in the Available State, then the state of the Custom Resource will be updated as Running\n  ","excerpt":"   Before installing the drivers, Dell CSI Operator tries to validate …","ref":"/csm-docs/v1/csidriver/troubleshooting/operator/","title":"Dell CSI Operator"},{"body":"   Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being created. If some mandatory environment variables are missing or there is a type mismatch, then the Operator will report an error during the reconciliation attempts.\nBecause of this, the status of the Custom Resource will change to “Failed” and the error captured in the “ErrorMessage” field in the status.\nFor example - If the PowerMax driver was installed in the namespace test-powermax and has the name powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the Custom Resource details.\nIf there was an error while installing the driver, then you would see a status like this:\nstatus:status:errorMessage:mandatoryEnv- X_CSI_K8S_CLUSTER_PREFIXnotspecifiedinuserspecstate:FailedThe state of the Custom Resource can also change to Failed because of any other prohibited updates or any failure while installing the driver. In order to recover from this failure, fix the error in the manifest and update/patch the Custom Resource\n  After an update to the driver, the controller pod may not have the latest desired specification.\nThis happens when the controller pod was in a failed state before applying the update. Even though the Dell CSI Operator updates the pod template specification for the StatefulSet, the StatefulSet controller does not apply the update to the pod. This happens because of the unique nature of StatefulSets where the controller tries to retain the last known working state.\nTo get around this problem, the Dell CSI Operator forces an update of the pod specification by deleting the older pod. In case the Dell CSI Operator fails to do so, delete the controller pod to force an update of the controller pod specification\n  The Status of the CSI Driver Custom Resource shows the state of the driver pods after installation. This state will not be updated automatically if there are any changes to the driver pods outside any Operator operations.\nAt times because of inconsistencies in fetching data from the Kubernetes cache, the state of some driver pods may not be updated correctly in the status. To force an update of the state, you can update the Custom Resource forcefully by setting forceUpdate to true. If all the driver pods are in the Available State, then the state of the Custom Resource will be updated as Running\n  ","excerpt":"   Before installing the drivers, Dell CSI Operator tries to validate …","ref":"/csm-docs/v2/csidriver/troubleshooting/operator/","title":"Dell CSI Operator"},{"body":"   Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being created. If some mandatory environment variables are missing or there is a type mismatch, then the Operator will report an error during the reconciliation attempts.\nBecause of this, the status of the Custom Resource will change to “Failed” and the error captured in the “ErrorMessage” field in the status.\nFor example - If the PowerMax driver was installed in the namespace test-powermax and has the name powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the Custom Resource details.\nIf there was an error while installing the driver, then you would see a status like this:\nstatus:status:errorMessage:mandatoryEnv- X_CSI_K8S_CLUSTER_PREFIXnotspecifiedinuserspecstate:FailedThe state of the Custom Resource can also change to Failed because of any other prohibited updates or any failure while installing the driver. In order to recover from this failure, fix the error in the manifest and update/patch the Custom Resource\n  After an update to the driver, the controller pod may not have the latest desired specification.\nThis happens when the controller pod was in a failed state before applying the update. Even though the Dell CSI Operator updates the pod template specification for the StatefulSet, the StatefulSet controller does not apply the update to the pod. This happens because of the unique nature of StatefulSets where the controller tries to retain the last known working state.\nTo get around this problem, the Dell CSI Operator forces an update of the pod specification by deleting the older pod. In case the Dell CSI Operator fails to do so, delete the controller pod to force an update of the controller pod specification\n  The Status of the CSI Driver Custom Resource shows the state of the driver pods after installation. This state will not be updated automatically if there are any changes to the driver pods outside any Operator operations.\nAt times because of inconsistencies in fetching data from the Kubernetes cache, the state of some driver pods may not be updated correctly in the status. To force an update of the state, you can update the Custom Resource forcefully by setting forceUpdate to true. If all the driver pods are in the Available State, then the state of the Custom Resource will be updated as Running\n  ","excerpt":"   Before installing the drivers, Dell CSI Operator tries to validate …","ref":"/csm-docs/v3/csidriver/troubleshooting/operator/","title":"Dell CSI Operator"},{"body":" Welcome to Dell Technologies Container Storage Modules documentation! Learn More          ","excerpt":" Welcome to Dell Technologies Container Storage Modules documentation! …","ref":"/csm-docs/","title":"Dell Technologies"},{"body":" This document version is no longer actively maintained. The site that you are currently viewing is an archived snapshot. For up-to-date documentation, see the latest version\n The Dell Technologies (Dell) Container Storage Modules (CSM) enables simple and consistent integration and automation experiences, extending enterprise storage capabilities to Kubernetes for cloud-native stateful applications. It reduces management complexity so developers can independently consume enterprise storage with ease and automate daily operations such as provisioning, snapshotting, replication, observability, authorization and, resiliency.\nCSM is made up of multiple components including modules (enterprise capabilities), CSI drivers (storage enablement) and, other related applications (deployment, feature controllers, etc).\nCSM Supported Modules and Dell CSI Drivers    Modules/Drivers CSM 1.2.1 CSM 1.2 CSM 1.1 CSM 1.0.1     Authorization 1.2 1.2 1.1 1.0   Observability 1.1.1 1.1 1.0.1 1.0.1   Replication 1.2 1.2 1.1 1.0   Resiliency 1.1 1.1 1.0.1 1.0.1   CSI Driver for PowerScale v2.2 v2.2 v2.1 v2.0   CSI Driver for Unity v2.2 v2.2 v2.1 v2.0   CSI Driver for PowerStore v2.2 v2.2 v2.1 v2.0   CSI Driver for PowerFlex v2.2 v2.2 v2.1 v2.0   CSI Driver for PowerMax v2.2 v2.2 v2.1 v2.0    CSM Modules Support Matrix for Dell CSI Drivers    CSM Module CSI PowerFlex v2.2 CSI PowerScale v2.2 CSI PowerStore v2.2 CSI PowerMax v2.2 CSI Unity XT v2.2     Authorization v1.2 ✔️ ✔️ ❌ ✔️ ❌   Observability v1.1.1 ✔️ ❌ ✔️ ❌ ❌   Replication v1.2 ❌ ✔️ ✔️ ✔️ ❌   Resilency v1.1 ✔️ ❌ ❌ ❌ ✔️    ","excerpt":" This document version is no longer actively maintained. The site that …","ref":"/csm-docs/v1/","title":"Documentation"},{"body":" This document version is no longer actively maintained. The site that you are currently viewing is an archived snapshot. For up-to-date documentation, see the latest version\n The Dell Technologies (Dell) Container Storage Modules (CSM) enables simple and consistent integration and automation experiences, extending enterprise storage capabilities to Kubernetes for cloud-native stateful applications. It reduces management complexity so developers can independently consume enterprise storage with ease and automate daily operations such as provisioning, snapshotting, replication, observability, authorization and, resiliency.\nCSM is made up of multiple components including modules (enterprise capabilities), CSI drivers (storage enablement) and, other related applications (deployment, feature controllers, etc).\nCSM Supported Modules and Dell CSI Drivers    Modules/Drivers CSM 1.2 CSM 1.1 CSM 1.0.1 CSM 1.0     Authorization 1.2 1.1 1.0 1.0   Observability 1.1 1.0.1 1.0.1 1.0   Replication 1.2 1.1 1.0 1.0   Resiliency 1.1 1.0.1 1.0.1 1.0   CSI Driver for PowerScale v2.2 v2.1 v2.0 v2.0   CSI Driver for Unity v2.2 v2.1 v2.0 v2.0   CSI Driver for PowerStore v2.2 v2.1 v2.0 v2.0   CSI Driver for PowerFlex v2.2 v2.1 v2.0 v2.0   CSI Driver for PowerMax v2.2 v2.1 v2.0 v2.0    CSM Modules Support Matrix for Dell CSI Drivers    CSM Module CSI PowerFlex v2.2 CSI PowerScale v2.2 CSI PowerStore v2.2 CSI PowerMax v2.2 CSI Unity XT v2.2     Authorization v1.2 ✔️ ✔️ ❌ ✔️ ❌   Observability v1.1 ✔️ ❌ ✔️ ❌ ❌   Replication v1.2 ❌ ✔️ ✔️ ✔️ ❌   Resilency v1.1 ✔️ ❌ ❌ ❌ ✔️    ","excerpt":" This document version is no longer actively maintained. The site that …","ref":"/csm-docs/v2/","title":"Documentation"},{"body":" This document version is no longer actively maintained. The site that you are currently viewing is an archived snapshot. For up-to-date documentation, see the latest version\n The Dell Container Storage Modules (CSM) enables simple and consistent integration and automation experiences, extending enterprise storage capabilities to Kubernetes for cloud-native stateful applications. It reduces management complexity so developers can independently consume enterprise storage with ease and automate daily operations such as provisioning, snapshotting, replication, observability, authorization and, resiliency.\nCSM is made up of multiple components including modules (enterprise capabilities), CSI drivers (storage enablement) and, other related applications (deployment, feature controllers, etc).\nCSM Supported Modules and Dell EMC CSI Drivers    Modules/Drivers CSM 1.1 CSM 1.0 Previous Older     Authorization 1.1 1.0 - -   Observability 1.0 1.0 - -   Replication 1.1 1.0 - -   Resiliency 1.0 1.0 - -   CSI Driver for PowerScale v2.1 v2.0 v1.6 v1.5   CSI Driver for Unity v2.1 v2.0 v1.6 v1.5   CSI Driver for PowerStore v2.1 v2.0 v1.4 v1.3   CSI Driver for PowerFlex v2.1 v2.0 v1.5 v1.4   CSI Driver for PowerMax v2.1 v2.0 v1.7 v1.6    ","excerpt":" This document version is no longer actively maintained. The site that …","ref":"/csm-docs/v3/","title":"Documentation"},{"body":"CSM Authorization can be installed by using the provided Helm v3 charts on Kubernetes platforms.\nThe following CSM Authorization components are installed in the specified namespace:\n proxy-service, which forwards requests from the CSI Driver to the backend storage array tenant-service, which configures tenants, role bindings, and generates JSON Web Tokens role-service, which configures roles for tenants to be bound to storage-service, which configures backend storage arrays for the proxy-server to foward requests to  The folloiwng third-party components are installed in the specified namespace:\n redis, which stores data regarding tenants and their volume ownership, quota, and revokation status redis-commander, a web management tool for Redis  The following third-party components are optionally installed in the specified namespace:\n cert-manager, which optionally provides a self-signed certificate to configure the CSM Authorization Ingresses nginx-ingress-controller, which fulfills the CSM Authorization Ingresses  Install CSM Authorization Steps\n  Run git clone https://github.com/dell/helm-charts.git to clone the git repository.\n  Ensure that you have created a namespace where you want to install CSM Authorization. You can run kubectl create namespace authorization to create a new one.\n  Prepare samples/csm-authorization/config.yaml which contains the JWT signing secret. The following table lists the configuration parameters.\n   Parameter Description Required Default     web.jwtsigningsecret String used to sign JSON Web Tokens true secret    Example:\nweb:jwtsigningsecret:randomString123After editing the file, run the following command to create a secret called karavi-config-secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config.yaml=samples/csm-authorization/config.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic karavi-config-secret -n authorization --from-file=config=samples/csm-authorization/config.yaml -o yaml --dry-run=client | kubectl replace -f -\n  Copy the default values.yaml file cp charts/csm-authorization/values.yaml myvalues.yaml\n  Look over all the fields in myvalues.yaml and fill in/adjust any as needed.\n     Parameter Description Required Default     ingress-nginx This section configures the enablement of the NGINX Ingress Controller. - -   enabled Enable/Disable deployment of the NGINX Ingress Controller. Set to false if you already have an Ingress Controller installed. No true   cert-manager This section configures the enablement of cert-manager. - -   enabled Enable/Disable deployment of cert-manager. Set to false if you already have cert-manager installed. No true   authorization This section configures the CSM-Authorization components. - -   authorization.images.proxyService The image to use for the proxy-service. Yes dellemc/csm-authorization-proxy:nightly   authorization.images.tenantService The image to use for the tenant-service. Yes dellemc/csm-authorization-tenant:nightly   authorization.images.roleService The image to use for the role-service. Yes dellemc/csm-authorization-proxy:nightly   authorization.images.storageService The image to use for the storage-service. Yes dellemc/csm-authorization-storage:nightly   authorization.images.opa The image to use for Open Policy Agent. Yes openpolicyagent/opa   authorization.images.opaKubeMgmt The image to use for Open Policy Agent kube-mgmt. Yes openpolicyagent/kube-mgmt:0.11   authorization.hostname The hostname to configure the self-signed certificate (if applicable) and the proxy, tenant, role, and storage service Ingresses. Yes csm-authorization.com   authorization.logLevel CSM Authorization log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes debug   authorization.zipkin.collectoruri The URI of the Zipkin instance to export traces. No -   authorization.zipkin.probability The ratio of traces to export. No -   authorization.proxyServerIngress.ingressClassName The ingressClassName of the proxy-service Ingress. Yes -   authorization.proxyServerIngress.hosts Additional host rules to be applied to the proxy-service Ingress. No -   authorization.proxyServerIngress.annotations Additional annotations for the proxy-service Ingress. No -   authorization.tenantServiceIngress.ingressClassName The ingressClassName of the tenant-service Ingress. Yes -   authorization.tenantServiceIngress.hosts Additional host rules to be applied to the tenant-service Ingress. No -   authorization.tenantServiceIngress.annotations Additional annotations for the tenant-service Ingress. No -   authorization.roleServiceIngress.ingressClassName The ingressClassName of the role-service Ingress. Yes -   authorization.roleServiceIngress.hosts Additional host rules to be applied to the role-service Ingress. No -   authorization.roleServiceIngress.annotations Additional annotations for the role-service Ingress. No -   authorization.storageServiceIngress.ingressClassName The ingressClassName of the storage-service Ingress. Yes -   authorization.storageServiceIngress.hosts Additional host rules to be applied to the storage-service Ingress. No -   authorization.storageServiceIngress.annotations Additional annotations for the storage-service Ingress. No -   redis This section configures Redis. - -   redis.images.redis The image to use for Redis. Yes redis:6.0.8-alpine   redis.images.commander The image to use for Redis Commander. Yes rediscommander/redis-commander:latest   redis.storageClass The storage class for Redis to use for persistence. If not supplied, the default storage class is used. No -    NOTE:\n The tenant, role, and storage services use GRPC. If the Ingress Controller requires annotations to support GRPC, they must be supplied.  Install the driver using helm.  To install CSM Authorization with the service Ingresses using your own certificate, run:\nhelm -n authorization install authorization -f myvalues.yaml charts/csm-authorization \\ --set-file authorization.certificate=\u003clocation-of-certificate-file\u003e \\ --set-file authorization.privateKey=\u003clocation-of-private-key-file\u003e To install CSM Authorization with the service Ingresses using a self-signed certificate generated via cert-manager, run:\nhelm -n authorization install authorization -f myvalues.yaml charts/csm-authorization Configuring the CSM Authorization Proxy Server The storage administrator must first configure the proxy server with the following:\n Storage systems Tenants Roles Role bindings  This is done using karavictl to connect to the storage, tenant, and role services. In this example, we will be referencing an installation using csm-authorization.com as the authorization.hostname value and the NGINX Ingress Controller accessed via the cluster’s master node.\nRun kubectl -n authorization get ingress and kubectl -n authorization get service to see the Ingress rules for these services and the exposed port for accessing these services via the LoadBalancer. For example:\n# kubectl -n authorization get ingress NAME CLASS HOSTS ADDRESS PORTS AGE proxy-server nginx csm-authorization.com 80, 443 86s role-service nginx role.csm-authorization.com 80, 443 86s storage-service nginx storage.csm-authorization.com 80, 443 86s tenant-service nginx tenant.csm-authorization.com 80, 443 86s # kubectl -n auth get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE authorization-cert-manager ClusterIP 10.104.35.150 \u003cnone\u003e 9402/TCP 28s authorization-cert-manager-webhook ClusterIP 10.97.179.94 \u003cnone\u003e 443/TCP 27s authorization-ingress-nginx-controller LoadBalancer 10.108.115.217 \u003cpending\u003e 80:30080/TCP,443:30016/TCP 27s authorization-ingress-nginx-controller-admission ClusterIP 10.103.143.215 \u003cnone\u003e 443/TCP 27s proxy-server ClusterIP 10.111.86.51 \u003cnone\u003e 8080/TCP 28s redis ClusterIP 10.111.158.17 \u003cnone\u003e 6379/TCP 28s redis-commander ClusterIP 10.107.22.41 \u003cnone\u003e 8081/TCP 27s role-service ClusterIP 10.96.113.230 \u003cnone\u003e 50051/TCP 27s storage-service ClusterIP 10.101.144.37 \u003cnone\u003e 50051/TCP 27s tenant-service ClusterIP 10.109.60.141 \u003cnone\u003e 50051/TCP 28s On the machine running karavictl, the /etc/hosts file needs to be updated with the Ingress hosts for the storage, tenant, and role services. For example:\n\u003cmaster_node_ip\u003e tenant.csm-authorization.com \u003cmaster_node_ip\u003e role.csm-authorization.com \u003cmaster_node_ip\u003e storage.csm-authorization.com The port that exposes these services is 30016.\nConfigure Storage A storage entity in CSM Authorization consists of the storage type (PowerFlex, PowerMax, PowerScale), the system ID, the API endpoint, and the credentials. For example, to create PowerFlex storage:\nkaravictl storage create --type powerflex --endpoint https://10.0.0.1 --system-id 11e4e7d35817bd0f --user admin --password Password123 --insecure --array-insecure --addr storage.csm-authorization.com:30016 NOTE:\n The insecure flag specifies to skip certificate validation when connecting to the CSM Authorization storage service. The array-insecure flag specifies to skip certificate validation when proxy-service connects to the backend storage array. Run karavictl storage create --help for help.  Configuring Tenants A tenant is a Kubernetes cluster that a role will be bound to. For example, to create a tenant named Finance:\nkaravictl tenant create --name Finance --insecure --addr tenant.csm-authorization.com:30016 NOTE:\n The insecure flag specifies to skip certificate validation when connecting to the tenant service. Run karavictl tenant create --help for help.  Configuring Roles A role consists of a name, the storage to use, and the quota limit for the storage pool to be used. For example, to create a role named FinanceRole using the PowerFlex storage created above with a quota limit of 100GB in storage pool myStoragePool:\nkaravictl role create --insecure --addr role.csm-authorization.com:30016 --role=FinanceRole=powerflex=11e4e7d35817bd0f=myStoragePool=100GB NOTE:\n The insecure flag specifies to skip certificate validation when connecting to the role service. Run karavictl role create --help for help.  Configuring Role Bindings A role binding binds a role to a tenant. For example, to bind the FinanceRole to the Finance tenant:\nkaravictl rolebinding create --tenant Finance --role FinanceRole --insecure --addr tenant.csm-authorization.com:30016 NOTE:\n The insecure flag specifies to skip certificate validation when connecting to the tenant service. Run karavictl rolebinding create --help for help.  Generating a Token Now that the tenant is bound to a role, a JSON Web Token can be generated for the tenant. For example, to generate a token for the Finance tenant:\nkaravictl generate token --tenant Finance --insecure --addr --addr tenant.csm-authorization.com:30016 { \"Token\": \"\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n name: proxy-authz-tokens\\ntype: Opaque\\ndata:\\n access: ZXlKaGJHY2lPaUpJVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SmhkV1FpT2lKcllYSmhkbWtpTENKbGVIQWlPakUyTlRNek1qUXhPRFlzSW1keWIzVndJam9pWm05dklpd2lhWE56SWpvaVkyOXRMbVJsYkd3dWEyRnlZWFpwSWl3aWNtOXNaWE1pT2lKaVlYSWlMQ0p6ZFdJaU9pSnJZWEpoZG1rdGRHVnVZVzUwSW4wLmJIODN1TldmaHoxc1FVaDcweVlfMlF3N1NTVnEyRzRKeGlyVHFMWVlEMkU=\\n refresh: ZXlKaGJHY2lPaUpJVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SmhkV1FpT2lKcllYSmhkbWtpTENKbGVIQWlPakUyTlRVNU1UWXhNallzSW1keWIzVndJam9pWm05dklpd2lhWE56SWpvaVkyOXRMbVJsYkd3dWEyRnlZWFpwSWl3aWNtOXNaWE1pT2lKaVlYSWlMQ0p6ZFdJaU9pSnJZWEpoZG1rdGRHVnVZVzUwSW4wLkxNbWVUSkZlX2dveXR0V0lUUDc5QWVaTy1kdmN5SHAwNUwyNXAtUm9ZZnM=\\n\" } With jq, you process the above response to filter the secret manifest. For example:\nkaravictl generate token --tenant Finance --insecure --addr --addr tenant.csm-authorization.com:30016 | jq -r '.Token' apiVersion: v1 kind: Secret metadata: name: proxy-authz-tokens type: Opaque data: access: ZXlKaGJHY2lPaUpJVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SmhkV1FpT2lKcllYSmhkbWtpTENKbGVIQWlPakUyTlRNek1qUTFOekVzSW1keWIzVndJam9pWm05dklpd2lhWE56SWpvaVkyOXRMbVJsYkd3dWEyRnlZWFpwSWl3aWNtOXNaWE1pT2lKaVlYSWlMQ0p6ZFdJaU9pSnJZWEpoZG1rdGRHVnVZVzUwSW4wLk4tNE42Q1pPbUptcVQtRDF5ZkNGdEZqSmRDRjcxNlh1SXlNVFVyckNOS1U= refresh: ZXlKaGJHY2lPaUpJVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SmhkV1FpT2lKcllYSmhkbWtpTENKbGVIQWlPakUyTlRVNU1UWTFNVEVzSW1keWIzVndJam9pWm05dklpd2lhWE56SWpvaVkyOXRMbVJsYkd3dWEyRnlZWFpwSWl3aWNtOXNaWE1pT2lKaVlYSWlMQ0p6ZFdJaU9pSnJZWEpoZG1rdGRHVnVZVzUwSW4wLkVxb3lXNld5ZEFLdU9mSmtkMkZaMk9TVThZMzlKUFc0YmhfNHc5R05ZNmM= This secret must be applied in the driver namespace. Continue reading the next section for configuring the driver to use CSM Authorization.\nConfiguring a Dell CSI Driver with CSM for Authorization The second part of CSM for Authorization deployment is to configure one or more of the supported CSI drivers. This is controlled by the Kubernetes tenant admin.\nConfiguring a Dell CSI Driver Given a setup where Kubernetes, a storage system, and the CSM for Authorization Proxy Server are deployed, follow the steps below to configure the CSI Drivers to work with the Authorization sidecar:\n  Apply the secret containing the token data into the driver namespace. It’s assumed that the Kubernetes administrator has the token secret manifest saved in /tmp/token.yaml.\n# It is assumed that array type powermax has the namespace \"powermax\", powerflex has the namepace \"vxflexos\", and powerscale has the namespace \"isilon\". kubectl apply -f /tmp/token.yaml -n powermax kubectl apply -f /tmp/token.yaml -n vxflexos kubectl apply -f /tmp/token.yaml -n isilon   Edit the following parameters in samples/secret/karavi-authorization-config.json file in CSI PowerFlex, CSI PowerMax, or CSI PowerScale driver and update/add connection information for one or more backend storage arrays. In an instance where multiple CSI drivers are configured on the same Kubernetes cluster, the port range in the endpoint parameter must be different for each driver.\n     Parameter Description Required Default     username Username for connecting to the backend storage array. This parameter is ignored. No -   password Password for connecting to to the backend storage array. This parameter is ignored. No -   intendedEndpoint HTTPS REST API endpoint of the backend storage array. Yes -   endpoint HTTPS localhost endpoint that the authorization sidecar will listen on. Yes https://localhost:9400   systemID System ID of the backend storage array. Yes \" \"   insecure A boolean that enables/disables certificate validation of the backend storage array. This parameter is not used. No true   isDefault A boolean that indicates if the array is the default array. This parameter is not used. No default value from values.yaml    Create the karavi-authorization-config secret using the following command:\nkubectl -n [CSI_DRIVER_NAMESPACE] create secret generic karavi-authorization-config --from-file=config=samples/secret/karavi-authorization-config.json -o yaml --dry-run=client | kubectl apply -f -\n Note:\n Create the driver secret as you would normally except update/add the connection information for communicating with the sidecar instead of the backend storage array and scrub the username and password For PowerScale, the systemID will be the clusterName of the array.  The isilon-creds secret has a mountEndpoint parameter which must be set to the hostname or IP address of the PowerScale OneFS API server, for example, 10.0.0.1.      Create the proxy-server-root-certificate secret.\nIf running in insecure mode, create the secret with empty data:\nkubectl -n [CSI_DRIVER_NAMESPACE] create secret generic proxy-server-root-certificate --from-literal=rootCertificate.pem= -o yaml --dry-run=client | kubectl apply -f -\nOtherwise, create the proxy-server-root-certificate secret with the appropriate file:\nkubectl -n [CSI_DRIVER_NAMESPACE] create secret generic proxy-server-root-certificate --from-file=rootCertificate.pem=/path/to/rootCA -o yaml --dry-run=client | kubectl apply -f -\n   Note: Follow the steps below for additional configurations to one or more of the supported CSI drivers.\n PowerFlex Please refer to step 5 in the installation steps for PowerFlex to edit the parameters in samples/config.yaml file to communicate with the sidecar.\n  Update endpoint to match the endpoint set in samples/secret/karavi-authorization-config.json\n  Create vxflexos-config secret using the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.yaml -o yaml --dry-run=client | kubectl apply -f -\n  Please refer to step 9 in the installation steps for PowerFlex to edit the parameters in myvalues.yaml file to communicate with the sidecar.\n Enable CSM for Authorization and provide proxyHost address\n  Install the CSI PowerFlex driver\n  PowerMax Please refer to step 7 in the installation steps for PowerMax to edit the parameters in my-powermax-settings.yaml to communicate with the sidecar.\n  Update endpoint to match the endpoint set in samples/secret/karavi-authorization-config.json\n  Enable CSM for Authorization and provide proxyHost address\n  Install the CSI PowerMax driver\n  PowerScale Please refer to step 5 in the installation steps for PowerScale to edit the parameters in my-isilon-settings.yaml to communicate with the sidecar.\n Update endpointPort to match the endpoint port number set in samples/secret/karavi-authorization-config.json  Notes:\n  In my-isilon-settings.yaml, endpointPort acts as a default value. If endpointPort is not specified in my-isilon-settings.yaml, then it should be specified in the endpoint parameter of samples/secret/secret.yaml. The isilon-creds secret has a mountEndpoint parameter which must be set to the hostname or IP address of the PowerScale OneFS API server, for example, 10.0.0.1.   Enable CSM for Authorization and provide proxyHost address  Please refer to step 6 in the installation steps for PowerScale to edit the parameters in samples/secret/secret.yaml file to communicate with the sidecar.\nUpdate endpoint to match the endpoint set in samples/secret/karavi-authorization-config.json   Note: Only add the endpoint port if it has not been set in my-isilon-settings.yaml.\n  Create the isilon-creds secret using the following command:\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\n  Install the CSI PowerScale driver\n  Updating CSM for Authorization Proxy Server Configuration CSM for Authorization has a subset of configuration parameters that can be updated dynamically:\n   Parameter Type Default Description     web.jwtsigningsecret String “secret” The secret used to sign JWT tokens    Updating configuration parameters can be done by editing the karavi-config-secret. The secret can be queried using k3s and kubectl like so:\nkubectl -n authorization get secret/karavi-config-secret\nTo update parameters, you must edit the base64 encoded data in the secret. The karavi-config-secret data can be decoded like so:\nkubectl -n authorization get secret/karavi-config-secret -o yaml | grep config.yaml | head -n 1 | awk '{print $2}' | base64 -d\nSave the output to a file or copy it to an editor to make changes. Once you are done with the changes, you must encode the data to base64. If your changes are in a file, you can encode it like so:\ncat \u003cfile\u003e | base64\nCopy the new, encoded data and edit the karavi-config-secret with the new data. Run this command to edit the secret:\nkubectl -n karavi edit secret/karavi-config-secret\nReplace the data in config.yaml under the data field with your new, encoded data. Save the changes and CSM Authorization will read the changed secret.\n Note: If you are updating the signing secret, the tenants need to be updated with new tokens via the karavictl generate token command.\n CSM for Authorization Proxy Server Dynamic Configuration Settings Some settings are not stored in the karavi-config-secret but in the csm-config-params ConfigMap, such as LOG_LEVEL and LOG_FORMAT. To update the CSM Authorization logging settings during runtime, run the below command, make your changes, and save the updated configMap data.\nkubectl -n authorization edit configmap/csm-config-params This edit will not update the logging level for the sidecar-proxy containers running in the CSI Driver pods. To update the sidecar-proxy logging levels, you must update the associated CSI Driver ConfigMap in a similar fashion:\nkubectl -n [CSM_CSI_DRVIER_NAMESPACE] edit configmap/\u003crelease_name\u003e-config-params Using PowerFlex as an example, kubectl -n vxflexos edit configmap/vxflexos-config-params can be used to update the logging level of the sidecar-proxy and the driver.\n","excerpt":"CSM Authorization can be installed by using the provided Helm v3 …","ref":"/csm-docs/docs/authorization/deployment/helm/","title":"Helm"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Mirantis Kubernetes Engine (MKE).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn MKE-based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on MKE-backed clusters may run any of the OS which we support with upstream clusters.\nMKE UI Examples ","excerpt":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and …","ref":"/csm-docs/docs/csidriver/partners/docker/","title":"MKE"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Mirantis Kubernetes Engine (MKE).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn MKE-based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on MKE-backed clusters may run any of the OS which we support with upstream clusters.\nMKE UI Examples ","excerpt":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and …","ref":"/csm-docs/v1/csidriver/partners/docker/","title":"MKE"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Mirantis Kubernetes Engine (MKE).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn MKE-based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on MKE-backed clusters may run any of the OS which we support with upstream clusters.\nMKE UI Examples ","excerpt":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and …","ref":"/csm-docs/v2/csidriver/partners/docker/","title":"MKE"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Mirantis Kubernetes Engine (MKE).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn MKE-based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on MKE-backed clusters may run any of the OS which we support with upstream clusters.\nMKE UI Examples ","excerpt":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and …","ref":"/csm-docs/v3/csidriver/partners/docker/","title":"MKE"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\n PowerFlex PowerMax PowerScale PowerStore Unity XT  As well as the Dell CSI Operator\n Dell CSI Operator  Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\n One Linux-based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry  If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry.    One of these will be required on both the system building the offline bundle as well as the system preparing for installation.    Tested version(s) are docker 19.03+ and podman 1.6.4+   git git will be used to manually clone one of the above repositories in order to create an offline bundle.    This is only needed on the system preparing the offline bundle.    Tested version(s) are git 1.8+ but any version should work.    Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\n Build an offline bundle Unpacking the offline bundle created in Step 1 and preparing for installation Perform either a Helm installation or Operator installation using the files obtained after unpacking in Step 2  NOTE: It is recommended to use the same build tool for packing and unpacking of images (either docker or podman).\nBuilding an offline bundle This needs to be performed on a Linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\n Perform a git clone of the desired repository. For a helm-based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle   For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory  The script will perform the following steps:\n Determine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator  The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\ngit clone https://github.com/dell/dell-csi-operator.git cd dell-csi-operator [root@user scripts]# ./csi-offline-bundle.sh -c * * Pulling and saving container images dellemc/csi-isilon:v2.0.0 dellemc/csi-isilon:v2.1.0 dellemc/csipowermax-reverseproxy:v1.4.0 dellemc/csi-powermax:v2.0.0 dellemc/csi-powermax:v2.1.0 dellemc/csi-powerstore:v2.0.0 dellemc/csi-powerstore:v2.1.0 dellemc/csi-unity:v2.0.0 dellemc/csi-unity:v2.1.0 localregistry:5028/csi-unity/csi-unity:20220303110841 dellemc/csi-vxflexos:v2.0.0 dellemc/csi-vxflexos:v2.1.0 localregistry:5035/csi-operator/dell-csi-operator:v1.7.0 dellemc/sdc:3.5.1.1 dellemc/sdc:3.5.1.1-1 dellemc/sdc:3.6 docker.io/busybox:1.32.0 ... ... * * Copying necessary files /root/dell-csi-operator/driverconfig /root/dell-csi-operator/deploy /root/dell-csi-operator/samples /root/dell-csi-operator/scripts /root/dell-csi-operator/OLM.md /root/dell-csi-operator/README.md /root/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/driverconfig/ dell-csi-operator-bundle/driverconfig/config.yaml dell-csi-operator-bundle/driverconfig/isilon_v200_v119.json dell-csi-operator-bundle/driverconfig/isilon_v200_v120.json dell-csi-operator-bundle/driverconfig/isilon_v200_v121.json dell-csi-operator-bundle/driverconfig/isilon_v200_v122.json dell-csi-operator-bundle/driverconfig/isilon_v210_v120.json dell-csi-operator-bundle/driverconfig/isilon_v210_v121.json dell-csi-operator-bundle/driverconfig/isilon_v210_v122.json dell-csi-operator-bundle/driverconfig/isilon_v220_v121.json dell-csi-operator-bundle/driverconfig/isilon_v220_v122.json dell-csi-operator-bundle/driverconfig/isilon_v220_v123.json dell-csi-operator-bundle/driverconfig/powermax_v200_v119.json ... ... * * Complete Offline bundle file is: /root/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking the offline bundle and preparing for installation This needs to be performed on a Linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for the driver or Operator installation, the following steps need to be performed:\n Copy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option  The script will then perform the following steps:\n Load the required container images into the local system Tag the images according to the user-supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images  An example of preparing the bundle for installation (192.168.75.40:5000 refers to an image registry accessible to Kubernetes/OpenShift):\ntar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md cd dell-csi-operator-bundle [root@user scripts]# ./csi-offline-bundle.sh -p -r localregistry:5000/csi-operator Preparing a offline bundle for installation * * Loading docker images 5b1fa8e3e100: Loading layer [==================================================\u003e] 3.697MB/3.697MB e20ed4c73206: Loading layer [==================================================\u003e] 17.22MB/17.22MB Loaded image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.0 d72a74c56330: Loading layer [==================================================\u003e] 3.031MB/3.031MB f2d2ab12e2a7: Loading layer [==================================================\u003e] 48.08MB/48.08MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.2 417cb9b79ade: Loading layer [==================================================\u003e] 3.062MB/3.062MB 61fefb35ccee: Loading layer [==================================================\u003e] 16.88MB/16.88MB Loaded image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0 7a5b9c0b4b14: Loading layer [==================================================\u003e] 3.031MB/3.031MB 1555ad6e2d44: Loading layer [==================================================\u003e] 49.86MB/49.86MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 2de1422d5d2d: Loading layer [==================================================\u003e] 54.56MB/54.56MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1 25a1c1010608: Loading layer [==================================================\u003e] 54.54MB/54.54MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v2.2.2 07363fa84210: Loading layer [==================================================\u003e] 3.062MB/3.062MB 5227e51ea570: Loading layer [==================================================\u003e] 54.92MB/54.92MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v3.0.0 cfb5cbeabdb2: Loading layer [==================================================\u003e] 55.38MB/55.38MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v3.1.0 ... ... * * Tagging and pushing images localregistry:5035/csi-operator/dell-csi-operator:v1.7.0 -\u003e localregistry:5000/csi-operator/dell-csi-operator:v1.7.0 dellemc/csi-isilon:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.0.0 dellemc/csi-isilon:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.1.0 dellemc/csipowermax-reverseproxy:v1.4.0 -\u003e localregistry:5000/csi-operator/csipowermax-reverseproxy:v1.4.0 dellemc/csi-powermax:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.0.0 dellemc/csi-powermax:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.1.0 dellemc/csi-powerstore:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.0.0 dellemc/csi-powerstore:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.1.0 dellemc/csi-unity:nightly -\u003e localregistry:5000/csi-operator/csi-unity:nightly dellemc/csi-unity:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.0.0 dellemc/csi-unity:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.1.0 dellemc/csi-vxflexos:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.0.0 dellemc/csi-vxflexos:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.1.0 dellemc/sdc:3.5.1.1 -\u003e localregistry:5000/csi-operator/sdc:3.5.1.1 dellemc/sdc:3.5.1.1-1 -\u003e localregistry:5000/csi-operator/sdc:3.5.1.1-1 dellemc/sdc:3.6 -\u003e localregistry:5000/csi-operator/sdc:3.6 docker.io/busybox:1.32.0 -\u003e localregistry:5000/csi-operator/busybox:1.32.0 ... ... * * Preparing operator files within /root/dell-csi-operator-bundle changing: localregistry:5000/csi-operator/dell-csi-operator:v1.7.0 -\u003e localregistry:5000/csi-operator/dell-csi-operator:v1.7.0 changing: dellemc/csi-isilon:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.0.0 changing: dellemc/csi-isilon:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.1.0 changing: dellemc/csipowermax-reverseproxy:v1.4.0 -\u003e localregistry:5000/csi-operator/csipowermax-reverseproxy:v1.4.0 changing: dellemc/csi-powermax:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.0.0 changing: dellemc/csi-powermax:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.1.0 changing: dellemc/csi-powerstore:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.0.0 changing: dellemc/csi-powerstore:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.1.0 changing: dellemc/csi-unity:nightly -\u003e localregistry:5000/csi-operator/csi-unity:nightly changing: dellemc/csi-unity:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.0.0 changing: dellemc/csi-unity:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.1.0 changing: dellemc/csi-vxflexos:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.0.0 changing: dellemc/csi-vxflexos:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.1.0 changing: dellemc/sdc:3.5.1.1 -\u003e localregistry:5000/csi-operator/sdc:3.5.1.1 changing: dellemc/sdc:3.5.1.1-1 -\u003e localregistry:5000/csi-operator/sdc:3.5.1.1-1 changing: dellemc/sdc:3.6 -\u003e localregistry:5000/csi-operator/sdc:3.6 changing: docker.io/busybox:1.32.0 -\u003e localregistry:5000/csi-operator/busybox:1.32.0 ... ... * * Complete Perform either a Helm installation or Operator installation Now that the required images are available and the Helm Charts/Operator configuration updated, you can proceed by following the usual installation procedure as documented either via Helm or Operator.\nNOTES:\n Offline bundle installation is only supported with manual installs i.e. without using Operator Lifecycle Manager. Installation should be done using the files that are obtained after unpacking the offline bundle (dell-csi-operator-bundle.tar.gz) as the image tags in the manifests are modified to point to the internal registry. Offline bundle installs operator in default namespace via install.sh script. Make sure that the current context in kubeconfig file has the namespace set to default.  ","excerpt":"The csi-offline-bundle.sh script can be used to create a package …","ref":"/csm-docs/docs/csidriver/installation/offline/","title":"Offline Installation of Dell CSI Storage Providers"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\n PowerFlex PowerMax PowerScale PowerStore Unity  As well as the Dell CSI Operator\n Dell CSI Operator  Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\n One Linux-based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry  If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry.    One of these will be required on both the system building the offline bundle as well as the system preparing for installation.    Tested version(s) are docker 19.03+ and podman 1.6.4+   git git will be used to manually clone one of the above repositories in order to create an offline bundle.    This is only needed on the system preparing the offline bundle.    Tested version(s) are git 1.8+ but any version should work.    Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\n Build an offline bundle Unpacking the offline bundle created in Step 1 and preparing for installation Perform either a Helm installation or Operator installation using the files obtained after unpacking in Step 2  NOTE: It is recommended to use the same build tool for packing and unpacking of images (either docker or podman).\nBuilding an offline bundle This needs to be performed on a Linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\n Perform a git clone of the desired repository. For a helm-based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle   For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory  The script will perform the following steps:\n Determine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator  The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\ngit clone https://github.com/dell/dell-csi-operator.git cd dell-csi-operator [root@user scripts]# ./csi-offline-bundle.sh -c * * Pulling and saving container images dellemc/csi-isilon:v2.0.0 dellemc/csi-isilon:v2.1.0 dellemc/csipowermax-reverseproxy:v1.4.0 dellemc/csi-powermax:v2.0.0 dellemc/csi-powermax:v2.1.0 dellemc/csi-powerstore:v2.0.0 dellemc/csi-powerstore:v2.1.0 dellemc/csi-unity:v2.0.0 dellemc/csi-unity:v2.1.0 localregistry:5028/csi-unity/csi-unity:20220303110841 dellemc/csi-vxflexos:v2.0.0 dellemc/csi-vxflexos:v2.1.0 localregistry:5035/csi-operator/dell-csi-operator:v1.7.0 dellemc/sdc:3.5.1.1 dellemc/sdc:3.5.1.1-1 dellemc/sdc:3.6 docker.io/busybox:1.32.0 ... ... * * Copying necessary files /root/dell-csi-operator/driverconfig /root/dell-csi-operator/deploy /root/dell-csi-operator/samples /root/dell-csi-operator/scripts /root/dell-csi-operator/OLM.md /root/dell-csi-operator/README.md /root/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/driverconfig/ dell-csi-operator-bundle/driverconfig/config.yaml dell-csi-operator-bundle/driverconfig/isilon_v200_v119.json dell-csi-operator-bundle/driverconfig/isilon_v200_v120.json dell-csi-operator-bundle/driverconfig/isilon_v200_v121.json dell-csi-operator-bundle/driverconfig/isilon_v200_v122.json dell-csi-operator-bundle/driverconfig/isilon_v210_v120.json dell-csi-operator-bundle/driverconfig/isilon_v210_v121.json dell-csi-operator-bundle/driverconfig/isilon_v210_v122.json dell-csi-operator-bundle/driverconfig/isilon_v220_v121.json dell-csi-operator-bundle/driverconfig/isilon_v220_v122.json dell-csi-operator-bundle/driverconfig/isilon_v220_v123.json dell-csi-operator-bundle/driverconfig/powermax_v200_v119.json ... ... * * Complete Offline bundle file is: /root/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking the offline bundle and preparing for installation This needs to be performed on a Linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for the driver or Operator installation, the following steps need to be performed:\n Copy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option  The script will then perform the following steps:\n Load the required container images into the local system Tag the images according to the user-supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images  An example of preparing the bundle for installation (192.168.75.40:5000 refers to an image registry accessible to Kubernetes/OpenShift):\ntar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md cd dell-csi-operator-bundle [root@user scripts]# ./csi-offline-bundle.sh -p -r localregistry:5000/csi-operator Preparing a offline bundle for installation * * Loading docker images 5b1fa8e3e100: Loading layer [==================================================\u003e] 3.697MB/3.697MB e20ed4c73206: Loading layer [==================================================\u003e] 17.22MB/17.22MB Loaded image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.0 d72a74c56330: Loading layer [==================================================\u003e] 3.031MB/3.031MB f2d2ab12e2a7: Loading layer [==================================================\u003e] 48.08MB/48.08MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.2 417cb9b79ade: Loading layer [==================================================\u003e] 3.062MB/3.062MB 61fefb35ccee: Loading layer [==================================================\u003e] 16.88MB/16.88MB Loaded image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0 7a5b9c0b4b14: Loading layer [==================================================\u003e] 3.031MB/3.031MB 1555ad6e2d44: Loading layer [==================================================\u003e] 49.86MB/49.86MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 2de1422d5d2d: Loading layer [==================================================\u003e] 54.56MB/54.56MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1 25a1c1010608: Loading layer [==================================================\u003e] 54.54MB/54.54MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v2.2.2 07363fa84210: Loading layer [==================================================\u003e] 3.062MB/3.062MB 5227e51ea570: Loading layer [==================================================\u003e] 54.92MB/54.92MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v3.0.0 cfb5cbeabdb2: Loading layer [==================================================\u003e] 55.38MB/55.38MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v3.1.0 ... ... * * Tagging and pushing images localregistry:5035/csi-operator/dell-csi-operator:v1.7.0 -\u003e localregistry:5000/csi-operator/dell-csi-operator:v1.7.0 dellemc/csi-isilon:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.0.0 dellemc/csi-isilon:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.1.0 dellemc/csipowermax-reverseproxy:v1.4.0 -\u003e localregistry:5000/csi-operator/csipowermax-reverseproxy:v1.4.0 dellemc/csi-powermax:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.0.0 dellemc/csi-powermax:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.1.0 dellemc/csi-powerstore:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.0.0 dellemc/csi-powerstore:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.1.0 dellemc/csi-unity:nightly -\u003e localregistry:5000/csi-operator/csi-unity:nightly dellemc/csi-unity:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.0.0 dellemc/csi-unity:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.1.0 dellemc/csi-vxflexos:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.0.0 dellemc/csi-vxflexos:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.1.0 dellemc/sdc:3.5.1.1 -\u003e localregistry:5000/csi-operator/sdc:3.5.1.1 dellemc/sdc:3.5.1.1-1 -\u003e localregistry:5000/csi-operator/sdc:3.5.1.1-1 dellemc/sdc:3.6 -\u003e localregistry:5000/csi-operator/sdc:3.6 docker.io/busybox:1.32.0 -\u003e localregistry:5000/csi-operator/busybox:1.32.0 ... ... * * Preparing operator files within /root/dell-csi-operator-bundle changing: localregistry:5000/csi-operator/dell-csi-operator:v1.7.0 -\u003e localregistry:5000/csi-operator/dell-csi-operator:v1.7.0 changing: dellemc/csi-isilon:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.0.0 changing: dellemc/csi-isilon:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.1.0 changing: dellemc/csipowermax-reverseproxy:v1.4.0 -\u003e localregistry:5000/csi-operator/csipowermax-reverseproxy:v1.4.0 changing: dellemc/csi-powermax:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.0.0 changing: dellemc/csi-powermax:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.1.0 changing: dellemc/csi-powerstore:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.0.0 changing: dellemc/csi-powerstore:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.1.0 changing: dellemc/csi-unity:nightly -\u003e localregistry:5000/csi-operator/csi-unity:nightly changing: dellemc/csi-unity:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.0.0 changing: dellemc/csi-unity:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.1.0 changing: dellemc/csi-vxflexos:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.0.0 changing: dellemc/csi-vxflexos:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.1.0 changing: dellemc/sdc:3.5.1.1 -\u003e localregistry:5000/csi-operator/sdc:3.5.1.1 changing: dellemc/sdc:3.5.1.1-1 -\u003e localregistry:5000/csi-operator/sdc:3.5.1.1-1 changing: dellemc/sdc:3.6 -\u003e localregistry:5000/csi-operator/sdc:3.6 changing: docker.io/busybox:1.32.0 -\u003e localregistry:5000/csi-operator/busybox:1.32.0 ... ... * * Complete Perform either a Helm installation or Operator installation Now that the required images are available and the Helm Charts/Operator configuration updated, you can proceed by following the usual installation procedure as documented either via Helm or Operator.\nNOTES:\n Offline bundle installation is only supported with manual installs i.e. without using Operator Lifecycle Manager. Installation should be done using the files that are obtained after unpacking the offline bundle (dell-csi-operator-bundle.tar.gz) as the image tags in the manifests are modified to point to the internal registry. Offline bundle installs operator in default namespace via install.sh script. Make sure that the current context in kubeconfig file has the namespace set to default.  ","excerpt":"The csi-offline-bundle.sh script can be used to create a package …","ref":"/csm-docs/v1/csidriver/installation/offline/","title":"Offline Installation of Dell CSI Storage Providers"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\n PowerFlex PowerMax PowerScale PowerStore Unity  As well as the Dell CSI Operator\n Dell CSI Operator  Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\n One Linux-based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry  If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry.    One of these will be required on both the system building the offline bundle as well as the system preparing for installation.    Tested version(s) are docker 19.03+ and podman 1.6.4+   git git will be used to manually clone one of the above repositories in order to create an offline bundle.    This is only needed on the system preparing the offline bundle.    Tested version(s) are git 1.8+ but any version should work.    Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\n Build an offline bundle Unpacking the offline bundle created in Step 1 and preparing for installation Perform either a Helm installation or Operator installation using the files obtained after unpacking in Step 2  NOTE: It is recommended to use the same build tool for packing and unpacking of images (either docker or podman).\nBuilding an offline bundle This needs to be performed on a Linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\n Perform a git clone of the desired repository. For a helm-based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle   For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory  The script will perform the following steps:\n Determine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator  The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\ngit clone https://github.com/dell/dell-csi-operator.git cd dell-csi-operator [root@user scripts]# ./csi-offline-bundle.sh -c * * Pulling and saving container images dellemc/csi-isilon:v2.0.0 dellemc/csi-isilon:v2.1.0 dellemc/csipowermax-reverseproxy:v1.4.0 dellemc/csi-powermax:v2.0.0 dellemc/csi-powermax:v2.1.0 dellemc/csi-powerstore:v2.0.0 dellemc/csi-powerstore:v2.1.0 dellemc/csi-unity:v2.0.0 dellemc/csi-unity:v2.1.0 localregistry:5028/csi-unity/csi-unity:20220303110841 dellemc/csi-vxflexos:v2.0.0 dellemc/csi-vxflexos:v2.1.0 localregistry:5035/csi-operator/dell-csi-operator:v1.7.0 dellemc/sdc:3.5.1.1 dellemc/sdc:3.5.1.1-1 dellemc/sdc:3.6 docker.io/busybox:1.32.0 ... ... * * Copying necessary files /root/dell-csi-operator/driverconfig /root/dell-csi-operator/deploy /root/dell-csi-operator/samples /root/dell-csi-operator/scripts /root/dell-csi-operator/OLM.md /root/dell-csi-operator/README.md /root/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/driverconfig/ dell-csi-operator-bundle/driverconfig/config.yaml dell-csi-operator-bundle/driverconfig/isilon_v200_v119.json dell-csi-operator-bundle/driverconfig/isilon_v200_v120.json dell-csi-operator-bundle/driverconfig/isilon_v200_v121.json dell-csi-operator-bundle/driverconfig/isilon_v200_v122.json dell-csi-operator-bundle/driverconfig/isilon_v210_v120.json dell-csi-operator-bundle/driverconfig/isilon_v210_v121.json dell-csi-operator-bundle/driverconfig/isilon_v210_v122.json dell-csi-operator-bundle/driverconfig/isilon_v220_v121.json dell-csi-operator-bundle/driverconfig/isilon_v220_v122.json dell-csi-operator-bundle/driverconfig/isilon_v220_v123.json dell-csi-operator-bundle/driverconfig/powermax_v200_v119.json ... ... * * Complete Offline bundle file is: /root/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking the offline bundle and preparing for installation This needs to be performed on a Linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for the driver or Operator installation, the following steps need to be performed:\n Copy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option  The script will then perform the following steps:\n Load the required container images into the local system Tag the images according to the user-supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images  An example of preparing the bundle for installation (192.168.75.40:5000 refers to an image registry accessible to Kubernetes/OpenShift):\ntar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md cd dell-csi-operator-bundle [root@user scripts]# ./csi-offline-bundle.sh -p -r localregistry:5000/csi-operator Preparing a offline bundle for installation * * Loading docker images 5b1fa8e3e100: Loading layer [==================================================\u003e] 3.697MB/3.697MB e20ed4c73206: Loading layer [==================================================\u003e] 17.22MB/17.22MB Loaded image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.0 d72a74c56330: Loading layer [==================================================\u003e] 3.031MB/3.031MB f2d2ab12e2a7: Loading layer [==================================================\u003e] 48.08MB/48.08MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.2 417cb9b79ade: Loading layer [==================================================\u003e] 3.062MB/3.062MB 61fefb35ccee: Loading layer [==================================================\u003e] 16.88MB/16.88MB Loaded image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0 7a5b9c0b4b14: Loading layer [==================================================\u003e] 3.031MB/3.031MB 1555ad6e2d44: Loading layer [==================================================\u003e] 49.86MB/49.86MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 2de1422d5d2d: Loading layer [==================================================\u003e] 54.56MB/54.56MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1 25a1c1010608: Loading layer [==================================================\u003e] 54.54MB/54.54MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v2.2.2 07363fa84210: Loading layer [==================================================\u003e] 3.062MB/3.062MB 5227e51ea570: Loading layer [==================================================\u003e] 54.92MB/54.92MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v3.0.0 cfb5cbeabdb2: Loading layer [==================================================\u003e] 55.38MB/55.38MB Loaded image: k8s.gcr.io/sig-storage/csi-provisioner:v3.1.0 ... ... * * Tagging and pushing images localregistry:5035/csi-operator/dell-csi-operator:v1.7.0 -\u003e localregistry:5000/csi-operator/dell-csi-operator:v1.7.0 dellemc/csi-isilon:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.0.0 dellemc/csi-isilon:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.1.0 dellemc/csipowermax-reverseproxy:v1.4.0 -\u003e localregistry:5000/csi-operator/csipowermax-reverseproxy:v1.4.0 dellemc/csi-powermax:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.0.0 dellemc/csi-powermax:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.1.0 dellemc/csi-powerstore:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.0.0 dellemc/csi-powerstore:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.1.0 dellemc/csi-unity:nightly -\u003e localregistry:5000/csi-operator/csi-unity:nightly dellemc/csi-unity:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.0.0 dellemc/csi-unity:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.1.0 dellemc/csi-vxflexos:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.0.0 dellemc/csi-vxflexos:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.1.0 dellemc/sdc:3.5.1.1 -\u003e localregistry:5000/csi-operator/sdc:3.5.1.1 dellemc/sdc:3.5.1.1-1 -\u003e localregistry:5000/csi-operator/sdc:3.5.1.1-1 dellemc/sdc:3.6 -\u003e localregistry:5000/csi-operator/sdc:3.6 docker.io/busybox:1.32.0 -\u003e localregistry:5000/csi-operator/busybox:1.32.0 ... ... * * Preparing operator files within /root/dell-csi-operator-bundle changing: localregistry:5000/csi-operator/dell-csi-operator:v1.7.0 -\u003e localregistry:5000/csi-operator/dell-csi-operator:v1.7.0 changing: dellemc/csi-isilon:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.0.0 changing: dellemc/csi-isilon:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-isilon:v2.1.0 changing: dellemc/csipowermax-reverseproxy:v1.4.0 -\u003e localregistry:5000/csi-operator/csipowermax-reverseproxy:v1.4.0 changing: dellemc/csi-powermax:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.0.0 changing: dellemc/csi-powermax:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-powermax:v2.1.0 changing: dellemc/csi-powerstore:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.0.0 changing: dellemc/csi-powerstore:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-powerstore:v2.1.0 changing: dellemc/csi-unity:nightly -\u003e localregistry:5000/csi-operator/csi-unity:nightly changing: dellemc/csi-unity:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.0.0 changing: dellemc/csi-unity:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-unity:v2.1.0 changing: dellemc/csi-vxflexos:v2.0.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.0.0 changing: dellemc/csi-vxflexos:v2.1.0 -\u003e localregistry:5000/csi-operator/csi-vxflexos:v2.1.0 changing: dellemc/sdc:3.5.1.1 -\u003e localregistry:5000/csi-operator/sdc:3.5.1.1 changing: dellemc/sdc:3.5.1.1-1 -\u003e localregistry:5000/csi-operator/sdc:3.5.1.1-1 changing: dellemc/sdc:3.6 -\u003e localregistry:5000/csi-operator/sdc:3.6 changing: docker.io/busybox:1.32.0 -\u003e localregistry:5000/csi-operator/busybox:1.32.0 ... ... * * Complete Perform either a Helm installation or Operator installation Now that the required images are available and the Helm Charts/Operator configuration updated, you can proceed by following the usual installation procedure as documented either via Helm or Operator.\nNOTES:\n Offline bundle installation is only supported with manual installs i.e. without using Operator Lifecycle Manager. Installation should be done using the files that are obtained after unpacking the offline bundle (dell-csi-operator-bundle.tar.gz) as the image tags in the manifests are modified to point to the internal registry. Offline bundle installs operator in default namespace via install.sh script. Make sure that the current context in kubeconfig file has the namespace set to default.  ","excerpt":"The csi-offline-bundle.sh script can be used to create a package …","ref":"/csm-docs/v2/csidriver/installation/offline/","title":"Offline Installation of Dell CSI Storage Providers"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell EMC CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\n PowerFlex PowerMax PowerScale PowerStore Unity  As well as the Dell CSI Operator\n Dell CSI Operator  Dependencies Multiple Linux-based systems may be required to create and process an offline bundle for use.\n One Linux-based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One Linux-based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry  If one Linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry.    One of these will be required on both the system building the offline bundle as well as the system preparing for installation.    Tested version(s) are docker 19.03+ and podman 1.6.4+   git git will be used to manually clone one of the above repositories in order to create an offline bundle.    This is only needed on the system preparing the offline bundle.    Tested version(s) are git 1.8+ but any version should work.    Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\n Build an offline bundle Unpacking the offline bundle created in Step 1 and preparing for installation Perform either a Helm installation or Operator installation using the files obtained after unpacking in Step 2  Building an offline bundle This needs to be performed on a Linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\n Perform a git clone of the desired repository. For a helm-based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle   For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory  The script will perform the following steps:\n Determine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator  The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\n[user@anothersystem /home/user]# git clone https://github.com/dell/dell-csi-operator.git [user@anothersystem /home/user]# cd dell-csi-operator [user@system /home/user/dell-csi-operator]# scripts/csi-offline-bundle.sh -c * * Building image manifest file * * Pulling container images dellemc/csi-isilon:v1.4.0.000R dellemc/csi-isilon:v1.5.0 dellemc/csi-isilon:v1.6.0 dellemc/csipowermax-reverseproxy:v1.3.0 dellemc/csi-powermax:v1.5.0.000R dellemc/csi-powermax:v1.6.0 dellemc/csi-powermax:v1.7.0 dellemc/csi-powerstore:v1.2.0.000R dellemc/csi-powerstore:v1.3.0 dellemc/csi-powerstore:v1.4.0 dellemc/csi-unity:v1.4.0.000R dellemc/csi-unity:v1.5.0 dellemc/csi-unity:v1.6.0 dellemc/csi-vxflexos:v1.3.0.000R dellemc/csi-vxflexos:v1.4.0 dellemc/csi-vxflexos:v1.5.0 dellemc/dell-csi-operator:v1.4.0 dellemc/sdc:3.5.1.1 dellemc/sdc:3.5.1.1-1 docker.io/busybox:1.32.0 k8s.gcr.io/sig-storage/csi-attacher:v3.0.0 k8s.gcr.io/sig-storage/csi-attacher:v3.1.0 k8s.gcr.io/sig-storage/csi-attacher:v3.2.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.0.2 k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1 k8s.gcr.io/sig-storage/csi-resizer:v1.2.0 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3 k8s.gcr.io/sig-storage/csi-snapshotter:v4.0.0 k8s.gcr.io/sig-storage/csi-snapshotter:v4.1.0 quay.io/k8scsi/csi-resizer:v1.0.0 quay.io/k8scsi/csi-resizer:v1.1.0 * * Saving images * * Copying necessary files /dell/git/dell-csi-operator/config /dell/git/dell-csi-operator/deploy /dell/git/dell-csi-operator/samples /dell/git/dell-csi-operator/scripts /dell/git/dell-csi-operator/README.md /dell/git/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md * * Complete Offline bundle file is: /dell/git/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking the offline bundle and preparing for installation This needs to be performed on a Linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for the driver or Operator installation, the following steps need to be performed:\n Copy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u003cfilename\u003e Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option  The script will then perform the following steps:\n Load the required container images into the local system Tag the images according to the user-supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images  An example of preparing the bundle for installation (192.168.75.40:5000 refers to an image registry accessible to Kubernetes/OpenShift):\n[user@anothersystem /tmp]# tar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u003clisting of files included in bundle\u003e ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md [user@anothersystem /tmp]# cd dell-csi-operator-bundle [user@anothersystem /tmp/dell-csi-operator-bundle]# scripts/csi-offline-bundle.sh -p -r 192.168.75.40:5000/operator Preparing an offline bundle for installation * * Loading docker images * * Tagging and pushing images dellemc/csi-isilon:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.4.0.000R dellemc/csi-isilon:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.5.0 dellemc/csi-isilon:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.6.0 dellemc/csipowermax-reverseproxy:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csipowermax-reverseproxy:v1.3.0 dellemc/csi-powermax:v1.5.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.5.0.000R dellemc/csi-powermax:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.6.0 dellemc/csi-powermax:v1.7.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.7.0 dellemc/csi-powerstore:v1.2.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.2.0.000R dellemc/csi-powerstore:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.3.0 dellemc/csi-powerstore:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.4.0 dellemc/csi-unity:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.4.0.000R dellemc/csi-unity:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.5.0 dellemc/csi-unity:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.6.0 dellemc/csi-vxflexos:v1.3.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.3.0.000R dellemc/csi-vxflexos:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.4.0 dellemc/csi-vxflexos:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.5.0 dellemc/dell-csi-operator:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/dell-csi-operator:v1.4.0 dellemc/sdc:3.5.1.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1 dellemc/sdc:3.5.1.1-1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1-1 docker.io/busybox:1.32.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/busybox:1.32.0 k8s.gcr.io/sig-storage/csi-attacher:v3.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.0.0 k8s.gcr.io/sig-storage/csi-attacher:v3.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.1.0 k8s.gcr.io/sig-storage/csi-attacher:v3.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.2.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.0.1 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.1.0 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.2.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.0.2 k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.1.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.2.1 k8s.gcr.io/sig-storage/csi-resizer:v1.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.2.0 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.2 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.3 k8s.gcr.io/sig-storage/csi-snapshotter:v4.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.0.0 k8s.gcr.io/sig-storage/csi-snapshotter:v4.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.1.0 quay.io/k8scsi/csi-resizer:v1.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.0.0 quay.io/k8scsi/csi-resizer:v1.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.1.0 * * Preparing operator files within /tmp/dell-csi-operator-bundle changing: dellemc/csi-isilon:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.4.0.000R changing: dellemc/csi-isilon:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.5.0 changing: dellemc/csi-isilon:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-isilon:v1.6.0 changing: dellemc/csipowermax-reverseproxy:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csipowermax-reverseproxy:v1.3.0 changing: dellemc/csi-powermax:v1.5.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.5.0.000R changing: dellemc/csi-powermax:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.6.0 changing: dellemc/csi-powermax:v1.7.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powermax:v1.7.0 changing: dellemc/csi-powerstore:v1.2.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.2.0.000R changing: dellemc/csi-powerstore:v1.3.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.3.0 changing: dellemc/csi-powerstore:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-powerstore:v1.4.0 changing: dellemc/csi-unity:v1.4.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.4.0.000R changing: dellemc/csi-unity:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.5.0 changing: dellemc/csi-unity:v1.6.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-unity:v1.6.0 changing: dellemc/csi-vxflexos:v1.3.0.000R -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.3.0.000R changing: dellemc/csi-vxflexos:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.4.0 changing: dellemc/csi-vxflexos:v1.5.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-vxflexos:v1.5.0 changing: dellemc/dell-csi-operator:v1.4.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/dell-csi-operator:v1.4.0 changing: dellemc/sdc:3.5.1.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1 changing: dellemc/sdc:3.5.1.1-1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/sdc:3.5.1.1-1 changing: docker.io/busybox:1.32.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/busybox:1.32.0 changing: k8s.gcr.io/sig-storage/csi-attacher:v3.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.0.0 changing: k8s.gcr.io/sig-storage/csi-attacher:v3.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.1.0 changing: k8s.gcr.io/sig-storage/csi-attacher:v3.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-attacher:v3.2.1 changing: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.0.1 changing: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.1.0 changing: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-node-driver-registrar:v2.2.0 changing: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.0.2 changing: k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.1.0 changing: k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-provisioner:v2.2.1 changing: k8s.gcr.io/sig-storage/csi-resizer:v1.2.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.2.0 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.2 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v3.0.3 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v4.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.0.0 changing: k8s.gcr.io/sig-storage/csi-snapshotter:v4.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-snapshotter:v4.1.0 changing: quay.io/k8scsi/csi-resizer:v1.0.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.0.0 changing: quay.io/k8scsi/csi-resizer:v1.1.0 -\u003e amaas-eos-mw1.cec.lab.emc.com:5028/csi-operator/csi-resizer:v1.1.0 * * Complete Perform either a Helm installation or Operator installation Now that the required images are available and the Helm Charts/Operator configuration updated, you can proceed by following the usual installation procedure as documented either via Helm or Operator.\nNOTES:\n Offline bundle installation is only supported with manual installs i.e. without using Operator Lifecycle Manager. Installation should be done using the files that are obtained after unpacking the offline bundle (dell-csi-operator-bundle.tar.gz) as the image tags in the manifests are modified to point to the internal registry. Offline bundle installs operator in default namespace via install.sh script. Make sure that the current context in kubeconfig file has the namespace set to default.  ","excerpt":"The csi-offline-bundle.sh script can be used to create a package …","ref":"/csm-docs/v3/csidriver/installation/offline/","title":"Offline Installation of Dell EMC CSI Storage Providers"},{"body":"Release Notes - Dell CSI Operator 1.8.0  Note: There will be a delay in certification of Dell CSI Operator 1.8.0 and it will not be available for download from the Red Hat OpenShift certified catalog right away. The operator will still be available for download from the Red Hat OpenShift Community Catalog soon after the 1.8.0 release.\n New Features/Changes  Added support for Kubernetes 1.24. Added support for OpenShift 4.10.  Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\nSupport The Dell CSI Operator image is available on Dockerhub and is officially supported by Dell. For any CSI operator and driver issues, questions or feedback, please follow our support process.\n","excerpt":"Release Notes - Dell CSI Operator 1.8.0  Note: There will be a delay …","ref":"/csm-docs/docs/csidriver/release/operator/","title":"Operator"},{"body":"Release Notes - Dell CSI Operator 1.7.0  Note: There will be a delay in certification of Dell CSI Operator 1.7.0 and it will not be available for download from the Red Hat OpenShift certified catalog right away. The operator will still be available for download from the Red Hat OpenShift Community Catalog soon after the 1.7.0 release.\n New Features/Changes  Added support for Kubernetes 1.23.  Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\nSupport The Dell CSI Operator image is available on Dockerhub and is officially supported by Dell. For any CSI operator and driver issues, questions or feedback, please follow our support process.\n","excerpt":"Release Notes - Dell CSI Operator 1.7.0  Note: There will be a delay …","ref":"/csm-docs/v1/csidriver/release/operator/","title":"Operator"},{"body":"Release Notes - Dell CSI Operator 1.7.0  Note: There will be a delay in certification of Dell CSI Operator 1.7.0 and it will not be available for download from the Red Hat OpenShift certified catalog right away. The operator will still be available for download from the Red Hat OpenShift Community Catalog soon after the 1.7.0 release.\n New Features/Changes  Added support for Kubernetes 1.23.  Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no known issues in this release.\nSupport The Dell CSI Operator image is available on Dockerhub and is officially supported by Dell. For any CSI operator and driver issues, questions or feedback, please follow our support process.\n","excerpt":"Release Notes - Dell CSI Operator 1.7.0  Note: There will be a delay …","ref":"/csm-docs/v2/csidriver/release/operator/","title":"Operator"},{"body":"Release Notes - Dell CSI Operator 1.6.0  Note: There will be a delay in certification of Dell CSI Operator 1.6.0 and it will not be available for download from the Red Hat OpenShift certified catalog right away. The operator will still be available for download from the Red Hat OpenShift Community Catalog soon after the 1.6.0 release.\n New Features/Changes  Added support for OpenShift v4.9.  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     A warning message will be listed in the events for cluster scoped objects if the driver is not upgraded after an operator upgrade. This happens because of the fix provided by Kubernetes in 1.20 for one of the known issue. After an operator upgrade, the objects will get updated automatically after 45 mins in case of no driver upgrade.    Support The Dell CSI Operator image is available on Dockerhub and is officially supported by Dell EMC. For any CSI operator and driver issues, questions or feedback, please follow our support process.\n","excerpt":"Release Notes - Dell CSI Operator 1.6.0  Note: There will be a delay …","ref":"/csm-docs/v3/csidriver/release/operator/","title":"Operator"},{"body":"The CSI Driver for Dell PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell PowerFlex Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements that must be met before installing the CSI Driver for Dell PowerFlex:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Enable Zero Padding on PowerFlex Mount propagation is enabled on container runtime that is being used Install PowerFlex Storage Data Client If using Snapshot feature, satisfy all Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure If enabling CSM for Authorization, please refer to the Authorization deployment steps first  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nEnable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. For more information to configure this setting, see Dell PowerFlex documentation.\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run the node portion of the CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment; currently only Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions.\nOptional: For a typical install, you will pull SDC kernel modules from the Dell FTP site, which is set up by default. Some users might want to mirror this repository to a local location. The PowerFlex KB article has instructions on how to do this.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\ncontroller:snapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here: v5.0.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v5.0.x\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\n When using Kubernetes 1.21/1.22/1.23 it is recommended to use 5.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone -b v2.3.0 https://github.com/dell/csi-powerflex.git to clone the git repository.\n  Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace vxflexos to create a new one.\n  Collect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the scripts directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the values for these parameters as they must be entered into samples/config.yaml.\n  Prepare samples/config.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing PowerFlex system. If authorization is enabled, username will be ignored. true -   password Password for accessing PowerFlex system. If authorization is enabled, password will be ignored. true -   systemID System name/ID of PowerFlex system. true -   allSystemNames List of previous names of powerflex array if used for PV create false -   endpoint REST API gateway HTTPS endpoint for PowerFlex system. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on true -   skipCertificateValidation Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. true true   isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list. false false   mdm mdm defines the MDM(s) that SDC should register with on start. This should be a list of MDM IP addresses or hostnames separated by comma. true -    Example: samples/config.yaml\n  - username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:trueisDefault:truemdm:\"10.0.0.3,10.0.0.4\"NOTE: To use multiple arrays, copy and paste section above for each array. Make sure isDefault is set to true for only one array.\nAfter editing the file, run the following command to create a secret called `vxflexos-config`: `kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/config.yaml` Use the following command to replace or update the secret: `kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/config.yaml -o yaml --dry-run=client | kubectl replace -f -` *NOTE:* - The user needs to validate the YAML syntax and array-related key/values while replacing the vxflexos-creds secret. - If you want to create a new array or update the MDM values in the secret, you will need to reinstall the driver. If you change other details, such as login information, the secret will dynamically update -- see [dynamic-array-configuration](../../../features/powerflex#dynamic-array-configuration) for more details. - Old `json` format of the array configuration file is still supported in this release. If you already have your configuration in `json` format, you may continue to maintain it or you may transfer this configuration to `yaml` format and replace/update the secret. - \"insecure\" parameter has been changed to \"skipCertificateValidation\" as insecure is deprecated and will be removed from use in config.yaml or secret.yaml in a future release. Users can continue to use any one of \"insecure\" or \"skipCertificateValidation\" for now. The driver would return an error if both parameters are used. - Please note that log configuration parameters from v1.5 will no longer work in v2.0 and higher. Please refer to the [Dynamic Logging Configuration](../../../features/powerflex#dynamic-logging-configuration) section in Features for more information. - If the user is using complex K8s version like \"v1.21.3-mirantis-1\", use below kubeVersion check in helm/csi-unity/Chart.yaml file. kubeVersion: \"\u003e= 1.21.0-0 \u003c 1.24.0-0\"   Default logging options are set during Helm install. To see possible configuration options, see the Dynamic Logging Configuration section in Features.\n  If using automated SDC deployment:\n Check the SDC container image is the correct version for your version of PowerFlex.    Copy the default values.yaml file cd helm \u0026\u0026 cp csi-vxflexos/values.yaml myvalues.yaml\n  If you are using a custom image, check the version and driverRepository fields in myvalues.yaml to make sure that they are pointing to the correct image repository and driver version. These two fields are spliced together to form the image name, as shown here: \u003cdriverRepository\u003e/csi-vxflexos:v\u003cversion\u003e\n  Look over all the other fields myvalues.yaml and fill in/adjust any as needed. All the fields are described here:\n     Parameter Description Required Default     version Set to verify the values file version matches driver version and used to pull the image as part of the image name. Yes 2.0.0   driverRepository Set to give the repository containing the driver image (used as part of the image name). Yes dellemc   powerflexSdc Set to give the location of the SDC image used if automatic SDC deployment is being utilized. No dellemc/sdc:3.6   certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. No 0   logLevel CSI driver log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes “debug”   logFormat CSI driver log format. Allowed values: “TEXT” or “JSON”. Yes “TEXT”   kubeletConfigDir kubelet config directory path. Ensure that the config.yaml file is present at this path. Yes /var/lib/kubelet   defaultFsType Used to set the default FS type which will be used for mount volumes if FsType is not specified in the storage class. Allowed values: ext4, xfs. Yes ext4   fsGroupPolicy Defines which FS Group policy mode to be used. Supported modes areNone, File, and ReadWriteOnceWithFSType. No “ReadWriteOnceWithFSType”   imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Allowed values: Always, IfNotPresent, Never. Yes IfNotPresent   enablesnapshotcgdelete A boolean that, when enabled, will delete all snapshots in a consistency group everytime a snap in the group is deleted. Yes false   enablelistvolumesnapshot A boolean that, when enabled, will allow list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap). It is recommend this be false unless instructed otherwise. Yes false   allowRWOMultiPodAccess Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. Yes false   controller This section allows the configuration of controller-specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - -   volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. Yes “k8s”   controllerCount Set to deploy multiple controller instances. If the controller count is greater than the number of available nodes, excess pods remain in a pending state. It should be greater than 0. You can increase the number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2   snapshot.enabled A boolean that enable/disable volume snapshot feature. No true   resizer.enabled A boolean that enable/disable volume expansion feature. No true   nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. Yes \" \"   tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install the controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. Yes \" \"   healthMonitor This section configures the optional deployment of the external health monitor sidecar, for controller side volume health monitoring. - -   enabled Enable/Disable deployment of external health monitor sidecar. No false   volumeHealthMonitorInterval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s   node This section allows the configuration of node-specific parameters. - -   healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false   nodeSelector Defines what nodes would be selected for pods of node daemonset. Leave as blank to use all nodes. Yes \" \"   tolerations Defines tolerations that would be applied to node daemonset. Leave as blank to install node driver only on worker nodes. Yes \" \"   monitor This section allows the configuration of the SDC monitoring pod. - -   enabled Set to enable the usage of the monitoring pod. Yes false   hostNetwork Set whether the monitor pod should run on the host network or not. Yes true   hostPID Set whether the monitor pod should run in the host namespace or not. Yes true   vgsnapshotter This section allows the configuration of the volume group snapshotter(vgsnapshotter) pod. - -   enabled A boolean that enable/disable vg snapshotter feature. No false   image Image for vg snapshotter. No \" \"   podmon Podmon is an optional feature under development and tech preview. Enable this feature only after contact support for additional information. - -   enabled A boolean that enable/disable podmon feature. No false   image image for podmon. No \" \"   authorization Authorization is an optional feature to apply credential shielding of the backend PowerFlex. - -   enabled A boolean that enables/disables authorization feature. No false   sidecarProxyImage Image for csm-authorization-sidecar. No \" \"   proxyHost Hostname of the csm-authorization server. No Empty   skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. No true    Install the driver using csi-install.sh bash script by running cd dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml. Alternatively, to do a helm install solely with Helm charts (without shell scripts), refer to helm/README.md.  NOTE:\n  For detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder.\n  Install script will validate MDM IP(s) in vxflexos-config secret and creates a new field consumed by the init container and sdc-monitor container\n  This install script also runs the verify.sh script. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes.\n  It is mandatory to run install script after changes to MDM configuration in vxflexos-config secret. Refer dynamic-array-configuration\n  If an extended Kubernetes version is being used (e.g. v1.21.3-mirantis-1) and is failing the version check in Helm even though it falls in the allowed range, then you must go into helm/csi-vxflexos/Chart.yaml and replace the standard kubeVersion check with the commented-out alternative. Please note that this will also allow the use of pre-release alpha and beta versions of Kubernetes, which is not supported.\n  (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.\n Mount options are specified in storageclass yaml under mkfsFormatOption. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Certificate validation for PowerFlex Gateway REST API calls This topic provides details about setting up the certificate for the CSI Driver for Dell PowerFlex.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name vxflexos-certs-0 to vxflexos-certs-n based on the “.Values.certSecretCount” parameter present in the namespace vxflexos.\nThis secret contains the X509 certificates of the CA which signed PowerFlex gateway SSL certificate in PEM format.\nThe CSI driver exposes an install parameter in config.yaml, skipCertificateValidation, which determines if the driver performs client-side verification of the gateway certificates.\nskipCertificateValidation parameter is set to true by default, and the driver does not verify the gateway certificates.\nIf skipCertificateValidation is set to false, then the secret vxflexos-certs-n must contain the CA certificate for the array gateway.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the gateway certificate is self-signed or if you are using an embedded gateway, then perform the following steps.\n  To fetch the certificate, run the following command.\n `openssl s_client -showcerts -connect \u003cGateway IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem`  Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem\n  Run the following command to create the cert secret with index ‘0’:\n `kubectl create secret generic vxflexos-certs-0 --from-file=cert-0=ca_cert_0.pem -n vxflexos`  Use the following command to replace the secret:\n `kubectl create secret generic vxflexos-certs-0 -n vxflexos --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -`    Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: vxflexos-certs-1, vxflexos-certs-2, etc)\n  Notes:\n “vxflexos” is the namespace for Helm-based installation but namespace can be user-defined in operator-based installation. User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation. Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver. Updating vxflexos-certs-n secrets is a manual process, unlike vxflexos-config. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.  Storage Classes For CSI driver for PowerFlex version 1.4 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class: There are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\n Edit storageclass.yaml if you need ext4 filesystem and storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have. Replace \u003cSYSTEM_ID\u003e with the system ID you have. Note there are two appearances in the file. Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f storageclass.yaml or kubectl create -f storageclass-xfs.yaml  NOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerFlex v1.5, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerFlex v2.2 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerFlex to 1.5 or higher, before upgrading to 2.3.\n","excerpt":"The CSI Driver for Dell PowerFlex can be deployed by using the …","ref":"/csm-docs/docs/csidriver/installation/helm/powerflex/","title":"PowerFlex"},{"body":"Installing CSI Driver for PowerFlex via Operator The CSI Driver for Dell PowerFlex can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: SDC Deployment for Operator  This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non-supported versions of the OS also do the manual SDC deployment steps given below. Refer to https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When the driver is created, MDM value for initContainers in driver CR is set by the operator from mdm attributes in the driver configuration file, config.yaml. An example of config.yaml is below in this document. Do not set MDM value for initContainers in the driver CR file manually. Note: To use an sdc-binary module from customer ftp site:  Create a secret, sdc-repo-secret.yaml to contain the credentials for the private repo. To generate the base64 encoding of a credential:    echo-n\u003ccredential\u003e|base64-isecret sample to use:\napiVersion:v1kind:Secretmetadata:name:sdc-repo-credsnamespace:vxflexostype:Opaquedata:# set username to the base64 encoded username, sdc default isusername:\u003cusernameinbase64\u003e # set password to the base64 encoded password, sdc default ispassword:\u003cpasswordinbase64\u003e Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml. Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml. Please note the following:  If using sidecar, you will need to edit the value fields under the HOST_PID and MDM fields by filling the empty quotes with host PID and the MDM IPs. If not using sidecar, please leave this commented out – otherwise, the empty fields will cause errors.    Example CR: config/samples/vxflex_v220_ops_48.yaml sideCars:# Comment the following section if you don't want to run the monitoring sidecar- name:sdc-monitorenvs:- name:HOST_PIDvalue:\"1\"- name:MDMvalue:\"\"- name:external-health-monitorargs:[\"--monitor-interval=60s\"]initContainers:- image:dellemc/sdc:3.6imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\"10.x.x.x,10.x.x.x\"Note: Please comment the sdc-monitor sidecar section if you are not using it. Blank values for MDM will result in error. Do not comment the external-health-monitor argument.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  Install Driver   Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e command using the desired name to create the namespace.\n  Prepare the config.yaml for driver configuration.\nExample: config.yaml\n# Username for accessing PowerFlex system.\t# Required: true- username:\"admin\"# Password for accessing PowerFlex system.\t# Required: truepassword:\"password\"# System name/ID of PowerFlex system.\t# Required: truesystemID:\"ID1\"# REST API gateway HTTPS endpoint for PowerFlex system.# Required: trueendpoint:\"https://127.0.0.1\"# Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface.# Allowed values: true or false# Required: true# Default value: trueskipCertificateValidation:true# indicates if this array is the default array# needed for backwards compatibility# only one array is allowed to have this set to true # Required: false# Default value: falseisDefault:true# defines the MDM(s) that SDC should register with on start.# Allowed values: a list of IP addresses or hostnames separated by comma.# Required: true# Default value: none mdm:\"10.0.0.1,10.0.0.2\"# Defines all system names used to create powerflex volumes# Required: false# Default value: noneAllSystemNames:\"name1,name2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"AllSystemNames:\"name1,name2\"After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.yaml -o yaml --dry-run=client | kubectl replace -f -\nNote:\n System ID, MDM configuration, etc. now are taken directly from config.yaml. MDM provided in the input_sample_file.yaml will be overidden with MDM values in config.yaml. Please provide MDM values in input_sample_file.yaml so that it will be overidden by default value.    Create a Custom Resource (CR) for PowerFlex using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\n   Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, excess pods will become stay in a pending state. Defaults are 2 which allows for Controller high availability. Yes 2   fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType”   Common parameters for node and controller      X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false   X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false   X_CSI_DEBUG To enable debug mode No true   X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false      Execute the kubectl create -f \u003cinput_sample_file.yaml\u003e command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.\n Example CR for PowerFlex Driver    apiVersion:storage.dell.com/v1kind:CSIVXFlexOSmetadata:name:test-vxflexosnamespace:test-vxflexosspec:driver:configVersion:v2.3.0replicas:1dnsPolicy:ClusterFirstWithHostNetforceUpdate:falsefsGroupPolicy:Filecommon:image:\"dellemc/csi-vxflexos:v2.3.0\"imagePullPolicy:IfNotPresentenvs:- name:X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOTvalue:\"false\"- name:X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETEvalue:\"false\"- name:X_CSI_DEBUGvalue:\"true\"- name:X_CSI_ALLOW_RWO_MULTI_POD_ACCESSvalue:\"false\"sideCars:# comment the following section if you don't want to run the monitoring sidecar- name:sdc-monitorenvs:- name:HOST_PIDvalue:\"1\"- name:MDMvalue:\"\"# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin.# Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\".# - name: external-health-monitor# args: [\"--monitor-interval=60s\"]controller:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition.# Install the 'external-health-monitor' sidecar accordingly.# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"node:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"initContainers:- image:dellemc/sdc:3.6imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\"10.xx.xx.xx,10.xx.xx.xx\"#provide MDM value--- apiVersion: v1kind:ConfigMapmetadata:name:vxflexos-config-paramsnamespace:test-vxflexosdata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"debug\"CSI_LOG_FORMAT:\"TEXT\"```### Pre-Requisite for installation with OLMPleaserunthefollowingcommandsforcreatingtherequiredConfigMapbeforeinstallingthedell-csi-operatorusingOLM.```yaml1.gitclonehttps://github.com/dell/dell-csi-operator.git2.cddell-csi-operator3.tar-czfconfig.tar.gzdriverconfig/# Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM 4.kubectlcreateconfigmapdell-csi-operator-config--from-fileconfig.tar.gz-n\u003coperator-namespace\u003eVolume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin.# Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\".# - name: external-health-monitor# args: [\"--monitor-interval=60s\"]# Install the 'external-health-monitor' sidecar accordingly.# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: falsecontroller:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"node:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"","excerpt":"Installing CSI Driver for PowerFlex via Operator The CSI Driver for …","ref":"/csm-docs/docs/csidriver/installation/operator/powerflex/","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\n Navigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following:  Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured.  Results\nAn outline of this workflow is described below:\n The 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below:  kind:PersistentVolumeClaimapiVersion:v1metadata:name:pvolnamespace:helmtest-vxflexosspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GistorageClassName:vxflexosThe volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use a storage class named vxflexos. This step yields a mounted ext4 file system. You can create the vxflexos and vxflexos-xfs storage classes by using the yamls located in samples/storageclass. If you compare pvol0.yaml and pvol1.yaml, you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos.   NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.\n Test creating snapshots Test the workflow for snapshot creation.\n NOTE: Starting with version 2.0, CSI Driver for PowerFlex helm tests are designed to work exclusively with v1 snapshots.\n Steps\n Start the 2vols container and leave it running.  Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly.   Run sh snaptest.sh to start the test.  This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1namespace:helmtest-vxflexosspec:volumeSnapshotClassName:vxflexos-snapclasssource:persistentVolumeClaimName:pvol0Results\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0, then the created snapshot is named pvol0-snap1.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n helmtest-vxflexos.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell PowerFlex installation does not create this class. You will need to create instance of VolumeSnapshotClass from one of default samples in `samples/volumesnapshotclass’ directory.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\n Run sh snaprestoretest.sh to start the test.  This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updated directory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\n Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly.  Results\nAn outline of this workflow is described below:\n The snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim, which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below:  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:helmtest-vxflexosspec:storageClassName:vxflexosdataSource:name:pvol0-snap1kind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiNOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\n","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/docs/csidriver/installation/test/powerflex/","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v2.3.0 New Features/Changes  Added support to configure fsGroupPolicy Removed beta volumesnapshotclass sample files. Added support for Kubernetes 1.24. Added support for OpenShift 4.10. Fixed handling of idempotent snapshots.  Fixed Issues  Added label to driver node pod for Resiliency protection. Updated values file to use patched image of vg-snapshotter.  Known Issues    Issue Workaround     Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100   When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node.    Note:  Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI PowerFlex v2.3.0 New Features/Changes  Added …","ref":"/csm-docs/docs/csidriver/release/powerflex/","title":"PowerFlex"},{"body":"   Symptoms Prevention, Resolution or Workaround     The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that need to pull an image of the driver.   When you run the command kubectl describe pods vxflexos-controller-* –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n vxflexos vxflexos-controller-* driver logs show that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system.   The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID.   The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script.   CreateVolume error System  is not configured in the driver Powerflex name if used for systemID in StorageClass ensure same name is also used in array config systemID   Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on a worker node, and ensure your container run time manager is properly configured to be utilized with SElinux.   Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux.   Installation of the driver on Kubernetes v1.21/v1.22/v1.23 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.21/v1.22/v1.23 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements   The kubectl logs -n vxflexos vxflexos-controller-* driver logs show x509: certificate signed by unknown authority A self assigned certificate is used for PowerFlex array. See certificate validation for PowerFlex Gateway   When you run the command kubectl apply -f snapclass-v1.yaml, you get the error error: unable to recognize \"snapclass-v1.yaml\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Check to make sure that the v1 snapshotter CRDs are installed, and not the v1beta1 CRDs, which are no longer supported.   The controller pod is stuck and producing errors such as” Failed to watch *v1.VolumeSnapshotContent: failed to list *v1.VolumeSnapshotContent: the server could not find the requested resource (get volumesnapshotcontents.snapshot.storage.k8s.io) Make sure that v1 snapshotter CRDs and v1 snapclass are installed, and not v1beta1, which is no longer supported.   Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.21.0 \u003c= 1.23.0 which is incompatible with Kubernetes V1.21.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart at helm/csi-vxflexos/Chart.yaml and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported.   Volume metrics are missing Enable Volume Health Monitoring   When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node.     Note: vxflexos-controller-* is the controller pod that acquires leader lease\n ","excerpt":"   Symptoms Prevention, Resolution or Workaround     The installation …","ref":"/csm-docs/docs/csidriver/troubleshooting/powerflex/","title":"PowerFlex"},{"body":"The CSI Driver for Dell PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell PowerFlex Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements that must be met before installing the CSI Driver for Dell PowerFlex:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Enable Zero Padding on PowerFlex Mount propagation is enabled on container runtime that is being used Install PowerFlex Storage Data Client If using Snapshot feature, satisfy all Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure If enabling CSM for Authorization, please refer to the Authorization deployment steps first  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nEnable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. For more information to configure this setting, see Dell PowerFlex documentation.\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run the node portion of the CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment; currently only Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions.\nOptional: For a typical install, you will pull SDC kernel modules from the Dell FTP site, which is set up by default. Some users might want to mirror this repository to a local location. The PowerFlex KB article has instructions on how to do this.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\ncontroller:snapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here: v5.0.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v5.0.x\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\n When using Kubernetes 1.21/1.22/1.23 it is recommended to use 5.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone -b v2.2.0 https://github.com/dell/csi-powerflex.git to clone the git repository.\n  Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace vxflexos to create a new one.\n  Collect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the scripts directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the values for these parameters as they must be entered into samples/config.yaml.\n  Prepare samples/config.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing PowerFlex system. If authorization is enabled, username will be ignored. true -   password Password for accessing PowerFlex system. If authorization is enabled, password will be ignored. true -   systemID System name/ID of PowerFlex system. true -   allSystemNames List of previous names of powerflex array if used for PV create false -   endpoint REST API gateway HTTPS endpoint for PowerFlex system. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on true -   skipCertificateValidation Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. true true   isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list. false false   mdm mdm defines the MDM(s) that SDC should register with on start. This should be a list of MDM IP addresses or hostnames separated by comma. true -    Example: samples/config.yaml\n# Username for accessing PowerFlex system.\t# If authorization is enabled, username will be ignored.- username:\"admin\"# Password for accessing PowerFlex system.\t# If authorization is enabled, password will be ignored.password:\"password\"# System name/ID of PowerFlex system.\tsystemID:\"ID1\"# Previous names of PowerFlex system if used for PV.allSystemNames:\"pflex-1,pflex-2\"# REST API gateway HTTPS endpoint for PowerFlex system.# If authorization is enabled, endpoint should be the HTTPS localhost endpoint that # the authorization sidecar will listen onendpoint:\"https://127.0.0.1\"# Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface.# Allowed values: true or false# Default value: trueskipCertificateValidation:true# indicates if this array is the default array# needed for backwards compatibility# only one array is allowed to have this set to true # Default value: falseisDefault:true# defines the MDM(s) that SDC should register with on start.# Allowed values: a list of IP addresses or hostnames separated by comma.# Default value: none mdm:\"10.0.0.1,10.0.0.2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"After editing the file, run the following command to create a secret called vxflexos-config:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/config.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/config.yaml -o yaml --dry-run=client | kubectl replace -f -\nNOTE:\n The user needs to validate the YAML syntax and array-related key/values while replacing the vxflexos-creds secret. If you want to create a new array or update the MDM values in the secret, you will need to reinstall the driver. If you change other details, such as login information, the secret will dynamically update – see dynamic-array-configuration for more details. Old json format of the array configuration file is still supported in this release. If you already have your configuration in json format, you may continue to maintain it or you may transfer this configuration to yaml format and replace/update the secret. “insecure” parameter has been changed to “skipCertificateValidation” as insecure is deprecated and will be removed from use in config.yaml or secret.yaml in a future release. Users can continue to use any one of “insecure” or “skipCertificateValidation” for now. The driver would return an error if both parameters are used. Please note that log configuration parameters from v1.5 will no longer work in v2.0 and higher. Please refer to the Dynamic Logging Configuration section in Features for more information. If the user is using complex K8s version like “v1.21.3-mirantis-1”, use below kubeVersion check in helm/csi-unity/Chart.yaml file. kubeVersion: “\u003e= 1.21.0-0 \u003c 1.24.0-0”    Default logging options are set during Helm install. To see possible configuration options, see the Dynamic Logging Configuration section in Features.\n  If using automated SDC deployment:\n Check the SDC container image is the correct version for your version of PowerFlex.    Copy the default values.yaml file cd helm \u0026\u0026 cp csi-vxflexos/values.yaml myvalues.yaml\n  If you are using a custom image, check the version and driverRepository fields in myvalues.yaml to make sure that they are pointing to the correct image repository and driver version. These two fields are spliced together to form the image name, as shown here: \u003cdriverRepository\u003e/csi-vxflexos:v\u003cversion\u003e\n  Look over all the other fields myvalues.yaml and fill in/adjust any as needed. All the fields are described here:\n     Parameter Description Required Default     version Set to verify the values file version matches driver version and used to pull the image as part of the image name. Yes 2.0.0   driverRepository Set to give the repository containing the driver image (used as part of the image name). Yes dellemc   powerflexSdc Set to give the location of the SDC image used if automatic SDC deployment is being utilized. No dellemc/sdc:3.6   certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. No 0   logLevel CSI driver log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes “debug”   logFormat CSI driver log format. Allowed values: “TEXT” or “JSON”. Yes “TEXT”   kubeletConfigDir kubelet config directory path. Ensure that the config.yaml file is present at this path. Yes /var/lib/kubelet   defaultFsType Used to set the default FS type which will be used for mount volumes if FsType is not specified in the storage class. Allowed values: ext4, xfs. Yes ext4   imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Allowed values: Always, IfNotPresent, Never. Yes IfNotPresent   enablesnapshotcgdelete A boolean that, when enabled, will delete all snapshots in a consistency group everytime a snap in the group is deleted. Yes false   enablelistvolumesnapshot A boolean that, when enabled, will allow list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap). It is recommend this be false unless instructed otherwise. Yes false   allowRWOMultiPodAccess Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. Yes false   controller This section allows the configuration of controller-specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - -   volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. Yes “k8s”   controllerCount Set to deploy multiple controller instances. If the controller count is greater than the number of available nodes, excess pods remain in a pending state. It should be greater than 0. You can increase the number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2   snapshot.enabled A boolean that enable/disable volume snapshot feature. No true   resizer.enabled A boolean that enable/disable volume expansion feature. No true   nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. Yes \" \"   tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install the controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. Yes \" \"   healthMonitor This section configures the optional deployment of the external health monitor sidecar, for controller side volume health monitoring. - -   enabled Enable/Disable deployment of external health monitor sidecar. No false   volumeHealthMonitorInterval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s   node This section allows the configuration of node-specific parameters. - -   nodeSelector Defines what nodes would be selected for pods of node daemonset. Leave as blank to use all nodes. Yes \" \"   tolerations Defines tolerations that would be applied to node daemonset. Leave as blank to install node driver only on worker nodes. Yes \" \"   monitor This section allows the configuration of the SDC monitoring pod. - -   enabled Set to enable the usage of the monitoring pod. Yes false   hostNetwork Set whether the monitor pod should run on the host network or not. Yes true   hostPID Set whether the monitor pod should run in the host namespace or not. Yes true   healthMonitor This section configures node side volume health monitoring - -   enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false   vgsnapshotter This section allows the configuration of the volume group snapshotter(vgsnapshotter) pod. - -   enabled A boolean that enable/disable vg snapshotter feature. No false   image Image for vg snapshotter. No \" \"   podmon Podmon is an optional feature under development and tech preview. Enable this feature only after contact support for additional information. - -   enabled A boolean that enable/disable podmon feature. No false   image image for podmon. No \" \"   authorization Authorization is an optional feature to apply credential shielding of the backend PowerFlex. - -   enabled A boolean that enables/disables authorization feature. No false   sidecarProxyImage Image for csm-authorization-sidecar. No \" \"   proxyHost Hostname of the csm-authorization server. No Empty   skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. No true    Install the driver using csi-install.sh bash script by running cd dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml. Alternatively, to do a helm install solely with Helm charts (without shell scripts), refer to helm/README.md.  NOTE:\n  For detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder.\n  Install script will validate MDM IP(s) in vxflexos-config secret and creates a new field consumed by the init container and sdc-monitor container\n  This install script also runs the verify.sh script. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes.\n  It is mandatory to run install script after changes to MDM configuration in vxflexos-config secret. Refer dynamic-array-configuration\n  If an extended Kubernetes version is being used (e.g. v1.21.3-mirantis-1) and is failing the version check in Helm even though it falls in the allowed range, then you must go into helm/csi-vxflexos/Chart.yaml and replace the standard kubeVersion check with the commented-out alternative. Please note that this will also allow the use of pre-release alpha and beta versions of Kubernetes, which is not supported.\n  (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.\n Mount options are specified in storageclass yaml under mkfsFormatOption. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Certificate validation for PowerFlex Gateway REST API calls This topic provides details about setting up the certificate for the CSI Driver for Dell PowerFlex.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name vxflexos-certs-0 to vxflexos-certs-n based on the “.Values.certSecretCount” parameter present in the namespace vxflexos.\nThis secret contains the X509 certificates of the CA which signed PowerFlex gateway SSL certificate in PEM format.\nThe CSI driver exposes an install parameter in config.yaml, skipCertificateValidation, which determines if the driver performs client-side verification of the gateway certificates.\nskipCertificateValidation parameter is set to true by default, and the driver does not verify the gateway certificates.\nIf skipCertificateValidation is set to false, then the secret vxflexos-certs-n must contain the CA certificate for the array gateway.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the gateway certificate is self-signed or if you are using an embedded gateway, then perform the following steps.\n  To fetch the certificate, run the following command.\n `openssl s_client -showcerts -connect \u003cGateway IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem`  Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem\n  Run the following command to create the cert secret with index ‘0’:\n `kubectl create secret generic vxflexos-certs-0 --from-file=cert-0=ca_cert_0.pem -n vxflexos`  Use the following command to replace the secret:\n `kubectl create secret generic vxflexos-certs-0 -n vxflexos --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -`    Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: vxflexos-certs-1, vxflexos-certs-2, etc)\n  Notes:\n “vxflexos” is the namespace for Helm-based installation but namespace can be user-defined in operator-based installation. User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation. Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver. Updating vxflexos-certs-n secrets is a manual process, unlike vxflexos-config. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.  Storage Classes For CSI driver for PowerFlex version 1.4 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class: There are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\n Edit storageclass.yaml if you need ext4 filesystem and storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have. Replace \u003cSYSTEM_ID\u003e with the system ID you have. Note there are two appearances in the file. Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f storageclass.yaml or kubectl create -f storageclass-xfs.yaml  NOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerFlex v1.5, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerFlex v2.1 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerFlex to 1.5 or higher, before upgrading to 2.2.\n","excerpt":"The CSI Driver for Dell PowerFlex can be deployed by using the …","ref":"/csm-docs/v1/csidriver/installation/helm/powerflex/","title":"PowerFlex"},{"body":"Installing CSI Driver for PowerFlex via Operator The CSI Driver for Dell PowerFlex can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: SDC Deployment for Operator  This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non-supported versions of the OS also do the manual SDC deployment steps given below. Refer to https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When the driver is created, MDM value for initContainers in driver CR is set by the operator from mdm attributes in the driver configuration file, config.yaml. An example of config.yaml is below in this document. Do not set MDM value for initContainers in the driver CR file manually. Note: To use an sdc-binary module from customer ftp site:  Create a secret, sdc-repo-secret.yaml to contain the credentials for the private repo. To generate the base64 encoding of a credential:    echo-n\u003ccredential\u003e|base64-isecret sample to use:\napiVersion:v1kind:Secretmetadata:name:sdc-repo-credsnamespace:vxflexostype:Opaquedata:# set username to the base64 encoded username, sdc default isusername:\u003cusernameinbase64\u003e # set password to the base64 encoded password, sdc default ispassword:\u003cpasswordinbase64\u003e Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml. Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml. Please note the following:  If using sidecar, you will need to edit the value fields under the HOST_PID and MDM fields by filling the empty quotes with host PID and the MDM IPs. If not using sidecar, please leave this commented out – otherwise, the empty fields will cause errors.    Example CR: config/samples/vxflex_v220_ops_48.yaml sideCars:# Comment the following section if you don't want to run the monitoring sidecar- name:sdc-monitorenvs:- name:HOST_PIDvalue:\"1\"- name:MDMvalue:\"\"- name:external-health-monitorargs:[\"--monitor-interval=60s\"]initContainers:- image:dellemc/sdc:3.6imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\"10.x.x.x,10.x.x.x\"Note: Please comment the sdc-monitor sidecar section if you are not using it. Blank values for MDM will result in error. Do not comment the external-health-monitor argument.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  Install Driver   Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e command using the desired name to create the namespace.\n  Prepare the config.yaml for driver configuration.\nExample: config.yaml\n# Username for accessing PowerFlex system.\t# Required: true- username:\"admin\"# Password for accessing PowerFlex system.\t# Required: truepassword:\"password\"# System name/ID of PowerFlex system.\t# Required: truesystemID:\"ID1\"# REST API gateway HTTPS endpoint for PowerFlex system.# Required: trueendpoint:\"https://127.0.0.1\"# Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface.# Allowed values: true or false# Required: true# Default value: trueskipCertificateValidation:true# indicates if this array is the default array# needed for backwards compatibility# only one array is allowed to have this set to true # Required: false# Default value: falseisDefault:true# defines the MDM(s) that SDC should register with on start.# Allowed values: a list of IP addresses or hostnames separated by comma.# Required: true# Default value: none mdm:\"10.0.0.1,10.0.0.2\"# Defines all system names used to create powerflex volumes# Required: false# Default value: noneAllSystemNames:\"name1,name2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"AllSystemNames:\"name1,name2\"After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.yaml -o yaml --dry-run=client | kubectl replace -f -\nNote:\n System ID, MDM configuration, etc. now are taken directly from config.yaml. MDM provided in the input_sample_file.yaml will be overidden with MDM values in config.yaml. Please provide MDM values in input_sample_file.yaml so that it will be overidden by default value.    Create a Custom Resource (CR) for PowerFlex using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\n   Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, excess pods will become stay in a pending state. Defaults are 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false   X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false   X_CSI_DEBUG To enable debug mode No true   X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false      Execute the kubectl create -f \u003cinput_sample_file.yaml\u003e command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.\n Example CR for PowerFlex Driver apiVersion:storage.dell.com/v1    kind: CSIVXFlexOS metadata: name: test-vxflexos namespace: test-vxflexos spec: driver: configVersion: v2.2.0 replicas: 1 dnsPolicy: ClusterFirstWithHostNet forceUpdate: false common: image: “dellemc/csi-vxflexos:v2.2.0” imagePullPolicy: IfNotPresent envs: - name: X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT value: “false” - name: X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE value: “false” - name: X_CSI_DEBUG value: “true” - name: X_CSI_ALLOW_RWO_MULTI_POD_ACCESS value: “false” sideCars: # comment the following section if you don’t want to run the monitoring sidecar - name: sdc-monitor envs: - name: HOST_PID value: “1” - name: MDM value: \"”\n # Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" initContainers: - image: dellemc/sdc:3.6 imagePullPolicy: IfNotPresent name: sdc envs: - name: MDM value: \"10.xx.xx.xx,10.xx.xx.xx\" #provide MDM value --- apiVersion: v1 kind: ConfigMap metadata: name: vxflexos-config-params namespace: test-vxflexos data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"debug\" CSI_LOG_FORMAT: \"TEXT\" ```  Pre-Requisite for installation with OLM  Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.  1.gitclonehttps://github.com/dell/dell-csi-operator.git2.cddell-csi-operator3.tar-czfconfig.tar.gzdriverconfig/# Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM 4.kubectlcreateconfigmapdell-csi-operator-config--from-fileconfig.tar.gz-n\u003coperator-namespace\u003eVolume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin.# Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\".# - name: external-health-monitor# args: [\"--monitor-interval=60s\"]# Install the 'external-health-monitor' sidecar accordingly.# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: falsecontroller:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"node:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"","excerpt":"Installing CSI Driver for PowerFlex via Operator The CSI Driver for …","ref":"/csm-docs/v1/csidriver/installation/operator/powerflex/","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\n Navigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following:  Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured.  Results\nAn outline of this workflow is described below:\n The 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below:  kind:PersistentVolumeClaimapiVersion:v1metadata:name:pvolnamespace:helmtest-vxflexosspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GistorageClassName:vxflexosThe volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use a storage class named vxflexos. This step yields a mounted ext4 file system. You can create the vxflexos and vxflexos-xfs storage classes by using the yamls located in samples/storageclass. If you compare pvol0.yaml and pvol1.yaml, you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos.   NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.\n Test creating snapshots Test the workflow for snapshot creation.\n NOTE: Starting with version 2.0, CSI Driver for PowerFlex helm tests are designed to work exclusively with v1 snapshots.\n Steps\n Start the 2vols container and leave it running.  Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly.   Run sh snaptest.sh to start the test.  This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1namespace:helmtest-vxflexosspec:volumeSnapshotClassName:vxflexos-snapclasssource:persistentVolumeClaimName:pvol0Results\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0, then the created snapshot is named pvol0-snap1.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n helmtest-vxflexos.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell PowerFlex installation does not create this class. You will need to create instance of VolumeSnapshotClass from one of default samples in `samples/volumesnapshotclass’ directory.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\n Run sh snaprestoretest.sh to start the test.  This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updated directory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\n Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly.  Results\nAn outline of this workflow is described below:\n The snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim, which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below:  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:helmtest-vxflexosspec:storageClassName:vxflexosdataSource:name:pvol0-snap1kind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiNOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\n","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v1/csidriver/installation/test/powerflex/","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v2.2.0 New Features/Changes  Added support for Kubernetes 1.23. Added support for Amazon Elastic Kubernetes Service Anywhere.  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100    Note:  Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI PowerFlex v2.2.0 New Features/Changes  Added …","ref":"/csm-docs/v1/csidriver/release/powerflex/","title":"PowerFlex"},{"body":"   Symptoms Prevention, Resolution or Workaround     The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that need to pull an image of the driver.   When you run the command kubectl describe pods vxflexos-controller-* –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n vxflexos vxflexos-controller-* driver logs show that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system.   The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID.   The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script.   CreateVolume error System  is not configured in the driver Powerflex name if used for systemID in StorageClass ensure same name is also used in array config systemID   Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on a worker node, and ensure your container run time manager is properly configured to be utilized with SElinux.   Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux.   Installation of the driver on Kubernetes v1.21/v1.22/v1.23 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.21/v1.22/v1.23 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements   The kubectl logs -n vxflexos vxflexos-controller-* driver logs show x509: certificate signed by unknown authority A self assigned certificate is used for PowerFlex array. See certificate validation for PowerFlex Gateway   When you run the command kubectl apply -f snapclass-v1.yaml, you get the error error: unable to recognize \"snapclass-v1.yaml\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Check to make sure that the v1 snapshotter CRDs are installed, and not the v1beta1 CRDs, which are no longer supported.   The controller pod is stuck and producing errors such as” Failed to watch *v1.VolumeSnapshotContent: failed to list *v1.VolumeSnapshotContent: the server could not find the requested resource (get volumesnapshotcontents.snapshot.storage.k8s.io) Make sure that v1 snapshotter CRDs and v1 snapclass are installed, and not v1beta1, which is no longer supported.   Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.21.0 \u003c= 1.23.0 which is incompatible with Kubernetes V1.21.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart at helm/csi-vxflexos/Chart.yaml and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported.   Volume metrics are missing Enable Volume Health Monitoring     Note: vxflexos-controller-* is the controller pod that acquires leader lease\n ","excerpt":"   Symptoms Prevention, Resolution or Workaround     The installation …","ref":"/csm-docs/v1/csidriver/troubleshooting/powerflex/","title":"PowerFlex"},{"body":"The CSI Driver for Dell PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell PowerFlex Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements that must be met before installing the CSI Driver for Dell PowerFlex:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Enable Zero Padding on PowerFlex Mount propagation is enabled on container runtime that is being used Install PowerFlex Storage Data Client If using Snapshot feature, satisfy all Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure If enabling CSM for Authorization, please refer to the Authorization deployment steps first  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nEnable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. For more information to configure this setting, see Dell PowerFlex documentation.\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run the node portion of the CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment; currently only Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions.\nOptional: For a typical install, you will pull SDC kernel modules from the Dell FTP site, which is set up by default. Some users might want to mirror this repository to a local location. The PowerFlex KB article has instructions on how to do this.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\ncontroller:snapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here: v5.0.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v5.0.x\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\n When using Kubernetes 1.21/1.22/1.23 it is recommended to use 5.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone -b v2.2.0 https://github.com/dell/csi-powerflex.git to clone the git repository.\n  Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace vxflexos to create a new one.\n  Collect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the scripts directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the values for these parameters as they must be entered into samples/config.yaml.\n  Prepare samples/config.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing PowerFlex system. If authorization is enabled, username will be ignored. true -   password Password for accessing PowerFlex system. If authorization is enabled, password will be ignored. true -   systemID System name/ID of PowerFlex system. true -   allSystemNames List of previous names of powerflex array if used for PV create false -   endpoint REST API gateway HTTPS endpoint for PowerFlex system. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on true -   skipCertificateValidation Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. true true   isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list. false false   mdm mdm defines the MDM(s) that SDC should register with on start. This should be a list of MDM IP addresses or hostnames separated by comma. true -    Example: samples/config.yaml\n# Username for accessing PowerFlex system.\t# If authorization is enabled, username will be ignored.- username:\"admin\"# Password for accessing PowerFlex system.\t# If authorization is enabled, password will be ignored.password:\"password\"# System name/ID of PowerFlex system.\tsystemID:\"ID1\"# Previous names of PowerFlex system if used for PV.allSystemNames:\"pflex-1,pflex-2\"# REST API gateway HTTPS endpoint for PowerFlex system.# If authorization is enabled, endpoint should be the HTTPS localhost endpoint that # the authorization sidecar will listen onendpoint:\"https://127.0.0.1\"# Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface.# Allowed values: true or false# Default value: trueskipCertificateValidation:true# indicates if this array is the default array# needed for backwards compatibility# only one array is allowed to have this set to true # Default value: falseisDefault:true# defines the MDM(s) that SDC should register with on start.# Allowed values: a list of IP addresses or hostnames separated by comma.# Default value: none mdm:\"10.0.0.1,10.0.0.2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"After editing the file, run the following command to create a secret called vxflexos-config:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/config.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/config.yaml -o yaml --dry-run=client | kubectl replace -f -\nNOTE:\n The user needs to validate the YAML syntax and array-related key/values while replacing the vxflexos-creds secret. If you want to create a new array or update the MDM values in the secret, you will need to reinstall the driver. If you change other details, such as login information, the secret will dynamically update – see dynamic-array-configuration for more details. Old json format of the array configuration file is still supported in this release. If you already have your configuration in json format, you may continue to maintain it or you may transfer this configuration to yaml format and replace/update the secret. “insecure” parameter has been changed to “skipCertificateValidation” as insecure is deprecated and will be removed from use in config.yaml or secret.yaml in a future release. Users can continue to use any one of “insecure” or “skipCertificateValidation” for now. The driver would return an error if both parameters are used. Please note that log configuration parameters from v1.5 will no longer work in v2.0 and higher. Please refer to the Dynamic Logging Configuration section in Features for more information. If the user is using complex K8s version like “v1.21.3-mirantis-1”, use below kubeVersion check in helm/csi-unity/Chart.yaml file. kubeVersion: “\u003e= 1.21.0-0 \u003c 1.24.0-0”    Default logging options are set during Helm install. To see possible configuration options, see the Dynamic Logging Configuration section in Features.\n  If using automated SDC deployment:\n Check the SDC container image is the correct version for your version of PowerFlex.    Copy the default values.yaml file cd helm \u0026\u0026 cp csi-vxflexos/values.yaml myvalues.yaml\n  If you are using a custom image, check the version and driverRepository fields in myvalues.yaml to make sure that they are pointing to the correct image repository and driver version. These two fields are spliced together to form the image name, as shown here: \u003cdriverRepository\u003e/csi-vxflexos:v\u003cversion\u003e\n  Look over all the other fields myvalues.yaml and fill in/adjust any as needed. All the fields are described here:\n     Parameter Description Required Default     version Set to verify the values file version matches driver version and used to pull the image as part of the image name. Yes 2.0.0   driverRepository Set to give the repository containing the driver image (used as part of the image name). Yes dellemc   powerflexSdc Set to give the location of the SDC image used if automatic SDC deployment is being utilized. No dellemc/sdc:3.6   certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. No 0   logLevel CSI driver log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes “debug”   logFormat CSI driver log format. Allowed values: “TEXT” or “JSON”. Yes “TEXT”   kubeletConfigDir kubelet config directory path. Ensure that the config.yaml file is present at this path. Yes /var/lib/kubelet   defaultFsType Used to set the default FS type which will be used for mount volumes if FsType is not specified in the storage class. Allowed values: ext4, xfs. Yes ext4   imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Allowed values: Always, IfNotPresent, Never. Yes IfNotPresent   enablesnapshotcgdelete A boolean that, when enabled, will delete all snapshots in a consistency group everytime a snap in the group is deleted. Yes false   enablelistvolumesnapshot A boolean that, when enabled, will allow list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap). It is recommend this be false unless instructed otherwise. Yes false   allowRWOMultiPodAccess Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. Yes false   controller This section allows the configuration of controller-specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - -   volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. Yes “k8s”   controllerCount Set to deploy multiple controller instances. If the controller count is greater than the number of available nodes, excess pods remain in a pending state. It should be greater than 0. You can increase the number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2   snapshot.enabled A boolean that enable/disable volume snapshot feature. No true   resizer.enabled A boolean that enable/disable volume expansion feature. No true   nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. Yes \" \"   tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install the controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. Yes \" \"   healthMonitor This section configures the optional deployment of the external health monitor sidecar, for controller side volume health monitoring. - -   enabled Enable/Disable deployment of external health monitor sidecar. No false   volumeHealthMonitorInterval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s   node This section allows the configuration of node-specific parameters. - -   nodeSelector Defines what nodes would be selected for pods of node daemonset. Leave as blank to use all nodes. Yes \" \"   tolerations Defines tolerations that would be applied to node daemonset. Leave as blank to install node driver only on worker nodes. Yes \" \"   monitor This section allows the configuration of the SDC monitoring pod. - -   enabled Set to enable the usage of the monitoring pod. Yes false   hostNetwork Set whether the monitor pod should run on the host network or not. Yes true   hostPID Set whether the monitor pod should run in the host namespace or not. Yes true   healthMonitor This section configures node side volume health monitoring - -   enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false   vgsnapshotter This section allows the configuration of the volume group snapshotter(vgsnapshotter) pod. - -   enabled A boolean that enable/disable vg snapshotter feature. No false   image Image for vg snapshotter. No \" \"   podmon Podmon is an optional feature under development and tech preview. Enable this feature only after contact support for additional information. - -   enabled A boolean that enable/disable podmon feature. No false   image image for podmon. No \" \"   authorization Authorization is an optional feature to apply credential shielding of the backend PowerFlex. - -   enabled A boolean that enables/disables authorization feature. No false   sidecarProxyImage Image for csm-authorization-sidecar. No \" \"   proxyHost Hostname of the csm-authorization server. No Empty   skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. No true    Install the driver using csi-install.sh bash script by running cd dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml. Alternatively, to do a helm install solely with Helm charts (without shell scripts), refer to helm/README.md.  NOTE:\n  For detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder.\n  Install script will validate MDM IP(s) in vxflexos-config secret and creates a new field consumed by the init container and sdc-monitor container\n  This install script also runs the verify.sh script. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes.\n  It is mandatory to run install script after changes to MDM configuration in vxflexos-config secret. Refer dynamic-array-configuration\n  If an extended Kubernetes version is being used (e.g. v1.21.3-mirantis-1) and is failing the version check in Helm even though it falls in the allowed range, then you must go into helm/csi-vxflexos/Chart.yaml and replace the standard kubeVersion check with the commented-out alternative. Please note that this will also allow the use of pre-release alpha and beta versions of Kubernetes, which is not supported.\n  (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.\n Mount options are specified in storageclass yaml under mkfsFormatOption. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Certificate validation for PowerFlex Gateway REST API calls This topic provides details about setting up the certificate for the CSI Driver for Dell PowerFlex.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name vxflexos-certs-0 to vxflexos-certs-n based on the “.Values.certSecretCount” parameter present in the namespace vxflexos.\nThis secret contains the X509 certificates of the CA which signed PowerFlex gateway SSL certificate in PEM format.\nThe CSI driver exposes an install parameter in config.yaml, skipCertificateValidation, which determines if the driver performs client-side verification of the gateway certificates.\nskipCertificateValidation parameter is set to true by default, and the driver does not verify the gateway certificates.\nIf skipCertificateValidation is set to false, then the secret vxflexos-certs-n must contain the CA certificate for the array gateway.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the gateway certificate is self-signed or if you are using an embedded gateway, then perform the following steps.\n  To fetch the certificate, run the following command.\n `openssl s_client -showcerts -connect \u003cGateway IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem`  Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem\n  Run the following command to create the cert secret with index ‘0’:\n `kubectl create secret generic vxflexos-certs-0 --from-file=cert-0=ca_cert_0.pem -n vxflexos`  Use the following command to replace the secret:\n `kubectl create secret generic vxflexos-certs-0 -n vxflexos --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -`    Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: vxflexos-certs-1, vxflexos-certs-2, etc)\n  Notes:\n “vxflexos” is the namespace for Helm-based installation but namespace can be user-defined in operator-based installation. User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation. Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver. Updating vxflexos-certs-n secrets is a manual process, unlike vxflexos-config. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.  Storage Classes For CSI driver for PowerFlex version 1.4 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class: There are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\n Edit storageclass.yaml if you need ext4 filesystem and storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have. Replace \u003cSYSTEM_ID\u003e with the system ID you have. Note there are two appearances in the file. Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f storageclass.yaml or kubectl create -f storageclass-xfs.yaml  NOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerFlex v1.5, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerFlex v2.1 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerFlex to 1.5 or higher, before upgrading to 2.2.\n","excerpt":"The CSI Driver for Dell PowerFlex can be deployed by using the …","ref":"/csm-docs/v2/csidriver/installation/helm/powerflex/","title":"PowerFlex"},{"body":"Installing CSI Driver for PowerFlex via Operator The CSI Driver for Dell PowerFlex can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: SDC Deployment for Operator  This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non-supported versions of the OS also do the manual SDC deployment steps given below. Refer to https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When the driver is created, MDM value for initContainers in driver CR is set by the operator from mdm attributes in the driver configuration file, config.yaml. An example of config.yaml is below in this document. Do not set MDM value for initContainers in the driver CR file manually. Note: To use an sdc-binary module from customer ftp site:  Create a secret, sdc-repo-secret.yaml to contain the credentials for the private repo. To generate the base64 encoding of a credential:    echo-n\u003ccredential\u003e|base64-isecret sample to use:\napiVersion:v1kind:Secretmetadata:name:sdc-repo-credsnamespace:vxflexostype:Opaquedata:# set username to the base64 encoded username, sdc default isusername:\u003cusernameinbase64\u003e # set password to the base64 encoded password, sdc default ispassword:\u003cpasswordinbase64\u003e Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml. Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml. Please note the following:  If using sidecar, you will need to edit the value fields under the HOST_PID and MDM fields by filling the empty quotes with host PID and the MDM IPs. If not using sidecar, please leave this commented out – otherwise, the empty fields will cause errors.    Example CR: config/samples/vxflex_v220_ops_48.yaml sideCars:# Comment the following section if you don't want to run the monitoring sidecar- name:sdc-monitorenvs:- name:HOST_PIDvalue:\"1\"- name:MDMvalue:\"\"- name:external-health-monitorargs:[\"--monitor-interval=60s\"]initContainers:- image:dellemc/sdc:3.6imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\"10.x.x.x,10.x.x.x\"Note: Please comment the sdc-monitor sidecar section if you are not using it. Blank values for MDM will result in error. Do not comment the external-health-monitor argument.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  Install Driver   Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e command using the desired name to create the namespace.\n  Prepare the config.yaml for driver configuration.\nExample: config.yaml\n# Username for accessing PowerFlex system.\t# Required: true- username:\"admin\"# Password for accessing PowerFlex system.\t# Required: truepassword:\"password\"# System name/ID of PowerFlex system.\t# Required: truesystemID:\"ID1\"# REST API gateway HTTPS endpoint for PowerFlex system.# Required: trueendpoint:\"https://127.0.0.1\"# Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface.# Allowed values: true or false# Required: true# Default value: trueskipCertificateValidation:true# indicates if this array is the default array# needed for backwards compatibility# only one array is allowed to have this set to true # Required: false# Default value: falseisDefault:true# defines the MDM(s) that SDC should register with on start.# Allowed values: a list of IP addresses or hostnames separated by comma.# Required: true# Default value: none mdm:\"10.0.0.1,10.0.0.2\"# Defines all system names used to create powerflex volumes# Required: false# Default value: noneAllSystemNames:\"name1,name2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"AllSystemNames:\"name1,name2\"After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.yaml -o yaml --dry-run=client | kubectl replace -f -\nNote:\n System ID, MDM configuration, etc. now are taken directly from config.yaml. MDM provided in the input_sample_file.yaml will be overidden with MDM values in config.yaml. Please provide MDM values in input_sample_file.yaml so that it will be overidden by default value.    Create a Custom Resource (CR) for PowerFlex using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\n   Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, excess pods will become stay in a pending state. Defaults are 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false   X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false   X_CSI_DEBUG To enable debug mode No true   X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false      Execute the kubectl create -f \u003cinput_sample_file.yaml\u003e command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.\n Example CR for PowerFlex Driver apiVersion:storage.dell.com/v1    kind: CSIVXFlexOS metadata: name: test-vxflexos namespace: test-vxflexos spec: driver: configVersion: v2.2.0 replicas: 1 dnsPolicy: ClusterFirstWithHostNet forceUpdate: false common: image: “dellemc/csi-vxflexos:v2.2.0” imagePullPolicy: IfNotPresent envs: - name: X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT value: “false” - name: X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE value: “false” - name: X_CSI_DEBUG value: “true” - name: X_CSI_ALLOW_RWO_MULTI_POD_ACCESS value: “false” sideCars: # comment the following section if you don’t want to run the monitoring sidecar - name: sdc-monitor envs: - name: HOST_PID value: “1” - name: MDM value: \"”\n # Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" initContainers: - image: dellemc/sdc:3.6 imagePullPolicy: IfNotPresent name: sdc envs: - name: MDM value: \"10.xx.xx.xx,10.xx.xx.xx\" #provide MDM value --- apiVersion: v1 kind: ConfigMap metadata: name: vxflexos-config-params namespace: test-vxflexos data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"debug\" CSI_LOG_FORMAT: \"TEXT\" ```  Pre-Requisite for installation with OLM  Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.  1.gitclonehttps://github.com/dell/dell-csi-operator.git2.cddell-csi-operator3.tar-czfconfig.tar.gzdriverconfig/# Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM 4.kubectlcreateconfigmapdell-csi-operator-config--from-fileconfig.tar.gz-n\u003coperator-namespace\u003eVolume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin.# Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\".# - name: external-health-monitor# args: [\"--monitor-interval=60s\"]# Install the 'external-health-monitor' sidecar accordingly.# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: falsecontroller:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"node:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"","excerpt":"Installing CSI Driver for PowerFlex via Operator The CSI Driver for …","ref":"/csm-docs/v2/csidriver/installation/operator/powerflex/","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\n Navigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following:  Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured.  Results\nAn outline of this workflow is described below:\n The 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below:  kind:PersistentVolumeClaimapiVersion:v1metadata:name:pvolnamespace:helmtest-vxflexosspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GistorageClassName:vxflexosThe volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use a storage class named vxflexos. This step yields a mounted ext4 file system. You can create the vxflexos and vxflexos-xfs storage classes by using the yamls located in samples/storageclass. If you compare pvol0.yaml and pvol1.yaml, you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos.   NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.\n Test creating snapshots Test the workflow for snapshot creation.\n NOTE: Starting with version 2.0, CSI Driver for PowerFlex helm tests are designed to work exclusively with v1 snapshots.\n Steps\n Start the 2vols container and leave it running.  Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly.   Run sh snaptest.sh to start the test.  This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1namespace:helmtest-vxflexosspec:volumeSnapshotClassName:vxflexos-snapclasssource:persistentVolumeClaimName:pvol0Results\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0, then the created snapshot is named pvol0-snap1.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n helmtest-vxflexos.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell PowerFlex installation does not create this class. You will need to create instance of VolumeSnapshotClass from one of default samples in `samples/volumesnapshotclass’ directory.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\n Run sh snaprestoretest.sh to start the test.  This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updated directory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\n Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly.  Results\nAn outline of this workflow is described below:\n The snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim, which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below:  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:helmtest-vxflexosspec:storageClassName:vxflexosdataSource:name:pvol0-snap1kind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiNOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\n","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v2/csidriver/installation/test/powerflex/","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v2.2.0 New Features/Changes  Added support for Kubernetes 1.23. Added support for Amazon Elastic Kubernetes Service Anywhere.  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100    Note:  Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI PowerFlex v2.2.0 New Features/Changes  Added …","ref":"/csm-docs/v2/csidriver/release/powerflex/","title":"PowerFlex"},{"body":"   Symptoms Prevention, Resolution or Workaround     The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that need to pull an image of the driver.   When you run the command kubectl describe pods vxflexos-controller-* –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n vxflexos vxflexos-controller-* driver logs show that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system.   The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID.   The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script.   CreateVolume error System  is not configured in the driver Powerflex name if used for systemID in StorageClass ensure same name is also used in array config systemID   Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on a worker node, and ensure your container run time manager is properly configured to be utilized with SElinux.   Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux.   Installation of the driver on Kubernetes v1.21/v1.22/v1.23 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.21/v1.22/v1.23 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements   The kubectl logs -n vxflexos vxflexos-controller-* driver logs show x509: certificate signed by unknown authority A self assigned certificate is used for PowerFlex array. See certificate validation for PowerFlex Gateway   When you run the command kubectl apply -f snapclass-v1.yaml, you get the error error: unable to recognize \"snapclass-v1.yaml\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Check to make sure that the v1 snapshotter CRDs are installed, and not the v1beta1 CRDs, which are no longer supported.   The controller pod is stuck and producing errors such as” Failed to watch *v1.VolumeSnapshotContent: failed to list *v1.VolumeSnapshotContent: the server could not find the requested resource (get volumesnapshotcontents.snapshot.storage.k8s.io) Make sure that v1 snapshotter CRDs and v1 snapclass are installed, and not v1beta1, which is no longer supported.   Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.21.0 \u003c= 1.23.0 which is incompatible with Kubernetes V1.21.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart at helm/csi-vxflexos/Chart.yaml and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported.   Volume metrics are missing Enable Volume Health Monitoring     Note: vxflexos-controller-* is the controller pod that acquires leader lease\n ","excerpt":"   Symptoms Prevention, Resolution or Workaround     The installation …","ref":"/csm-docs/v2/csidriver/troubleshooting/powerflex/","title":"PowerFlex"},{"body":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell EMC PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell EMC PowerFlex Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements that must be met before installing the CSI Driver for Dell EMC PowerFlex:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Enable Zero Padding on PowerFlex Mount propagation is enabled on container runtime that is being used Install PowerFlex Storage Data Client If using Snapshot feature, satisfy all Volume Snapshot requirements A user must exist on the array with a role \u003e= FrontEndConfigure If enabling CSM for Authorization, please refer to the Authorization deployment steps first  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nEnable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. For more information to configure this setting, see Dell EMC PowerFlex documentation.\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run the node portion of the CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment; currently only Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer to https://hub.docker.com/r/dellemc/sdc for supported OS versions.\nOptional: For a typical install, you will pull SDC kernel modules from the Dell EMC FTP site, which is set up by default. Some users might want to mirror this repository to a local location. The PowerFlex KB article has instructions on how to do this.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\ncontroller:snapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here: v4.2.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v4.2.x\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n When using Kubernetes 1.20/1.21/1.22 it is recommended to use 4.2.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone -b v2.1.0 https://github.com/dell/csi-powerflex.git to clone the git repository.\n  Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace vxflexos to create a new one.\n  Collect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the scripts directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the values for these parameters as they must be entered into samples/config.yaml.\n  Prepare samples/config.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing PowerFlex system. If authorization is enabled, username will be ignored. true -   password Password for accessing PowerFlex system. If authorization is enabled, password will be ignored. true -   systemID System name/ID of PowerFlex system. true -   allSystemNames List of previous names of powerflex array if used for PV create false -   endpoint REST API gateway HTTPS endpoint for PowerFlex system. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on true -   skipCertificateValidation Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface. true true   isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list. false false   mdm mdm defines the MDM(s) that SDC should register with on start. This should be a list of MDM IP addresses or hostnames separated by comma. true -    Example: samples/config.yaml\n# Username for accessing PowerFlex system.\t# If authorization is enabled, username will be ignored.- username:\"admin\"# Password for accessing PowerFlex system.\t# If authorization is enabled, password will be ignored.password:\"password\"# System name/ID of PowerFlex system.\tsystemID:\"ID1\"# Previous names of PowerFlex system if used for PV.allSystemNames:\"pflex-1,pflex-2\"# REST API gateway HTTPS endpoint for PowerFlex system.# If authorization is enabled, endpoint should be the HTTPS localhost endpoint that # the authorization sidecar will listen onendpoint:\"https://127.0.0.1\"# Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface.# Allowed values: true or false# Default value: trueskipCertificateValidation:true# indicates if this array is the default array# needed for backwards compatibility# only one array is allowed to have this set to true # Default value: falseisDefault:true# defines the MDM(s) that SDC should register with on start.# Allowed values: a list of IP addresses or hostnames separated by comma.# Default value: none mdm:\"10.0.0.1,10.0.0.2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"After editing the file, run the following command to create a secret called vxflexos-config:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/config.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=samples/config.yaml -o yaml --dry-run=client | kubectl replace -f -\nNOTE:\n The user needs to validate the YAML syntax and array-related key/values while replacing the vxflexos-creds secret. If you want to create a new array or update the MDM values in the secret, you will need to reinstall the driver. If you change other details, such as login information, the secret will dynamically update – see dynamic-array-configuration for more details. Old json format of the array configuration file is still supported in this release. If you already have your configuration in json format, you may continue to maintain it or you may transfer this configuration to yaml format and replace/update the secret. “insecure” parameter has been changed to “skipCertificateValidation” as insecure is deprecated and will be removed from use in config.yaml or secret.yaml in a future release. Users can continue to use any one of “insecure” or “skipCertificateValidation” for now. The driver would return an error if both parameters are used. Please note that log configuration parameters from v1.5 will no longer work in v2.0 and higher. Please refer to the Dynamic Logging Configuration section in Features for more information.    Default logging options are set during Helm install. To see possible configuration options, see the Dynamic Logging Configuration section in Features.\n  If using automated SDC deployment:\n Check the SDC container image is the correct version for your version of PowerFlex.    Copy the default values.yaml file cd helm \u0026\u0026 cp csi-vxflexos/values.yaml myvalues.yaml\n  If you are using a custom image, check the version and driverRepository fields in myvalues.yaml to make sure that they are pointing to the correct image repository and driver version. These two fields are spliced together to form the image name, as shown here: \u003cdriverRepository\u003e/csi-vxflexos:v\u003cversion\u003e\n  Look over all the other fields myvalues.yaml and fill in/adjust any as needed. All the fields are described here:\n     Parameter Description Required Default     version Set to verify the values file version matches driver version and used to pull the image as part of the image name. Yes 2.0.0   driverRepository Set to give the repository containing the driver image (used as part of the image name). Yes dellemc   powerflexSdc Set to give the location of the SDC image used if automatic SDC deployment is being utilized. No dellemc/sdc:3.6   certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. No 0   logLevel CSI driver log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes “debug”   logFormat CSI driver log format. Allowed values: “TEXT” or “JSON”. Yes “TEXT”   kubeletConfigDir kubelet config directory path. Ensure that the config.yaml file is present at this path. Yes /var/lib/kubelet   defaultFsType Used to set the default FS type which will be used for mount volumes if FsType is not specified in the storage class. Allowed values: ext4, xfs. Yes ext4   imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Allowed values: Always, IfNotPresent, Never. Yes IfNotPresent   enablesnapshotcgdelete A boolean that, when enabled, will delete all snapshots in a consistency group everytime a snap in the group is deleted. Yes false   enablelistvolumesnapshot A boolean that, when enabled, will allow list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap). It is recommend this be false unless instructed otherwise. Yes false   allowRWOMultiPodAccess Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. Yes false   controller This section allows the configuration of controller-specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - -   volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. Yes “k8s”   controllerCount Set to deploy multiple controller instances. If the controller count is greater than the number of available nodes, excess pods remain in a pending state. It should be greater than 0. You can increase the number of available nodes by configuring the “controller” section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2   snapshot.enabled A boolean that enable/disable volume snapshot feature. No true   resizer.enabled A boolean that enable/disable volume expansion feature. No true   nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. Yes \" \"   tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install the controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. Yes \" \"   healthMonitor This section configures the optional deployment of the external health monitor sidecar, for controller side volume health monitoring. - -   enabled Enable/Disable deployment of external health monitor sidecar. No false   volumeHealthMonitorInterval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s   node This section allows the configuration of node-specific parameters. - -   nodeSelector Defines what nodes would be selected for pods of node daemonset. Leave as blank to use all nodes. Yes \" \"   tolerations Defines tolerations that would be applied to node daemonset. Leave as blank to install node driver only on worker nodes. Yes \" \"   monitor This section allows the configuration of the SDC monitoring pod. - -   enabled Set to enable the usage of the monitoring pod. Yes false   hostNetwork Set whether the monitor pod should run on the host network or not. Yes true   hostPID Set whether the monitor pod should run in the host namespace or not. Yes true   healthMonitor This section configures node side volume health monitoring - -   enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false   vgsnapshotter This section allows the configuration of the volume group snapshotter(vgsnapshotter) pod. - -   enabled A boolean that enable/disable vg snapshotter feature. No false   image Image for vg snapshotter. No \" \"   podmon Podmon is an optional feature under development and tech preview. Enable this feature only after contact support for additional information. - -   enabled A boolean that enable/disable podmon feature. No false   image image for podmon. No \" \"   authorization Authorization is an optional feature to apply credential shielding of the backend PowerFlex. - -   enabled A boolean that enables/disables authorization feature. No false   sidecarProxyImage Image for csm-authorization-sidecar. No \" \"   proxyHost Hostname of the csm-authorization server. No Empty   skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. No true    Install the driver using csi-install.sh bash script by running cd dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml. Alternatively, to do a helm install solely with Helm charts (without shell scripts), refer to helm/README.md.  NOTE:\n  For detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder.\n  Install script will validate MDM IP(s) in vxflexos-config secret and creates a new field consumed by the init container and sdc-monitor container\n  This install script also runs the verify.sh script. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes.\n  It is mandatory to run install script after changes to MDM configuration in vxflexos-config secret. Refer dynamic-array-configuration\n  (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.\n Mount options are specified in storageclass yaml under mkfsFormatOption. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Certificate validation for PowerFlex Gateway REST API calls This topic provides details about setting up the certificate for the CSI Driver for Dell EMC PowerFlex.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name vxflexos-certs-0 to vxflexos-certs-n based on the “.Values.certSecretCount” parameter present in the namespace vxflexos.\nThis secret contains the X509 certificates of the CA which signed PowerFlex gateway SSL certificate in PEM format.\nThe CSI driver exposes an install parameter in config.yaml, skipCertificateValidation, which determines if the driver performs client-side verification of the gateway certificates.\nskipCertificateValidation parameter is set to true by default, and the driver does not verify the gateway certificates.\nIf skipCertificateValidation is set to false, then the secret vxflexos-certs-n must contain the CA certificate for the array gateway.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the gateway certificate is self-signed or if you are using an embedded gateway, then perform the following steps.\n  To fetch the certificate, run the following command.\n `openssl s_client -showcerts -connect \u003cGateway IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem`  Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem\n  Run the following command to create the cert secret with index ‘0’:\n `kubectl create secret generic vxflexos-certs-0 --from-file=cert-0=ca_cert_0.pem -n vxflexos`  Use the following command to replace the secret:\n `kubectl create secret generic vxflexos-certs-0 -n vxflexos --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -`    Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: vxflexos-certs-1, vxflexos-certs-2, etc)\n  Notes:\n “vxflexos” is the namespace for Helm-based installation but namespace can be user-defined in operator-based installation. User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation. Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver. Updating vxflexos-certs-n secrets is a manual process, unlike vxflexos-config. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.  Storage Classes For CSI driver for PowerFlex version 1.4 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class: There are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\n Edit storageclass.yaml if you need ext4 filesystem and storageclass-xfs.yaml if you want xfs filesystem. Replace \u003cSTORAGE_POOL\u003e with the storage pool you have. Replace \u003cSYSTEM_ID\u003e with the system ID you have. Note there are two appearances in the file. Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f storageclass.yaml or kubectl create -f storageclass-xfs.yaml  NOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerFlex v1.5, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/ folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nNOTE Support for v1beta1 snapshots is being discontinued in this release.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerFlex v2.0 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerFlex to 1.5 or higher, before upgrading to 2.1.\n","excerpt":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the …","ref":"/csm-docs/v3/csidriver/installation/helm/powerflex/","title":"PowerFlex"},{"body":"Installing CSI Driver for PowerFlex via Operator The CSI Driver for Dell EMC PowerFlex can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: SDC Deployment for Operator  This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non-supported versions of the OS also do the manual SDC deployment steps given below. Refer to https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When the driver is created, MDM value for initContainers in driver CR is set by the operator from mdm attributes in the driver configuration file, config.yaml. An example of config.yaml is below in this document. Do not set MDM value for initContainers in the driver CR file manually. Note: To use an sdc-binary module from customer ftp site:  Create a secret, sdc-repo-secret.yaml to contain the credentials for the private repo. To generate the base64 encoding of a credential:    echo-n\u003ccredential\u003e|base64-isecret sample to use:\napiVersion:v1kind:Secretmetadata:name:sdc-repo-credsnamespace:vxflexostype:Opaquedata:# set username to the base64 encoded username, sdc default isusername:\u003cusernameinbase64\u003e # set password to the base64 encoded password, sdc default ispassword:\u003cpasswordinbase64\u003e Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml. Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml. Please note the following:  If using sidecar, you will need to edit the value fields under the HOST_PID and MDM fields by filling the empty quotes with host PID and the MDM IPs. If not using sidecar, please leave this commented out – otherwise, the empty fields will cause errors.    Example CR: config/samples/vxflex_v220_ops_48.yaml sideCars:# Comment the following section if you don't want to run the monitoring sidecar- name:sdc-monitorenvs:- name:HOST_PIDvalue:\"1\"- name:MDMvalue:\"\"- name:external-health-monitorargs:[\"--monitor-interval=60s\"]initContainers:- image:dellemc/sdc:3.6imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\"10.x.x.x,10.x.x.x\"Note: Please comment the sdc-monitor sidecar section if you are not using it. Blank values for MDM will result in error. Do not comment the external-health-monitor argument.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and CentOS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  Install Driver   Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e command using the desired name to create the namespace.\n  Prepare the config.yaml for driver configuration.\nExample: config.yaml\n# Username for accessing PowerFlex system.\t# Required: true- username:\"admin\"# Password for accessing PowerFlex system.\t# Required: truepassword:\"password\"# System name/ID of PowerFlex system.\t# Required: truesystemID:\"ID1\"# REST API gateway HTTPS endpoint for PowerFlex system.# Required: trueendpoint:\"https://127.0.0.1\"# Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface.# Allowed values: true or false# Required: true# Default value: trueskipCertificateValidation:true# indicates if this array is the default array# needed for backwards compatibility# only one array is allowed to have this set to true # Required: false# Default value: falseisDefault:true# defines the MDM(s) that SDC should register with on start.# Allowed values: a list of IP addresses or hostnames separated by comma.# Required: true# Default value: none mdm:\"10.0.0.1,10.0.0.2\"# Defines all system names used to create powerflex volumes# Required: false# Default value: noneAllSystemNames:\"name1,name2\"- username:\"admin\"password:\"Password123\"systemID:\"ID2\"endpoint:\"https://127.0.0.2\"skipCertificateValidation:truemdm:\"10.0.0.3,10.0.0.4\"AllSystemNames:\"name1,name2\"After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n \u003cdriver-namespace\u003e --from-file=config=config.yaml -o yaml --dry-run=client | kubectl replace -f -\nNote:\n System ID, MDM configuration, etc. now are taken directly from config.yaml. MDM provided in the input_sample_file.yaml will be overidden with MDM values in config.yaml. Please provide MDM values in input_sample_file.yaml so that it will be overidden by default value.    Create a Custom Resource (CR) for PowerFlex using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\n   Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, excess pods will become stay in a pending state. Defaults are 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false   X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false   X_CSI_DEBUG To enable debug mode No true   X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to “true” will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false      Execute the kubectl create -f \u003cinput_sample_file.yaml\u003e command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.\n Example CR for PowerFlex Driver apiVersion:storage.dell.com/v1    kind: CSIVXFlexOS metadata: name: test-vxflexos namespace: test-vxflexos spec: driver: configVersion: v2.2.0 replicas: 1 dnsPolicy: ClusterFirstWithHostNet forceUpdate: false common: image: “dellemc/csi-vxflexos:v2.2.0” imagePullPolicy: IfNotPresent envs: - name: X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT value: “false” - name: X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE value: “false” - name: X_CSI_DEBUG value: “true” - name: X_CSI_ALLOW_RWO_MULTI_POD_ACCESS value: “false” sideCars: # comment the following section if you don’t want to run the monitoring sidecar - name: sdc-monitor envs: - name: HOST_PID value: “1” - name: MDM value: \"”\n # Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" initContainers: - image: dellemc/sdc:3.6 imagePullPolicy: IfNotPresent name: sdc envs: - name: MDM value: \"10.xx.xx.xx,10.xx.xx.xx\" #provide MDM value --- apiVersion: v1 kind: ConfigMap metadata: name: vxflexos-config-params namespace: test-vxflexos data: driver-config-params.yaml: | CSI_LOG_LEVEL: \"debug\" CSI_LOG_FORMAT: \"TEXT\" ```  Pre-Requisite for installation with OLM  Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.  1.gitclonehttps://github.com/dell/dell-csi-operator.git2.cddell-csi-operator3.tar-czfconfig.tar.gzdriverconfig/# Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM 4.kubectlcreateconfigmapdell-csi-operator-config--from-fileconfig.tar.gz-n\u003coperator-namespace\u003eVolume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin.# Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\".# - name: external-health-monitor# args: [\"--monitor-interval=60s\"]# Install the 'external-health-monitor' sidecar accordingly.# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: falsecontroller:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"node:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"","excerpt":"Installing CSI Driver for PowerFlex via Operator The CSI Driver for …","ref":"/csm-docs/v3/csidriver/installation/operator/powerflex/","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\n Navigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following:  Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \"docker.io/centos:latest\" Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured.  Results\nAn outline of this workflow is described below:\n The 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below:  kind:PersistentVolumeClaimapiVersion:v1metadata:name:pvolnamespace:helmtest-vxflexosspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GistorageClassName:vxflexosThe volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use a storage class named vxflexos. This step yields a mounted ext4 file system. You can create the vxflexos and vxflexos-xfs storage classes by using the yamls located in samples/storageclass. If you compare pvol0.yaml and pvol1.yaml, you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos.   NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.\n Test creating snapshots Test the workflow for snapshot creation.\n NOTE: Starting with version 2.0, CSI Driver for PowerFlex helm tests are designed to work exclusively with v1 snapshots.\n Steps\n Start the 2vols container and leave it running.  Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly.   Run sh snaptest.sh to start the test.  This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1namespace:helmtest-vxflexosspec:volumeSnapshotClassName:vxflexos-snapclasssource:persistentVolumeClaimName:pvol0Results\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0, then the created snapshot is named pvol0-snap1.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n helmtest-vxflexos.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell EMC PowerFlex installation does not create this class. You will need to create instance of VolumeSnapshotClass from one of default samples in `samples/volumesnapshotclass’ directory.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\n Run sh snaprestoretest.sh to start the test.  This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updated directory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\n Helm tests are designed assuming users are using the storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from these values, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the snapshotclass name: vxflexos-snapclass If your snapshotclass name differs from the default values, update snap1.yaml and snap2.yaml accordingly.  Results\nAn outline of this workflow is described below:\n The snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim, which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below:  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:helmtest-vxflexosspec:storageClassName:vxflexosdataSource:name:pvol0-snap1kind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiNOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot’s name in snap1.yaml.\n","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v3/csidriver/installation/test/powerflex/","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v2.1.0 New Features/Changes  Added support for OpenShift v4.9. Added support for CSI spec 1.5. Added support for new access modes in CSI Spec 1.5. Added support for PV/PVC metrics. Added support for CSM Authorization sidecar via Helm. Added v1 extensions to vg snaphot from v1alpha2. Added support to update helm charts to do a helm install without shell scripts. Added support for volume health monitoring Removed support for Fedora CoreOS  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100    Note:  Support for kurbernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode introduced in the release will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI PowerFlex v2.1.0 New Features/Changes  Added …","ref":"/csm-docs/v3/csidriver/release/powerflex/","title":"PowerFlex"},{"body":"   Symptoms Prevention, Resolution or Workaround     The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that need to pull an image of the driver.   When you run the command kubectl describe pods vxflexos-controller-* –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n vxflexos vxflexos-controller-* driver logs show that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system.   The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID.   The kubectl logs vxflexos-controller-* -n vxflexos driver logs show that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script.   CreateVolume error System  is not configured in the driver Powerflex name if used for systemID in StorageClass ensure same name is also used in array config systemID   Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on a worker node, and ensure your container run time manager is properly configured to be utilized with SElinux.   Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux.   Installation of the driver on Kubernetes v1.20/v1.21/v1.22 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20/1.21/v1.22 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements   The kubectl logs -n vxflexos vxflexos-controller-* driver logs show x509: certificate signed by unknown authority A self assigned certificate is used for PowerFlex array. See certificate validation for PowerFlex Gateway   When you run the command kubectl apply -f snapclass-v1.yaml, you get the error error: unable to recognize \"snapclass-v1.yaml\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Check to make sure that the v1 snapshotter CRDs are installed, and not the v1beta1 CRDs, which are no longer supported.   The controller pod is stuck and producing errors such as” Failed to watch *v1.VolumeSnapshotContent: failed to list *v1.VolumeSnapshotContent: the server could not find the requested resource (get volumesnapshotcontents.snapshot.storage.k8s.io) Make sure that v1 snapshotter CRDs and v1 snapclass are installed, and not v1beta1, which is no longer supported.     Note: vxflexos-controller-* is the controller pod that acquires leader lease\n ","excerpt":"   Symptoms Prevention, Resolution or Workaround     The installation …","ref":"/csm-docs/v3/csidriver/troubleshooting/powerflex/","title":"PowerFlex"},{"body":"CSI Driver for Dell PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, see the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume (optional) Kubernetes External health monitor, which provides volume health status (optional) CSI PowerMax ReverseProxy, which maximizes CSI driver and Unisphere performance (optional) Dell CSI Replicator, which provides Replication capability.  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell PowerMax Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following requirements must be met before installing CSI Driver for Dell PowerMax:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Fibre Channel requirements iSCSI requirements Certificate validation for Unisphere REST API calls Mount propagation is enabled on container runtime that is being used Linux multipathing requirements If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first  Install Helm 3 Install Helm 3 on the master node before you install CSI Driver for Dell PowerMax.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.\nFibre Channel Requirements CSI Driver for Dell PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.  iSCSI Requirements The CSI Driver for Dell PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\n All Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell PowerMax array. All the port group names supplied to the driver must exist on each Dell PowerMax with the same name.  For more information about configuring iSCSI, see Dell Host Connectivity guide.\nCertificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\n  To fetch the certificate, run openssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null 2\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem\nNOTE: The IP address varies for each user.\n  To create the secret, run kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax\n  Ports in the port group There are no restrictions to how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell PowerMax to ensure that you have multiple paths to your data volumes.\nLinux multipathing requirements CSI Driver for Dell PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\n All the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file.  As a best practice, use the following options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. For installation, use v5.0.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers to support Volume snapshots.\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster, irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v5.0.x\nNOTE:\n The manifests available on GitHub install the snapshotter image: quay.io/k8scsi/csi-snapshotter:v4.0.x The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication:enabled:trueReplication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\n Run git clone -b v2.3.0 https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the `samples/secret/secret.yaml file, point to the correct namespace, and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax privileges.\n Create the secret by running kubectl create -f samples/secret/secret.yaml. If you are going to install the new CSI PowerMax ReverseProxy service, create a TLS secret with the name - csireverseproxy-tls-secret which holds an SSL certificate and the corresponding private key in the namespace where you are installing the driver. Copy the default values.yaml file cd helm \u0026\u0026 cp csi-powermax/values.yaml my-powermax-settings.yaml Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml     Parameter Description Required Default     global This section refers to configuration options for both CSI PowerMax Driver and Reverse Proxy - -   defaultCredentialsSecret This secret name refers to:\n1. The Unisphere credentials if the driver is installed without proxy or with proxy in Linked mode.\n2. The proxy credentials if the driver is installed with proxy in StandAlone mode.\n3. The default Unisphere credentials if credentialsSecret is not specified for a management server. Yes powermax-creds   storageArrays This section refers to the list of arrays managed by the driver and Reverse Proxy in StandAlone mode. - -   storageArrayId This refers to PowerMax Symmetrix ID. Yes 000000000001   endpoint This refers to the URL of the Unisphere server managing storageArrayId. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes if Reverse Proxy mode is StandAlone https://primary-1.unisphe.re:8443   backupEndpoint This refers to the URL of the backup Unisphere server managing storageArrayId, if Reverse Proxy is installed in StandAlone mode. If authorization is enabled, backupEndpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on No https://backup-1.unisphe.re:8443   managementServers This section refers to the list of configurations for Unisphere servers managing powermax arrays. - -   endpoint This refers to the URL of the Unisphere server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes https://primary-1.unisphe.re:8443   credentialsSecret This refers to the user credentials for endpoint No primary-1-secret   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty   limits This refers to various limits for Reverse Proxy No -   maxActiveRead This refers to the maximum concurrent READ request handled by the reverse proxy. No 5   maxActiveWrite This refers to the maximum concurrent WRITE request handled by the reverse proxy. No 4   maxOutStandingRead This refers to maximum queued READ request when reverse proxy receives more than maxActiveRead requests. No 50   maxOutStandingWrite This refers to maximum queued WRITE request when reverse proxy receives more than maxActiveWrite requests. No 50   kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent   clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC”   logLevel CSI driver log level. Allowed values: “error”, “warn”/“warning”, “info”, “debug”. Yes “debug”   logFormat CSI driver log format. Allowed values: “TEXT” or “JSON”. Yes “TEXT”   kubeletConfigDir kubelet config directory path. Ensure that the config.yaml file is present at this path. Yes /var/lib/kubelet   defaultFsType Used to set the default FS type for external provisioner Yes ext4   portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3”   skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True”   transportProtocol Set the preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty   nodeNameTemplate Used to specify a template that will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, leave this value empty. No Empty   modifyHostName Change any existing host names. When nodenametemplate is set, it changes the name to the specified format else it uses driver default host name format. No false   powerMaxDebug Enables low level and http traffic logging between the CSI driver and Unisphere. Don’t enable this unless asked to do so by the support team. No false   enableCHAP Determine if the driver is going to configure SCSI node databases on the nodes with the CHAP credentials. If enabled, the CHAP secret must be provided in the credentials secret and set to the key “chapsecret” No false   fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType”   version Current version of the driver. Don’t modify this value as this value will be used by the install script. Yes v2.3.0   images Defines the container images used by the driver. - -   driverRepository Defines the registry of the container image used for the driver. Yes dellemc   controller Allows configuration of the controller-specific parameters. - -   controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2   volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s”   snapshot.enabled Enable/Disable volume snapshot feature Yes true   snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot”   resizer.enabled Enable/Disable volume expansion feature Yes true   healthMonitor.enabled Allows to enable/disable volume health monitor No false   healthMonitor.interval Interval of monitoring volume health condition No 60s   nodeSelector Define node selection constraints for pods of controller deployment No    tolerations Define tolerations for the controller deployment, if required No    node Allows configuration of the node-specific parameters. - -   tolerations Add tolerations as per requirement No -   nodeSelector Add node selectors as per requirement No -   healthMonitor.enabled Allows to enable/disable volume health monitor No false   topologyControl.enabled Allows to enable/disable topology control to filter topology keys No false   csireverseproxy This section refers to the configuration options for CSI PowerMax Reverse Proxy - -   enabled Boolean parameter which indicates if CSI PowerMax Reverse Proxy is going to be configured and installed.\nNOTE: If not enabled, then there is no requirement to configure any of the following values. No “False”   image This refers to the image of the CSI Powermax Reverse Proxy container. Yes dellemc/csipowermax-reverseproxy:v2.1.0   tlsSecret This refers to the TLS secret of the Reverse Proxy Server. Yes csirevproxy-tls-secret   deployAsSidecar If set to true, the Reverse Proxy is installed as a sidecar to the driver’s controller pod otherwise it is installed as a separate deployment. Yes “True”   port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation Yes 2222   mode This refers to the installation mode of Reverse Proxy. It can be set to:\n1. Linked: In this mode, the Reverse Proxy communicates with a primary or a backup Unisphere managing the same set of arrays.\n2. StandAlone: In this mode, the Reverse Proxy communicates with multiple arrays managed by different Unispheres. Yes “StandAlone”   authorization Authorization is an optional feature to apply credential shielding of the backend PowerMax. - -   enabled A boolean that enables/disables authorization feature. No false   sidecarProxyImage Image for csm-authorization-sidecar. No \" \"   proxyHost Hostname of the csm-authorization server. No Empty   skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. No true   migration Migration is an optional feature to enable migration between storage classes - -   enabled A boolean that enables/disables migration feature. No false   image Image for dell-csi-migrator sidecar. No \" \"   migrationPrefix enables migration sidecar to read required information from the storage class fields No migration.storage.dell.com   replication Replication is an optional feature to enable replication \u0026 disaster recovery capabilities of PowerMax to Kubernetes clusters. - -   enabled A boolean that enables/disables replication feature. No false   image Image for dell-csi-replicator sidecar. No \" \"   replicationContextPrefix enables side cars to read required information from the volume context No powermax   replicationPrefix Determine if replication is enabled No replication.storage.dell.com    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml Or you can also install the driver using standalone helm chart using the command helm install --values my-powermax-settings.yaml --namespace powermax powermax ./csi-powermax  Note:\n For detailed instructions on how to run the install scripts, see the readme document in the dell-csi-helm-installer folder. There are a set of samples provided here to help you configure the driver with reverse proxy This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option In order to enable authorization, there should be an authorization proxy server already installed. PowerMax Array username must have role as StorageAdmin to be able to perform CRUD operations. If the user is using complex K8s version like “v1.22.3-mirantis-1”, use below kubeVersion check in helm Chart file. kubeVersion: “\u003e= 1.22.0-0 \u003c 1.25.0-0”. User should provide all boolean values with double-quotes. This applies only for values.yaml. Example: “true”/“false”. controllerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails. Endpoint should not have any special character at the end apart from port number.  Storage Classes Starting CSI PowerMax v1.6, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests has been provided in the samples/storageclass folder. Please use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. To continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Volume Snapshot Class Starting with CSI PowerMax v1.7.0, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerMax v2.1.0 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerMax to 1.7.0 or higher, before upgrading to 2.3.0.\nSample values file The following sections have useful snippets from values.yaml file which provides more information on how to configure the CSI PowerMax driver along with CSI PowerMax ReverseProxy in various modes\nCSI PowerMax driver without Proxy In this mode, the CSI PowerMax driver can only connect to a single Unisphere server. So, you just specify a list of storage arrays and the address of the Unisphere server\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"- storageArrayId:\"000000000002\"managementServers:- endpoint:https://unisphere-address:8443 Note: If you provide multiple endpoints in the list of management servers, the installer will only use the first server in the list\n CSI PowerMax driver with Proxy in Linked mode In this mode, the CSI PowerMax ReverseProxy acts as a passthrough for the RESTAPI calls and only provides limited functionality such as rate limiting, backup Unisphere server. The CSI PowerMax driver is still responsible for the authentication with the Unisphere server.\nThe first endpoint in the list of management servers is the primary Unisphere server and if you provide a second endpoint, then it will be considered as the backup Unisphere’s endpoint.\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"- storageArrayId:\"000000000002\"managementServers:- endpoint:https://primary-unisphere:8443skipCertificateValidation:falsecertSecret:primary-certlimits:maxActiveRead:5maxActiveWrite:4maxOutStandingRead:50maxOutStandingWrite:50- endpoint:https://backup-unisphere:8443#Optional# \"csireverseproxy\" refers to the subchart csireverseproxycsireverseproxy:# Set enabled to true if you want to use proxyenabled:trueimage:dellemc/csipowermax-reverseproxy:v1.4.0tlsSecret:csirevproxy-tls-secretdeployAsSidecar:trueport:2222mode:Linked Note: Since the driver is still responsible for authentication when used with Proxy in Linked mode, the credentials for both primary and backup Unisphere need to be the same.\n CSI PowerMax driver with Proxy in StandAlone mode This is the most advanced configuration which provides you with the capability to connect to Multiple Unisphere servers. You can specify primary and backup Unisphere servers for each storage array. If you have different credentials for your Unisphere servers, you can also specify different credential secrets.\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"endpoint:https://primary-1.unisphe.re:8443backupEndpoint:https://backup-1.unisphe.re:8443- storageArrayId:\"000000000002\"endpoint:https://primary-2.unisphe.re:8443backupEndpoint:https://backup-2.unisphe.re:8443managementServers:- endpoint:https://primary-1.unisphe.re:8443credentialsSecret:primary-1-secretskipCertificateValidation:falsecertSecret:primary-certlimits:maxActiveRead:5maxActiveWrite:4maxOutStandingRead:50maxOutStandingWrite:50- endpoint:https://backup-1.unisphe.re:8443credentialsSecret:backup-1-secretskipCertificateValidation:true- endpoint:https://primary-2.unisphe.re:8443credentialsSecret:primary-2-secretskipCertificateValidation:true- endpoint:https://backup-2.unisphe.re:8443credentialsSecret:backup-2-secretskipCertificateValidation:true# \"csireverseproxy\" refers to the subchart csireverseproxycsireverseproxy:# Set enabled to true if you want to use proxyenabled:trueimage:dellemc/csipowermax-reverseproxy:v1.4.0tlsSecret:csirevproxy-tls-secretdeployAsSidecar:trueport:2222mode:StandAlone Note: If the credential secret is missing from any management server details, the installer will try to use the defaultCredentialsSecret\n ","excerpt":"CSI Driver for Dell PowerMax can be deployed by using the provided …","ref":"/csm-docs/docs/csidriver/installation/helm/powermax/","title":"PowerMax"},{"body":"Installing CSI Driver for PowerMax via Operator CSI Driver for Dell PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Fibre Channel Requirements CSI Driver for Dell PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.  iSCSI Requirements The CSI Driver for Dell PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\n All Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell PowerMax array. All the port group names supplied to the driver must exist on each Dell PowerMax with the same name.  For more information about configuring iSCSI, see Dell Host Connectivity guide.\nCreate secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace. Create PowerMax credentials: Create a file called powermax-creds.yaml with the following content: apiVersion:v1kind:Secretmetadata:name:powermax-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards)# chapsecret: \u003cbase64 CHAP secret\u003eReplace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerMax using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:    Parameter Description Required Default     replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2   fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType”   Common parameters for node and controller      X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ   X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443   X_CSI_TRANSPORT_PROTOCOL Choose which transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No -   X_CSI_MANAGED_ARRAYS List of comma-separated array ID(s) which will be managed by the driver Yes -   X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Leave blank if not using reverse proxy No -   X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4   X_CSI_IG_MODIFY_HOSTNAME Change any existing host names. When nodenametemplate is set, it changes the name to the specified format else it uses driver default host name format. No false   X_CSI_IG_NODENAME_TEMPLATE Provide a template for the CSI driver to use while creating the Host/IG on the array for the nodes in the cluster. It is of the format a-b-c-%foo%-xyz where foo will be replaced by host name of each node in the cluster. No -   X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No -   X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller and Node plugin. Provides details of volume status, usage and volume condition. As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar No false   Node parameters      X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false   X_CSI_TOPOLOGY_CONTROL_ENABLED Enable/Disabe topology control. It filters out arrays, associated transport protocol available to each node and creates topology keys based on any such user input. No false     Execute the following command to create the PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.  Note - If CSI driver is getting installed using OCP UI , create these two configmaps manually using the command oc create -f \u003cconfigfilename\u003e\n Configmap name powermax-config-params apiVersion:v1kind:ConfigMapmetadata:name:powermax-config-paramsnamespace:test-powermaxdata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"debug\"CSI_LOG_FORMAT:\"JSON\" Configmap name node-topology-config kind:ConfigMapmetadata:name:node-topology-confignamespace:test-powermaxdata:topologyConfig.yaml:| allowedConnections:- nodeName:\"node1\"rules:- \"000000000001:FC\"- \"000000000002:FC\"- nodeName:\"*\"rules:- \"000000000002:FC\"deniedConnections:- nodeName:\"node2\"rules:- \"000000000002:*\"- nodeName:\"node3\"rules:- \"*:*\"  CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is an optional component that can be installed with the CSI PowerMax driver. For more details on this feature see the related documentation.\nWhen you install CSI PowerMax ReverseProxy, dell-csi-operator will create a Deployment and ClusterIP service as part of the installation\nNote - To use the ReverseProxy with the CSI PowerMax driver, the ReverseProxy service should be created before you install the CSIPowerMax driver.\nPre-requisites Create a TLS secret that holds an SSL certificate and a private key which is required by the reverse proxy server. Use a tool such as openssl to generate this secret using the example below:\n openssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n powermax tls revproxy-certs --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec  tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs config : This section contains the details of the Reverse Proxy configuration mode : This value is set to Linked by default. Do not change this value linkConfig : This section contains the configuration of the Linked mode primary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if the primary Unisphere is unreachable url : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false standAloneConfig : This section contains the configuration of the StandAlone mode. Refer to the sample below for the detailed config   Note: Only one of the Linked or StandAlone configurations needs to be supplied. The appropriate mode needs to be set in the spec as well.\n Here is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion:storage.dell.com/v1kind:CSIPowerMaxRevProxymetadata:name:powermax-reverseproxy# \u003c- Name of the CSIPowerMaxRevProxy objectnamespace:test-powermax# \u003c- Set the namespace to where you will install the CSI PowerMax driverspec:# Image for CSI PowerMax ReverseProxyimage:dellemc/csipowermax-reverseproxy:v2.1.0# \u003c- CSI PowerMax Reverse Proxy imageimagePullPolicy:Always# TLS secret which contains SSL certificate and private key for the Reverse Proxy servertlsSecret:csirevproxy-tls-secretconfig:mode:LinkedlinkConfig:primary:url:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:true# This setting determines if client side Unisphere certificate validation is to be skippedcertSecret:\"\"# Provide this value if skipCertificateValidation is set to falsebackup:# This is an optional field and lets you configure a backup unisphere which can be used by proxy serverurl:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:truestandAloneConfig:# Set mode to \"StandAlone\" in order to use this configstorageArrays:- storageArrayId:\"000000000001\"# Unisphere server managing the PowerMax arrayprimaryURL:https://unisphere-1-addr:8443# proxyCredentialSecrets are used by the clients of the proxy to connect to it# If using proxy in the stand alone mode, then the driver must be provided the# same secret.# The format of the proxy credential secret are exactly the same as the unisphere credential secret# For using the proxy with the driver, use the same proxy credential secrets for# all the managed storage arraysproxyCredentialSecrets:- proxy-creds- storageArrayId:\"000000000002\"primaryURL:https://unisphere-2-addr:8443# An optional backup Unisphere server managing the same array# This can be used by the proxy to fall back to in case the primary# Unisphere is inaccessible temporarilybackupURL:unisphere-3-addr:8443proxyCredentialSecrets:- proxy-credsmanagementServers:- url:https://unisphere-1-addr:8443# Secret containing the credentials of the Unisphere serverarrayCredentialSecret:unsiphere-1-credsskipCertificateValidation:true- url:https://unisphere-2-addr:8443arrayCredentialSecret:unsiphere-2-credsskipCertificateValidation:true- url:https://unisphere-3-addr:8443arrayCredentialSecret:unsiphere-3-credsskipCertificateValidation:trueInstallation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service:\nkubectl create -f powermax_reverseproxy.yaml  You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u003cnamespace\u003e kubectl get svc -n \u003cnamespace\u003e  There is a new sample file - powermax_revproxy_standalone_with_driver.yaml in the samples folder which enables installation of CSI PowerMax ReverseProxy in StandAlone mode along with the CSI PowerMax driver. This mode enables the CSI PowerMax driver to connect to multiple Unisphere servers for managing multiple PowerMax arrays. Please follow the same steps described above to install ReverseProxy with this new sample file.\nDynamic Logging Configuration This feature is introduced in CSI Driver for powermax version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Sample CRD file for powermax apiVersion:storage.dell.com/v1kind:CSIPowerMaxmetadata:name:test-powermaxnamespace:test-powermaxspec:driver:# Config version for CSI PowerMax v2.3.0 driverconfigVersion:v2.3.0# replica: Define the number of PowerMax controller nodes# to deploy to the Kubernetes release# Allowed values: n, where n \u003e 0# Default value: Nonereplicas:2dnsPolicy:ClusterFirstWithHostNetforceUpdate:falsecommon:# Image for CSI PowerMax driver v2.3.0image:dellemc/csi-powermax:v2.3.0# imagePullPolicy: Policy to determine if the image should be pulled prior to starting the container.# Allowed values:# Always: Always pull the image.# IfNotPresent: Only pull the image if it does not already exist on the node.# Never: Never pull the image.# Default value: NoneimagePullPolicy:IfNotPresentenvs:# X_CSI_MANAGED_ARRAYS: Serial ID of the arrays that will be used for provisioning# Default value: None# Examples: \"000000000001\", \"000000000002\"- name:X_CSI_MANAGED_ARRAYSvalue:\"000000000000,000000000001\"# X_CSI_POWERMAX_ENDPOINT: Address of the Unisphere server that is managing the PowerMax arrays# Default value: None# Example: https://0.0.0.1:8443- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"# X_CSI_K8S_CLUSTER_PREFIX: Define a prefix that is appended onto# all resources created in the Array# This should be unique per K8s/CSI deployment# maximum length of this value is 3 characters# Default value: None# Examples: \"XYZ\", \"EMC\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"# X_CSI_POWERMAX_PORTGROUPS: Define the set of existing port groups that the driver will use.# It is a comma separated list of portgroup names.# Required only in case of iSCSI port groups# Allowed values: iSCSI Port Group names# Default value: None# Examples: \"pg1\", \"pg1, pg2\"- name:\"X_CSI_POWERMAX_PORTGROUPS\"value:\"\"# \"X_CSI_TRANSPORT_PROTOCOL\" can be \"FC\" or \"FIBRE\" for fibrechannel,# \"ISCSI\" for iSCSI, or \"\" for autoselection.# Allowed values:# \"FC\" - Fiber Channel protocol# \"FIBER\" - Fiber Channel protocol# \"ISCSI\" - iSCSI protocol# \"\" - Automatic selection of transport protocol# Default value: \"\" \u003cempty\u003e- name:\"X_CSI_TRANSPORT_PROTOCOL\"value:\"\"# X_CSI_POWERMAX_PROXY_SERVICE_NAME: Refers to the name of the proxy service in kubernetes# Set this to \"powermax-reverseproxy\" if you are installing the proxy# Allowed values: \"powermax-reverseproxy\"# default values: \"\" \u003cempty\u003e- name:\"X_CSI_POWERMAX_PROXY_SERVICE_NAME\"value:\"\"# X_CSI_GRPC_MAX_THREADS: Defines the maximum number of concurrent grpc requests.# Set this value to a higher number (max 50) if you are using the proxy# Allowed values: n, where n \u003e 4# default values: None- name:\"X_CSI_GRPC_MAX_THREADS\"value:\"4\"sideCars:# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin.# Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\" for controller plugin.# Also set the env variable node.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\" for node plugin.#- name: external-health-monitor# args: [\"--monitor-interval=300s\"]controller:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Determines if the controller plugin will monitor health of CSI volumes- volume status, volume condition# Install the 'external-health-monitor' sidecar accordingly.# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"node:envs:# X_CSI_POWERMAX_ISCSI_ENABLE_CHAP: Determine if the node plugin is going to configure# ISCSI node databases on the nodes with the CHAP credentials# If enabled, the CHAP secret must be provided in the credentials secret# and set to the key \"chapsecret\"# Allowed values:# \"true\" - CHAP is enabled# \"false\" - CHAP is disabled# Default value: \"false\"- name:\"X_CSI_POWERMAX_ISCSI_ENABLE_CHAP\"value:\"false\"# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin- volume usage, volume condition# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"# X_CSI_TOPOLOGY_CONTROL_ENABLED provides a way to filter topology keys on a node based on array and transport protocol# if enabled, user can create custom topology keys by editing node-topology-config configmap.# Allowed values:# true: enable the filtration based on config map# false: disable the filtration based on config map# Default value: false- name:X_CSI_TOPOLOGY_CONTROL_ENABLEDvalue:\"false\"---apiVersion:v1kind:ConfigMapmetadata:name:powermax-config-paramsnamespace:test-powermaxdata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"debug\"CSI_LOG_FORMAT:\"JSON\"---apiVersion:v1kind:ConfigMapmetadata:name:node-topology-confignamespace:test-powermaxdata:topologyConfig.yaml:| # allowedConnections contains a list of (node, array and protocol) info for user allowed configuration# For any given storage array ID and protocol on a Node, topology keys will be created for just those pair and# every other configuration is ignored# Please refer to the doc website about a detailed explanation of each configuration parameter# and the various possible inputsallowedConnections:# nodeName: Name of the node on which user wants to apply given rules# Allowed values:# nodeName - name of a specific node# * - all the nodes# Examples: \"node1\", \"*\"- nodeName:\"node1\"# rules is a list of 'StorageArrayID:TransportProtocol' pair. ':' is required between both value# Allowed values:# StorageArrayID:# - SymmetrixID : for specific storage array# - \"*\" :- for all the arrays connected to the node# TransportProtocol:# - FC : Fibre Channel protocol# - ISCSI : iSCSI protocol# - \"*\" - for all the possible Transport Protocol# Examples: \"000000000001:FC\", \"000000000002:*\", \"*:FC\", \"*:*\"rules:- \"000000000001:FC\"- \"000000000002:FC\"- nodeName:\"*\"rules:- \"000000000002:FC\"# deniedConnections contains a list of (node, array and protocol) info for denied configurations by user# For any given storage array ID and protocol on a Node, topology keys will be created for every other configuration but# not these input pairsdeniedConnections:- nodeName:\"node2\"rules:- \"000000000002:*\"- nodeName:\"node3\"rules:- \"*:*\"Note:\n dell-csi-operator does not support the installation of CSI PowerMax ReverseProxy as a sidecar to the controller Pod. This facility is only present with dell-csi-helm-installer. Kubelet config dir path is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  Volume Health Monitoring This feature is introduced in CSI Driver for PowerMax version 2.2.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, set X_CSI_HEALTH_MONITOR_ENABLED to true in the driver manifest under controller and node section. Also, install the external-health-monitor from sideCars section for controller plugin. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" Support for custom topology keys This feature is introduced in CSI Driver for PowerMax version 2.3.0.\nOperator based installation Support for custom topology keys is optional and by default this feature is disabled for drivers when installed via operator.\nX_CSI_TOPOLOGY_CONTROL_ENABLED provides a way to filter topology keys on a node based on array and transport protocol. If enabled, user can create custom topology keys by editing node-topology-config configmap.\n To enable this feature, set X_CSI_TOPOLOGY_CONTROL_ENABLED to true in the driver manifest under node section.   # X_CSI_TOPOLOGY_CONTROL_ENABLED provides a way to filter topology keys on a node based on array and transport protocol # if enabled, user can create custom topology keys by editing node-topology-config configmap. # Allowed values: # true: enable the filtration based on config map # false: disable the filtration based on config map # Default value: false - name: X_CSI_TOPOLOGY_CONTROL_ENABLED value: \"false\" Edit the sample config map “node-topology-config” present in sample CRD with appropriate values:    Parameter Description     allowedConnections List of node, array and protocol info for user allowed configuration   allowedConnections.nodeName Name of the node on which user wants to apply given rules   allowedConnections.rules List of StorageArrayID:TransportProtocol pair   deniedConnections List of node, array and protocol info for user denied configuration   deniedConnections.nodeName Name of the node on which user wants to apply given rules   deniedConnections.rules List of StorageArrayID:TransportProtocol pair   \n        Note: Name of the configmap should always be node-topology-config.\n ","excerpt":"Installing CSI Driver for PowerMax via Operator CSI Driver for Dell …","ref":"/csm-docs/docs/csidriver/installation/operator/powermax/","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use CSI Driver for Dell PowerMax. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with a different number of volumes in a given namespace using the storageclass provided. To test the installation of the CSI driver, perform these tests:\n Volume clone test Volume test Snapshot test  Volume test Use this procedure to perform a volume test.\n  Create a namespace with the name test.\n  Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\n  Run the starttest.sh script and provide it with a test name. The following sample command can be used to run the 2vols test: ./starttest.sh -t 2vols -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e\nThis script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\n  Run the /stoptest.sh -t 2vols -n \u003ctest_namespace\u003e script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\n   NOTE: Helm tests have been designed assuming that users have created storageclass names like storageclass-name and storageclass-name-xfs. You can use kubectl get sc to check for the storageclass names.\n Volume clone test Use this procedure to perform a volume clone test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: bash volumeclonetest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum, and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test.   This is not supported for replicated volumes.\n Snapshot test Use this procedure to perform a snapshot test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot of that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test.   NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass. You must update the snapshot class name in the file snap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\n Volume Expansion test Use this procedure to perform a volume expansion test.\n Create a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates one PVC, and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC, and then recalculates the checksum Cleans up all the resources that were created as part of the test   Note: This is not applicable for replicated volumes.\n Setting Application Prefix Application prefix is the name of the application that can be used to group the PowerMax volumes. We can use it while naming storage group. To set the application prefix for PowerMax, please refer to the sample storage class https://github.com/dell/csi-powermax/blob/main/samples/storageclass/powermax.yaml.\n# Name of application to be used to group volumes# This is used in naming storage group# Optional: true, Default value: None# Examples: APP, app, sanity, testsApplicationPrefix:\u003capplicationprefix\u003e  Note: Supported length of storage group for PowerMax is 64 characters. Storage group name is of the format “csi-clusterprefix-application prefix-SLO name-SRP name-SG”. Based on the other inputs like clusterprefix,SLO name and SRP name maximum length of the ApplicationPrefix can vary.\n Consuming existing volumes with static provisioning Use this procedure to consume existing volumes with static provisioning.\n Open your Unisphere for Powermax, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, storage class is assumed as ‘powermax’, cluster prefix as ‘ABC’ and volume’s internal name as ‘00001’, array ID as ‘000000000001’, volume ID as ‘1abc23456’. The volume-handle should be in the format of csi-clusterPrefix-volumeNamePrefix-id-arrayID-volumeID.  apiVersion:v1kind:PersistentVolumemetadata:name:pvolnamespace:testspec:accessModes:- ReadWriteOncecapacity:storage:8Gicsi:driver:csi-powermax.dellemc.comvolumeHandle:csi-ABC-pmax-1abc23456-000000000001-00001persistentVolumeReclaimPolicy:RetainstorageClassName:powermaxvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:pvcnamespace:testspec:accessModes:- ReadWriteOnceresources:requests:storage:8GistorageClassName:powermaxvolumeMode:FilesystemvolumeName:pvolThen use this PVC as a volume in a pod.  apiVersion:v1kind:ServiceAccountmetadata:name:powermaxtestnamespace:test---kind:StatefulSetapiVersion:apps/v1metadata:name:powermaxtestnamespace:testspec:selector:matchLabels:app:powermaxtestserviceName:staticprovisioningtemplate:metadata:labels:app:powermaxtestspec:serviceAccount:powermaxtestcontainers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:pvcvolumes:- name:pvcpersistentVolumeClaim:claimName:pvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.   Note: CSI driver for PowerMax will create the necessary objects like Storage group, HostID and Masking View. They must not be created manually.\n ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/docs/csidriver/installation/test/powermax/","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v2.3.0 New Features/Changes  Updated deprecated StorageClass parameter fsType with csi.storage.k8s.io/fstype. Added support for Standalone Helm Charts. Removed beta volumesnapshotclass sample files. Added mapping of PV/PVC to namespace. Added support to configure fsGroupPolicy. Added support to filter topology keys based on user inputs. Added support for SRDF Metro group sharing multiple namespaces. Added support for Kubernetes 1.24. Added support for OpenShift 4.10. Added support to convert replicated volume to non-replicated volume and vice versa for Sync and Async modes.  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Delete Volume fails with the error message: volume is part of masking view This issue is due to limitations in Unisphere and occurs when Unisphere is overloaded. Currently, there is no workaround for this but it can be avoided by ensuring that Unisphere is not overloaded during such operations. The Unisphere team is assessing a fix for this in a future Unisphere release   Getting initiators list fails with context deadline error The following error can occur during the driver installation if a large number of initiators are present on the array. There is no workaround for this but it can be avoided by deleting stale initiators on the array   Unable to update Host: A problem occurred modifying the host resource This issue occurs when the nodes do not have unique hostnames or when an IP address/FQDN with same sub-domains are used as hostnames. The workaround is to use unique hostnames or FQDN with unique sub-domains   GetSnapVolumeList fails with context deadline error The following error can occur if a large number of snapshots are present on the array. There is no workaround for this but it can be avoided by deleting unused snapshots on the array   When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node    Note:  Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI PowerMax v2.3.0 New Features/Changes  Updated …","ref":"/csm-docs/docs/csidriver/release/powermax/","title":"PowerMax"},{"body":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that you have applied all the features to the indicated processes. Restart kubelet when remediated.   kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates that the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or log in to the docker registry   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver cannot authenticate Check your secret’s username and password   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver failed to connect to the U4P because it could not verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates   Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.22.0 \u003c 1.25.0 which is incompatible with Kubernetes V1.22.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported.   When a node goes down, the block volumes attached to the node cannot be attached to another node 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node.    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Warning about …","ref":"/csm-docs/docs/csidriver/troubleshooting/powermax/","title":"PowerMax"},{"body":"CSI Driver for Dell PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, see the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume (optional) Kubernetes External health monitor, which provides volume health status (optional) CSI PowerMax ReverseProxy, which maximizes CSI driver and Unisphere performance (optional) Dell CSI Replicator, which provides Replication capability.  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell PowerMax Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following requirements must be met before installing CSI Driver for Dell PowerMax:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Fibre Channel requirements iSCSI requirements Certificate validation for Unisphere REST API calls Mount propagation is enabled on container runtime that is being used Linux multipathing requirements If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first  Install Helm 3 Install Helm 3 on the master node before you install CSI Driver for Dell PowerMax.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.\nFibre Channel Requirements CSI Driver for Dell PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.  iSCSI Requirements The CSI Driver for Dell PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\n All Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell PowerMax array. All the port group names supplied to the driver must exist on each Dell PowerMax with the same name.  For more information about configuring iSCSI, see Dell Host Connectivity guide.\nCertificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\n  To fetch the certificate, run openssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null 2\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem\nNOTE: The IP address varies for each user.\n  To create the secret, run kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax\n  Ports in the port group There are no restrictions to how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell PowerMax to ensure that you have multiple paths to your data volumes.\nLinux multipathing requirements CSI Driver for Dell PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\n All the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file.  As a best practice, use the following options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. For installation, use v5.0.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers to support Volume snapshots.\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster, irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v5.0.x\nNOTE:\n The manifests available on GitHub install the snapshotter image: quay.io/k8scsi/csi-snapshotter:v4.0.x The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication:enabled:trueReplication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\n Run git clone -b v2.2.0 https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the `samples/secret/secret.yaml file, point to the correct namespace, and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax privileges.\n Create the secret by running kubectl create -f samples/secret/secret.yaml. If you are going to install the new CSI PowerMax ReverseProxy service, create a TLS secret with the name - csireverseproxy-tls-secret which holds an SSL certificate and the corresponding private key in the namespace where you are installing the driver. Copy the default values.yaml file cd helm \u0026\u0026 cp csi-powermax/values.yaml my-powermax-settings.yaml Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml     Parameter Description Required Default     kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent   clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC”   defaultFsType Used to set the default FS type for external provisioner Yes ext4   portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3”   storageResourcePool This parameter must mention one of the SRPs on the PowerMax array that the symmetrixID specifies. This value is used to create the default storage class. Yes “SRP_1”   serviceLevel This parameter must mention one of the Service Levels on the PowerMax array. This value is used to create the default storage class. Yes “Bronze”   skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True”   transportProtocol Set the preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty   nodeNameTemplate Used to specify a template that will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, leave this value empty. No Empty   controller Allows configuration of the controller-specific parameters. - -   controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2   volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s”   snapshot.enabled Enable/Disable volume snapshot feature Yes true   snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot”   resizer.enabled Enable/Disable volume expansion feature Yes true   healthMonitor.enabled Allows to enable/disable volume health monitor No false   healthMonitor.interval Interval of monitoring volume health condition No 60s   nodeSelector Define node selection constraints for pods of controller deployment No    tolerations Define tolerations for the controller deployment, if required No    node Allows configuration of the node-specific parameters. - -   tolerations Add tolerations as per requirement No -   nodeSelector Add node selectors as per requirement No -   healthMonitor.enabled Allows to enable/disable volume health monitor No false   global This section refers to configuration options for both CSI PowerMax Driver and Reverse Proxy - -   defaultCredentialsSecret This secret name refers to:\n1. The Unisphere credentials if the driver is installed without proxy or with proxy in Linked mode.\n2. The proxy credentials if the driver is installed with proxy in StandAlone mode.\n3. The default Unisphere credentials if credentialsSecret is not specified for a management server. Yes powermax-creds   storageArrays This section refers to the list of arrays managed by the driver and Reverse Proxy in StandAlone mode. - -   storageArrayId This refers to PowerMax Symmetrix ID. Yes 000000000001   endpoint This refers to the URL of the Unisphere server managing storageArrayId. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes if Reverse Proxy mode is StandAlone https://primary-1.unisphe.re:8443   backupEndpoint This refers to the URL of the backup Unisphere server managing storageArrayId, if Reverse Proxy is installed in StandAlone mode. If authorization is enabled, backupEndpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on No https://backup-1.unisphe.re:8443   managementServers This section refers to the list of configurations for Unisphere servers managing powermax arrays. - -   endpoint This refers to the URL of the Unisphere server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes https://primary-1.unisphe.re:8443   credentialsSecret This refers to the user credentials for endpoint No primary-1-secret   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty   limits This refers to various limits for Reverse Proxy No -   maxActiveRead This refers to the maximum concurrent READ request handled by the reverse proxy. No 5   maxActiveWrite This refers to the maximum concurrent WRITE request handled by the reverse proxy. No 4   maxOutStandingRead This refers to maximum queued READ request when reverse proxy receives more than maxActiveRead requests. No 50   maxOutStandingWrite This refers to maximum queued WRITE request when reverse proxy receives more than maxActiveWrite requests. No 50   csireverseproxy This section refers to the configuration options for CSI PowerMax Reverse Proxy - -   enabled Boolean parameter which indicates if CSI PowerMax Reverse Proxy is going to be configured and installed.\nNOTE: If not enabled, then there is no requirement to configure any of the following values. No “False”   image This refers to the image of the CSI Powermax Reverse Proxy container. Yes dellemc/csipowermax-reverseproxy:v1.4.0   tlsSecret This refers to the TLS secret of the Reverse Proxy Server. Yes csirevproxy-tls-secret   deployAsSidecar If set to true, the Reverse Proxy is installed as a sidecar to the driver’s controller pod otherwise it is installed as a separate deployment. Yes “True”   port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation Yes 2222   mode This refers to the installation mode of Reverse Proxy. It can be set to:\n1. Linked: In this mode, the Reverse Proxy communicates with a primary or a backup Unisphere managing the same set of arrays.\n2. StandAlone: In this mode, the Reverse Proxy communicates with multiple arrays managed by different Unispheres. Yes “StandAlone”   authorization Authorization is an optional feature to apply credential shielding of the backend PowerMax. - -   enabled A boolean that enables/disables authorization feature. No false   sidecarProxyImage Image for csm-authorization-sidecar. No \" \"   proxyHost Hostname of the csm-authorization server. No Empty   skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. No true    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml  Note:\n For detailed instructions on how to run the install scripts, see the readme document in the dell-csi-helm-installer folder. There are a set of samples provided here to help you configure the driver with reverse proxy This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option In order to enable authorization, there should be an authorization proxy server already installed.  Storage Classes Starting CSI PowerMax v1.6, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests has been provided in the samples/storageclass folder. Please use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. To continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Volume Snapshot Class Starting with CSI PowerMax v1.7, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerMax v2.1 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerMax to 1.7 or higher, before upgrading to 2.2.\nSample values file The following sections have useful snippets from values.yaml file which provides more information on how to configure the CSI PowerMax driver along with CSI PowerMax ReverseProxy in various modes\nCSI PowerMax driver without Proxy In this mode, the CSI PowerMax driver can only connect to a single Unisphere server. So, you just specify a list of storage arrays and the address of the Unisphere server\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"- storageArrayId:\"000000000002\"managementServers:- endpoint:https://unisphere-address:8443 Note: If you provide multiple endpoints in the list of management servers, the installer will only use the first server in the list\n CSI PowerMax driver with Proxy in Linked mode In this mode, the CSI PowerMax ReverseProxy acts as a passthrough for the RESTAPI calls and only provides limited functionality such as rate limiting, backup Unisphere server. The CSI PowerMax driver is still responsible for the authentication with the Unisphere server.\nThe first endpoint in the list of management servers is the primary Unisphere server and if you provide a second endpoint, then it will be considered as the backup Unisphere’s endpoint.\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"- storageArrayId:\"000000000002\"managementServers:- endpoint:https://primary-unisphere:8443skipCertificateValidation:falsecertSecret:primary-certlimits:maxActiveRead:5maxActiveWrite:4maxOutStandingRead:50maxOutStandingWrite:50- endpoint:https://backup-unisphere:8443#Optional# \"csireverseproxy\" refers to the subchart csireverseproxycsireverseproxy:# Set enabled to true if you want to use proxyenabled:trueimage:dellemc/csipowermax-reverseproxy:v1.4.0tlsSecret:csirevproxy-tls-secretdeployAsSidecar:trueport:2222mode:Linked Note: Since the driver is still responsible for authentication when used with Proxy in Linked mode, the credentials for both primary and backup Unisphere need to be the same.\n CSI PowerMax driver with Proxy in StandAlone mode This is the most advanced configuration which provides you with the capability to connect to Multiple Unisphere servers. You can specify primary and backup Unisphere servers for each storage array. If you have different credentials for your Unisphere servers, you can also specify different credential secrets.\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"endpoint:https://primary-1.unisphe.re:8443backupEndpoint:https://backup-1.unisphe.re:8443- storageArrayId:\"000000000002\"endpoint:https://primary-2.unisphe.re:8443backupEndpoint:https://backup-2.unisphe.re:8443managementServers:- endpoint:https://primary-1.unisphe.re:8443credentialsSecret:primary-1-secretskipCertificateValidation:falsecertSecret:primary-certlimits:maxActiveRead:5maxActiveWrite:4maxOutStandingRead:50maxOutStandingWrite:50- endpoint:https://backup-1.unisphe.re:8443credentialsSecret:backup-1-secretskipCertificateValidation:true- endpoint:https://primary-2.unisphe.re:8443credentialsSecret:primary-2-secretskipCertificateValidation:true- endpoint:https://backup-2.unisphe.re:8443credentialsSecret:backup-2-secretskipCertificateValidation:true# \"csireverseproxy\" refers to the subchart csireverseproxycsireverseproxy:# Set enabled to true if you want to use proxyenabled:trueimage:dellemc/csipowermax-reverseproxy:v1.4.0tlsSecret:csirevproxy-tls-secretdeployAsSidecar:trueport:2222mode:StandAlone Note: If the credential secret is missing from any management server details, the installer will try to use the defaultCredentialsSecret\n ","excerpt":"CSI Driver for Dell PowerMax can be deployed by using the provided …","ref":"/csm-docs/v1/csidriver/installation/helm/powermax/","title":"PowerMax"},{"body":"Installing CSI Driver for PowerMax via Operator CSI Driver for Dell PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Create secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace. Create PowerMax credentials: Create a file called powermax-creds.yaml with the following content: apiVersion:v1kind:Secretmetadata:name:powermax-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards)# chapsecret: \u003cbase64 CHAP secret\u003eReplace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerMax using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:    Parameter Description Required Default     replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ   X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443   X_CSI_TRANSPORT_PROTOCOL Choose which transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No -   X_CSI_MANAGED_ARRAYS List of comma-separated array ID(s) which will be managed by the driver Yes -   X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Leave blank if not using reverse proxy No -   X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4   X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No -   X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller and Node plugin. Provides details of volume status, usage and volume condition. As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar No false   Node parameters      X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false     Execute the following command to create the PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.  CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is an optional component that can be installed with the CSI PowerMax driver. For more details on this feature see the related documentation.\nWhen you install CSI PowerMax ReverseProxy, dell-csi-operator will create a Deployment and ClusterIP service as part of the installation\nNote - To use the ReverseProxy with the CSI PowerMax driver, the ReverseProxy service should be created before you install the CSIPowerMax driver.\nPre-requisites Create a TLS secret that holds an SSL certificate and a private key which is required by the reverse proxy server. Use a tool such as openssl to generate this secret using the example below:\n openssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n powermax tls revproxy-certs --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec  tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs config : This section contains the details of the Reverse Proxy configuration mode : This value is set to Linked by default. Do not change this value linkConfig : This section contains the configuration of the Linked mode primary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if the primary Unisphere is unreachable url : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false standAloneConfig : This section contains the configuration of the StandAlone mode. Refer to the sample below for the detailed config   Note: Only one of the Linked or StandAlone configurations needs to be supplied. The appropriate mode needs to be set in the spec as well.\n Here is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion:storage.dell.com/v1kind:CSIPowerMaxRevProxymetadata:name:powermax-reverseproxy# \u003c- Name of the CSIPowerMaxRevProxy objectnamespace:test-powermax# \u003c- Set the namespace to where you will install the CSI PowerMax driverspec:# Image for CSI PowerMax ReverseProxyimage:dellemc/csipowermax-reverseproxy:v1.4.0# \u003c- CSI PowerMax Reverse Proxy imageimagePullPolicy:Always# TLS secret which contains SSL certificate and private key for the Reverse Proxy servertlsSecret:csirevproxy-tls-secretconfig:mode:LinkedlinkConfig:primary:url:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:true# This setting determines if client side Unisphere certificate validation is to be skippedcertSecret:\"\"# Provide this value if skipCertificateValidation is set to falsebackup:# This is an optional field and lets you configure a backup unisphere which can be used by proxy serverurl:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:truestandAloneConfig:# Set mode to \"StandAlone\" in order to use this configstorageArrays:- storageArrayId:\"000000000001\"# Unisphere server managing the PowerMax arrayprimaryURL:https://unisphere-1-addr:8443# proxyCredentialSecrets are used by the clients of the proxy to connect to it# If using proxy in the stand alone mode, then the driver must be provided the# same secret.# The format of the proxy credential secret are exactly the same as the unisphere credential secret# For using the proxy with the driver, use the same proxy credential secrets for# all the managed storage arraysproxyCredentialSecrets:- proxy-creds- storageArrayId:\"000000000002\"primaryURL:https://unisphere-2-addr:8443# An optional backup Unisphere server managing the same array# This can be used by the proxy to fall back to in case the primary# Unisphere is inaccessible temporarilybackupURL:unisphere-3-addr:8443proxyCredentialSecrets:- proxy-credsmanagementServers:- url:https://unisphere-1-addr:8443# Secret containing the credentials of the Unisphere serverarrayCredentialSecret:unsiphere-1-credsskipCertificateValidation:true- url:https://unisphere-2-addr:8443arrayCredentialSecret:unsiphere-2-credsskipCertificateValidation:true- url:https://unisphere-3-addr:8443arrayCredentialSecret:unsiphere-3-credsskipCertificateValidation:trueInstallation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service:\nkubectl create -f powermax_reverseproxy.yaml  You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u003cnamespace\u003e kubectl get svc -n \u003cnamespace\u003e  There is a new sample file - powermax_revproxy_standalone_with_driver.yaml in the samples folder which enables installation of CSI PowerMax ReverseProxy in StandAlone mode along with the CSI PowerMax driver. This mode enables the CSI PowerMax driver to connect to multiple Unisphere servers for managing multiple PowerMax arrays. Please follow the same steps described above to install ReverseProxy with this new sample file.\nDynamic Logging Configuration This feature is introduced in CSI Driver for powermax version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Sample CRD file for powermax apiVersion:storage.dell.com/v1kind:CSIPowerMaxmetadata:name:test-powermaxnamespace:test-powermaxspec:driver:# Config version for CSI PowerMax v2.2.0 driverconfigVersion:v2.2.0# replica: Define the number of PowerMax controller nodes# to deploy to the Kubernetes release# Allowed values: n, where n \u003e 0# Default value: Nonereplicas:2dnsPolicy:ClusterFirstWithHostNetforceUpdate:falsecommon:# Image for CSI PowerMax driver v2.2.0image:dellemc/csi-powermax:v2.2.0# imagePullPolicy: Policy to determine if the image should be pulled prior to starting the container.# Allowed values:# Always: Always pull the image.# IfNotPresent: Only pull the image if it does not already exist on the node.# Never: Never pull the image.# Default value: NoneimagePullPolicy:IfNotPresentenvs:# X_CSI_MANAGED_ARRAYS: Serial ID of the arrays that will be used for provisioning# Default value: None# Examples: \"000000000001\", \"000000000002\"- name:X_CSI_MANAGED_ARRAYSvalue:\"000000000000,000000000001\"# X_CSI_POWERMAX_ENDPOINT: Address of the Unisphere server that is managing the PowerMax arrays# Default value: None# Example: https://0.0.0.1:8443- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"# X_CSI_K8S_CLUSTER_PREFIX: Define a prefix that is appended onto# all resources created in the Array# This should be unique per K8s/CSI deployment# maximum length of this value is 3 characters# Default value: None# Examples: \"XYZ\", \"EMC\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"# X_CSI_POWERMAX_PORTGROUPS: Define the set of existing port groups that the driver will use.# It is a comma separated list of portgroup names.# Required only in case of iSCSI port groups# Allowed values: iSCSI Port Group names# Default value: None# Examples: \"pg1\", \"pg1, pg2\"- name:\"X_CSI_POWERMAX_PORTGROUPS\"value:\"\"# \"X_CSI_TRANSPORT_PROTOCOL\" can be \"FC\" or \"FIBRE\" for fibrechannel,# \"ISCSI\" for iSCSI, or \"\" for autoselection.# Allowed values:# \"FC\" - Fiber Channel protocol# \"FIBER\" - Fiber Channel protocol# \"ISCSI\" - iSCSI protocol# \"\" - Automatic selection of transport protocol# Default value: \"\" \u003cempty\u003e- name:\"X_CSI_TRANSPORT_PROTOCOL\"value:\"\"# X_CSI_POWERMAX_PROXY_SERVICE_NAME: Refers to the name of the proxy service in kubernetes# Set this to \"powermax-reverseproxy\" if you are installing the proxy# Allowed values: \"powermax-reverseproxy\"# default values: \"\" \u003cempty\u003e- name:\"X_CSI_POWERMAX_PROXY_SERVICE_NAME\"value:\"\"# X_CSI_GRPC_MAX_THREADS: Defines the maximum number of concurrent grpc requests.# Set this value to a higher number (max 50) if you are using the proxy# Allowed values: n, where n \u003e 4# default values: None- name:\"X_CSI_GRPC_MAX_THREADS\"value:\"4\"sideCars:# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin.# Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\" for controller plugin.# Also set the env variable node.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\" for node plugin.#- name: external-health-monitor# args: [\"--monitor-interval=300s\"]controller:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Determines if the controller plugin will monitor health of CSI volumes- volume status, volume condition# Install the 'external-health-monitor' sidecar accordingly.# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"node:envs:# X_CSI_POWERMAX_ISCSI_ENABLE_CHAP: Determine if the node plugin is going to configure# ISCSI node databases on the nodes with the CHAP credentials# If enabled, the CHAP secret must be provided in the credentials secret# and set to the key \"chapsecret\"# Allowed values:# \"true\" - CHAP is enabled# \"false\" - CHAP is disabled# Default value: \"false\"- name:\"X_CSI_POWERMAX_ISCSI_ENABLE_CHAP\"value:\"false\"# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin- volume usage, volume condition# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"---apiVersion:v1kind:ConfigMapmetadata:name:powermax-config-paramsnamespace:test-powermaxdata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"debug\"CSI_LOG_FORMAT:\"JSON\"Note:\n dell-csi-operator does not support the installation of CSI PowerMax ReverseProxy as a sidecar to the controller Pod. This facility is only present with dell-csi-helm-installer. Kubelet config dir path is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  Volume Health Monitoring This feature is introduced in CSI Driver for PowerMax version 2.2.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, set X_CSI_HEALTH_MONITOR_ENABLED to true in the driver manifest under controller and node section. Also, install the external-health-monitor from sideCars section for controller plugin. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\"  ","excerpt":"Installing CSI Driver for PowerMax via Operator CSI Driver for Dell …","ref":"/csm-docs/v1/csidriver/installation/operator/powermax/","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use CSI Driver for Dell PowerMax. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with a different number of volumes in a given namespace using the storageclass provided. To test the installation of the CSI driver, perform these tests:\n Volume clone test Volume test Snapshot test  Volume test Use this procedure to perform a volume test.\n  Create a namespace with the name test.\n  Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\n  Run the starttest.sh script and provide it with a test name. The following sample command can be used to run the 2vols test: ./starttest.sh -t 2vols -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e\nThis script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\n  Run the /stoptest.sh -t 2vols -n \u003ctest_namespace\u003e script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\n   NOTE: Helm tests have been designed assuming that users have created storageclass names like storageclass-name and storageclass-name-xfs. You can use kubectl get sc to check for the storageclass names.\n Volume clone test Use this procedure to perform a volume clone test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: bash volumeclonetest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum, and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test.   This is not supported for replicated volumes.\n Snapshot test Use this procedure to perform a snapshot test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot of that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test.   NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass. You must update the snapshot class name in the file snap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\n Volume Expansion test Use this procedure to perform a volume expansion test.\n Create a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates one PVC, and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC, and then recalculates the checksum Cleans up all the resources that were created as part of the test   Note: This is not applicable for replicated volumes.\n Setting Application Prefix Application prefix is the name of the application that can be used to group the PowerMax volumes. We can use it while naming storage group. To set the application prefix for PowerMax, please refer to the sample storage class https://github.com/dell/csi-powermax/blob/main/samples/storageclass/powermax.yaml.\n# Name of application to be used to group volumes# This is used in naming storage group# Optional: true, Default value: None# Examples: APP, app, sanity, testsApplicationPrefix:\u003capplicationprefix\u003e  Note: Supported length of storage group for PowerMax is 64 characters. Storage group name is of the format “csi-clusterprefix-application prefix-SLO name-SRP name-SG”. Based on the other inputs like clusterprefix,SLO name and SRP name maximum length of the ApplicationPrefix can vary.\n Consuming existing volumes with static provisioning Use this procedure to consume existing volumes with static provisioning.\n Open your Unisphere for Powermax, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, storage class is assumed as ‘powermax’, cluster prefix as ‘ABC’ and volume’s internal name as ‘00001’, array ID as ‘000000000001’, volume ID as ‘1abc23456’. The volume-handle should be in the format of csi-clusterPrefix-volumeNamePrefix-id-arrayID-volumeID.  apiVersion:v1kind:PersistentVolumemetadata:name:pvolnamespace:testspec:accessModes:- ReadWriteOncecapacity:storage:8Gicsi:driver:csi-powermax.dellemc.comvolumeHandle:csi-ABC-pmax-1abc23456-000000000001-00001persistentVolumeReclaimPolicy:RetainstorageClassName:powermaxvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:pvcnamespace:testspec:accessModes:- ReadWriteOnceresources:requests:storage:8GistorageClassName:powermaxvolumeMode:FilesystemvolumeName:pvolThen use this PVC as a volume in a pod.  apiVersion:v1kind:ServiceAccountmetadata:name:powermaxtestnamespace:test---kind:StatefulSetapiVersion:apps/v1metadata:name:powermaxtestnamespace:testspec:selector:matchLabels:app:powermaxtestserviceName:staticprovisioningtemplate:metadata:labels:app:powermaxtestspec:serviceAccount:powermaxtestcontainers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:pvcvolumes:- name:pvcpersistentVolumeClaim:claimName:pvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.   Note: CSI driver for PowerMax will create the necessary objects like Storage group, HostID and Masking View. They must not be created manually.\n ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v1/csidriver/installation/test/powermax/","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v2.2.0 New Features/Changes  Added support for new access modes in CSI Spec 1.5. Added support for Volume Health Monitoring. Added support for Kubernetes 1.23.  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Delete Volume fails with the error message: volume is part of masking view This issue is due to limitations in Unisphere and occurs when Unisphere is overloaded. Currently, there is no workaround for this but it can be avoided by ensuring that Unisphere is not overloaded during such operations. The Unisphere team is assessing a fix for this in a future Unisphere release   Getting initiators list fails with context deadline error The following error can occur during the driver installation if a large number of initiators are present on the array. There is no workaround for this but it can be avoided by deleting stale initiators on the array   Unable to update Host: A problem occurred modifying the host resource This issue occurs when the nodes do not have unique hostnames or when an IP address/FQDN with same sub-domains are used as hostnames. The workaround is to use unique hostnames or FQDN with unique sub-domains   GetSnapVolumeList fails with context deadline error The following error can occur if a large number of snapshots are present on the array. There is no workaround for this but it can be avoided by deleting unused snapshots on the array    Note:  Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode introduced in the release will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters. Expansion of volumes and cloning of volumes are not supported for replicated volumes.  ","excerpt":"Release Notes - CSI PowerMax v2.2.0 New Features/Changes  Added …","ref":"/csm-docs/v1/csidriver/release/powermax/","title":"PowerMax"},{"body":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that you have applied all the features to the indicated processes. Restart kubelet when remediated.   kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates that the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or log in to the docker registry   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver cannot authenticate Check your secret’s username and password   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver failed to connect to the U4P because it could not verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Warning about …","ref":"/csm-docs/v1/csidriver/troubleshooting/powermax/","title":"PowerMax"},{"body":"CSI Driver for Dell PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, see the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume (optional) Kubernetes External health monitor, which provides volume health status (optional) CSI PowerMax ReverseProxy, which maximizes CSI driver and Unisphere performance (optional) Dell CSI Replicator, which provides Replication capability.  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell PowerMax Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following requirements must be met before installing CSI Driver for Dell PowerMax:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Fibre Channel requirements iSCSI requirements Certificate validation for Unisphere REST API calls Mount propagation is enabled on container runtime that is being used Linux multipathing requirements If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first  Install Helm 3 Install Helm 3 on the master node before you install CSI Driver for Dell PowerMax.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.\nFibre Channel Requirements CSI Driver for Dell PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.  iSCSI Requirements The CSI Driver for Dell PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\n All Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell PowerMax array. All the port group names supplied to the driver must exist on each Dell PowerMax with the same name.  For more information about configuring iSCSI, see Dell Host Connectivity guide.\nCertificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\n  To fetch the certificate, run openssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null 2\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem\nNOTE: The IP address varies for each user.\n  To create the secret, run kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax\n  Ports in the port group There are no restrictions to how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell PowerMax to ensure that you have multiple paths to your data volumes.\nLinux multipathing requirements CSI Driver for Dell PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\n All the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file.  As a best practice, use the following options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. For installation, use v5.0.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers to support Volume snapshots.\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster, irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v5.0.x\nNOTE:\n The manifests available on GitHub install the snapshotter image: quay.io/k8scsi/csi-snapshotter:v4.0.x The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication:enabled:trueReplication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\n Run git clone -b v2.2.0 https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the `samples/secret/secret.yaml file, point to the correct namespace, and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax privileges.\n Create the secret by running kubectl create -f samples/secret/secret.yaml. If you are going to install the new CSI PowerMax ReverseProxy service, create a TLS secret with the name - csireverseproxy-tls-secret which holds an SSL certificate and the corresponding private key in the namespace where you are installing the driver. Copy the default values.yaml file cd helm \u0026\u0026 cp csi-powermax/values.yaml my-powermax-settings.yaml Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml     Parameter Description Required Default     kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent   clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC”   defaultFsType Used to set the default FS type for external provisioner Yes ext4   portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3”   storageResourcePool This parameter must mention one of the SRPs on the PowerMax array that the symmetrixID specifies. This value is used to create the default storage class. Yes “SRP_1”   serviceLevel This parameter must mention one of the Service Levels on the PowerMax array. This value is used to create the default storage class. Yes “Bronze”   skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True”   transportProtocol Set the preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty   nodeNameTemplate Used to specify a template that will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, leave this value empty. No Empty   controller Allows configuration of the controller-specific parameters. - -   controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2   volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s”   snapshot.enabled Enable/Disable volume snapshot feature Yes true   snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot”   resizer.enabled Enable/Disable volume expansion feature Yes true   healthMonitor.enabled Allows to enable/disable volume health monitor No false   healthMonitor.interval Interval of monitoring volume health condition No 60s   nodeSelector Define node selection constraints for pods of controller deployment No    tolerations Define tolerations for the controller deployment, if required No    node Allows configuration of the node-specific parameters. - -   tolerations Add tolerations as per requirement No -   nodeSelector Add node selectors as per requirement No -   healthMonitor.enabled Allows to enable/disable volume health monitor No false   global This section refers to configuration options for both CSI PowerMax Driver and Reverse Proxy - -   defaultCredentialsSecret This secret name refers to:\n1. The Unisphere credentials if the driver is installed without proxy or with proxy in Linked mode.\n2. The proxy credentials if the driver is installed with proxy in StandAlone mode.\n3. The default Unisphere credentials if credentialsSecret is not specified for a management server. Yes powermax-creds   storageArrays This section refers to the list of arrays managed by the driver and Reverse Proxy in StandAlone mode. - -   storageArrayId This refers to PowerMax Symmetrix ID. Yes 000000000001   endpoint This refers to the URL of the Unisphere server managing storageArrayId. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes if Reverse Proxy mode is StandAlone https://primary-1.unisphe.re:8443   backupEndpoint This refers to the URL of the backup Unisphere server managing storageArrayId, if Reverse Proxy is installed in StandAlone mode. If authorization is enabled, backupEndpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on No https://backup-1.unisphe.re:8443   managementServers This section refers to the list of configurations for Unisphere servers managing powermax arrays. - -   endpoint This refers to the URL of the Unisphere server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes https://primary-1.unisphe.re:8443   credentialsSecret This refers to the user credentials for endpoint No primary-1-secret   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty   limits This refers to various limits for Reverse Proxy No -   maxActiveRead This refers to the maximum concurrent READ request handled by the reverse proxy. No 5   maxActiveWrite This refers to the maximum concurrent WRITE request handled by the reverse proxy. No 4   maxOutStandingRead This refers to maximum queued READ request when reverse proxy receives more than maxActiveRead requests. No 50   maxOutStandingWrite This refers to maximum queued WRITE request when reverse proxy receives more than maxActiveWrite requests. No 50   csireverseproxy This section refers to the configuration options for CSI PowerMax Reverse Proxy - -   enabled Boolean parameter which indicates if CSI PowerMax Reverse Proxy is going to be configured and installed.\nNOTE: If not enabled, then there is no requirement to configure any of the following values. No “False”   image This refers to the image of the CSI Powermax Reverse Proxy container. Yes dellemc/csipowermax-reverseproxy:v1.4.0   tlsSecret This refers to the TLS secret of the Reverse Proxy Server. Yes csirevproxy-tls-secret   deployAsSidecar If set to true, the Reverse Proxy is installed as a sidecar to the driver’s controller pod otherwise it is installed as a separate deployment. Yes “True”   port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation Yes 2222   mode This refers to the installation mode of Reverse Proxy. It can be set to:\n1. Linked: In this mode, the Reverse Proxy communicates with a primary or a backup Unisphere managing the same set of arrays.\n2. StandAlone: In this mode, the Reverse Proxy communicates with multiple arrays managed by different Unispheres. Yes “StandAlone”   authorization Authorization is an optional feature to apply credential shielding of the backend PowerMax. - -   enabled A boolean that enables/disables authorization feature. No false   sidecarProxyImage Image for csm-authorization-sidecar. No \" \"   proxyHost Hostname of the csm-authorization server. No Empty   skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. No true    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml  Note:\n For detailed instructions on how to run the install scripts, see the readme document in the dell-csi-helm-installer folder. There are a set of samples provided here to help you configure the driver with reverse proxy This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option In order to enable authorization, there should be an authorization proxy server already installed.  Storage Classes Starting CSI PowerMax v1.6, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests has been provided in the samples/storageclass folder. Please use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. To continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Volume Snapshot Class Starting with CSI PowerMax v1.7, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerMax v2.1 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerMax to 1.7 or higher, before upgrading to 2.2.\nSample values file The following sections have useful snippets from values.yaml file which provides more information on how to configure the CSI PowerMax driver along with CSI PowerMax ReverseProxy in various modes\nCSI PowerMax driver without Proxy In this mode, the CSI PowerMax driver can only connect to a single Unisphere server. So, you just specify a list of storage arrays and the address of the Unisphere server\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"- storageArrayId:\"000000000002\"managementServers:- endpoint:https://unisphere-address:8443 Note: If you provide multiple endpoints in the list of management servers, the installer will only use the first server in the list\n CSI PowerMax driver with Proxy in Linked mode In this mode, the CSI PowerMax ReverseProxy acts as a passthrough for the RESTAPI calls and only provides limited functionality such as rate limiting, backup Unisphere server. The CSI PowerMax driver is still responsible for the authentication with the Unisphere server.\nThe first endpoint in the list of management servers is the primary Unisphere server and if you provide a second endpoint, then it will be considered as the backup Unisphere’s endpoint.\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"- storageArrayId:\"000000000002\"managementServers:- endpoint:https://primary-unisphere:8443skipCertificateValidation:falsecertSecret:primary-certlimits:maxActiveRead:5maxActiveWrite:4maxOutStandingRead:50maxOutStandingWrite:50- endpoint:https://backup-unisphere:8443#Optional# \"csireverseproxy\" refers to the subchart csireverseproxycsireverseproxy:# Set enabled to true if you want to use proxyenabled:trueimage:dellemc/csipowermax-reverseproxy:v1.4.0tlsSecret:csirevproxy-tls-secretdeployAsSidecar:trueport:2222mode:Linked Note: Since the driver is still responsible for authentication when used with Proxy in Linked mode, the credentials for both primary and backup Unisphere need to be the same.\n CSI PowerMax driver with Proxy in StandAlone mode This is the most advanced configuration which provides you with the capability to connect to Multiple Unisphere servers. You can specify primary and backup Unisphere servers for each storage array. If you have different credentials for your Unisphere servers, you can also specify different credential secrets.\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"endpoint:https://primary-1.unisphe.re:8443backupEndpoint:https://backup-1.unisphe.re:8443- storageArrayId:\"000000000002\"endpoint:https://primary-2.unisphe.re:8443backupEndpoint:https://backup-2.unisphe.re:8443managementServers:- endpoint:https://primary-1.unisphe.re:8443credentialsSecret:primary-1-secretskipCertificateValidation:falsecertSecret:primary-certlimits:maxActiveRead:5maxActiveWrite:4maxOutStandingRead:50maxOutStandingWrite:50- endpoint:https://backup-1.unisphe.re:8443credentialsSecret:backup-1-secretskipCertificateValidation:true- endpoint:https://primary-2.unisphe.re:8443credentialsSecret:primary-2-secretskipCertificateValidation:true- endpoint:https://backup-2.unisphe.re:8443credentialsSecret:backup-2-secretskipCertificateValidation:true# \"csireverseproxy\" refers to the subchart csireverseproxycsireverseproxy:# Set enabled to true if you want to use proxyenabled:trueimage:dellemc/csipowermax-reverseproxy:v1.4.0tlsSecret:csirevproxy-tls-secretdeployAsSidecar:trueport:2222mode:StandAlone Note: If the credential secret is missing from any management server details, the installer will try to use the defaultCredentialsSecret\n ","excerpt":"CSI Driver for Dell PowerMax can be deployed by using the provided …","ref":"/csm-docs/v2/csidriver/installation/helm/powermax/","title":"PowerMax"},{"body":"Installing CSI Driver for PowerMax via Operator CSI Driver for Dell PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Create secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace. Create PowerMax credentials: Create a file called powermax-creds.yaml with the following content: apiVersion:v1kind:Secretmetadata:name:powermax-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards)# chapsecret: \u003cbase64 CHAP secret\u003eReplace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerMax using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:    Parameter Description Required Default     replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ   X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443   X_CSI_TRANSPORT_PROTOCOL Choose which transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No -   X_CSI_MANAGED_ARRAYS List of comma-separated array ID(s) which will be managed by the driver Yes -   X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Leave blank if not using reverse proxy No -   X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4   X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No -   X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller and Node plugin. Provides details of volume status, usage and volume condition. As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar No false   Node parameters      X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false     Execute the following command to create the PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.  CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is an optional component that can be installed with the CSI PowerMax driver. For more details on this feature see the related documentation.\nWhen you install CSI PowerMax ReverseProxy, dell-csi-operator will create a Deployment and ClusterIP service as part of the installation\nNote - To use the ReverseProxy with the CSI PowerMax driver, the ReverseProxy service should be created before you install the CSIPowerMax driver.\nPre-requisites Create a TLS secret that holds an SSL certificate and a private key which is required by the reverse proxy server. Use a tool such as openssl to generate this secret using the example below:\n openssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n powermax tls revproxy-certs --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec  tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs config : This section contains the details of the Reverse Proxy configuration mode : This value is set to Linked by default. Do not change this value linkConfig : This section contains the configuration of the Linked mode primary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if the primary Unisphere is unreachable url : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false standAloneConfig : This section contains the configuration of the StandAlone mode. Refer to the sample below for the detailed config   Note: Only one of the Linked or StandAlone configurations needs to be supplied. The appropriate mode needs to be set in the spec as well.\n Here is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion:storage.dell.com/v1kind:CSIPowerMaxRevProxymetadata:name:powermax-reverseproxy# \u003c- Name of the CSIPowerMaxRevProxy objectnamespace:test-powermax# \u003c- Set the namespace to where you will install the CSI PowerMax driverspec:# Image for CSI PowerMax ReverseProxyimage:dellemc/csipowermax-reverseproxy:v1.4.0# \u003c- CSI PowerMax Reverse Proxy imageimagePullPolicy:Always# TLS secret which contains SSL certificate and private key for the Reverse Proxy servertlsSecret:csirevproxy-tls-secretconfig:mode:LinkedlinkConfig:primary:url:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:true# This setting determines if client side Unisphere certificate validation is to be skippedcertSecret:\"\"# Provide this value if skipCertificateValidation is set to falsebackup:# This is an optional field and lets you configure a backup unisphere which can be used by proxy serverurl:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:truestandAloneConfig:# Set mode to \"StandAlone\" in order to use this configstorageArrays:- storageArrayId:\"000000000001\"# Unisphere server managing the PowerMax arrayprimaryURL:https://unisphere-1-addr:8443# proxyCredentialSecrets are used by the clients of the proxy to connect to it# If using proxy in the stand alone mode, then the driver must be provided the# same secret.# The format of the proxy credential secret are exactly the same as the unisphere credential secret# For using the proxy with the driver, use the same proxy credential secrets for# all the managed storage arraysproxyCredentialSecrets:- proxy-creds- storageArrayId:\"000000000002\"primaryURL:https://unisphere-2-addr:8443# An optional backup Unisphere server managing the same array# This can be used by the proxy to fall back to in case the primary# Unisphere is inaccessible temporarilybackupURL:unisphere-3-addr:8443proxyCredentialSecrets:- proxy-credsmanagementServers:- url:https://unisphere-1-addr:8443# Secret containing the credentials of the Unisphere serverarrayCredentialSecret:unsiphere-1-credsskipCertificateValidation:true- url:https://unisphere-2-addr:8443arrayCredentialSecret:unsiphere-2-credsskipCertificateValidation:true- url:https://unisphere-3-addr:8443arrayCredentialSecret:unsiphere-3-credsskipCertificateValidation:trueInstallation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service:\nkubectl create -f powermax_reverseproxy.yaml  You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u003cnamespace\u003e kubectl get svc -n \u003cnamespace\u003e  There is a new sample file - powermax_revproxy_standalone_with_driver.yaml in the samples folder which enables installation of CSI PowerMax ReverseProxy in StandAlone mode along with the CSI PowerMax driver. This mode enables the CSI PowerMax driver to connect to multiple Unisphere servers for managing multiple PowerMax arrays. Please follow the same steps described above to install ReverseProxy with this new sample file.\nDynamic Logging Configuration This feature is introduced in CSI Driver for powermax version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Sample CRD file for powermax apiVersion:storage.dell.com/v1kind:CSIPowerMaxmetadata:name:test-powermaxnamespace:test-powermaxspec:driver:# Config version for CSI PowerMax v2.2.0 driverconfigVersion:v2.2.0# replica: Define the number of PowerMax controller nodes# to deploy to the Kubernetes release# Allowed values: n, where n \u003e 0# Default value: Nonereplicas:2dnsPolicy:ClusterFirstWithHostNetforceUpdate:falsecommon:# Image for CSI PowerMax driver v2.2.0image:dellemc/csi-powermax:v2.2.0# imagePullPolicy: Policy to determine if the image should be pulled prior to starting the container.# Allowed values:# Always: Always pull the image.# IfNotPresent: Only pull the image if it does not already exist on the node.# Never: Never pull the image.# Default value: NoneimagePullPolicy:IfNotPresentenvs:# X_CSI_MANAGED_ARRAYS: Serial ID of the arrays that will be used for provisioning# Default value: None# Examples: \"000000000001\", \"000000000002\"- name:X_CSI_MANAGED_ARRAYSvalue:\"000000000000,000000000001\"# X_CSI_POWERMAX_ENDPOINT: Address of the Unisphere server that is managing the PowerMax arrays# Default value: None# Example: https://0.0.0.1:8443- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"# X_CSI_K8S_CLUSTER_PREFIX: Define a prefix that is appended onto# all resources created in the Array# This should be unique per K8s/CSI deployment# maximum length of this value is 3 characters# Default value: None# Examples: \"XYZ\", \"EMC\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"# X_CSI_POWERMAX_PORTGROUPS: Define the set of existing port groups that the driver will use.# It is a comma separated list of portgroup names.# Required only in case of iSCSI port groups# Allowed values: iSCSI Port Group names# Default value: None# Examples: \"pg1\", \"pg1, pg2\"- name:\"X_CSI_POWERMAX_PORTGROUPS\"value:\"\"# \"X_CSI_TRANSPORT_PROTOCOL\" can be \"FC\" or \"FIBRE\" for fibrechannel,# \"ISCSI\" for iSCSI, or \"\" for autoselection.# Allowed values:# \"FC\" - Fiber Channel protocol# \"FIBER\" - Fiber Channel protocol# \"ISCSI\" - iSCSI protocol# \"\" - Automatic selection of transport protocol# Default value: \"\" \u003cempty\u003e- name:\"X_CSI_TRANSPORT_PROTOCOL\"value:\"\"# X_CSI_POWERMAX_PROXY_SERVICE_NAME: Refers to the name of the proxy service in kubernetes# Set this to \"powermax-reverseproxy\" if you are installing the proxy# Allowed values: \"powermax-reverseproxy\"# default values: \"\" \u003cempty\u003e- name:\"X_CSI_POWERMAX_PROXY_SERVICE_NAME\"value:\"\"# X_CSI_GRPC_MAX_THREADS: Defines the maximum number of concurrent grpc requests.# Set this value to a higher number (max 50) if you are using the proxy# Allowed values: n, where n \u003e 4# default values: None- name:\"X_CSI_GRPC_MAX_THREADS\"value:\"4\"sideCars:# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin.# Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\" for controller plugin.# Also set the env variable node.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\" for node plugin.#- name: external-health-monitor# args: [\"--monitor-interval=300s\"]controller:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Determines if the controller plugin will monitor health of CSI volumes- volume status, volume condition# Install the 'external-health-monitor' sidecar accordingly.# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"node:envs:# X_CSI_POWERMAX_ISCSI_ENABLE_CHAP: Determine if the node plugin is going to configure# ISCSI node databases on the nodes with the CHAP credentials# If enabled, the CHAP secret must be provided in the credentials secret# and set to the key \"chapsecret\"# Allowed values:# \"true\" - CHAP is enabled# \"false\" - CHAP is disabled# Default value: \"false\"- name:\"X_CSI_POWERMAX_ISCSI_ENABLE_CHAP\"value:\"false\"# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin- volume usage, volume condition# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"---apiVersion:v1kind:ConfigMapmetadata:name:powermax-config-paramsnamespace:test-powermaxdata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"debug\"CSI_LOG_FORMAT:\"JSON\"Note:\n dell-csi-operator does not support the installation of CSI PowerMax ReverseProxy as a sidecar to the controller Pod. This facility is only present with dell-csi-helm-installer. Kubelet config dir path is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  Volume Health Monitoring This feature is introduced in CSI Driver for PowerMax version 2.2.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator.\nTo enable this feature, set X_CSI_HEALTH_MONITOR_ENABLED to true in the driver manifest under controller and node section. Also, install the external-health-monitor from sideCars section for controller plugin. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false controller: envs: - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"true\"  ","excerpt":"Installing CSI Driver for PowerMax via Operator CSI Driver for Dell …","ref":"/csm-docs/v2/csidriver/installation/operator/powermax/","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use CSI Driver for Dell PowerMax. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with a different number of volumes in a given namespace using the storageclass provided. To test the installation of the CSI driver, perform these tests:\n Volume clone test Volume test Snapshot test  Volume test Use this procedure to perform a volume test.\n  Create a namespace with the name test.\n  Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\n  Run the starttest.sh script and provide it with a test name. The following sample command can be used to run the 2vols test: ./starttest.sh -t 2vols -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e\nThis script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\n  Run the /stoptest.sh -t 2vols -n \u003ctest_namespace\u003e script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\n   NOTE: Helm tests have been designed assuming that users have created storageclass names like storageclass-name and storageclass-name-xfs. You can use kubectl get sc to check for the storageclass names.\n Volume clone test Use this procedure to perform a volume clone test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: bash volumeclonetest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum, and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test.  Snapshot test Use this procedure to perform a snapshot test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot of that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test.   NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass. You must update the snapshot class name in the file snap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\n Volume Expansion test Use this procedure to perform a volume expansion test.\n Create a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates one PVC, and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC, and then recalculates the checksum Cleans up all the resources that were created as part of the test  Setting Application Prefix Application prefix is the name of the application that can be used to group the PowerMax volumes. We can use it while naming storage group. To set the application prefix for PowerMax, please refer to the sample storage class https://github.com/dell/csi-powermax/blob/main/samples/storageclass/powermax.yaml.\n# Name of application to be used to group volumes# This is used in naming storage group# Optional: true, Default value: None# Examples: APP, app, sanity, testsApplicationPrefix:\u003capplicationprefix\u003e  Note: Supported length of storage group for PowerMax is 64 characters. Storage group name is of the format “csi-clusterprefix-application prefix-SLO name-SRP name-SG”. Based on the other inputs like clusterprefix,SLO name and SRP name maximum length of the ApplicationPrefix can vary.\n Consuming existing volumes with static provisioning Use this procedure to consume existing volumes with static provisioning.\n Open your Unisphere for Powermax, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, storage class is assumed as ‘powermax’, cluster prefix as ‘ABC’ and volume’s internal name as ‘00001’, array ID as ‘000000000001’, volume ID as ‘1abc23456’. The volume-handle should be in the format of csi-clusterPrefix-volumeNamePrefix-id-arrayID-volumeID.  apiVersion:v1kind:PersistentVolumemetadata:name:pvolnamespace:testspec:accessModes:- ReadWriteOncecapacity:storage:8Gicsi:driver:csi-powermax.dellemc.comvolumeHandle:csi-ABC-pmax-1abc23456-000000000001-00001persistentVolumeReclaimPolicy:RetainstorageClassName:powermaxvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:pvcnamespace:testspec:accessModes:- ReadWriteOnceresources:requests:storage:8GistorageClassName:powermaxvolumeMode:FilesystemvolumeName:pvolThen use this PVC as a volume in a pod.  apiVersion:v1kind:ServiceAccountmetadata:name:powermaxtestnamespace:test---kind:StatefulSetapiVersion:apps/v1metadata:name:powermaxtestnamespace:testspec:selector:matchLabels:app:powermaxtestserviceName:staticprovisioningtemplate:metadata:labels:app:powermaxtestspec:serviceAccount:powermaxtestcontainers:- name:testimage:docker.io/centos:latestcommand:[\"/bin/sleep\",\"3600\"]volumeMounts:- mountPath:\"/data\"name:pvcvolumes:- name:pvcpersistentVolumeClaim:claimName:pvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.   Note: CSI driver for PowerMax will create the necessary objects like Storage group, HostID and Masking View. They must not be created manually.\n ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v2/csidriver/installation/test/powermax/","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v2.2.0 New Features/Changes  Added support for new access modes in CSI Spec 1.5. Added support for Volume Health Monitoring. Added support for Kubernetes 1.23.  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Delete Volume fails with the error message: volume is part of masking view This issue is due to limitations in Unisphere and occurs when Unisphere is overloaded. Currently, there is no workaround for this but it can be avoided by ensuring that Unisphere is not overloaded during such operations. The Unisphere team is assessing a fix for this in a future Unisphere release   Getting initiators list fails with context deadline error The following error can occur during the driver installation if a large number of initiators are present on the array. There is no workaround for this but it can be avoided by deleting stale initiators on the array   Unable to update Host: A problem occurred modifying the host resource This issue occurs when the nodes do not have unique hostnames or when an IP address/FQDN with same sub-domains are used as hostnames. The workaround is to use unique hostnames or FQDN with unique sub-domains   GetSnapVolumeList fails with context deadline error The following error can occur if a large number of snapshots are present on the array. There is no workaround for this but it can be avoided by deleting unused snapshots on the array    Note:  Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode introduced in the release will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI PowerMax v2.2.0 New Features/Changes  Added …","ref":"/csm-docs/v2/csidriver/release/powermax/","title":"PowerMax"},{"body":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that you have applied all the features to the indicated processes. Restart kubelet when remediated.   kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates that the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or log in to the docker registry   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver cannot authenticate Check your secret’s username and password   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver failed to connect to the U4P because it could not verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Warning about …","ref":"/csm-docs/v2/csidriver/troubleshooting/powermax/","title":"PowerMax"},{"body":"CSI Driver for Dell EMC PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, see the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell EMC PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume CSI PowerMax ReverseProxy (optional)  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell EMC PowerMax Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following requirements must be met before installing CSI Driver for Dell EMC PowerMax:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Fibre Channel requirements iSCSI requirements Certificate validation for Unisphere REST API calls Mount propagation is enabled on container runtime that is being used Linux multipathing requirements If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first  Install Helm 3 Install Helm 3 on the master node before you install CSI Driver for Dell EMC PowerMax.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.\nFibre Channel Requirements CSI Driver for Dell EMC PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install CSI Driver:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.  iSCSI Requirements The CSI Driver for Dell EMC PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\n All Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell EMC PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell EMC PowerMax array. All the port group names supplied to the driver must exist on each Dell EMC PowerMax with the same name.  For more information about configuring iSCSI, see Dell EMC Host Connectivity guide.\nCertificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\n  To fetch the certificate, run openssl s_client -showcerts -connect [Unisphere IP]:8443 \u003c/dev/null 2\u003e /dev/null | openssl x509 -outform PEM \u003e ca_cert.pem\nNOTE: The IP address varies for each user.\n  To create the secret, run kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax\n  Ports in the port group There are no restrictions to how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell EMC PowerMax to ensure that you have multiple paths to your data volumes.\nLinux multipathing requirements CSI Driver for Dell EMC PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\n All the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file.  As a best practice, use the following options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 (Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. For installation, use v4.2.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers to support Volume snapshots.\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster, irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v4.2.x\nNOTE:\n The manifests available on GitHub install the snapshotter image: quay.io/k8scsi/csi-snapshotter:v4.0.x The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 4.2.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication:enabled:trueReplication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\n Run git clone -b v2.1.0 https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the `samples/secret/secret.yaml file, point to the correct namespace, and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \"myusername\" | base64 echo -n \"mypassword\" | base64 where myusername and mypassword are credentials for a user with PowerMax privileges.\n Create the secret by running kubectl create -f samples/secret/secret.yaml. If you are going to install the new CSI PowerMax ReverseProxy service, create a TLS secret with the name - csireverseproxy-tls-secret which holds an SSL certificate and the corresponding private key in the namespace where you are installing the driver. Copy the default values.yaml file cd helm \u0026\u0026 cp csi-powermax/values.yaml my-powermax-settings.yaml Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml     Parameter Description Required Default     kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent   clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes “ABC”   defaultFsType Used to set the default FS type for external provisioner Yes ext4   portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only “PortGroup1, PortGroup2, PortGroup3”   storageResourcePool This parameter must mention one of the SRPs on the PowerMax array that the symmetrixID specifies. This value is used to create the default storage class. Yes “SRP_1”   serviceLevel This parameter must mention one of the Service Levels on the PowerMax array. This value is used to create the default storage class. Yes “Bronze”   skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No “True”   transportProtocol Set the preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty   nodeNameTemplate Used to specify a template that will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, leave this value empty. No Empty   controller Allows configuration of the controller-specific parameters. - -   controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2   volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s”   snapshot.enabled Enable/Disable volume snapshot feature Yes true   snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot”   resizer.enabled Enable/Disable volume expansion feature Yes true   nodeSelector Define node selection constraints for pods of controller deployment No    tolerations Define tolerations for the controller deployment, if required No    node Allows configuration of the node-specific parameters. - -   tolerations Add tolerations as per requirement No -   nodeSelector Add node selectors as per requirement No -   global This section refers to configuration options for both CSI PowerMax Driver and Reverse Proxy - -   defaultCredentialsSecret This secret name refers to:\n1. The Unisphere credentials if the driver is installed without proxy or with proxy in Linked mode.\n2. The proxy credentials if the driver is installed with proxy in StandAlone mode.\n3. The default Unisphere credentials if credentialsSecret is not specified for a management server. Yes powermax-creds   storageArrays This section refers to the list of arrays managed by the driver and Reverse Proxy in StandAlone mode. - -   storageArrayId This refers to PowerMax Symmetrix ID. Yes 000000000001   endpoint This refers to the URL of the Unisphere server managing storageArrayId. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes if Reverse Proxy mode is StandAlone https://primary-1.unisphe.re:8443   backupEndpoint This refers to the URL of the backup Unisphere server managing storageArrayId, if Reverse Proxy is installed in StandAlone mode. If authorization is enabled, backupEndpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on No https://backup-1.unisphe.re:8443   managementServers This section refers to the list of configurations for Unisphere servers managing powermax arrays. - -   endpoint This refers to the URL of the Unisphere server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes https://primary-1.unisphe.re:8443   credentialsSecret This refers to the user credentials for endpoint No primary-1-secret   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. No “True”   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty   limits This refers to various limits for Reverse Proxy No -   maxActiveRead This refers to the maximum concurrent READ request handled by the reverse proxy. No 5   maxActiveWrite This refers to the maximum concurrent WRITE request handled by the reverse proxy. No 4   maxOutStandingRead This refers to maximum queued READ request when reverse proxy receives more than maxActiveRead requests. No 50   maxOutStandingWrite This refers to maximum queued WRITE request when reverse proxy receives more than maxActiveWrite requests. No 50   csireverseproxy This section refers to the configuration options for CSI PowerMax Reverse Proxy - -   enabled Boolean parameter which indicates if CSI PowerMax Reverse Proxy is going to be configured and installed.\nNOTE: If not enabled, then there is no requirement to configure any of the following values. No “False”   image This refers to the image of the CSI Powermax Reverse Proxy container. Yes dellemc/csipowermax-reverseproxy:v1.4.0   tlsSecret This refers to the TLS secret of the Reverse Proxy Server. Yes csirevproxy-tls-secret   deployAsSidecar If set to true, the Reverse Proxy is installed as a sidecar to the driver’s controller pod otherwise it is installed as a separate deployment. Yes “True”   port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation Yes 2222   mode This refers to the installation mode of Reverse Proxy. It can be set to:\n1. Linked: In this mode, the Reverse Proxy communicates with a primary or a backup Unisphere managing the same set of arrays.\n2. StandAlone: In this mode, the Reverse Proxy communicates with multiple arrays managed by different Unispheres. Yes “StandAlone”   authorization Authorization is an optional feature to apply credential shielding of the backend PowerMax. - -   enabled A boolean that enables/disables authorization feature. No false   sidecarProxyImage Image for csm-authorization-sidecar. No \" \"   proxyHost Hostname of the csm-authorization server. No Empty   skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. No true    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml  Note:\n For detailed instructions on how to run the install scripts, see the readme document in the dell-csi-helm-installer folder. There are a set of samples provided here to help you configure the driver with reverse proxy This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option In order to enable authorization, there should be an authorization proxy server already installed.  Storage Classes Starting CSI PowerMax v1.6, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests has been provided in the samples/storageclass folder. Please use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. To continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Volume Snapshot Class Starting with CSI PowerMax v1.7, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerMax v2.0 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerMax to 1.7 or higher, before upgrading to 2.1.\nSample values file The following sections have useful snippets from values.yaml file which provides more information on how to configure the CSI PowerMax driver along with CSI PowerMax ReverseProxy in various modes\nCSI PowerMax driver without Proxy In this mode, the CSI PowerMax driver can only connect to a single Unisphere server. So, you just specify a list of storage arrays and the address of the Unisphere server\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"- storageArrayId:\"000000000002\"managementServers:- endpoint:https://unisphere-address:8443 Note: If you provide multiple endpoints in the list of management servers, the installer will only use the first server in the list\n CSI PowerMax driver with Proxy in Linked mode In this mode, the CSI PowerMax ReverseProxy acts as a passthrough for the RESTAPI calls and only provides limited functionality such as rate limiting, backup Unisphere server. The CSI PowerMax driver is still responsible for the authentication with the Unisphere server.\nThe first endpoint in the list of management servers is the primary Unisphere server and if you provide a second endpoint, then it will be considered as the backup Unisphere’s endpoint.\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"- storageArrayId:\"000000000002\"managementServers:- endpoint:https://primary-unisphere:8443skipCertificateValidation:falsecertSecret:primary-certlimits:maxActiveRead:5maxActiveWrite:4maxOutStandingRead:50maxOutStandingWrite:50- endpoint:https://backup-unisphere:8443#Optional# \"csireverseproxy\" refers to the subchart csireverseproxycsireverseproxy:# Set enabled to true if you want to use proxyenabled:trueimage:dellemc/csipowermax-reverseproxy:v1.4.0tlsSecret:csirevproxy-tls-secretdeployAsSidecar:trueport:2222mode:Linked Note: Since the driver is still responsible for authentication when used with Proxy in Linked mode, the credentials for both primary and backup Unisphere need to be the same.\n CSI PowerMax driver with Proxy in StandAlone mode This is the most advanced configuration which provides you with the capability to connect to Multiple Unisphere servers. You can specify primary and backup Unisphere servers for each storage array. If you have different credentials for your Unisphere servers, you can also specify different credential secrets.\nglobal:defaultCredentialsSecret:powermax-credsstorageArrays:- storageArrayId:\"000000000001\"endpoint:https://primary-1.unisphe.re:8443backupEndpoint:https://backup-1.unisphe.re:8443- storageArrayId:\"000000000002\"endpoint:https://primary-2.unisphe.re:8443backupEndpoint:https://backup-2.unisphe.re:8443managementServers:- endpoint:https://primary-1.unisphe.re:8443credentialsSecret:primary-1-secretskipCertificateValidation:falsecertSecret:primary-certlimits:maxActiveRead:5maxActiveWrite:4maxOutStandingRead:50maxOutStandingWrite:50- endpoint:https://backup-1.unisphe.re:8443credentialsSecret:backup-1-secretskipCertificateValidation:true- endpoint:https://primary-2.unisphe.re:8443credentialsSecret:primary-2-secretskipCertificateValidation:true- endpoint:https://backup-2.unisphe.re:8443credentialsSecret:backup-2-secretskipCertificateValidation:true# \"csireverseproxy\" refers to the subchart csireverseproxycsireverseproxy:# Set enabled to true if you want to use proxyenabled:trueimage:dellemc/csipowermax-reverseproxy:v1.4.0tlsSecret:csirevproxy-tls-secretdeployAsSidecar:trueport:2222mode:StandAlone Note: If the credential secret is missing from any management server details, the installer will try to use the defaultCredentialsSecret\n ","excerpt":"CSI Driver for Dell EMC PowerMax can be deployed by using the provided …","ref":"/csm-docs/v3/csidriver/installation/helm/powermax/","title":"PowerMax"},{"body":"Installing CSI Driver for PowerMax via Operator CSI Driver for Dell EMC PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Create secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver  Create namespace: Run kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace. Create PowerMax credentials: Create a file called powermax-creds.yaml with the following content: apiVersion:v1kind:Secretmetadata:name:powermax-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u003cdriver-namespace\u003e type: Opaquedata:# set username to the base64 encoded usernameusername:\u003cbase64username\u003e # set password to the base64 encoded passwordpassword:\u003cbase64password\u003e # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards)# chapsecret: \u003cbase64 CHAP secret\u003eReplace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \"myusername\" | base64 echo -n \"mypassword\" | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \"mychapsecret\" | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerMax using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:    Parameter Description Required Default     replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ   X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443   X_CSI_TRANSPORT_PROTOCOL Choose which transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: “PortGroup1,PortGroup2” No -   X_CSI_MANAGED_ARRAYS List of comma-separated array ID(s) which will be managed by the driver Yes -   X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Leave blank if not using reverse proxy No -   X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4   X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No -   Node parameters      X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false     Execute the following command to create the PowerMax custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerMax driver.  CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is an optional component that can be installed with the CSI PowerMax driver. For more details on this feature see the related documentation.\nWhen you install CSI PowerMax ReverseProxy, dell-csi-operator will create a Deployment and ClusterIP service as part of the installation\nNote - To use the ReverseProxy with the CSI PowerMax driver, the ReverseProxy service should be created before you install the CSIPowerMax driver.\nPre-requisites Create a TLS secret that holds an SSL certificate and a private key which is required by the reverse proxy server. Use a tool such as openssl to generate this secret using the example below:\n openssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n powermax tls revproxy-certs --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec  tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs config : This section contains the details of the Reverse Proxy configuration mode : This value is set to Linked by default. Do not change this value linkConfig : This section contains the configuration of the Linked mode primary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if the primary Unisphere is unreachable url : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false standAloneConfig : This section contains the configuration of the StandAlone mode. Refer to the sample below for the detailed config   Note: Only one of the Linked or StandAlone configurations needs to be supplied. The appropriate mode needs to be set in the spec as well.\n Here is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion:storage.dell.com/v1kind:CSIPowerMaxRevProxymetadata:name:powermax-reverseproxy# \u003c- Name of the CSIPowerMaxRevProxy objectnamespace:test-powermax# \u003c- Set the namespace to where you will install the CSI PowerMax driverspec:# Image for CSI PowerMax ReverseProxyimage:dellemc/csipowermax-reverseproxy:v1.4.0# \u003c- CSI PowerMax Reverse Proxy imageimagePullPolicy:Always# TLS secret which contains SSL certificate and private key for the Reverse Proxy servertlsSecret:csirevproxy-tls-secretconfig:mode:LinkedlinkConfig:primary:url:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:true# This setting determines if client side Unisphere certificate validation is to be skippedcertSecret:\"\"# Provide this value if skipCertificateValidation is set to falsebackup:# This is an optional field and lets you configure a backup unisphere which can be used by proxy serverurl:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:truestandAloneConfig:# Set mode to \"StandAlone\" in order to use this configstorageArrays:- storageArrayId:\"000000000001\"# Unisphere server managing the PowerMax arrayprimaryURL:https://unisphere-1-addr:8443# proxyCredentialSecrets are used by the clients of the proxy to connect to it# If using proxy in the stand alone mode, then the driver must be provided the# same secret.# The format of the proxy credential secret are exactly the same as the unisphere credential secret# For using the proxy with the driver, use the same proxy credential secrets for# all the managed storage arraysproxyCredentialSecrets:- proxy-creds- storageArrayId:\"000000000002\"primaryURL:https://unisphere-2-addr:8443# An optional backup Unisphere server managing the same array# This can be used by the proxy to fall back to in case the primary# Unisphere is inaccessible temporarilybackupURL:unisphere-3-addr:8443proxyCredentialSecrets:- proxy-credsmanagementServers:- url:https://unisphere-1-addr:8443# Secret containing the credentials of the Unisphere serverarrayCredentialSecret:unsiphere-1-credsskipCertificateValidation:true- url:https://unisphere-2-addr:8443arrayCredentialSecret:unsiphere-2-credsskipCertificateValidation:true- url:https://unisphere-3-addr:8443arrayCredentialSecret:unsiphere-3-credsskipCertificateValidation:trueInstallation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service:\nkubectl create -f powermax_reverseproxy.yaml  You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u003cnamespace\u003e kubectl get svc -n \u003cnamespace\u003e  There is a new sample file - powermax_revproxy_standalone_with_driver.yaml in the samples folder which enables installation of CSI PowerMax ReverseProxy in StandAlone mode along with the CSI PowerMax driver. This mode enables the CSI PowerMax driver to connect to multiple Unisphere servers for managing multiple PowerMax arrays. Please follow the same steps described above to install ReverseProxy with this new sample file.\nDynamic Logging Configuration This feature is introduced in CSI Driver for powermax version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powermax-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powermax-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n powermax powermax-config-params Sample CRD file for powermax apiVersion:storage.dell.com/v1kind:CSIPowerMaxmetadata:name:test-powermaxnamespace:test-powermaxspec:driver:# Config version for CSI PowerMax v2.1.0 driverconfigVersion:v2.1.0# replica: Define the number of PowerMax controller nodes# to deploy to the Kubernetes release# Allowed values: n, where n \u003e 0# Default value: Nonereplicas:2dnsPolicy:ClusterFirstWithHostNetforceUpdate:falsecommon:# Image for CSI PowerMax driver v2.1.0image:dellemc/csi-powermax:v2.1.0# imagePullPolicy: Policy to determine if the image should be pulled prior to starting the container.# Allowed values:# Always: Always pull the image.# IfNotPresent: Only pull the image if it does not already exist on the node.# Never: Never pull the image.# Default value: NoneimagePullPolicy:IfNotPresentenvs:# X_CSI_MANAGED_ARRAYS: Serial ID of the arrays that will be used for provisioning# Default value: None# Examples: \"000000000001\", \"000000000002\"- name:X_CSI_MANAGED_ARRAYSvalue:\"000000000000,000000000001\"# X_CSI_POWERMAX_ENDPOINT: Address of the Unisphere server that is managing the PowerMax arrays# Default value: None# Example: https://0.0.0.1:8443- name:X_CSI_POWERMAX_ENDPOINTvalue:\"https://0.0.0.0:8443/\"# X_CSI_K8S_CLUSTER_PREFIX: Define a prefix that is appended onto# all resources created in the Array# This should be unique per K8s/CSI deployment# maximum length of this value is 3 characters# Default value: None# Examples: \"XYZ\", \"EMC\"# Examples: \"XYZ\", \"EMC\"- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\"XYZ\"# X_CSI_POWERMAX_PORTGROUPS: Define the set of existing port groups that the driver will use.# It is a comma separated list of portgroup names.# Required only in case of iSCSI port groups# Allowed values: iSCSI Port Group names# Default value: None# Examples: \"pg1\", \"pg1, pg2\"- name:\"X_CSI_POWERMAX_PORTGROUPS\"value:\"\"# \"X_CSI_TRANSPORT_PROTOCOL\" can be \"FC\" or \"FIBRE\" for fibrechannel,# \"ISCSI\" for iSCSI, or \"\" for autoselection.# Allowed values:# \"FC\" - Fiber Channel protocol# \"FIBER\" - Fiber Channel protocol# \"ISCSI\" - iSCSI protocol# \"\" - Automatic selection of transport protocol# Default value: \"\" \u003cempty\u003e- name:\"X_CSI_TRANSPORT_PROTOCOL\"value:\"\"# X_CSI_POWERMAX_PROXY_SERVICE_NAME: Refers to the name of the proxy service in kubernetes# Set this to \"powermax-reverseproxy\" if you are installing the proxy# Allowed values: \"powermax-reverseproxy\"# default values: \"\" \u003cempty\u003e- name:\"X_CSI_POWERMAX_PROXY_SERVICE_NAME\"value:\"\"# X_CSI_GRPC_MAX_THREADS: Defines the maximum number of concurrent grpc requests.# Set this value to a higher number (max 50) if you are using the proxy# Allowed values: n, where n \u003e 4# default values: None- name:\"X_CSI_GRPC_MAX_THREADS\"value:\"4\"node:envs:# X_CSI_POWERMAX_ISCSI_ENABLE_CHAP: Determine if the driver is going to configure# ISCSI node databases on the nodes with the CHAP credentials# If enabled, the CHAP secret must be provided in the credentials secret# and set to the key \"chapsecret\"# Allowed values:# \"true\" - CHAP is enabled# \"false\" - CHAP is disabled# Default value: \"false\"- name:\"X_CSI_POWERMAX_ISCSI_ENABLE_CHAP\"value:\"false\"---apiVersion:v1kind:ConfigMapmetadata:name:powermax-config-paramsnamespace:test-powermaxdata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"debug\"CSI_LOG_FORMAT:\"JSON\"Note:\n dell-csi-operator does not support the installation of CSI PowerMax ReverseProxy as a sidecar to the controller Pod. This facility is only present with dell-csi-helm-installer. Kubelet config dir path is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  ","excerpt":"Installing CSI Driver for PowerMax via Operator CSI Driver for Dell …","ref":"/csm-docs/v3/csidriver/installation/operator/powermax/","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell EMC PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use CSI Driver for Dell EMC PowerMax. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with a different number of volumes in a given namespace using the storageclass provided. To test the installation of the CSI driver, perform these tests:\n Volume clone test Volume test Snapshot test  Volume test Use this procedure to perform a volume test.\n  Create a namespace with the name test.\n  Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\n  Run the starttest.sh script and provide it with a test name. The following sample command can be used to run the 2vols test: ./starttest.sh -t 2vols -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e\nThis script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\n  Run the /stoptest.sh -t 2vols -n \u003ctest_namespace\u003e script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\n   NOTE: Helm tests have been designed assuming that users have created storageclass names like storageclass-name and storageclass-name-xfs. You can use kubectl get sc to check for the storageclass names.\n Volume clone test Use this procedure to perform a volume clone test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: bash volumeclonetest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum, and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test.  Snapshot test Use this procedure to perform a snapshot test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot of that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test.   NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass. You must update the snapshot class name in the file snap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\n Volume Expansion test Use this procedure to perform a volume expansion test.\n Create a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh -n \u003ctest_namespace\u003e -s \u003cstorageclass-name\u003e  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates one PVC, and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC, and then recalculates the checksum Cleans up all the resources that were created as part of the test  Setting Application Prefix Application prefix is the name of the application that can be used to group the PowerMax volumes. We can use it while naming storage group. To set the application prefix for PowerMax, please refer to the sample storage class https://github.com/dell/csi-powermax/blob/main/samples/storageclass/powermax.yaml.\n# Name of application to be used to group volumes# This is used in naming storage group# Optional: true, Default value: None# Examples: APP, app, sanity, testsApplicationPrefix:\u003capplicationprefix\u003e  Note: Supported length of storage group for PowerMax is 64 characters. Storage group name is of the format “csi-clusterprefix-application prefix-SLO name-SRP name-SG”. Based on the other inputs like clusterprefix,SLO name and SRP name maximum length of the ApplicationPrefix can vary.\n ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v3/csidriver/installation/test/powermax/","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v2.1.0 New Features/Changes  Added support for OpenShift v4.9. Added support for CSI spec 1.5. Added v2 suffix to the module names. Added support for CSM Authorization sidecar via Helm  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Delete Volume fails with the error message: volume is part of masking view This issue is due to limitations in Unisphere and occurs when Unisphere is overloaded. Currently, there is no workaround for this but it can be avoided by ensuring that Unisphere is not overloaded during such operations. The Unisphere team is assessing a fix for this in a future Unisphere release   Getting initiators list fails with context deadline error The following error can occur during the driver installation if a large number of initiators are present on the array. There is no workaround for this but it can be avoided by deleting stale initiators on the array   Unable to update Host: A problem occurred modifying the host resource This issue occurs when the nodes do not have unique hostnames or when an IP address/FQDN with same sub-domains are used as hostnames. The workaround is to use unique hostnames or FQDN with unique sub-domains    ","excerpt":"Release Notes - CSI PowerMax v2.1.0 New Features/Changes  Added …","ref":"/csm-docs/v3/csidriver/release/powermax/","title":"PowerMax"},{"body":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that you have applied all the features to the indicated processes. Restart kubelet when remediated.   kubectl describe pod powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e indicates that the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or log in to the docker registry   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver cannot authenticate Check your secret’s username and password   kubectl logs powermax-controller-\u003cxyz\u003e –n \u003cnamespace\u003e driver logs show that the driver failed to connect to the U4P because it could not verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Warning about …","ref":"/csm-docs/v3/csidriver/troubleshooting/powermax/","title":"PowerMax"},{"body":"The CSI Driver for Dell PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell PowerScale:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first If enabling CSM for Replication, please refer to the Replication deployment steps first If enabling CSM for Resiliency, please refer to the Resiliency deployment steps first  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerScale.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\n(Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\ncontroller:snapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here:v5.0.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v5.0.x\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via helm. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\ncontroller:healthMonitor:# enabled: Enable/Disable health monitor of CSI volumes# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: Noneenabled:false# healthMonitorInterval: Interval of monitoring volume health condition# Allowed values: Number followed by unit (s,m,h)# Examples: 60s, 5m, 1h# Default value: 60sinterval:60snode:healthMonitor:# enabled: Enable/Disable health monitor of CSI volumes- volume usage, volume condition# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: Noneenabled:falseInstallation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller.  (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication:enabled:trueReplication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\n  Run git clone -b v2.3.0 https://github.com/dell/csi-powerscale.git to clone the git repository.\n  Ensure that you have created the namespace where you want to install the driver. You can run kubectl create namespace isilon to create a new one. The use of “isilon” as the namespace is just an example. You can choose any name for the namespace.\n  Collect information from the PowerScale Systems like IP address, IsiPath, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml.\n  Copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\n  Edit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     logLevel CSI driver log level No “debug”   certSecretCount Defines the number of certificate secrets, which the user is going to create for SSL authentication. (isilon-cert-0..isilon-cert-(n-1)); Minimum value should be 1. Yes 1   allowedNetworks Defines the list of networks that can be used for NFS I/O traffic, CIDR format must be used. No [ ]   maxIsilonVolumesPerNode Defines the default value for a maximum number of volumes that the controller can publish to the node. If the value is zero CO SHALL decide how many volumes of this type can be published by the controller to the node. This limit is applicable to all the nodes in the cluster for which node label ‘max-isilon-volumes-per-node’ is not set. Yes 0   imagePullPolicy Defines the policy to determine if the image should be pulled prior to starting the container Yes IfNotPresent   verbose Indicates what content of the OneFS REST API message should be logged in debug level logs Yes 1   kubeletConfigDir Specify kubelet config dir path Yes “/var/lib/kubelet”   enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish a connection to Array. This requires enableCustomTopology to be enabled. No false   fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType”   podmonAPIPort Defines the port which csi-driver will use within the cluster to support podmon No 8083   maxPathLen Defines the maximum length of path for a volume No 192   controller Configure controller pod specific parameters     controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2   volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s”   snapshot.enabled Enable/Disable volume snapshot feature Yes true   snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot”   resizer.enabled Enable/Disable volume expansion feature Yes true   healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume status, volume condition Yes false   healthMonitor.interval Interval of monitoring volume health condition Yes 60s   nodeSelector Define node selection constraints for pods of controller deployment No    tolerations Define tolerations for the controller deployment, if required No    leader-election-lease-duration Duration, that non-leader candidates will wait to force acquire leadership No 20s   leader-election-renew-deadline Duration, that the acting leader will retry refreshing leadership before giving up No 15s   leader-election-retry-period Duration, the LeaderElector clients should wait between tries of actions No 5s   node Configure node pod specific parameters     nodeSelector Define node selection constraints for pods of node daemonset No    tolerations Define tolerations for the node daemonset, if required No    dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet   healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition Yes false   PLATFORM ATTRIBUTES      endpointPort Define the HTTPs port number of the PowerScale OneFS API server. If authorization is enabled, endpointPort should be the HTTPS localhost port that the authorization sidecar will listen on. This value acts as a default value for endpointPort, if not specified for a cluster config in secret. No 8080   skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. This value acts as a default value for skipCertificateValidation, if not specified for a cluster config in secret. No true   isiAuthType Indicates the authentication method to be used. If set to 1 then it follows as session-based authentication else basic authentication No 0   isiAccessZone Define the name of the access zone a volume can be created in. If storageclass is missing with AccessZone parameter, then value of isiAccessZone is used for the same. No System   enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. No true   isiPath Define the base path for the volumes to be created on PowerScale cluster. This value acts as a default value for isiPath, if not specified for a cluster config in secret No /ifs/data/csi   noProbeOnStart Define whether the controller/node plugin should probe all the PowerScale clusters during driver initialization No false   autoProbe Specify if automatically probe the PowerScale cluster if not done already during CSI calls No true   authorization Authorization is an optional feature to apply credential shielding of the backend PowerScale. - -   enabled A boolean that enables/disables authorization feature. No false   sidecarProxyImage Image for csm-authorization-sidecar. No \" \"   proxyHost Hostname of the csm-authorization server. No Empty   skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. No true   podmon Podmon is an optional feature under development and tech preview. Enable this feature only after contact support for additional information. - -   enabled A boolean that enable/disable podmon feature. No false   image image for podmon. No \" \"    NOTE:\n ControllerCount parameter value must not exceed the number of nodes in the Kubernetes cluster. Otherwise, some of the controller pods remain in a “Pending” state till new nodes are available for scheduling. The installer exits with a WARNING on the same. Whenever the certSecretCount parameter changes in my-isilon-setting.yaml user needs to reinstall the driver. In order to enable authorization, there should be an authorization proxy server already installed.    Edit following parameters in samples/secret/secret.yaml file and update/add connection/authentication information for one or more PowerScale clusters.\n   Parameter Description Required Default     clusterName Logical name of PoweScale cluster against which volume CRUD operations are performed through this secret. Yes -   username username for connecting to PowerScale OneFS API server Yes -   password password for connecting to PowerScale OneFS API server Yes -   endpoint HTTPS endpoint of the PowerScale OneFS API server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes -   isDefault Indicates if this is a default cluster (would be used by storage classes without ClusterName parameter). Only one of the cluster config should be marked as default. No false   Optional parameters Following parameters are Optional. If specified will override default values from values.yaml.     skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. No default value from values.yaml   endpointPort Specify the HTTPs port number of the PowerScale OneFS API server No default value from values.yaml   isiPath The base path for the volumes to be created on PowerScale cluster. Note: IsiPath parameter in storageclass, if present will override this attribute. No default value from values.yaml   mountEndpoint Endpoint of the PowerScale OneFS API server, for example, 10.0.0.1. This must be specified if CSM-Authorization is enabled. No -    The username specified in secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\n   Privilege Type     ISI_PRIV_LOGIN_PAPI Read Only   ISI_PRIV_NFS Read Write   ISI_PRIV_QUOTA Read Write   ISI_PRIV_SNAPSHOT Read Write   ISI_PRIV_IFS_RESTORE Read Only   ISI_PRIV_NS_IFS_ACCESS Read Only   ISI_PRIV_IFS_BACKUP Read Only      Create isilon-creds secret using the following command:  kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\nNOTE:\n If any key/value is present in all my-isilon-settings.yaml, secret, and storageClass, then the values provided in storageClass parameters take precedence. The user has to validate the yaml syntax and array-related key/values while replacing or appending the isilon-creds secret. The driver will continue to use previous values in case of an error found in the yaml file. For the key isiIP/endpoint, the user can give either IP address or FQDN. Also, the user can prefix ‘https’ (For example, https://192.168.1.1) with the value. The isilon-creds secret has a mountEndpoint parameter which should only be updated and used when Authorization is enabled.   Install OneFS CA certificates by following the instructions from the next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and an empty secret must be created for the successful installation of CSI Driver for Dell PowerScale.\nkubectl create -f empty-secret.yaml This command will create a new secret called isilon-certs-0 in isilon namespace.\n  Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace isilon --values ../helm/my-isilon-settings.yaml (assuming that the current working directory is ‘helm’ and my-isilon-settings.yaml is also present under ‘helm’ directory)\n  Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘skipCertificateValidation’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘skipCertificateValidation’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘skipCertificateValidation’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘skipCertificateValidation’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure  To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem To create the certs secret, run kubectl create secret generic isilon-certs-0 --from-file=cert-0=ca_cert_0.pem -n isilon Use the following command to replace the secret  kubectl create secret generic isilon-certs-0 -n isilon --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -  NOTES:\n The OneFS IP can be with or without a port, depends upon the configuration of OneFS API server. The commands are based on the namespace ‘isilon’ It is highly recommended that ca_cert.pem file(s) having the naming convention as ca_cert_number.pem (example: ca_cert_0, ca_cert_1), where this number starts from 0 and grows as the number of OneFS arrays grows. The cert secret created out of these pem files must have the naming convention as isilon-certs-number (example: isilon-certs-0, isilon-certs-1, and so on.); The number must start from zero and must grow in incremental order. The number of the secrets created out of pem files should match certSecretCount value in myvalues.yaml or my-isilon-settings.yaml.  Dynamic update of array details via secret.yaml CSI Driver for Dell PowerScale now provides supports for Multi cluster. Now users can link the single CSI Driver to multiple OneFS Clusters by updating secret.yaml. Users can now update the isilon-creds secret by editing the secret.yaml and executing the following command\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\nNote: Updating isilon-certs-x secrets is a manual process, unlike isilon-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nStorage Classes The CSI driver for Dell PowerScale version 1.5 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A sample storage class manifest is available at samples/storageclass/isilon.yaml. Use this sample manifest to create a storageclass to provision storage; uncomment/ update the manifest as per the requirements.\nWhat happens to my existing storage classes? Upgrading from CSI PowerScale v2.2 driver: The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nNOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Volume Snapshot Class Starting CSI PowerScale v1.6, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. Sample volume snapshot class manifests are available at samples/volumesnapshotclass/. Use these sample manifests to create a volumesnapshotclass for creating volume snapshots; uncomment/ update the manifests as per the requirements.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerScale v2.2 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerScale to 1.6 or higher before upgrading to 2.2.\n","excerpt":"The CSI Driver for Dell PowerScale can be deployed by using the …","ref":"/csm-docs/docs/csidriver/installation/helm/isilon/","title":"PowerScale"},{"body":"Installing CSI Driver for PowerScale via Operator The CSI Driver for Dell PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerScale via Operator.\nListing installed drivers with the CSI Isilon CRD User can query for CSI-PowerScale driver using the following command: kubectl get csiisilon --all-namespaces\nInstall Driver   Create namespace.\nExecute kubectl create namespace test-isilon to create the test-isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘test-isilon’.\n  Create isilon-creds secret by using secret.yaml file format only.\n2.1 Create a yaml file called secret.yaml with the following content:\n isilonClusters: # logical name of PowerScale Cluster - clusterName: \"cluster1\" # username for connecting to PowerScale OneFS API server # Default value: None username: \"user\" # password for connecting to PowerScale OneFS API server password: \"password\" # HTTPS endpoint of the PowerScale OneFS API server # Default value: None # Examples: \"1.2.3.4\", \"https://1.2.3.4\", \"https://abc.myonefs.com\" endpoint: \"1.2.3.4\" # Is this a default cluster (would be used by storage classes without ClusterName parameter) # Allowed values: # true: mark this cluster config as default # false: mark this cluster config as not default # Default value: false isDefault: true # Specify whether the PowerScale OneFS API server's certificate chain and host name should be verified. # Allowed values: # true: skip OneFS API server's certificate verification # false: verify OneFS API server's certificates # Default value: default value specified in values.yaml # skipCertificateValidation: true # The base path for the volumes to be created on PowerScale cluster # This will be used if a storage class does not have the IsiPath parameter specified. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Default value: default value specified in values.yaml # Examples: \"/ifs/data/csi\", \"/ifs/engineering\" # isiPath: \"/ifs/data/csi\" # The permissions for isi volume directory path # This will be used if a storage class does not have the IsiVolumePathPermissions parameter specified. # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" # isiVolumePathPermissions: \"0777\" - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" endpointPort: \"8080\" Replace the values for the given keys as per your environment. After creating the secret.yaml, the following command can be used to create the secret,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nUse the following command to replace or update the secret\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the isilon-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\n  Create isilon-certs-n secret. Please refer this section for creating cert-secrets.\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:isilon-certs-0namespace:isilontype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\n  Create a CR (Custom Resource) for PowerScale using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\n   Parameter Description Required Default     dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet   fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType”   X_CSI_MAX_PATH_LIMIT Defines the maximum length of path for a volume No 192   Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_ISI_SKIP_CERTIFICATE_VALIDATION Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty   X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISI_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    X_CSI_ISI_VOLUME_PATH_PERMISSIONS The permissions for isi volume directory path Yes 0777   X_CSI_ISI_AUTH_TYPE Indicates the authentication method to be used. If set to 1 then it follows as session-based authentication else basic authentication No 0   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    nodeSelector Define node selection constraints for pods of controller deployment No    X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin. Provides details of volume status and volume condition. As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar No false   Node parameters      X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes 0   X_CSI_MODE Driver starting mode No node   X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from node plugin. Provides details of volume usage No false   Side car parameters      leader-election-lease-duration Duration, that non-leader candidates will wait to force acquire leadership No 20s   leader-election-renew-deadline Duration, that the acting leader will retry refreshing leadership before giving up No 15s   leader-election-retry-period Duration, the LeaderElector clients should wait between tries of actions No 5s      Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\n  Note :\n From CSI-PowerScale v1.6.0 and higher, Storage class and VolumeSnapshotClass will not be created as part of driver deployment. The user has to create Storageclass and Volume Snapshot Class. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  Volume Health Monitoring This feature is introduced in CSI Driver for PowerScale version 2.1.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin.# Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\".# - name: external-health-monitor# args: [\"--monitor-interval=60s\"]# Install the 'external-health-monitor' sidecar accordingly.# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: falsecontroller:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"node:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"","excerpt":"Installing CSI Driver for PowerScale via Operator The CSI Driver for …","ref":"/csm-docs/docs/csidriver/installation/operator/isilon/","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\n  Creating a storage class:\nCreate a file storageclass.yaml using sample yaml file located at samples/storageclass/isilon.yaml. Update/uncomment the attributes in this sample file as per the requirements.\nExecute the following command to create a storage class:\nkubectl create -f $PWD/storageclass.yaml Result: After executing the above command storage class will be created in the default namespace, and the user can see the storage class by executing kubectl get sc.\n  Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at samples/persistentvolumeclaim/pvc.yaml\nExecute the following command to create volume:\nkubectl create -f $PWD/pvc.yaml\nResult: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing the command kubectl get pvc.\n  Note: The status of the volume can be either Bound or Pending depending on the VolumeBindingMode specified on the storage class.\n Attach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at samples/pod/.\nExecute the following command to mount the volume to the Kubernetes node:\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, a new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for the host to be part of clients/rootclients field of export created for volume and used by nginx application.\n  Create Snapshot\nVolumeSnapshotClass is needed for creating the volume snapshots. Starting from v1.6, CSI Driver for PowerScale will not create any default Volume Snapshot class.\nSo the user has to create a volume snapshot class. The required sample files are present under samples/volumesnapshotclass/. Choose the file based on Kubernetes version.\nExecute either one of the following commands to create a volume snapshot class.\nkubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1.yaml OR kubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1beta1.yaml\nThe above-said command will create a volume snapshotclass with the name isilon-snapclass.\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snapshot-of-test-pvc.yaml. The sample file for snapshot creation is located at samples/volumesnapshot/.\nExecute the following command to create snapshot:\nkubectl create -f samples/volumesnapshot/snapshot-of-test-pvc.yaml\nThe spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is test-pvc, then the created snapshot is named snapshot-of-test-pvc. Verify the PowerScale system for the newly created snapshot.\n  Note:\n User can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. User has to make sure that the IsiPath in the parameters section of the volume snapshot class is matching with a corresponding storage class.   Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in the spec dataSource field.\nThe sample file for volume creation from the snapshot is located at samples/persistentvolumeclaim/pvc-from-snapshot.yaml .\nExecute the following command to create a snapshot:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-snapshot.yaml Verify the PowerScale system for newly created volume from the snapshot.\n  Delete Snapshot\nExecute the following commands to delete the snapshot:\nkubectl get volumesnapshot kubectl delete volumesnapshot snapshot-of-test-pvc   Create a new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in the spec dataSource field.\nThe sample file for volume creation from volume is located at samples/persistentvolumeclaim/pvc-from-pvc.yaml\nExecute the following command to create a pvc from another pvc:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-pvc.yaml Verify the PowerScale system for newly created volume from volume.\n  To Unattach the volume from Host\nDelete the nginx application to Unattach the volume from the host:\nkubectl delete -f nginx.yaml\n  To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc   ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/docs/csidriver/installation/test/powerscale/","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v2.3.0 New Features/Changes  Removed beta volumesnapshotclass sample files. Added support for Kubernetes 1.24. Added support to increase volume path limit. Added support for OpenShift 4.10. Added support for CSM Resiliency sidecar via Helm.  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Resolution or workaround, if known     If the length of the nodeID exceeds 128 characters, the driver fails to update the CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn’t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in the CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future Kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581 Note: In kubernetes 1.22 this limit has been relaxed to 192 characters.   If some older NFS exports /terminated worker nodes still in NFS export client list, CSI driver tries to add a new worker node it fails (For RWX volume). User need to manually clean the export client list from old entries to make successful addition of new worker nodes.   Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100   fsGroupPolicy may not work as expected without root privileges for NFS onlyhttps://github.com/kubernetes/examples/issues/260 To get the desired behavior set “RootClientEnabled” = “true” in the storage class parameter    Note:  Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI Driver for PowerScale v2.3.0 New Features/Changes …","ref":"/csm-docs/docs/csidriver/release/powerscale/","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms Prevention, Resolution or Workaround     The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password for corresponding cluster   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs- secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in the production environment.   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver error: create volume failed, Access denied. create directory as requested This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations.   Volume/filesystem is allowed to mount by any host in the network, though that host is not a part of the export of that particular volume under /ifs directory “Dell PowerScale: OneFS NFS Design Considerations and Best Practices”: There is a default shared directory (ifs) of OneFS, which lets clients running Windows, UNIX, Linux, or Mac OS X access the same directories and files. It is recommended to disable the ifs shared directory in a production environment and create dedicated NFS exports and SMB shares for your workload.   Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class is not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.   While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.   If the hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to driver version 1.4.0 or later there is a possibility of “localhost” as a stale entry in export Recommended setup: User should not map a hostname to loopback IP in /etc/hosts file   Driver node pod is in “CrashLoopBackOff” as “Node ID” generated is not with proper FQDN. This might be due to “dnsPolicy” implemented on the driver node pod which may differ with different networks. This parameter is configurable in both helm and Operator installer and the user can try with different “dnsPolicy” according to the environment.   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver Authentication failed. Trying to re-authenticate when using Session-based authentication The issue has been resolved from OneFS 9.3 onwards, for OneFS versions prior to 9.3 for session-based authentication either smart connect can be created against a single node of Isilon or CSI Driver can be installed/pointed to a particular node of the Isilon else basic authentication can be used by setting isiAuthType in values.yaml to 0   When an attempt is made to create more than one ReadOnly PVC from the same volume snapshot, the second and subsequent requests result in PVCs in state Pending, with a warning another RO volume from this snapshot is already present. This is because the driver allows only one RO volume from a specific snapshot at any point in time. This is to allow faster creation(within a few seconds) of a RO PVC from a volume snapshot irrespective of the size of the volume snapshot. Wait for the deletion of the first RO PVC created from the same volume snapshot.   While attaching a ReadOnly PVC from a volume snapshot to a pod, the mount operation will fail with error mounting ... failed, reason given by server: No such file or directory, if RO volume’s access zone(non System access zone) on Isilon is configured with a dedicated service IP(which is same as AzServiceIP storage class parameter). This operation results in accessing the snapshot base directory(/ifs) and results in overstepping the RO volume’s access zone’s base directory, which the OneFS doesn’t allow. Provide a service ip that belongs to RO volume’s access zone which set the highest level /ifs as its zone base directory.    ","excerpt":"Here are some installation failures that might be encountered and how …","ref":"/csm-docs/docs/csidriver/troubleshooting/powerscale/","title":"PowerScale"},{"body":"Installing CSI Driver for PowerScale via Dell CSM Operator The CSI Driver for Dell PowerScale can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerScale via Operator.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using the following command: kubectl get csm --all-namespaces\nPrerequisite   Create namespace. Execute kubectl create namespace test-isilon to create the test-isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘test-isilon’.\n  Create isilon-creds secret by creating a yaml file called secret.yaml with the following content:\n isilonClusters: # logical name of PowerScale Cluster - clusterName: \"cluster1\" # username for connecting to PowerScale OneFS API server # Default value: None username: \"user\" # password for connecting to PowerScale OneFS API server password: \"password\" # HTTPS endpoint of the PowerScale OneFS API server # Default value: None # Examples: \"1.2.3.4\", \"https://1.2.3.4\", \"https://abc.myonefs.com\" endpoint: \"1.2.3.4\" # Is this a default cluster (would be used by storage classes without ClusterName parameter) # Allowed values: # true: mark this cluster config as default # false: mark this cluster config as not default # Default value: false isDefault: true # Specify whether the PowerScale OneFS API server's certificate chain and host name should be verified. # Allowed values: # true: skip OneFS API server's certificate verification # false: verify OneFS API server's certificates # Default value: default value specified in values.yaml # skipCertificateValidation: true # The base path for the volumes to be created on PowerScale cluster # This will be used if a storage class does not have the IsiPath parameter specified. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Default value: default value specified in values.yaml # Examples: \"/ifs/data/csi\", \"/ifs/engineering\" # isiPath: \"/ifs/data/csi\" # The permissions for isi volume directory path # This will be used if a storage class does not have the IsiVolumePathPermissions parameter specified. # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" # isiVolumePathPermissions: \"0777\" - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" endpointPort: \"8080\" Replace the values for the given keys as per your environment. After creating the secret.yaml, the following command can be used to create the secret,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nUse the following command to replace or update the secret\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the isilon-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\n  Create isilon-certs-n secret. Please refer this section for creating cert-secrets.\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:isilon-certs-0namespace:isilontype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\n  Install Driver   Follow all the prerequisites above\n  Create a CR (Custom Resource) for PowerScale using the sample files provided here. This file can be modified to use custom parameters if needed.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\n   Parameter Description Required Default     dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet   Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_ISI_SKIP_CERTIFICATE_VALIDATION Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty   X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISI_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    X_CSI_ISI_VOLUME_PATH_PERMISSIONS The permissions for isi volume directory path Yes 0777   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    Node parameters      X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes 0   X_CSI_MODE Driver starting mode No node      Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\n  Verify the CSI Driver installation\n  Note :\n “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  ","excerpt":"Installing CSI Driver for PowerScale via Dell CSM Operator The CSI …","ref":"/csm-docs/docs/deployment/csmoperator/drivers/powerscale/","title":"PowerScale"},{"body":"The CSI Driver for Dell PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell PowerScale:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first If enabling CSM for Replication, please refer to the Replication deployment steps first  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerScale.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\n(Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\ncontroller:snapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here:v5.0.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v5.0.x\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via helm. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\ncontroller:healthMonitor:# enabled: Enable/Disable health monitor of CSI volumes# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: Noneenabled:false# healthMonitorInterval: Interval of monitoring volume health condition# Allowed values: Number followed by unit (s,m,h)# Examples: 60s, 5m, 1h# Default value: 60sinterval:60snode:healthMonitor:# enabled: Enable/Disable health monitor of CSI volumes- volume usage, volume condition# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: Noneenabled:falseInstallation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller.  (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication:enabled:trueReplication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\n  Run git clone -b v2.2.0 https://github.com/dell/csi-powerscale.git to clone the git repository.\n  Ensure that you have created the namespace where you want to install the driver. You can run kubectl create namespace isilon to create a new one. The use of “isilon” as the namespace is just an example. You can choose any name for the namespace.\n  Collect information from the PowerScale Systems like IP address, IsiPath, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml.\n  Copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\n  Edit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     logLevel CSI driver log level No “debug”   certSecretCount Defines the number of certificate secrets, which the user is going to create for SSL authentication. (isilon-cert-0..isilon-cert-(n-1)); Minimum value should be 1. Yes 1   allowedNetworks Defines the list of networks that can be used for NFS I/O traffic, CIDR format must be used. No [ ]   maxIsilonVolumesPerNode Defines the default value for a maximum number of volumes that the controller can publish to the node. If the value is zero CO SHALL decide how many volumes of this type can be published by the controller to the node. This limit is applicable to all the nodes in the cluster for which node label ‘max-isilon-volumes-per-node’ is not set. Yes 0   imagePullPolicy Defines the policy to determine if the image should be pulled prior to starting the container Yes IfNotPresent   verbose Indicates what content of the OneFS REST API message should be logged in debug level logs Yes 1   kubeletConfigDir Specify kubelet config dir path Yes “/var/lib/kubelet”   enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish a connection to Array. This requires enableCustomTopology to be enabled. No false   fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType”   controller Configure controller pod specific parameters     controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2   volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s”   snapshot.enabled Enable/Disable volume snapshot feature Yes true   snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot”   resizer.enabled Enable/Disable volume expansion feature Yes true   healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume status, volume condition Yes false   healthMonitor.interval Interval of monitoring volume health condition Yes 60s   nodeSelector Define node selection constraints for pods of controller deployment No    tolerations Define tolerations for the controller deployment, if required No    leader-election-lease-duration Duration, that non-leader candidates will wait to force acquire leadership No 20s   leader-election-renew-deadline Duration, that the acting leader will retry refreshing leadership before giving up No 15s   leader-election-retry-period Duration, the LeaderElector clients should wait between tries of actions No 5s   node Configure node pod specific parameters     nodeSelector Define node selection constraints for pods of node daemonset No    tolerations Define tolerations for the node daemonset, if required No    dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet   healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition Yes false   PLATFORM ATTRIBUTES      endpointPort Define the HTTPs port number of the PowerScale OneFS API server. If authorization is enabled, endpointPort should be the HTTPS localhost port that the authorization sidecar will listen on. This value acts as a default value for endpointPort, if not specified for a cluster config in secret. No 8080   skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. This value acts as a default value for skipCertificateValidation, if not specified for a cluster config in secret. No true   isiAuthType Indicates the authentication method to be used. If set to 1 then it follows as session-based authentication else basic authentication No 0   isiAccessZone Define the name of the access zone a volume can be created in. If storageclass is missing with AccessZone parameter, then value of isiAccessZone is used for the same. No System   enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. No true   isiPath Define the base path for the volumes to be created on PowerScale cluster. This value acts as a default value for isiPath, if not specified for a cluster config in secret No /ifs/data/csi   noProbeOnStart Define whether the controller/node plugin should probe all the PowerScale clusters during driver initialization No false   autoProbe Specify if automatically probe the PowerScale cluster if not done already during CSI calls No true   authorization Authorization is an optional feature to apply credential shielding of the backend PowerScale. - -   enabled A boolean that enables/disables authorization feature. No false   sidecarProxyImage Image for csm-authorization-sidecar. No \" \"   proxyHost Hostname of the csm-authorization server. No Empty   skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. No true    NOTE:\n ControllerCount parameter value must not exceed the number of nodes in the Kubernetes cluster. Otherwise, some of the controller pods remain in a “Pending” state till new nodes are available for scheduling. The installer exits with a WARNING on the same. Whenever the certSecretCount parameter changes in my-isilon-setting.yaml user needs to reinstall the driver. In order to enable authorization, there should be an authorization proxy server already installed.    Edit following parameters in samples/secret/secret.yaml file and update/add connection/authentication information for one or more PowerScale clusters.\n   Parameter Description Required Default     clusterName Logical name of PoweScale cluster against which volume CRUD operations are performed through this secret. Yes -   username username for connecting to PowerScale OneFS API server Yes -   password password for connecting to PowerScale OneFS API server Yes -   endpoint HTTPS endpoint of the PowerScale OneFS API server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes -   isDefault Indicates if this is a default cluster (would be used by storage classes without ClusterName parameter). Only one of the cluster config should be marked as default. No false   Optional parameters Following parameters are Optional. If specified will override default values from values.yaml.     skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. No default value from values.yaml   endpointPort Specify the HTTPs port number of the PowerScale OneFS API server No default value from values.yaml   isiPath The base path for the volumes to be created on PowerScale cluster. Note: IsiPath parameter in storageclass, if present will override this attribute. No default value from values.yaml   mountEndpoint Endpoint of the PowerScale OneFS API server, for example, 10.0.0.1. This must be specified if CSM-Authorization is enabled. No -    The username specified in secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\n   Privilege Type     ISI_PRIV_LOGIN_PAPI Read Only   ISI_PRIV_NFS Read Write   ISI_PRIV_QUOTA Read Write   ISI_PRIV_SNAPSHOT Read Write   ISI_PRIV_IFS_RESTORE Read Only   ISI_PRIV_NS_IFS_ACCESS Read Only   ISI_PRIV_IFS_BACKUP Read Only      Create isilon-creds secret using the following command:  kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\nNOTE:\n If any key/value is present in all my-isilon-settings.yaml, secret, and storageClass, then the values provided in storageClass parameters take precedence. The user has to validate the yaml syntax and array-related key/values while replacing or appending the isilon-creds secret. The driver will continue to use previous values in case of an error found in the yaml file. For the key isiIP/endpoint, the user can give either IP address or FQDN. Also, the user can prefix ‘https’ (For example, https://192.168.1.1) with the value. The isilon-creds secret has a mountEndpoint parameter which should only be updated and used when Authorization is enabled.   Install OneFS CA certificates by following the instructions from the next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and an empty secret must be created for the successful installation of CSI Driver for Dell PowerScale.\nkubectl create -f empty-secret.yaml This command will create a new secret called isilon-certs-0 in isilon namespace.\n  Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace isilon --values ../helm/my-isilon-settings.yaml (assuming that the current working directory is ‘helm’ and my-isilon-settings.yaml is also present under ‘helm’ directory)\n  Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘skipCertificateValidation’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘skipCertificateValidation’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘skipCertificateValidation’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘skipCertificateValidation’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure  To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem To create the certs secret, run kubectl create secret generic isilon-certs-0 --from-file=cert-0=ca_cert_0.pem -n isilon Use the following command to replace the secret  kubectl create secret generic isilon-certs-0 -n isilon --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -  NOTES:\n The OneFS IP can be with or without a port, depends upon the configuration of OneFS API server. The commands are based on the namespace ‘isilon’ It is highly recommended that ca_cert.pem file(s) having the naming convention as ca_cert_number.pem (example: ca_cert_0, ca_cert_1), where this number starts from 0 and grows as the number of OneFS arrays grows. The cert secret created out of these pem files must have the naming convention as isilon-certs-number (example: isilon-certs-0, isilon-certs-1, and so on.); The number must start from zero and must grow in incremental order. The number of the secrets created out of pem files should match certSecretCount value in myvalues.yaml or my-isilon-settings.yaml.  Dynamic update of array details via secret.yaml CSI Driver for Dell PowerScale now provides supports for Multi cluster. Now users can link the single CSI Driver to multiple OneFS Clusters by updating secret.yaml. Users can now update the isilon-creds secret by editing the secret.yaml and executing the following command\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\nNote: Updating isilon-certs-x secrets is a manual process, unlike isilon-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nStorage Classes The CSI driver for Dell PowerScale version 1.5 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A sample storage class manifest is available at samples/storageclass/isilon.yaml. Use this sample manifest to create a storageclass to provision storage; uncomment/ update the manifest as per the requirements.\nWhat happens to my existing storage classes? Upgrading from CSI PowerScale v2.1 driver: The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nNOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Volume Snapshot Class Starting CSI PowerScale v1.6, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. Sample volume snapshot class manifests are available at samples/volumesnapshotclass/. Use these sample manifests to create a volumesnapshotclass for creating volume snapshots; uncomment/ update the manifests as per the requirements.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerScale v2.1 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerScale to 1.6 or higher before upgrading to 2.2.\n","excerpt":"The CSI Driver for Dell PowerScale can be deployed by using the …","ref":"/csm-docs/v1/csidriver/installation/helm/isilon/","title":"PowerScale"},{"body":"Installing CSI Driver for PowerScale via Operator The CSI Driver for Dell PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerScale via Operator.\nListing installed drivers with the CSI Isilon CRD User can query for CSI-PowerScale driver using the following command: kubectl get csiisilon --all-namespaces\nInstall Driver   Create namespace.\nExecute kubectl create namespace test-isilon to create the test-isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘test-isilon’.\n  Create isilon-creds secret by using secret.yaml file format only.\n2.1 Create a yaml file called secret.yaml with the following content:\n isilonClusters: # logical name of PowerScale Cluster - clusterName: \"cluster1\" # username for connecting to PowerScale OneFS API server # Default value: None username: \"user\" # password for connecting to PowerScale OneFS API server password: \"password\" # HTTPS endpoint of the PowerScale OneFS API server # Default value: None # Examples: \"1.2.3.4\", \"https://1.2.3.4\", \"https://abc.myonefs.com\" endpoint: \"1.2.3.4\" # Is this a default cluster (would be used by storage classes without ClusterName parameter) # Allowed values: # true: mark this cluster config as default # false: mark this cluster config as not default # Default value: false isDefault: true # Specify whether the PowerScale OneFS API server's certificate chain and host name should be verified. # Allowed values: # true: skip OneFS API server's certificate verification # false: verify OneFS API server's certificates # Default value: default value specified in values.yaml # skipCertificateValidation: true # The base path for the volumes to be created on PowerScale cluster # This will be used if a storage class does not have the IsiPath parameter specified. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Default value: default value specified in values.yaml # Examples: \"/ifs/data/csi\", \"/ifs/engineering\" # isiPath: \"/ifs/data/csi\" # The permissions for isi volume directory path # This will be used if a storage class does not have the IsiVolumePathPermissions parameter specified. # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" # isiVolumePathPermissions: \"0777\" - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" endpointPort: \"8080\" Replace the values for the given keys as per your environment. After creating the secret.yaml, the following command can be used to create the secret,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nUse the following command to replace or update the secret\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the isilon-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\n  Create isilon-certs-n secret. Please refer this section for creating cert-secrets.\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:isilon-certs-0namespace:isilontype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\n  Create a CR (Custom Resource) for PowerScale using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\n   Parameter Description Required Default     dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet   fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType”   Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_ISI_SKIP_CERTIFICATE_VALIDATION Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty   X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISI_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    X_CSI_ISI_VOLUME_PATH_PERMISSIONS The permissions for isi volume directory path Yes 0777   X_CSI_ISI_AUTH_TYPE Indicates the authentication method to be used. If set to 1 then it follows as session-based authentication else basic authentication No 0   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    nodeSelector Define node selection constraints for pods of controller deployment No    X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin. Provides details of volume status and volume condition. As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar No false   Node parameters      X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes 0   X_CSI_MODE Driver starting mode No node   X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from node plugin. Provides details of volume usage No false   Side car parameters      leader-election-lease-duration Duration, that non-leader candidates will wait to force acquire leadership No 20s   leader-election-renew-deadline Duration, that the acting leader will retry refreshing leadership before giving up No 15s   leader-election-retry-period Duration, the LeaderElector clients should wait between tries of actions No 5s      Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\n  Note :\n From CSI-PowerScale v1.6.0 and higher, Storage class and VolumeSnapshotClass will not be created as part of driver deployment. The user has to create Storageclass and Volume Snapshot Class. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  Volume Health Monitoring This feature is introduced in CSI Driver for unity version 2.1.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin.# Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\".# - name: external-health-monitor# args: [\"--monitor-interval=60s\"]# Install the 'external-health-monitor' sidecar accordingly.# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: falsecontroller:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"node:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"","excerpt":"Installing CSI Driver for PowerScale via Operator The CSI Driver for …","ref":"/csm-docs/v1/csidriver/installation/operator/isilon/","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\n  Creating a storage class:\nCreate a file storageclass.yaml using sample yaml file located at samples/storageclass/isilon.yaml. Update/uncomment the attributes in this sample file as per the requirements.\nExecute the following command to create a storage class:\nkubectl create -f $PWD/storageclass.yaml Result: After executing the above command storage class will be created in the default namespace, and the user can see the storage class by executing kubectl get sc.\n  Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at samples/persistentvolumeclaim/pvc.yaml\nExecute the following command to create volume:\nkubectl create -f $PWD/pvc.yaml\nResult: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing the command kubectl get pvc.\n  Note: The status of the volume can be either Bound or Pending depending on the VolumeBindingMode specified on the storage class.\n Attach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at samples/pod/.\nExecute the following command to mount the volume to the Kubernetes node:\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, a new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for the host to be part of clients/rootclients field of export created for volume and used by nginx application.\n  Create Snapshot\nVolumeSnapshotClass is needed for creating the volume snapshots. Starting from v1.6, CSI Driver for PowerScale will not create any default Volume Snapshot class.\nSo the user has to create a volume snapshot class. The required sample files are present under samples/volumesnapshotclass/. Choose the file based on Kubernetes version.\nExecute either one of the following commands to create a volume snapshot class.\nkubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1.yaml OR kubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1beta1.yaml\nThe above-said command will create a volume snapshotclass with the name isilon-snapclass.\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snapshot-of-test-pvc.yaml. The sample file for snapshot creation is located at samples/volumesnapshot/.\nExecute the following command to create snapshot:\nkubectl create -f samples/volumesnapshot/snapshot-of-test-pvc.yaml\nThe spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is test-pvc, then the created snapshot is named snapshot-of-test-pvc. Verify the PowerScale system for the newly created snapshot.\n  Note:\n User can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. User has to make sure that the IsiPath in the parameters section of the volume snapshot class is matching with a corresponding storage class.   Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in the spec dataSource field.\nThe sample file for volume creation from the snapshot is located at samples/persistentvolumeclaim/pvc-from-snapshot.yaml .\nExecute the following command to create a snapshot:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-snapshot.yaml Verify the PowerScale system for newly created volume from the snapshot.\n  Delete Snapshot\nExecute the following commands to delete the snapshot:\nkubectl get volumesnapshot kubectl delete volumesnapshot snapshot-of-test-pvc   Create a new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in the spec dataSource field.\nThe sample file for volume creation from volume is located at samples/persistentvolumeclaim/pvc-from-pvc.yaml\nExecute the following command to create a pvc from another pvc:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-pvc.yaml Verify the PowerScale system for newly created volume from volume.\n  To Unattach the volume from Host\nDelete the nginx application to Unattach the volume from the host:\nkubectl delete -f nginx.yaml\n  To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc   ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v1/csidriver/installation/test/powerscale/","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v2.2.0 New Features/Changes  Added support for Replication. Added support for Kubernetes 1.23. Added support to configure fsGroupPolicy. Added support for session based authentication along with basic authentication for PowerScale.  Fixed Issues  CSI Driver installation fails with the error message “error getting FQDN”.  Known Issues    Issue Resolution or workaround, if known     If the length of the nodeID exceeds 128 characters, the driver fails to update the CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn’t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in the CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future Kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581 Note: In kubernetes 1.22 this limit has been relaxed to 192 characters.   If some older NFS exports /terminated worker nodes still in NFS export client list, CSI driver tries to add a new worker node it fails (For RWX volume). User need to manually clean the export client list from old entries to make successful addition of new worker nodes.   Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100   fsGroupPolicy may not work as expected without root privileges for NFS onlyhttps://github.com/kubernetes/examples/issues/260 To get the desired behavior set “RootClientEnabled” = “true” in the storage class parameter    Note:  Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI Driver for PowerScale v2.2.0 New Features/Changes …","ref":"/csm-docs/v1/csidriver/release/powerscale/","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms Prevention, Resolution or Workaround     The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password for corresponding cluster   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs- secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in the production environment.   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver error: create volume failed, Access denied. create directory as requested This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations.   Volume/filesystem is allowed to mount by any host in the network, though that host is not a part of the export of that particular volume under /ifs directory “Dell PowerScale: OneFS NFS Design Considerations and Best Practices”: There is a default shared directory (ifs) of OneFS, which lets clients running Windows, UNIX, Linux, or Mac OS X access the same directories and files. It is recommended to disable the ifs shared directory in a production environment and create dedicated NFS exports and SMB shares for your workload.   Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class is not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.   While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.   If the hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to driver version 1.4.0 or later there is a possibility of “localhost” as a stale entry in export Recommended setup: User should not map a hostname to loopback IP in /etc/hosts file   Driver node pod is in “CrashLoopBackOff” as “Node ID” generated is not with proper FQDN. This might be due to “dnsPolicy” implemented on the driver node pod which may differ with different networks. This parameter is configurable in both helm and Operator installer and the user can try with different “dnsPolicy” according to the environment.   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver Authentication failed. Trying to re-authenticate when using Session-based authentication The issue has been resolved from OneFS 9.3 onwards, for OneFS versions prior to 9.3 for session-based authentication either smart connect can be created against a single node of Isilon or CSI Driver can be installed/pointed to a particular node of the Isilon else basic authentication can be used by setting isiAuthType in values.yaml to 0   When an attempt is made to create more than one ReadOnly PVC from the same volume snapshot, the second and subsequent requests result in PVCs in state Pending, with a warning another RO volume from this snapshot is already present. This is because the driver allows only one RO volume from a specific snapshot at any point in time. This is to allow faster creation(within a few seconds) of a RO PVC from a volume snapshot irrespective of the size of the volume snapshot. Wait for the deletion of the first RO PVC created from the same volume snapshot.   While attaching a ReadOnly PVC from a volume snapshot to a pod, the mount operation will fail with error mounting ... failed, reason given by server: No such file or directory, if RO volume’s access zone(non System access zone) on Isilon is configured with a dedicated service IP(which is same as AzServiceIP storage class parameter). This operation results in accessing the snapshot base directory(/ifs) and results in overstepping the RO volume’s access zone’s base directory, which the OneFS doesn’t allow. Provide a service ip that belongs to RO volume’s access zone which set the highest level /ifs as its zone base directory.    ","excerpt":"Here are some installation failures that might be encountered and how …","ref":"/csm-docs/v1/csidriver/troubleshooting/powerscale/","title":"PowerScale"},{"body":"Installing CSI Driver for PowerScale via Dell CSM Operator The CSI Driver for Dell PowerScale can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerScale via Operator.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using the following command: kubectl get csm --all-namespaces\nPrerequisite   Create namespace. Execute kubectl create namespace test-isilon to create the test-isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘test-isilon’.\n  Create isilon-creds secret by creating a yaml file called secret.yaml with the following content:\n isilonClusters: # logical name of PowerScale Cluster - clusterName: \"cluster1\" # username for connecting to PowerScale OneFS API server # Default value: None username: \"user\" # password for connecting to PowerScale OneFS API server password: \"password\" # HTTPS endpoint of the PowerScale OneFS API server # Default value: None # Examples: \"1.2.3.4\", \"https://1.2.3.4\", \"https://abc.myonefs.com\" endpoint: \"1.2.3.4\" # Is this a default cluster (would be used by storage classes without ClusterName parameter) # Allowed values: # true: mark this cluster config as default # false: mark this cluster config as not default # Default value: false isDefault: true # Specify whether the PowerScale OneFS API server's certificate chain and host name should be verified. # Allowed values: # true: skip OneFS API server's certificate verification # false: verify OneFS API server's certificates # Default value: default value specified in values.yaml # skipCertificateValidation: true # The base path for the volumes to be created on PowerScale cluster # This will be used if a storage class does not have the IsiPath parameter specified. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Default value: default value specified in values.yaml # Examples: \"/ifs/data/csi\", \"/ifs/engineering\" # isiPath: \"/ifs/data/csi\" # The permissions for isi volume directory path # This will be used if a storage class does not have the IsiVolumePathPermissions parameter specified. # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" # isiVolumePathPermissions: \"0777\" - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" endpointPort: \"8080\" Replace the values for the given keys as per your environment. After creating the secret.yaml, the following command can be used to create the secret,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nUse the following command to replace or update the secret\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the isilon-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\n  Create isilon-certs-n secret. Please refer this section for creating cert-secrets.\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:isilon-certs-0namespace:isilontype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\n  Install Driver   Follow all the prerequisites above\n  Create a CR (Custom Resource) for PowerScale using the sample files provided here. This file can be modified to use custom parameters if needed.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\n   Parameter Description Required Default     dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet   Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_ISI_SKIP_CERTIFICATE_VALIDATION Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty   X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISI_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    X_CSI_ISI_VOLUME_PATH_PERMISSIONS The permissions for isi volume directory path Yes 0777   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    Node parameters      X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes 0   X_CSI_MODE Driver starting mode No node      Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\n  Verify the CSI Driver installation\n  Note :\n “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  ","excerpt":"Installing CSI Driver for PowerScale via Dell CSM Operator The CSI …","ref":"/csm-docs/v1/deployment/csmoperator/drivers/powerscale/","title":"PowerScale"},{"body":"The CSI Driver for Dell PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell PowerScale:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first If enabling CSM for Replication, please refer to the Replication deployment steps first  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerScale.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\n(Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\ncontroller:snapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here:v5.0.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v5.0.x\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via helm. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\ncontroller:healthMonitor:# enabled: Enable/Disable health monitor of CSI volumes# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: Noneenabled:false# healthMonitorInterval: Interval of monitoring volume health condition# Allowed values: Number followed by unit (s,m,h)# Examples: 60s, 5m, 1h# Default value: 60sinterval:60snode:healthMonitor:# enabled: Enable/Disable health monitor of CSI volumes- volume usage, volume condition# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: Noneenabled:falseInstallation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller.  (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication:enabled:trueReplication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in the csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\n  Run git clone -b v2.2.0 https://github.com/dell/csi-powerscale.git to clone the git repository.\n  Ensure that you have created the namespace where you want to install the driver. You can run kubectl create namespace isilon to create a new one. The use of “isilon” as the namespace is just an example. You can choose any name for the namespace.\n  Collect information from the PowerScale Systems like IP address, IsiPath, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml.\n  Copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\n  Edit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     logLevel CSI driver log level No “debug”   certSecretCount Defines the number of certificate secrets, which the user is going to create for SSL authentication. (isilon-cert-0..isilon-cert-(n-1)); Minimum value should be 1. Yes 1   allowedNetworks Defines the list of networks that can be used for NFS I/O traffic, CIDR format must be used. No [ ]   maxIsilonVolumesPerNode Defines the default value for a maximum number of volumes that the controller can publish to the node. If the value is zero CO SHALL decide how many volumes of this type can be published by the controller to the node. This limit is applicable to all the nodes in the cluster for which node label ‘max-isilon-volumes-per-node’ is not set. Yes 0   imagePullPolicy Defines the policy to determine if the image should be pulled prior to starting the container Yes IfNotPresent   verbose Indicates what content of the OneFS REST API message should be logged in debug level logs Yes 1   kubeletConfigDir Specify kubelet config dir path Yes “/var/lib/kubelet”   enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish a connection to Array. This requires enableCustomTopology to be enabled. No false   fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType”   controller Configure controller pod specific parameters     controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2   volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s”   snapshot.enabled Enable/Disable volume snapshot feature Yes true   snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot”   resizer.enabled Enable/Disable volume expansion feature Yes true   healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume status, volume condition Yes false   healthMonitor.interval Interval of monitoring volume health condition Yes 60s   nodeSelector Define node selection constraints for pods of controller deployment No    tolerations Define tolerations for the controller deployment, if required No    leader-election-lease-duration Duration, that non-leader candidates will wait to force acquire leadership No 20s   leader-election-renew-deadline Duration, that the acting leader will retry refreshing leadership before giving up No 15s   leader-election-retry-period Duration, the LeaderElector clients should wait between tries of actions No 5s   node Configure node pod specific parameters     nodeSelector Define node selection constraints for pods of node daemonset No    tolerations Define tolerations for the node daemonset, if required No    dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet   healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition Yes false   PLATFORM ATTRIBUTES      endpointPort Define the HTTPs port number of the PowerScale OneFS API server. If authorization is enabled, endpointPort should be the HTTPS localhost port that the authorization sidecar will listen on. This value acts as a default value for endpointPort, if not specified for a cluster config in secret. No 8080   skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. This value acts as a default value for skipCertificateValidation, if not specified for a cluster config in secret. No true   isiAuthType Indicates the authentication method to be used. If set to 1 then it follows as session-based authentication else basic authentication No 0   isiAccessZone Define the name of the access zone a volume can be created in. If storageclass is missing with AccessZone parameter, then value of isiAccessZone is used for the same. No System   enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. No true   isiPath Define the base path for the volumes to be created on PowerScale cluster. This value acts as a default value for isiPath, if not specified for a cluster config in secret No /ifs/data/csi   noProbeOnStart Define whether the controller/node plugin should probe all the PowerScale clusters during driver initialization No false   autoProbe Specify if automatically probe the PowerScale cluster if not done already during CSI calls No true   authorization Authorization is an optional feature to apply credential shielding of the backend PowerScale. - -   enabled A boolean that enables/disables authorization feature. No false   sidecarProxyImage Image for csm-authorization-sidecar. No \" \"   proxyHost Hostname of the csm-authorization server. No Empty   skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. No true    NOTE:\n ControllerCount parameter value must not exceed the number of nodes in the Kubernetes cluster. Otherwise, some of the controller pods remain in a “Pending” state till new nodes are available for scheduling. The installer exits with a WARNING on the same. Whenever the certSecretCount parameter changes in my-isilon-setting.yaml user needs to reinstall the driver. In order to enable authorization, there should be an authorization proxy server already installed.    Edit following parameters in samples/secret/secret.yaml file and update/add connection/authentication information for one or more PowerScale clusters.\n   Parameter Description Required Default     clusterName Logical name of PoweScale cluster against which volume CRUD operations are performed through this secret. Yes -   username username for connecting to PowerScale OneFS API server Yes -   password password for connecting to PowerScale OneFS API server Yes -   endpoint HTTPS endpoint of the PowerScale OneFS API server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes -   isDefault Indicates if this is a default cluster (would be used by storage classes without ClusterName parameter). Only one of the cluster config should be marked as default. No false   Optional parameters Following parameters are Optional. If specified will override default values from values.yaml.     skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. No default value from values.yaml   endpointPort Specify the HTTPs port number of the PowerScale OneFS API server No default value from values.yaml   isiPath The base path for the volumes to be created on PowerScale cluster. Note: IsiPath parameter in storageclass, if present will override this attribute. No default value from values.yaml   mountEndpoint Endpoint of the PowerScale OneFS API server, for example, 10.0.0.1. This must be specified if CSM-Authorization is enabled. No -    The username specified in secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\n   Privilege Type     ISI_PRIV_LOGIN_PAPI Read Only   ISI_PRIV_NFS Read Write   ISI_PRIV_QUOTA Read Write   ISI_PRIV_SNAPSHOT Read Write   ISI_PRIV_IFS_RESTORE Read Only   ISI_PRIV_NS_IFS_ACCESS Read Only   ISI_PRIV_IFS_BACKUP Read Only      Create isilon-creds secret using the following command:  kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\nNOTE:\n If any key/value is present in all my-isilon-settings.yaml, secret, and storageClass, then the values provided in storageClass parameters take precedence. The user has to validate the yaml syntax and array-related key/values while replacing or appending the isilon-creds secret. The driver will continue to use previous values in case of an error found in the yaml file. For the key isiIP/endpoint, the user can give either IP address or FQDN. Also, the user can prefix ‘https’ (For example, https://192.168.1.1) with the value. The isilon-creds secret has a mountEndpoint parameter which should only be updated and used when Authorization is enabled.   Install OneFS CA certificates by following the instructions from the next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and an empty secret must be created for the successful installation of CSI Driver for Dell PowerScale.\nkubectl create -f empty-secret.yaml This command will create a new secret called isilon-certs-0 in isilon namespace.\n  Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace isilon --values ../helm/my-isilon-settings.yaml (assuming that the current working directory is ‘helm’ and my-isilon-settings.yaml is also present under ‘helm’ directory)\n  Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘skipCertificateValidation’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘skipCertificateValidation’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘skipCertificateValidation’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘skipCertificateValidation’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure  To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem To create the certs secret, run kubectl create secret generic isilon-certs-0 --from-file=cert-0=ca_cert_0.pem -n isilon Use the following command to replace the secret  kubectl create secret generic isilon-certs-0 -n isilon --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -  NOTES:\n The OneFS IP can be with or without a port, depends upon the configuration of OneFS API server. The commands are based on the namespace ‘isilon’ It is highly recommended that ca_cert.pem file(s) having the naming convention as ca_cert_number.pem (example: ca_cert_0, ca_cert_1), where this number starts from 0 and grows as the number of OneFS arrays grows. The cert secret created out of these pem files must have the naming convention as isilon-certs-number (example: isilon-certs-0, isilon-certs-1, and so on.); The number must start from zero and must grow in incremental order. The number of the secrets created out of pem files should match certSecretCount value in myvalues.yaml or my-isilon-settings.yaml.  Dynamic update of array details via secret.yaml CSI Driver for Dell PowerScale now provides supports for Multi cluster. Now users can link the single CSI Driver to multiple OneFS Clusters by updating secret.yaml. Users can now update the isilon-creds secret by editing the secret.yaml and executing the following command\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\nNote: Updating isilon-certs-x secrets is a manual process, unlike isilon-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nStorage Classes The CSI driver for Dell PowerScale version 1.5 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A sample storage class manifest is available at samples/storageclass/isilon.yaml. Use this sample manifest to create a storageclass to provision storage; uncomment/ update the manifest as per the requirements.\nWhat happens to my existing storage classes? Upgrading from CSI PowerScale v2.1 driver: The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nNOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Volume Snapshot Class Starting CSI PowerScale v1.6, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. Sample volume snapshot class manifests are available at samples/volumesnapshotclass/. Use these sample manifests to create a volumesnapshotclass for creating volume snapshots; uncomment/ update the manifests as per the requirements.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerScale v2.1 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerScale to 1.6 or higher before upgrading to 2.2.\n","excerpt":"The CSI Driver for Dell PowerScale can be deployed by using the …","ref":"/csm-docs/v2/csidriver/installation/helm/isilon/","title":"PowerScale"},{"body":"Installing CSI Driver for PowerScale via Operator The CSI Driver for Dell PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerScale via Operator.\nListing installed drivers with the CSI Isilon CRD User can query for CSI-PowerScale driver using the following command: kubectl get csiisilon --all-namespaces\nInstall Driver   Create namespace.\nExecute kubectl create namespace test-isilon to create the test-isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘test-isilon’.\n  Create isilon-creds secret by using secret.yaml file format only.\n2.1 Create a yaml file called secret.yaml with the following content:\n isilonClusters: # logical name of PowerScale Cluster - clusterName: \"cluster1\" # username for connecting to PowerScale OneFS API server # Default value: None username: \"user\" # password for connecting to PowerScale OneFS API server password: \"password\" # HTTPS endpoint of the PowerScale OneFS API server # Default value: None # Examples: \"1.2.3.4\", \"https://1.2.3.4\", \"https://abc.myonefs.com\" endpoint: \"1.2.3.4\" # Is this a default cluster (would be used by storage classes without ClusterName parameter) # Allowed values: # true: mark this cluster config as default # false: mark this cluster config as not default # Default value: false isDefault: true # Specify whether the PowerScale OneFS API server's certificate chain and host name should be verified. # Allowed values: # true: skip OneFS API server's certificate verification # false: verify OneFS API server's certificates # Default value: default value specified in values.yaml # skipCertificateValidation: true # The base path for the volumes to be created on PowerScale cluster # This will be used if a storage class does not have the IsiPath parameter specified. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Default value: default value specified in values.yaml # Examples: \"/ifs/data/csi\", \"/ifs/engineering\" # isiPath: \"/ifs/data/csi\" # The permissions for isi volume directory path # This will be used if a storage class does not have the IsiVolumePathPermissions parameter specified. # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" # isiVolumePathPermissions: \"0777\" - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" endpointPort: \"8080\" Replace the values for the given keys as per your environment. After creating the secret.yaml, the following command can be used to create the secret,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nUse the following command to replace or update the secret\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the isilon-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\n  Create isilon-certs-n secret. Please refer this section for creating cert-secrets.\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:isilon-certs-0namespace:isilontype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\n  Create a CR (Custom Resource) for PowerScale using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\n   Parameter Description Required Default     dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet   fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType”   Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_ISI_SKIP_CERTIFICATE_VALIDATION Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty   X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISI_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    X_CSI_ISI_VOLUME_PATH_PERMISSIONS The permissions for isi volume directory path Yes 0777   X_CSI_ISI_AUTH_TYPE Indicates the authentication method to be used. If set to 1 then it follows as session-based authentication else basic authentication No 0   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    nodeSelector Define node selection constraints for pods of controller deployment No    X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin. Provides details of volume status and volume condition. As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar No false   Node parameters      X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes 0   X_CSI_MODE Driver starting mode No node   X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from node plugin. Provides details of volume usage No false   Side car parameters      leader-election-lease-duration Duration, that non-leader candidates will wait to force acquire leadership No 20s   leader-election-renew-deadline Duration, that the acting leader will retry refreshing leadership before giving up No 15s   leader-election-retry-period Duration, the LeaderElector clients should wait between tries of actions No 5s      Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\n  Note :\n From CSI-PowerScale v1.6.0 and higher, Storage class and VolumeSnapshotClass will not be created as part of driver deployment. The user has to create Storageclass and Volume Snapshot Class. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  Volume Health Monitoring This feature is introduced in CSI Driver for unity version 2.1.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin.# Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\".# - name: external-health-monitor# args: [\"--monitor-interval=60s\"]# Install the 'external-health-monitor' sidecar accordingly.# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: falsecontroller:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"node:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"true\"","excerpt":"Installing CSI Driver for PowerScale via Operator The CSI Driver for …","ref":"/csm-docs/v2/csidriver/installation/operator/isilon/","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\n  Creating a storage class:\nCreate a file storageclass.yaml using sample yaml file located at samples/storageclass/isilon.yaml. Update/uncomment the attributes in this sample file as per the requirements.\nExecute the following command to create a storage class:\nkubectl create -f $PWD/storageclass.yaml Result: After executing the above command storage class will be created in the default namespace, and the user can see the storage class by executing kubectl get sc.\n  Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at samples/persistentvolumeclaim/pvc.yaml\nExecute the following command to create volume:\nkubectl create -f $PWD/pvc.yaml\nResult: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing the command kubectl get pvc.\n  Note: The status of the volume can be either Bound or Pending depending on the VolumeBindingMode specified on the storage class.\n Attach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at samples/pod/.\nExecute the following command to mount the volume to the Kubernetes node:\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, a new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for the host to be part of clients/rootclients field of export created for volume and used by nginx application.\n  Create Snapshot\nVolumeSnapshotClass is needed for creating the volume snapshots. Starting from v1.6, CSI Driver for PowerScale will not create any default Volume Snapshot class.\nSo the user has to create a volume snapshot class. The required sample files are present under samples/volumesnapshotclass/. Choose the file based on Kubernetes version.\nExecute either one of the following commands to create a volume snapshot class.\nkubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1.yaml OR kubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1beta1.yaml\nThe above-said command will create a volume snapshotclass with the name isilon-snapclass.\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snapshot-of-test-pvc.yaml. The sample file for snapshot creation is located at samples/volumesnapshot/.\nExecute the following command to create snapshot:\nkubectl create -f samples/volumesnapshot/snapshot-of-test-pvc.yaml\nThe spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is test-pvc, then the created snapshot is named snapshot-of-test-pvc. Verify the PowerScale system for the newly created snapshot.\n  Note:\n User can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. User has to make sure that the IsiPath in the parameters section of the volume snapshot class is matching with a corresponding storage class.   Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in the spec dataSource field.\nThe sample file for volume creation from the snapshot is located at samples/persistentvolumeclaim/pvc-from-snapshot.yaml .\nExecute the following command to create a snapshot:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-snapshot.yaml Verify the PowerScale system for newly created volume from the snapshot.\n  Delete Snapshot\nExecute the following commands to delete the snapshot:\nkubectl get volumesnapshot kubectl delete volumesnapshot snapshot-of-test-pvc   Create a new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in the spec dataSource field.\nThe sample file for volume creation from volume is located at samples/persistentvolumeclaim/pvc-from-pvc.yaml\nExecute the following command to create a pvc from another pvc:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-pvc.yaml Verify the PowerScale system for newly created volume from volume.\n  To Unattach the volume from Host\nDelete the nginx application to Unattach the volume from the host:\nkubectl delete -f nginx.yaml\n  To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc   ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v2/csidriver/installation/test/powerscale/","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v2.2.0 New Features/Changes  Added support for Replication. Added support for Kubernetes 1.23. Added support to configure fsGroupPolicy. Added support for session based authentication along with basic authentication for PowerScale.  Fixed Issues  CSI Driver installation fails with the error message “error getting FQDN”.  Known Issues    Issue Resolution or workaround, if known     If the length of the nodeID exceeds 128 characters, the driver fails to update the CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn’t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in the CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future Kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581 Note: In kubernetes 1.22 this limit has been relaxed to 192 characters.   If some older NFS exports /terminated worker nodes still in NFS export client list, CSI driver tries to add a new worker node it fails (For RWX volume). User need to manually clean the export client list from old entries to make successful addition of new worker nodes.   Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100   fsGroupPolicy may not work as expected without root privileges for NFS onlyhttps://github.com/kubernetes/examples/issues/260 To get the desired behavior set “RootClientEnabled” = “true” in the storage class parameter    Note:  Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI Driver for PowerScale v2.2.0 New Features/Changes …","ref":"/csm-docs/v2/csidriver/release/powerscale/","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms Prevention, Resolution or Workaround     The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password for corresponding cluster   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs- secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in the production environment.   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver error: create volume failed, Access denied. create directory as requested This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations.   Volume/filesystem is allowed to mount by any host in the network, though that host is not a part of the export of that particular volume under /ifs directory “Dell PowerScale: OneFS NFS Design Considerations and Best Practices”: There is a default shared directory (ifs) of OneFS, which lets clients running Windows, UNIX, Linux, or Mac OS X access the same directories and files. It is recommended to disable the ifs shared directory in a production environment and create dedicated NFS exports and SMB shares for your workload.   Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class is not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.   While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.   If the hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to driver version 1.4.0 or later there is a possibility of “localhost” as a stale entry in export Recommended setup: User should not map a hostname to loopback IP in /etc/hosts file   Driver node pod is in “CrashLoopBackOff” as “Node ID” generated is not with proper FQDN. This might be due to “dnsPolicy” implemented on the driver node pod which may differ with different networks. This parameter is configurable in both helm and Operator installer and the user can try with different “dnsPolicy” according to the environment.   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver Authentication failed. Trying to re-authenticate when using Session-based authentication The issue has been resolved from OneFS 9.3 onwards, for OneFS versions prior to 9.3 for session-based authentication either smart connect can be created against a single node of Isilon or CSI Driver can be installed/pointed to a particular node of the Isilon else basic authentication can be used by setting isiAuthType in values.yaml to 0   When an attempt is made to create more than one ReadOnly PVC from the same volume snapshot, the second and subsequent requests result in PVCs in state Pending, with a warning another RO volume from this snapshot is already present. This is because the driver allows only one RO volume from a specific snapshot at any point in time. This is to allow faster creation(within a few seconds) of a RO PVC from a volume snapshot irrespective of the size of the volume snapshot. Wait for the deletion of the first RO PVC created from the same volume snapshot.   While attaching a ReadOnly PVC from a volume snapshot to a pod, the mount operation will fail with error mounting ... failed, reason given by server: No such file or directory, if RO volume’s access zone(non System access zone) on Isilon is configured with a dedicated service IP(which is same as AzServiceIP storage class parameter). This operation results in accessing the snapshot base directory(/ifs) and results in overstepping the RO volume’s access zone’s base directory, which the OneFS doesn’t allow. Provide a service ip that belongs to RO volume’s access zone which set the highest level /ifs as its zone base directory.    ","excerpt":"Here are some installation failures that might be encountered and how …","ref":"/csm-docs/v2/csidriver/troubleshooting/powerscale/","title":"PowerScale"},{"body":"Installing CSI Driver for PowerScale via Dell CSM Operator The CSI Driver for Dell PowerScale can be installed via the Dell CSM Operator. To deploy the Operator, follow the instructions available here.\nNote that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerScale via Operator.\nListing installed drivers with the ContainerStorageModule CRD User can query for all Dell CSI drivers using the following command: kubectl get csm --all-namespaces\nInstall Driver   Create namespace. Execute kubectl create namespace test-isilon to create the test-isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘test-isilon’.\n  Create isilon-creds secret by creating a yaml file called secret.yaml with the following content:\n isilonClusters: # logical name of PowerScale Cluster - clusterName: \"cluster1\" # username for connecting to PowerScale OneFS API server # Default value: None username: \"user\" # password for connecting to PowerScale OneFS API server password: \"password\" # HTTPS endpoint of the PowerScale OneFS API server # Default value: None # Examples: \"1.2.3.4\", \"https://1.2.3.4\", \"https://abc.myonefs.com\" endpoint: \"1.2.3.4\" # Is this a default cluster (would be used by storage classes without ClusterName parameter) # Allowed values: # true: mark this cluster config as default # false: mark this cluster config as not default # Default value: false isDefault: true # Specify whether the PowerScale OneFS API server's certificate chain and host name should be verified. # Allowed values: # true: skip OneFS API server's certificate verification # false: verify OneFS API server's certificates # Default value: default value specified in values.yaml # skipCertificateValidation: true # The base path for the volumes to be created on PowerScale cluster # This will be used if a storage class does not have the IsiPath parameter specified. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Default value: default value specified in values.yaml # Examples: \"/ifs/data/csi\", \"/ifs/engineering\" # isiPath: \"/ifs/data/csi\" # The permissions for isi volume directory path # This will be used if a storage class does not have the IsiVolumePathPermissions parameter specified. # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" # isiVolumePathPermissions: \"0777\" - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" endpointPort: \"8080\" Replace the values for the given keys as per your environment. After creating the secret.yaml, the following command can be used to create the secret,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nUse the following command to replace or update the secret\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the isilon-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\n  Create isilon-certs-n secret. Please refer this section for creating cert-secrets.\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:isilon-certs-0namespace:isilontype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\n  Create a CR (Custom Resource) for PowerScale using the sample files provided here. This file can be modified to use custom parameters if needed.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\n   Parameter Description Required Default     dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet   Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_ISI_SKIP_CERTIFICATE_VALIDATION Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty   X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISI_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    X_CSI_ISI_VOLUME_PATH_PERMISSIONS The permissions for isi volume directory path Yes 0777   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    Node parameters      X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes 0   X_CSI_MODE Driver starting mode No node      Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\n  Verify the CSI Driver installation\n  Note :\n “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  ","excerpt":"Installing CSI Driver for PowerScale via Dell CSM Operator The CSI …","ref":"/csm-docs/v2/deployment/csmoperator/drivers/powerscale/","title":"PowerScale"},{"body":"The CSI Driver for Dell EMC PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell EMC PowerScale:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements If enabling CSM for Authorization, please refer to the Authorization deployment steps first  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerScale.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\n(Optional) Volume Snapshot Requirements Applicable only if you decided to enable snapshot feature in values.yaml\ncontroller:snapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Manifests are available here:v4.2.x\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available here: v4.2.x\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and the default snapshot controller by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 4.2.x version of snapshotter/snapshot-controller.  Install the Driver Steps\n  Run git clone -b v2.1.0 https://github.com/dell/csi-powerscale.git to clone the git repository.\n  Ensure that you have created the namespace where you want to install the driver. You can run kubectl create namespace isilon to create a new one. The use of “isilon” as the namespace is just an example. You can choose any name for the namespace.\n  Collect information from the PowerScale Systems like IP address, IsiPath, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml.\n  Copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\n  Edit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     logLevel CSI driver log level No “debug”   certSecretCount Defines the number of certificate secrets, which the user is going to create for SSL authentication. (isilon-cert-0..isilon-cert-(n-1)); Minimum value should be 1. Yes 1   allowedNetworks Defines the list of networks that can be used for NFS I/O traffic, CIDR format must be used. No [ ]   maxIsilonVolumesPerNode Defines the default value for a maximum number of volumes that the controller can publish to the node. If the value is zero CO SHALL decide how many volumes of this type can be published by the controller to the node. This limit is applicable to all the nodes in the cluster for which node label ‘max-isilon-volumes-per-node’ is not set. Yes 0   imagePullPolicy Defines the policy to determine if the image should be pulled prior to starting the container Yes IfNotPresent   verbose Indicates what content of the OneFS REST API message should be logged in debug level logs Yes 1   kubeletConfigDir Specify kubelet config dir path Yes “/var/lib/kubelet”   enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish a connection to Array. This requires enableCustomTopology to be enabled. No false   controller Configure controller pod specific parameters     controllerCount Defines the number of csi-powerscale controller pods to deploy to the Kubernetes release Yes 2   volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s”   snapshot.enabled Enable/Disable volume snapshot feature Yes true   snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot”   resizer.enabled Enable/Disable volume expansion feature Yes true   healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume status, volume condition Yes false   healthMonitor.interval Interval of monitoring volume health condition Yes 60s   nodeSelector Define node selection constraints for pods of controller deployment No    tolerations Define tolerations for the controller deployment, if required No    node Configure node pod specific parameters     nodeSelector Define node selection constraints for pods of node daemonset No    tolerations Define tolerations for the node daemonset, if required No    dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet   healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition Yes false   PLATFORM ATTRIBUTES      endpointPort Define the HTTPs port number of the PowerScale OneFS API server. If authorization is enabled, endpointPort should be the HTTPS localhost port that the authorization sidecar will listen on. This value acts as a default value for endpointPort, if not specified for a cluster config in secret. No 8080   skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. This value acts as a default value for skipCertificateValidation, if not specified for a cluster config in secret. No true   isiAccessZone Define the name of the access zone a volume can be created in. If storageclass is missing with AccessZone parameter, then value of isiAccessZone is used for the same. No System   enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. No true   isiPath Define the base path for the volumes to be created on PowerScale cluster. This value acts as a default value for isiPath, if not specified for a cluster config in secret No /ifs/data/csi   noProbeOnStart Define whether the controller/node plugin should probe all the PowerScale clusters during driver initialization No false   autoProbe Specify if automatically probe the PowerScale cluster if not done already during CSI calls No true   authorization Authorization is an optional feature to apply credential shielding of the backend PowerScale. - -   enabled A boolean that enables/disables authorization feature. No false   sidecarProxyImage Image for csm-authorization-sidecar. No \" \"   proxyHost Hostname of the csm-authorization server. No Empty   skipCertificateValidation A boolean that enables/disables certificate validation of the csm-authorization server. No true    NOTE:\n ControllerCount parameter value must not exceed the number of nodes in the Kubernetes cluster. Otherwise, some of the controller pods remain in a “Pending” state till new nodes are available for scheduling. The installer exits with a WARNING on the same. Whenever the certSecretCount parameter changes in my-isilon-setting.yaml user needs to reinstall the driver. In order to enable authorization, there should be an authorization proxy server already installed.    Edit following parameters in samples/secret/secret.yaml file and update/add connection/authentication information for one or more PowerScale clusters.\n   Parameter Description Required Default     clusterName Logical name of PoweScale cluster against which volume CRUD operations are performed through this secret. Yes -   username username for connecting to PowerScale OneFS API server Yes -   password password for connecting to PowerScale OneFS API server Yes -   endpoint HTTPS endpoint of the PowerScale OneFS API server. If authorization is enabled, endpoint should be the HTTPS localhost endpoint that the authorization sidecar will listen on Yes -   isDefault Indicates if this is a default cluster (would be used by storage classes without ClusterName parameter). Only one of the cluster config should be marked as default. No false   Optional parameters Following parameters are Optional. If specified will override default values from values.yaml.     skipCertificateValidation Specify whether the PowerScale OneFS API server’s certificate chain and hostname must be verified. No default value from values.yaml   endpointPort Specify the HTTPs port number of the PowerScale OneFS API server No default value from values.yaml   isiPath The base path for the volumes to be created on PowerScale cluster. Note: IsiPath parameter in storageclass, if present will override this attribute. No default value from values.yaml    The username specified in secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\n   Privilege Type     ISI_PRIV_LOGIN_PAPI Read Only   ISI_PRIV_NFS Read Write   ISI_PRIV_QUOTA Read Write   ISI_PRIV_SNAPSHOT Read Write   ISI_PRIV_IFS_RESTORE Read Only   ISI_PRIV_NS_IFS_ACCESS Read Only   ISI_PRIV_IFS_BACKUP Read Only      Create isilon-creds secret using the following command:  kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\nNOTE:\n If any key/value is present in all my-isilon-settings.yaml, secret, and storageClass, then the values provided in storageClass parameters take precedence. The user has to validate the yaml syntax and array-related key/values while replacing or appending the isilon-creds secret. The driver will continue to use previous values in case of an error found in the yaml file. For the key isiIP/endpoint, the user can give either IP address or FQDN. Also, the user can prefix ‘https’ (For example, https://192.168.1.1) with the value. The isilon-creds secret has a mountEndpoint parameter which should only be updated and used when Authorization is enabled.   Install OneFS CA certificates by following the instructions from the next section, if you want to validate OneFS API server’s certificates. If not, create an empty secret using the following command and an empty secret must be created for the successful installation of CSI Driver for Dell EMC PowerScale.\nkubectl create -f empty-secret.yaml This command will create a new secret called isilon-certs-0 in isilon namespace.\n  Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026\u0026 ./csi-install.sh --namespace isilon --values ../helm/my-isilon-settings.yaml (assuming that the current working directory is ‘helm’ and my-isilon-settings.yaml is also present under ‘helm’ directory)\n  Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter ‘skipCertificateValidation’ which determines if the driver performs client-side verification of the OneFS certificates. The ‘skipCertificateValidation’ parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the ‘skipCertificateValidation’ is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the ‘skipCertificateValidation’ parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure  To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem To create the certs secret, run kubectl create secret generic isilon-certs-0 --from-file=cert-0=ca_cert_0.pem -n isilon Use the following command to replace the secret  kubectl create secret generic isilon-certs-0 -n isilon --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -  NOTES:\n The OneFS IP can be with or without a port, depends upon the configuration of OneFS API server. The commands are based on the namespace ‘isilon’ It is highly recommended that ca_cert.pem file(s) having the naming convention as ca_cert_number.pem (example: ca_cert_0, ca_cert_1), where this number starts from 0 and grows as the number of OneFS arrays grows. The cert secret created out of these pem files must have the naming convention as isilon-certs-number (example: isilon-certs-0, isilon-certs-1, and so on.); The number must start from zero and must grow in incremental order. The number of the secrets created out of pem files should match certSecretCount value in myvalues.yaml or my-isilon-settings.yaml.  Dynamic update of array details via secret.yaml CSI Driver for Dell EMC PowerScale now provides supports for Multi cluster. Now users can link the single CSI Driver to multiple OneFS Clusters by updating secret.yaml. Users can now update the isilon-creds secret by editing the secret.yaml and executing the following command\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl apply -f -\nNote: Updating isilon-certs-x secrets is a manual process, unlike isilon-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nStorage Classes The CSI driver for Dell EMC PowerScale version 1.5 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A sample storage class manifest is available at samples/storageclass/isilon.yaml. Use this sample manifest to create a storageclass to provision storage; uncomment/ update the manifest as per the requirements.\nWhat happens to my existing storage classes? Upgrading from CSI PowerScale v2.0 driver The storage classes created as part of the installation have an annotation - “helm.sh/resource-policy”: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nNOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Volume Snapshot Class Starting CSI PowerScale v1.6, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. Sample volume snapshot class manifests are available at samples/volumesnapshotclass/. Use these sample manifests to create a volumesnapshotclass for creating volume snapshots; uncomment/ update the manifests as per the requirements.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerScale v2.0 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerScale to 1.6 or higher before upgrading to 2.1.\n","excerpt":"The CSI Driver for Dell EMC PowerScale can be deployed by using the …","ref":"/csm-docs/v3/csidriver/installation/helm/isilon/","title":"PowerScale"},{"body":"Installing CSI Driver for PowerScale via Operator The CSI Driver for Dell EMC PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the one specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nNote: MKE (Mirantis Kubernetes Engine) does not support the installation of CSI-PowerScale via Operator.\nListing installed drivers with the CSI Isilon CRD User can query for CSI-PowerScale driver using the following command: kubectl get csiisilon --all-namespaces\nInstall Driver   Create namespace.\nExecute kubectl create namespace test-isilon to create the test-isilon namespace (if not already present). Note that the namespace can be any user-defined name, in this example, we assume that the namespace is ‘test-isilon’.\n  Create isilon-creds secret by using secret.yaml file format only.\n2.1 Create a yaml file called secret.yaml with the following content:\n isilonClusters: # logical name of PowerScale Cluster - clusterName: \"cluster1\" # username for connecting to PowerScale OneFS API server # Default value: None username: \"user\" # password for connecting to PowerScale OneFS API server password: \"password\" # HTTPS endpoint of the PowerScale OneFS API server # Default value: None # Examples: \"1.2.3.4\", \"https://1.2.3.4\", \"https://abc.myonefs.com\" endpoint: \"1.2.3.4\" # Is this a default cluster (would be used by storage classes without ClusterName parameter) # Allowed values: # true: mark this cluster config as default # false: mark this cluster config as not default # Default value: false isDefault: true # Specify whether the PowerScale OneFS API server's certificate chain and host name should be verified. # Allowed values: # true: skip OneFS API server's certificate verification # false: verify OneFS API server's certificates # Default value: default value specified in values.yaml # skipCertificateValidation: true # The base path for the volumes to be created on PowerScale cluster # This will be used if a storage class does not have the IsiPath parameter specified. # Ensure that this path exists on PowerScale cluster. # Allowed values: unix absolute path # Default value: default value specified in values.yaml # Examples: \"/ifs/data/csi\", \"/ifs/engineering\" # isiPath: \"/ifs/data/csi\" # The permissions for isi volume directory path # This will be used if a storage class does not have the IsiVolumePathPermissions parameter specified. # Allowed values: valid octal mode number # Default value: \"0777\" # Examples: \"0777\", \"777\", \"0755\" # isiVolumePathPermissions: \"0777\" - clusterName: \"cluster2\" username: \"user\" password: \"password\" endpoint: \"1.2.3.4\" endpointPort: \"8080\" Replace the values for the given keys as per your environment. After creating the secret.yaml, the following command can be used to create the secret,\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml\nUse the following command to replace or update the secret\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the isilon-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\n  Create isilon-certs-n secret. Please refer this section for creating cert-secrets.\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:isilon-certs-0namespace:isilontype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\n  Create a CR (Custom Resource) for PowerScale using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\n   Parameter Description Required Default     dnsPolicy Determines the DNS Policy of the Node service Yes ClusterFirstWithHostNet   Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_ISI_SKIP_CERTIFICATE_VALIDATION Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ALLOWED_NETWORKS Custom networks for PowerScale export. List of networks that can be used for NFS I/O traffic, CIDR format should be used No empty   X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISI_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    X_CSI_ISI_VOLUME_PATH_PERMISSIONS The permissions for isi volume directory path Yes 0777   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    Node parameters      X_CSI_MAX_VOLUMES_PER_NODE Specify the default value for the maximum number of volumes that the controller can publish to the node Yes 0   X_CSI_MODE Driver starting mode No node      Execute the following command to create PowerScale custom resource: kubectl create -f \u003cinput_sample_file.yaml\u003e . This command will deploy the CSI-PowerScale driver in the namespace specified in the input YAML file.\n  Note :\n From CSI-PowerScale v1.6.0 and higher, Storage class and VolumeSnapshotClass will not be created as part of driver deployment. The user has to create Storageclass and Volume Snapshot Class. Node selector and node tolerations can be added in both controller parameters and node parameters section, based on the need. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  ","excerpt":"Installing CSI Driver for PowerScale via Operator The CSI Driver for …","ref":"/csm-docs/v3/csidriver/installation/operator/isilon/","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\n  Creating a storage class:\nCreate a file storageclass.yaml using sample yaml file located at samples/storageclass/isilon.yaml. Update/uncomment the attributes in this sample file as per the requirements.\nExecute the following command to create a storage class:\nkubectl create -f $PWD/storageclass.yaml Result: After executing the above command storage class will be created in the default namespace, and the user can see the storage class by executing kubectl get sc.\n  Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at samples/persistentvolumeclaim/pvc.yaml\nExecute the following command to create volume:\nkubectl create -f $PWD/pvc.yaml\nResult: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing the command kubectl get pvc.\n  Note: The status of the volume can be either Bound or Pending depending on the VolumeBindingMode specified on the storage class.\n Attach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at samples/pod/.\nExecute the following command to mount the volume to the Kubernetes node:\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, a new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for the host to be part of clients/rootclients field of export created for volume and used by nginx application.\n  Create Snapshot\nVolumeSnapshotClass is needed for creating the volume snapshots. Starting from v1.6, CSI Driver for PowerScale will not create any default Volume Snapshot class.\nSo the user has to create a volume snapshot class. The required sample files are present under samples/volumesnapshotclass/. Choose the file based on Kubernetes version.\nExecute either one of the following commands to create a volume snapshot class.\nkubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1.yaml OR kubectl create -f samples/volumesnapshotclass/isilon-volumesnapshotclass-v1beta1.yaml\nThe above-said command will create a volume snapshotclass with the name isilon-snapclass.\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snapshot-of-test-pvc.yaml. The sample file for snapshot creation is located at samples/volumesnapshot/.\nExecute the following command to create snapshot:\nkubectl create -f samples/volumesnapshot/snapshot-of-test-pvc.yaml\nThe spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is test-pvc, then the created snapshot is named snapshot-of-test-pvc. Verify the PowerScale system for the newly created snapshot.\n  Note:\n User can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. User has to make sure that the IsiPath in the parameters section of the volume snapshot class is matching with a corresponding storage class.   Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in the spec dataSource field.\nThe sample file for volume creation from the snapshot is located at samples/persistentvolumeclaim/pvc-from-snapshot.yaml .\nExecute the following command to create a snapshot:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-snapshot.yaml Verify the PowerScale system for newly created volume from the snapshot.\n  Delete Snapshot\nExecute the following commands to delete the snapshot:\nkubectl get volumesnapshot kubectl delete volumesnapshot snapshot-of-test-pvc   Create a new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in the spec dataSource field.\nThe sample file for volume creation from volume is located at samples/persistentvolumeclaim/pvc-from-pvc.yaml\nExecute the following command to create a pvc from another pvc:\nkubectl create -f samples/persistentvolumeclaim/pvc-from-pvc.yaml Verify the PowerScale system for newly created volume from volume.\n  To Unattach the volume from Host\nDelete the nginx application to Unattach the volume from the host:\nkubectl delete -f nginx.yaml\n  To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc   ","excerpt":"This section provides multiple methods to test driver functionality in …","ref":"/csm-docs/v3/csidriver/installation/test/powerscale/","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v2.1.0 New Features/Changes  Added support for OpenShift v4.9. Added support for CSI spec 1.5. Added support for new access modes in CSI Spec 1.5. Added support for PV/PVC metrics. Added ability to accept leader election timeout flags. Added support for Dell EMC PowerScale 9.3. Added support for volume health monitoring.  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Resolution or workaround, if known     If the length of the nodeID exceeds 128 characters, the driver fails to update the CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn’t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in the CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future Kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581 Note: In kubernetes 1.22 this limit has been relaxed to 192 characters.   If some older NFS exports /terminated worker nodes still in NFS export client list, CSI driver tries to add a new worker node it fails (For RWX volume). User need to manually clean the export client list from old entries to make successful additon of new worker nodes.   Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100    Note:  Support for kurbernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode introduced in the release will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI Driver for PowerScale v2.1.0 New Features/Changes …","ref":"/csm-docs/v3/csidriver/release/powerscale/","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms Prevention, Resolution or Workaround     The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret’s username and password for corresponding cluster   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn’t verify the certificates Check the isilon-certs- secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \"true\" for insecure connection. SSL validation is recommended in the production environment.   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver error: create volume failed, Access denied. create directory as requested This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations.   Volume/filesystem is allowed to mount by any host in the network, though that host is not a part of the export of that particular volume under /ifs directory “Dell EMC PowerScale: OneFS NFS Design Considerations and Best Practices”: There is a default shared directory (ifs) of OneFS, which lets clients running Windows, UNIX, Linux, or Mac OS X access the same directories and files. It is recommended to disable the ifs shared directory in a production environment and create dedicated NFS exports and SMB shares for your workload.   Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class is not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.   While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.   If the hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to driver version 1.4.0 or later there is a possibility of “localhost” as a stale entry in export Recommended setup: User should not map a hostname to loopback IP in /etc/hosts file   CSI Driver installation fails with the error message “error getting FQDN”. Map IP address of host with its FQDN in /etc/hosts file.   Driver node pod is in “CrashLoopBackOff” as “Node ID” generated is not with proper FQDN. This might be due to “dnsPolicy” implemented on the driver node pod which may differ with different networks. This parameter is configurable in both helm and Operator installer and the user can try with different “dnsPolicy” according to the environment.    ","excerpt":"Here are some installation failures that might be encountered and how …","ref":"/csm-docs/v3/csidriver/troubleshooting/powerscale/","title":"PowerScale"},{"body":"The CSI Driver for Dell PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers (Optional) Kubernetes External Snapshotter, which provides snapshot support (Optional) Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell PowerStore Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell PowerStore:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI or NVMe/TCP protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator or Set up the NVMe/TCP Initiator sections below. You can use NFS volumes without FC or iSCSI or NVMe/TCP configuration.   You can use either the Fibre Channel or iSCSI or NVMe/TCP protocol, but you do not need all the three.\n  If you want to use preconfigured iSCSI/FC hosts be sure to check that they are not part of any host group\n  Linux native multipathing requirements Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements Nonsecure registries are defined in Docker or other container runtimes, for CSI drivers that are hosted in a non-secure location. You can access your cluster with kubectl and helm. Ensure that your nodes support mounting NFS volumes.  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerStore.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell PowerStore supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell PowerStore:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done.  Set up the iSCSI Initiator The CSI Driver for Dell PowerStore v1.4 and higher supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\n Ensure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi.  For information about configuring iSCSI, see Dell PowerStore documentation on Dell Support.\nSet up the NVMe Initiator If you want to use the protocol, set up the NVMe initiators as follows:\n The driver requires NVMe management command-line interface (nvme-cli) to use configure, edit, view or start the NVMe client and target. The nvme-cli utility provides a command-line and interactive shell option. The NVMe CLI tool is installed in the host using the below command. sudo apt install nvme-cli  Requirements for NVMeTCP\n Modules including the nvme, nvme_core, nvme_fabrics, and nvme_tcp are required for using NVMe over Fabrics using TCP. Load the NVMe and NVMe-OF Modules using the below commands:  modprobe nvme modprobe nvme_tcp Requirements for NVMeFC\n NVMeFC Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done.  NOTE:\n Do not load the nvme_tcp module for NVMeFC  Linux multipathing requirements Dell PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell PowerStore.\nSet up Linux multipathing as follows:\n Ensure that all nodes have the Device Mapper Multipathing package installed.   You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\n  Enable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes.  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable the snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Use v5.0.x for the installation.\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available: Use v5.0.x for the installation.\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via helm. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\ncontroller:healthMonitor:# enabled: Enable/Disable health monitor of CSI volumes# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: Noneenabled:false# healthMonitorInterval: Interval of monitoring volume health condition# Allowed values: Number followed by unit (s,m,h)# Examples: 60s, 5m, 1h# Default value: 60svolumeHealthMonitorInterval:60snode:healthMonitor:# enabled: Enable/Disable health monitor of CSI volumes- volume usage, volume condition# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: Noneenabled:falseInstallation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is installed along with the driver and does not involve any extra configuration.  (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication:enabled:trueReplication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\n  Run git clone -b v2.3.0 https://github.com/dell/csi-powerstore.git to clone the git repository.\n  Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one. “csi-powerstore” is just an example. You can choose any name for the namespace. But make sure to align to the same namespace during the whole installation.\n  Check helm/csi-powerstore/driver-image.yaml and confirm the driver image points to new image.\n  Edit samples/secret/secret.yaml file and configure connection information for your PowerStore arrays changing following parameters:\n endpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what transport protocol we should use (FC, ISCSI, NVMeTCP, NVMeFC, None, or auto). nasName: defines what NAS should be used for NFS volumes. nfsAcls (Optional): defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.  Add more blocks similar to above for each PowerStore array if necessary.\n  Create storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n If you do not specify arrayID parameter in the storage class then the array that was specified as the default would be used for provisioning volumes.\n   Create the secret by running kubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml\n  Copy the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml\n  Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:\n     Parameter Description Required Default     logLevel Defines CSI driver log level No “debug”   logFormat Defines CSI driver log format No “JSON”   externalAccess Defines additional entries for hostAccess of NFS volumes, single IP address and subnet are valid entries No \" \"   kubeletConfigDir Defines kubelet config path for cluster Yes “/var/lib/kubelet”   imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Yes “IfNotPresent”   nfsAcls Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777”   connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No False   controller.controllerCount Defines number of replicas of controller deployment Yes 2   controller.volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csivol”   controller.snapshot.enabled Allows to enable/disable snapshotter sidecar with driver installation for snapshot feature No “true”   controller.snapshot.snapNamePrefix Defines prefix to apply to the names of a created snapshots No “csisnap”   controller.resizer.enabled Allows to enable/disable resizer sidecar with driver installation for volume expansion feature No “true”   controller.healthMonitor.enabled Allows to enable/disable volume health monitor No false   controller.healthMonitor.volumeHealthMonitorInterval Interval of monitoring volume health condition No 60s   controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \"   controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \"   node.nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node”   node.nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id”   node.healthMonitor.enabled Allows to enable/disable volume health monitor No false   node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \"   node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \"   fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType”   controller.vgsnapshot.enabled To enable or disable the volume group snapshot feature No “true”    Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml  After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n csi-powerstore    NOTE:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using node.nodeNamePrefix and the ID read from the file pointed to by node.nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes The CSI driver for Dell PowerStore version 1.3 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class:\nThere are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\n Edit the sample storage class yaml file and update following parameters:   arrayID: specifies what storage cluster the driver should use, if not specified driver will use storage cluster specified as default in samples/secret/secret.yaml FsType: specifies what filesystem type driver should use, possible variants ext3, ext4, xfs, nfs, if not specified driver will use ext4 by default. nfsAcls (Optional): defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. allowedTopologies (Optional): If you want you can also add topology constraints.  allowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/12.34.56.78-iscsi# replace \"-iscsi\" with \"-fc\", \"-nvmetcp\" or \"-nvmefc\" or \"-nfs\" at the end to use FC, NVMeTCP, NVMeFC or NFS enabled hosts# replace \"12.34.56.78\" with PowerStore endpoint IPvalues:- \"true\"Create your storage class by using kubectl:  kubectl create -f \u003cpath_to_storageclass_file\u003e NOTE: Deleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerStore v1.4.0, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerStore v2.1.0 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerStore to 1.4.0 or higher, before upgrading to 2.3.0.\nDynamically update the powerstore secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\nkubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\n","excerpt":"The CSI Driver for Dell PowerStore can be deployed by using the …","ref":"/csm-docs/docs/csidriver/installation/helm/powerstore/","title":"PowerStore"},{"body":"Installing CSI Driver for PowerStore via Operator The CSI Driver for Dell PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note: The deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver   Create namespace:\nRun kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\n  Create PowerStore array connection config:\nCreate a file called config.yaml with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIglobalID:\"unique\"# unique id of the PowerStore arrayusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIskipCertificateValidation:true# indicates if client side validation of (management)server's certificate can be skippedisDefault:true# treat current array as a default (would be used by storage classes without arrayID parameter)blockProtocol:\"auto\"# what SCSI transport protocol use on node side (FC, ISCSI, NVMeTCP, NVMeFC, None, or auto)nasName:\"nas-server\"# what NAS should be used for NFS volumesnfsAcls:\"0777\"# (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory.# NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.Change the parameters with relevant values for your PowerStore array.\nAdd more blocks similar to above for each PowerStore array if necessary.\n  Create Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLCombine both files and create Kubernetes secret by running the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f -   Create a Custom Resource (CR) for PowerStore using the sample files provided here.\n  Below is a sample CR:\napiVersion:storage.dell.com/v1kind:CSIPowerStoremetadata:name:test-powerstorenamespace:test-powerstorespec:driver:configVersion:v2.3.0replicas:2dnsPolicy:ClusterFirstWithHostNetforceUpdate:falsefsGroupPolicy:ReadWriteOnceWithFSTypecommon:image:\"dellemc/csi-powerstore:v2.3.0\"imagePullPolicy:IfNotPresentenvs:- name:X_CSI_POWERSTORE_NODE_NAME_PREFIXvalue:\"csi\"- name:X_CSI_FC_PORTS_FILTER_FILE_PATHvalue:\"/etc/fc-ports-filter\"sideCars:- name:external-health-monitorargs:[\"--monitor-interval=60s\"]controller:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"- name:X_CSI_NFS_ACLSvalue:\"0777\"nodeSelector:node-role.kubernetes.io/master:\"\"tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"node:envs:- name:\"X_CSI_POWERSTORE_ENABLE_CHAP\"value:\"true\"- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"nodeSelector:node-role.kubernetes.io/worker:\"\"tolerations:- key:\"node-role.kubernetes.io/worker\"operator:\"Exists\"effect:\"NoSchedule\"---apiVersion:v1kind:ConfigMapmetadata:name:powerstore-config-paramsnamespace:test-powerstoredata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"debug\"CSI_LOG_FORMAT:\"JSON\"Users must configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:     Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be pending state till new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2   namespace Specifies namespace where the drive will be installed Yes “test-powerstore”   fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType”   Common parameters for node and controller      X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node”   X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter”   Controller parameters      X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No \" \"   X_CSI_NFS_ACLS Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777”   Node parameters      X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false   6. Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerStore driver.      - After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e       Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\nsideCars:# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin.# Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\".- name:external-health-monitorargs:[\"--monitor-interval=60s\"]controller:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin- volume status, volume condition.# Install the 'external-health-monitor' sidecar accordingly.# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"node:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin- volume usage, volume condition# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params Note :\n “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  ","excerpt":"Installing CSI Driver for PowerStore via Operator The CSI Driver for …","ref":"/csm-docs/docs/csidriver/installation/operator/powerstore/","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs, and nfs storage classes and automatically mounts them to the pod.\n It assumes that you’ve created the same basic three storage classes from samples/storageclass folder without changing their names. If you’ve created different storage classes please edit tests/simple/simple.yaml and change PersistentVolumeClaim definitions to point to correct storage classes.\n Steps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/   You can find all the created resources in testpowerstore namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/csm-docs/docs/csidriver/installation/test/powerstore/","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v2.3.0 New Features/Changes  Support Volume Group Snapshots. Removed beta volumesnapshotclass sample files. Support Configurable Volume Attributes. Added support for Kubernetes 1.24. Added support for OpenShift 4.10. Added support for NVMe/FC protocol.  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Resolution or workaround, if known     Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100    fsGroupPolicy may not work as expected without root privileges for NFS onlyhttps://github.com/kubernetes/examples/issues/260 To get the desired behavior set “allowRoot: “true” in the storage class parameter   If the NVMeFC pod is not getting created and the host looses the ssh connection, causing the driver pods to go to error state remove the nvme_tcp module from the host incase of NVMeFC connection   When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node.    Note:  Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI PowerStore v2.3.0 New Features/Changes  Support …","ref":"/csm-docs/docs/csidriver/release/powerstore/","title":"PowerStore"},{"body":"   Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs show that the driver can’t connect to PowerStore API. Check if you’ve created a secret with correct credentials   Installation of the driver on Kubernetes supported versions fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.21/v1.22/v1.23 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements   If PVC is not getting created and getting the following error in PVC description: failed to provision volume with StorageClass \"powerstore-iscsi\": rpc error: code = Internal desc = : Unknown error: Check if you’ve created a secret with correct credentials   If the NVMeFC pod is not getting created and the host looses the ssh connection, causing the driver pods to go to error state remove the nvme_tcp module from the host incase of NVMeFC connection   When a node goes down, the block volumes attached to the node cannot be attached to another node 1. Force delete the pod running on the node that went down 2. Delete the volumeattachment to the node that went down. Now the volume can be attached to the new node.   If the pod creation for NVMe takes time when the connections between the host and the array are more than 2 and considerable volumes are mounted on the host Reduce the number of connections between the host and the array to 2.    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/docs/csidriver/troubleshooting/powerstore/","title":"PowerStore"},{"body":"The CSI Driver for Dell PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers (Optional) Kubernetes External Snapshotter, which provides snapshot support (Optional) Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell PowerStore Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell PowerStore:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI or NVMe/TCP protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator or Set up the NVMe/TCP Initiator sections below. You can use NFS volumes without FC or iSCSI or NVMe/TCP configuration.   You can use either the Fibre Channel or iSCSI or NVMe/TCP protocol, but you do not need all the three.\n  If you want to use preconfigured iSCSI/FC hosts be sure to check that they are not part of any host group\n  Linux native multipathing requirements Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements Nonsecure registries are defined in Docker or other container runtimes, for CSI drivers that are hosted in a non-secure location. You can access your cluster with kubectl and helm. Ensure that your nodes support mounting NFS volumes.  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerStore.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell PowerStore supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell PowerStore:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done.  Set up the iSCSI Initiator The CSI Driver for Dell PowerStore v1.4 and higher supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\n Ensure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi.  For information about configuring iSCSI, see Dell PowerStore documentation on Dell Support.\nSet up the NVMe/TCP Initiator If you want to use the protocol, set up the NVMe/TCP initiators as follows:\n  The driver requires NVMe management command-line interface (nvme-cli) to use configure, edit, view or start the NVMe client and target. The nvme-cli utility provides a command-line and interactive shell option. The NVMe CLI tool is installed in the host using the below command. sudo apt install nvme-cli\n  Modules including the nvme, nvme_core, nvme_fabrics, and nvme_tcp are required for using NVMe over Fabrics using TCP. Load the NVMe and NVMe-OF Modules using the below commands:\n  modprobe nvme modprobe nvme_tcp Linux multipathing requirements Dell PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell PowerStore.\nSet up Linux multipathing as follows:\n Ensure that all nodes have the Device Mapper Multipathing package installed.   You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\n  Enable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes.  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable the snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Use v5.0.x for the installation.\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available: Use v5.0.x for the installation.\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via helm. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\ncontroller:healthMonitor:# enabled: Enable/Disable health monitor of CSI volumes# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: Noneenabled:false# healthMonitorInterval: Interval of monitoring volume health condition# Allowed values: Number followed by unit (s,m,h)# Examples: 60s, 5m, 1h# Default value: 60svolumeHealthMonitorInterval:60snode:healthMonitor:# enabled: Enable/Disable health monitor of CSI volumes- volume usage, volume condition# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: Noneenabled:falseInstallation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is installed along with the driver and does not involve any extra configuration.  (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication:enabled:trueReplication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\n  Run git clone -b v2.2.0 https://github.com/dell/csi-powerstore.git to clone the git repository.\n  Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one. “csi-powerstore” is just an example. You can choose any name for the namespace. But make sure to align to the same namespace during the whole installation.\n  Check helm/csi-powerstore/driver-image.yaml and confirm the driver image points to new image.\n  Edit samples/secret/secret.yaml file and configure connection information for your PowerStore arrays changing following parameters:\n endpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what transport protocol we should use (FC, ISCSI, NVMeTCP, None, or auto). nasName: defines what NAS should be used for NFS volumes. nfsAcls (Optional): defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.  Add more blocks similar to above for each PowerStore array if necessary.\n  Create storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n If you do not specify arrayID parameter in the storage class then the array that was specified as the default would be used for provisioning volumes.\n   Create the secret by running kubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml\n  Copy the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml\n  Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:\n     Parameter Description Required Default     logLevel Defines CSI driver log level No “debug”   logFormat Defines CSI driver log format No “JSON”   externalAccess Defines additional entries for hostAccess of NFS volumes, single IP address and subnet are valid entries No \" \"   kubeletConfigDir Defines kubelet config path for cluster Yes “/var/lib/kubelet”   imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Yes “IfNotPresent”   nfsAcls Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777”   connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No False   controller.controllerCount Defines number of replicas of controller deployment Yes 2   controller.volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csivol”   controller.snapshot.enabled Allows to enable/disable snapshotter sidecar with driver installation for snapshot feature No “true”   controller.snapshot.snapNamePrefix Defines prefix to apply to the names of a created snapshots No “csisnap”   controller.resizer.enabled Allows to enable/disable resizer sidecar with driver installation for volume expansion feature No “true”   controller.healthMonitor.enabled Allows to enable/disable volume health monitor No false   controller.healthMonitor.volumeHealthMonitorInterval Interval of monitoring volume health condition No 60s   controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \"   controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \"   node.nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node”   node.nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id”   node.healthMonitor.enabled Allows to enable/disable volume health monitor No false   node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \"   node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \"   fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType”    Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml  After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n csi-powerstore    NOTE:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using node.nodeNamePrefix and the ID read from the file pointed to by node.nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes The CSI driver for Dell PowerStore version 1.3 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class:\nThere are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\n Edit the sample storage class yaml file and update following parameters:   arrayID: specifies what storage cluster the driver should use, if not specified driver will use storage cluster specified as default in samples/secret/secret.yaml FsType: specifies what filesystem type driver should use, possible variants ext3, ext4, xfs, nfs, if not specified driver will use ext4 by default. nfsAcls (Optional): defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. allowedTopologies (Optional): If you want you can also add topology constraints.  allowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/12.34.56.78-iscsi# replace \"-iscsi\" with \"-fc\", \"-nvme\" or \"-nfs\" at the end to use FC, NVMe or NFS enabled hosts# replace \"12.34.56.78\" with PowerStore endpoint IPvalues:- \"true\"Create your storage class by using kubectl:  kubectl create -f \u003cpath_to_storageclass_file\u003e NOTE: Deleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerStore v1.4, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerStore v2.1 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerStore to 1.4 or higher, before upgrading to 2.2.\nDynamically update the powerstore secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\nkubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\n","excerpt":"The CSI Driver for Dell PowerStore can be deployed by using the …","ref":"/csm-docs/v1/csidriver/installation/helm/powerstore/","title":"PowerStore"},{"body":"Installing CSI Driver for PowerStore via Operator The CSI Driver for Dell PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note: The deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver   Create namespace:\nRun kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\n  Create PowerStore array connection config:\nCreate a file called config.yaml with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIglobalID:\"unique\"# unique id of the PowerStore arrayusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIskipCertificateValidation:true# indicates if client side validation of (management)server's certificate can be skippedisDefault:true# treat current array as a default (would be used by storage classes without arrayID parameter)blockProtocol:\"auto\"# what SCSI transport protocol use on node side (FC, ISCSI, NVMeTCP, None, or auto)nasName:\"nas-server\"# what NAS should be used for NFS volumesnfsAcls:\"0777\"# (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory.# NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.Change the parameters with relevant values for your PowerStore array.\nAdd more blocks similar to above for each PowerStore array if necessary.\n  Create Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLCombine both files and create Kubernetes secret by running the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f -   Create a Custom Resource (CR) for PowerStore using the sample files provided here.\n  Below is a sample CR:\napiVersion:storage.dell.com/v1kind:CSIPowerStoremetadata:name:test-powerstorenamespace:test-powerstorespec:driver:configVersion:v2.2.0replicas:2dnsPolicy:ClusterFirstWithHostNetforceUpdate:falsefsGroupPolicy:ReadWriteOnceWithFSTypecommon:image:\"dellemc/csi-powerstore:v2.2.0\"imagePullPolicy:IfNotPresentenvs:- name:X_CSI_POWERSTORE_NODE_NAME_PREFIXvalue:\"csi\"- name:X_CSI_FC_PORTS_FILTER_FILE_PATHvalue:\"/etc/fc-ports-filter\"sideCars:- name:external-health-monitorargs:[\"--monitor-interval=60s\"]controller:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"- name:X_CSI_NFS_ACLSvalue:\"0777\"nodeSelector:node-role.kubernetes.io/master:\"\"tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"node:envs:- name:\"X_CSI_POWERSTORE_ENABLE_CHAP\"value:\"true\"- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"nodeSelector:node-role.kubernetes.io/worker:\"\"tolerations:- key:\"node-role.kubernetes.io/worker\"operator:\"Exists\"effect:\"NoSchedule\"---apiVersion:v1kind:ConfigMapmetadata:name:powerstore-config-paramsnamespace:test-powerstoredata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"debug\"CSI_LOG_FORMAT:\"JSON\"Users must configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:     Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be pending state till new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2   namespace Specifies namespace where the drive will be installed Yes “test-powerstore”   fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType”   Common parameters for node and controller      X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node”   X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter”   Controller parameters      X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No \" \"   X_CSI_NFS_ACLS Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777”   Node parameters      X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false   6. Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerStore driver.      - After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e       Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\nsideCars:# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin.# Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\".- name:external-health-monitorargs:[\"--monitor-interval=60s\"]controller:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin- volume status, volume condition.# Install the 'external-health-monitor' sidecar accordingly.# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"node:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin- volume usage, volume condition# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"Dynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params Note :\n “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  ","excerpt":"Installing CSI Driver for PowerStore via Operator The CSI Driver for …","ref":"/csm-docs/v1/csidriver/installation/operator/powerstore/","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs, and nfs storage classes and automatically mounts them to the pod.\n It assumes that you’ve created the same basic three storage classes from samples/storageclass folder without changing their names. If you’ve created different storage classes please edit tests/simple/simple.yaml and change PersistentVolumeClaim definitions to point to correct storage classes.\n Steps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/   You can find all the created resources in testpowerstore namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/csm-docs/v1/csidriver/installation/test/powerstore/","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v2.2.0 New Features/Changes  Added support for NVMe/TCP protocol. Added support for Kubernetes 1.23. Added support to configure fsGroupPolicy. Added support for configuring permissions using POSIX mode bits and NFSv4 ACLs on NFS mount directory.  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Resolution or workaround, if known     Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100    fsGroupPolicy may not work as expected without root privileges for NFS onlyhttps://github.com/kubernetes/examples/issues/260 To get the desired behavior set “allowRoot: “true” in the storage class parameter    Note:  Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI PowerStore v2.2.0 New Features/Changes  Added …","ref":"/csm-docs/v1/csidriver/release/powerstore/","title":"PowerStore"},{"body":"   Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs show that the driver can’t connect to PowerStore API. Check if you’ve created a secret with correct credentials   Installation of the driver on Kubernetes supported versions fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.21/v1.22/v1.23 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements   If PVC is not getting created and getting the following error in PVC description: failed to provision volume with StorageClass \"powerstore-iscsi\": rpc error: code = Internal desc = : Unknown error: Check if you’ve created a secret with correct credentials    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/v1/csidriver/troubleshooting/powerstore/","title":"PowerStore"},{"body":"The CSI Driver for Dell PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers (Optional) Kubernetes External Snapshotter, which provides snapshot support (Optional) Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell PowerStore Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell PowerStore:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI or NVMe/TCP protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator or Set up the NVMe/TCP Initiator sections below. You can use NFS volumes without FC or iSCSI or NVMe/TCP configuration.   You can use either the Fibre Channel or iSCSI or NVMe/TCP protocol, but you do not need all the three.\n  If you want to use preconfigured iSCSI/FC hosts be sure to check that they are not part of any host group\n  Linux native multipathing requirements Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements Nonsecure registries are defined in Docker or other container runtimes, for CSI drivers that are hosted in a non-secure location. You can access your cluster with kubectl and helm. Ensure that your nodes support mounting NFS volumes.  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell PowerStore.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell PowerStore supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell PowerStore:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done.  Set up the iSCSI Initiator The CSI Driver for Dell PowerStore v1.4 and higher supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\n Ensure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi.  For information about configuring iSCSI, see Dell PowerStore documentation on Dell Support.\nSet up the NVMe/TCP Initiator If you want to use the protocol, set up the NVMe/TCP initiators as follows:\n  The driver requires NVMe management command-line interface (nvme-cli) to use configure, edit, view or start the NVMe client and target. The nvme-cli utility provides a command-line and interactive shell option. The NVMe CLI tool is installed in the host using the below command. sudo apt install nvme-cli\n  Modules including the nvme, nvme_core, nvme_fabrics, and nvme_tcp are required for using NVMe over Fabrics using TCP. Load the NVMe and NVMe-OF Modules using the below commands:\n  modprobe nvme modprobe nvme_tcp Linux multipathing requirements Dell PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell PowerStore.\nSet up Linux multipathing as follows:\n Ensure that all nodes have the Device Mapper Multipathing package installed.   You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\n  Enable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes.  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable the snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Use v5.0.x for the installation.\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available: Use v5.0.x for the installation.\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via helm. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\ncontroller:healthMonitor:# enabled: Enable/Disable health monitor of CSI volumes# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: Noneenabled:false# healthMonitorInterval: Interval of monitoring volume health condition# Allowed values: Number followed by unit (s,m,h)# Examples: 60s, 5m, 1h# Default value: 60svolumeHealthMonitorInterval:60snode:healthMonitor:# enabled: Enable/Disable health monitor of CSI volumes- volume usage, volume condition# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: Noneenabled:falseInstallation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - NOTE:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is installed along with the driver and does not involve any extra configuration.  (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication:enabled:trueReplication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\n  Run git clone -b v2.2.0 https://github.com/dell/csi-powerstore.git to clone the git repository.\n  Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one. “csi-powerstore” is just an example. You can choose any name for the namespace. But make sure to align to the same namespace during the whole installation.\n  Check helm/csi-powerstore/driver-image.yaml and confirm the driver image points to new image.\n  Edit samples/secret/secret.yaml file and configure connection information for your PowerStore arrays changing following parameters:\n endpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what transport protocol we should use (FC, ISCSI, NVMeTCP, None, or auto). nasName: defines what NAS should be used for NFS volumes. nfsAcls (Optional): defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.  Add more blocks similar to above for each PowerStore array if necessary.\n  Create storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n If you do not specify arrayID parameter in the storage class then the array that was specified as the default would be used for provisioning volumes.\n   Create the secret by running kubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml\n  Copy the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml\n  Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:\n     Parameter Description Required Default     logLevel Defines CSI driver log level No “debug”   logFormat Defines CSI driver log format No “JSON”   externalAccess Defines additional entries for hostAccess of NFS volumes, single IP address and subnet are valid entries No \" \"   kubeletConfigDir Defines kubelet config path for cluster Yes “/var/lib/kubelet”   imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Yes “IfNotPresent”   nfsAcls Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777”   connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No False   controller.controllerCount Defines number of replicas of controller deployment Yes 2   controller.volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csivol”   controller.snapshot.enabled Allows to enable/disable snapshotter sidecar with driver installation for snapshot feature No “true”   controller.snapshot.snapNamePrefix Defines prefix to apply to the names of a created snapshots No “csisnap”   controller.resizer.enabled Allows to enable/disable resizer sidecar with driver installation for volume expansion feature No “true”   controller.healthMonitor.enabled Allows to enable/disable volume health monitor No false   controller.healthMonitor.volumeHealthMonitorInterval Interval of monitoring volume health condition No 60s   controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \"   controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \"   node.nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node”   node.nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id”   node.healthMonitor.enabled Allows to enable/disable volume health monitor No false   node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \"   node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \"   fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType”    Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml  After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n csi-powerstore    NOTE:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using node.nodeNamePrefix and the ID read from the file pointed to by node.nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes The CSI driver for Dell PowerStore version 1.3 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class:\nThere are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\n Edit the sample storage class yaml file and update following parameters:   arrayID: specifies what storage cluster the driver should use, if not specified driver will use storage cluster specified as default in samples/secret/secret.yaml FsType: specifies what filesystem type driver should use, possible variants ext3, ext4, xfs, nfs, if not specified driver will use ext4 by default. nfsAcls (Optional): defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. allowedTopologies (Optional): If you want you can also add topology constraints.  allowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/12.34.56.78-iscsi# replace \"-iscsi\" with \"-fc\", \"-nvme\" or \"-nfs\" at the end to use FC, NVMe or NFS enabled hosts# replace \"12.34.56.78\" with PowerStore endpoint IPvalues:- \"true\"Create your storage class by using kubectl:  kubectl create -f \u003cpath_to_storageclass_file\u003e NOTE: Deleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerStore v1.4, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerStore v2.1 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerStore to 1.4 or higher, before upgrading to 2.2.\nDynamically update the powerstore secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\nkubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\n","excerpt":"The CSI Driver for Dell PowerStore can be deployed by using the …","ref":"/csm-docs/v2/csidriver/installation/helm/powerstore/","title":"PowerStore"},{"body":"Installing CSI Driver for PowerStore via Operator The CSI Driver for Dell PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note: The deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver   Create namespace:\nRun kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\n  Create PowerStore array connection config:\nCreate a file called config.yaml with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIglobalID:\"unique\"# unique id of the PowerStore arrayusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIskipCertificateValidation:true# indicates if client side validation of (management)server's certificate can be skippedisDefault:true# treat current array as a default (would be used by storage classes without arrayID parameter)blockProtocol:\"auto\"# what SCSI transport protocol use on node side (FC, ISCSI, NVMeTCP, None, or auto)nasName:\"nas-server\"# what NAS should be used for NFS volumesnfsAcls:\"0777\"# (Optional) defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory.# NFSv4 ACls are supported for NFSv4 shares on NFSv4 enabled NAS servers only. POSIX ACLs are not supported and only POSIX mode bits are supported for NFSv3 shares.Change the parameters with relevant values for your PowerStore array.\nAdd more blocks similar to above for each PowerStore array if necessary.\n  Create Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLCombine both files and create Kubernetes secret by running the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f -   Create a Custom Resource (CR) for PowerStore using the sample files provided here.\n  Below is a sample CR:\napiVersion:storage.dell.com/v1kind:CSIPowerStoremetadata:name:test-powerstorenamespace:test-powerstorespec:driver:configVersion:v2.2.0replicas:2dnsPolicy:ClusterFirstWithHostNetforceUpdate:falsefsGroupPolicy:ReadWriteOnceWithFSTypecommon:image:\"dellemc/csi-powerstore:v2.2.0\"imagePullPolicy:IfNotPresentenvs:- name:X_CSI_POWERSTORE_NODE_NAME_PREFIXvalue:\"csi\"- name:X_CSI_FC_PORTS_FILTER_FILE_PATHvalue:\"/etc/fc-ports-filter\"sideCars:- name:external-health-monitorargs:[\"--monitor-interval=60s\"]controller:envs:- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"- name:X_CSI_NFS_ACLSvalue:\"0777\"nodeSelector:node-role.kubernetes.io/master:\"\"tolerations:- key:\"node-role.kubernetes.io/master\"operator:\"Exists\"effect:\"NoSchedule\"node:envs:- name:\"X_CSI_POWERSTORE_ENABLE_CHAP\"value:\"true\"- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"nodeSelector:node-role.kubernetes.io/worker:\"\"tolerations:- key:\"node-role.kubernetes.io/worker\"operator:\"Exists\"effect:\"NoSchedule\"---apiVersion:v1kind:ConfigMapmetadata:name:powerstore-config-paramsnamespace:test-powerstoredata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"debug\"CSI_LOG_FORMAT:\"JSON\"Users must configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:     Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be pending state till new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2   namespace Specifies namespace where the drive will be installed Yes “test-powerstore”   fsGroupPolicy Defines which FS Group policy mode to be used, Supported modes None, File and ReadWriteOnceWithFSType No “ReadWriteOnceWithFSType”   Common parameters for node and controller      X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node”   X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter”   Controller parameters      X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No \" \"   X_CSI_NFS_ACLS Defines permissions - POSIX mode bits or NFSv4 ACLs, to be set on NFS target mount directory. No “0777”   Node parameters      X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false   6. Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerStore driver.      - After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e       Volume Health Monitoring Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\nsideCars:# Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin.# Also set the env variable controller.envs.X_CSI_HEALTH_MONITOR_ENABLED to \"true\".- name:external-health-monitorargs:[\"--monitor-interval=60s\"]controller:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin- volume status, volume condition.# Install the 'external-health-monitor' sidecar accordingly.# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"node:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin- volume usage, volume condition# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"Dynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params Note :\n “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  ","excerpt":"Installing CSI Driver for PowerStore via Operator The CSI Driver for …","ref":"/csm-docs/v2/csidriver/installation/operator/powerstore/","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs, and nfs storage classes and automatically mounts them to the pod.\n It assumes that you’ve created the same basic three storage classes from samples/storageclass folder without changing their names. If you’ve created different storage classes please edit tests/simple/simple.yaml and change PersistentVolumeClaim definitions to point to correct storage classes.\n Steps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/   You can find all the created resources in testpowerstore namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/csm-docs/v2/csidriver/installation/test/powerstore/","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v2.2.0 New Features/Changes  Added support for NVMe/TCP protocol. Added support for Kubernetes 1.23. Added support to configure fsGroupPolicy. Added support for configuring permissions using POSIX mode bits and NFSv4 ACLs on NFS mount directory.  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Resolution or workaround, if known     Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100    fsGroupPolicy may not work as expected without root privileges for NFS onlyhttps://github.com/kubernetes/examples/issues/260 To get the desired behavior set “allowRoot: “true” in the storage class parameter    Note:  Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI PowerStore v2.2.0 New Features/Changes  Added …","ref":"/csm-docs/v2/csidriver/release/powerstore/","title":"PowerStore"},{"body":"   Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs show that the driver can’t connect to PowerStore API. Check if you’ve created a secret with correct credentials   Installation of the driver on Kubernetes supported versions fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.21/v1.22/v1.23 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements   If PVC is not getting created and getting the following error in PVC description: failed to provision volume with StorageClass \"powerstore-iscsi\": rpc error: code = Internal desc = : Unknown error: Check if you’ve created a secret with correct credentials    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/v2/csidriver/troubleshooting/powerstore/","title":"PowerStore"},{"body":"The CSI Driver for Dell EMC PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the specified namespace:\n CSI Driver for Dell EMC PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers (Optional) Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the specified namespace:\n CSI Driver for Dell EMC PowerStore Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell EMC PowerStore:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator sections below. You can use NFS volumes without FC or iSCSI configuration.   You can use either the Fibre Channel or iSCSI protocol, but you do not need both.\n  If you want to use preconfigured iSCSI/FC hosts be sure to check that they are not part of any host group\n  Linux native multipathing requirements Mount propagation is enabled on container runtime that is being used If using Snapshot feature, satisfy all Volume Snapshot requirements Nonsecure registries are defined in Docker or other container runtimes, for CSI drivers that are hosted in a non-secure location. You can access your cluster with kubectl and helm. Ensure that your nodes support mounting NFS volumes.  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerStore.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell EMC PowerStore supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell EMC PowerStore:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done.  Set up the iSCSI Initiator The CSI Driver for Dell EMC PowerStore v1.4 and higher supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\n Ensure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell EMC PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi.  For information about configuring iSCSI, see Dell EMC PowerStore documentation on Dell EMC Support.\nLinux multipathing requirements Dell EMC PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell EMC PowerStore.\nSet up Linux multipathing as follows:\n Ensure that all nodes have the Device Mapper Multipathing package installed.   You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\n  Enable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes.  (Optional) Volume Snapshot Requirements Applicable only if you decided to enable the snapshot feature in values.yaml\nsnapshot:enabled:trueVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Use v4.2.x for the installation.\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available: Use v4.2.x for the installation.\nNOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use 4.2.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is installed along with the driver and does not involve any extra configuration.  (Optional) Replication feature Requirements Applicable only if you decided to enable the Replication feature in values.yaml\nreplication:enabled:trueReplication CRD’s The CRDs for replication can be obtained and installed from the csm-replication project on Github. Use csm-replication/deploy/replicationcrds.all.yaml located in csm-replication git repo for the installation.\nCRDs should be configured during replication prepare stage with repctl as described in install-repctl\nInstall the Driver Steps\n  Run git clone -b v2.1.0 https://github.com/dell/csi-powerstore.git to clone the git repository.\n  Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one. “csi-powerstore” is just an example. You can choose any name for the namespace. But make sure to align to the same namespace during the whole installation.\n  Check helm/csi-powerstore/driver-image.yaml and confirm the driver image points to new image.\n  Edit samples/secret/secret.yaml file and configure connection information for your PowerStore arrays changing following parameters:\n endpoint: defines the full URL path to the PowerStore API. globalID: specifies what storage cluster the driver should use username, password: defines credentials for connecting to array. skipCertificateValidation: defines if we should use insecure connection or not. isDefault: defines if we should treat the current array as a default. blockProtocol: defines what SCSI transport protocol we should use (FC, ISCSI, None, or auto). nasName: defines what NAS should be used for NFS volumes.  Add more blocks similar to above for each PowerStore array if necessary.\n  Create storage classes using ones from samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u003cpath_to_storageclass_file\u003e\n If you do not specify arrayID parameter in the storage class then the array that was specified as the default would be used for provisioning volumes.\n   Create the secret by running kubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml\n  Copy the default values.yaml file cd dell-csi-helm-installer \u0026\u0026 cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml\n  Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:\n     Parameter Description Required Default     logLevel Defines CSI driver log level No “debug”   logFormat Defines CSI driver log format No “JSON”   externalAccess Defines additional entries for hostAccess of NFS volumes, single IP address and subnet are valid entries No \" \"   kubeletConfigDir Defines kubelet config path for cluster Yes “/var/lib/kubelet”   imagePullPolicy Policy to determine if the image should be pulled prior to starting the container. Yes “IfNotPresent”   connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No False   controller.controllerCount Defines number of replicas of controller deployment Yes 2   controller.volumeNamePrefix Defines the string added to each volume that the CSI driver creates No “csivol”   controller.snapshot.enabled Allows to enable/disable snapshotter sidecar with driver installation for snapshot feature No “true”   controller.snapshot.snapNamePrefix Defines prefix to apply to the names of a created snapshots No “csisnap”   controller.resizer.enabled Allows to enable/disable resizer sidecar with driver installation for volume expansion feature No “true”   controller.healthMonitor.enabled Allows to enable/disable volume health monitor No false   controller.healthMonitor.volumeHealthMonitorInterval Interval of monitoring volume health condition No 60s   controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \" \"   controller.tolerations Defines tolerations that would be applied to controller deployment Yes \" \"   node.nodeNamePrefix Defines the string added to each node that the CSI driver registers No “csi-node”   node.nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No “/etc/machine-id”   node.healthMonitor.enabled Allows to enable/disable volume health monitor No false   node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \" \"   node.tolerations Defines tolerations that would be applied to node daemonset Yes \" \"    Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml  After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n csi-powerstore    NOTE:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using node.nodeNamePrefix and the ID read from the file pointed to by node.nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. A hostname the driver uses for registration of adapters is in the form \u003cnodeNamePrefix\u003e-\u003cnodeID\u003e-\u003cnodeIP\u003e. By default, these are csi-node and the machine ID read from the file /etc/machine-id. To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment’s requirements for the specified option.    Storage Classes The CSI driver for Dell EMC PowerStore version 1.3 and later, dell-csi-helm-installer does not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class:\nThere are samples storage class yaml files available under samples/storageclass. These can be copied and modified as needed.\n Edit the sample storage class yaml file and update following parameters:   arrayID: specifies what storage cluster the driver should use, if not specified driver will use storage cluster specified as default in samples/secret/secret.yaml FsType: specifies what filesystem type driver should use, possible variants ext4, xfs, nfs, if not specified driver will use ext4 by default. allowedTopologies (Optional): If you want you can also add topology constraints.  allowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/12.34.56.78-iscsi# replace \"-iscsi\" with \"-fc\" or \"-nfs\" at the end to use FC or NFS enabled hosts# replace \"12.34.56.78\" with PowerStore endpoint IPvalues:- \"true\"Create your storage class by using kubectl:  kubectl create -f \u003cpath_to_storageclass_file\u003e NOTE: Deleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nVolume Snapshot Class Starting CSI PowerStore v1.4, dell-csi-helm-installer will not create any Volume Snapshot Class during the driver installation. There is a sample Volume Snapshot Class manifest present in the samples/volumesnapshotclass folder. Please use this sample to create a new Volume Snapshot Class to create Volume Snapshots.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI PowerStore v2.0 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI PowerStore to 1.4 or higher, before upgrading to 2.1.\nDynamically update the powerstore secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\nkubectl create secret generic powerstore-config -n csi-powerstore --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Dynamic Logging Configuration This feature is introduced in CSI Driver for PowerStore version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created, which contains attributes CSI_LOG_LEVEL which specifies the current log level of CSI driver and CSI_LOG_FORMAT which specifies the current log format of CSI driver.\nUsers can set the default log level by specifying log level to logLevel and log format to logFormat attribute in my-powerstore-settings.yaml during driver installation.\nTo change the log level or log format dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade Note: here my-powerstore-settings.yaml is a values.yaml file which user has used for driver installation.\n","excerpt":"The CSI Driver for Dell EMC PowerStore can be deployed by using the …","ref":"/csm-docs/v3/csidriver/installation/helm/powerstore/","title":"PowerStore"},{"body":"Installing CSI Driver for PowerStore via Operator The CSI Driver for Dell EMC PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note: The deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage the entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver   Create namespace:\nRun kubectl create namespace \u003cdriver-namespace\u003e using the desired name to create the namespace.\n  Create PowerStore array connection config:\nCreate a file called config.yaml with the following content\narrays:- endpoint:\"https://10.0.0.1/api/rest\"# full URL path to the PowerStore APIglobalID:\"unique\"# unique id of the PowerStore arrayusername:\"user\"# username for connecting to APIpassword:\"password\"# password for connecting to APIskipCertificateValidation:true# indicates if client side validation of (management)server's certificate can be skippedisDefault:true# treat current array as a default (would be used by storage classes without arrayID parameter)blockProtocol:\"auto\"# what SCSI transport protocol use on node side (FC, ISCSI, None, or auto)nasName:\"nas-server\"# what NAS should be used for NFS volumesChange the parameters with relevant values for your PowerStore array.\nAdd more blocks similar to above for each PowerStore array if necessary.\n  Create Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u003cdriver-namespace\u003etype:Opaquedata:config:CONFIG_YAMLCombine both files and create Kubernetes secret by running the following command:\nsed \"s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\" secret.yaml | kubectl apply -f -   Create a Custom Resource (CR) for PowerStore using the sample files provided here.\n  Users must configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:\n   Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be pending state till new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2   namespace Specifies namespace where the drive will be installed Yes “test-powerstore”   Common parameters for node and controller      X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes “csi-node”   X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No “/etc/fc-ports-filter”   Controller parameters      X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No \" \"   Node parameters      X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false      Execute the following command to create PowerStore custom resource:kubectl create -f \u003cinput_sample_file.yaml\u003e. The above command will deploy the CSI-PowerStore driver.\n After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u003cdriver-namespace\u003e    Dynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name powerstore-config-params is created using the manifest located in the sample file. This ConfigMap contains attributes CSI_LOG_LEVEL which specifies the current log level of the CSI driver and CSI_LOG_FORMAT which specifies the current log format of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap powerstore-config-params and update CSI_LOG_LEVEL to the desired log level and CSI_LOG_FORMAT to the desired log format.\nkubectl edit configmap -n csi-powerstore powerstore-config-params Note :\n “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  ","excerpt":"Installing CSI Driver for PowerStore via Operator The CSI Driver for …","ref":"/csm-docs/v3/csidriver/installation/operator/powerstore/","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs, and nfs storage classes and automatically mounts them to the pod.\n It assumes that you’ve created the same basic three storage classes from samples/storageclass folder without changing their names. If you’ve created different storage classes please edit tests/simple/simple.yaml and change PersistentVolumeClaim definitions to point to correct storage classes.\n Steps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/   You can find all the created resources in testpowerstore namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it’s in CrashLoopback state then the driver installation wasn’t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/csm-docs/v3/csidriver/installation/test/powerstore/","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v2.1.0 New Features/Changes  Added support for OpenShift v4.9. Added support for CSI spec 1.5. Added support for new access modes in CSI Spec 1.5. Added support for PV/PVC metrics. Added support for volume health monitoring.  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Resolution or workaround, if known     Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100     Note:  Support for kurbernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode introduced in the release will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI PowerStore v2.1.0 New Features/Changes  Added …","ref":"/csm-docs/v3/csidriver/release/powerstore/","title":"PowerStore"},{"body":"   Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods powerstore-controller-\u003csuffix\u003e –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \"insecure-registries\" :[ \"hostname.cloudapp.net:5000\" ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n csi-powerstore powerstore-node-\u003csuffix\u003e driver logs show that the driver can’t connect to PowerStore API. Check if you’ve created a secret with correct credentials   Installation of the driver on Kubernetes supported versions fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \"\": no matches for kind \"VolumeSnapshotClass\" in version \"snapshot.storage.k8s.io/v1\" Kubernetes v1.20/1.21/v1.22 requires v1 version of snapshot CRDs to be created in cluster, see the Volume Snapshot Requirements   If PVC is not getting created and getting the following error in PVC description: failed to provision volume with StorageClass \"powerstore-iscsi\": rpc error: code = Internal desc = : Unknown error: Check if you’ve created a secret with correct credentials    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/v3/csidriver/troubleshooting/powerstore/","title":"PowerStore"},{"body":"Getting started with Kubernetes on Dell Storage   Dell CSI Operator deployment in OpenShift   ","excerpt":"Getting started with Kubernetes on Dell Storage   Dell CSI Operator …","ref":"/csm-docs/docs/grasp/video/","title":"Quick video lessons"},{"body":"Getting started with Kubernetes on Dell Storage   Dell CSI Operator deployment in OpenShift   ","excerpt":"Getting started with Kubernetes on Dell Storage   Dell CSI Operator …","ref":"/csm-docs/v1/grasp/video/","title":"Quick video lessons"},{"body":"Getting started with Kubernetes on Dell Storage   Dell CSI Operator deployment in OpenShift   ","excerpt":"Getting started with Kubernetes on Dell Storage   Dell CSI Operator …","ref":"/csm-docs/v2/grasp/video/","title":"Quick video lessons"},{"body":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI Operator deployment in OpenShift   ","excerpt":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI …","ref":"/csm-docs/v3/grasp/video/","title":"Quick video lessons"},{"body":"The CSM Replication module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. Dell CSM Operator will deploy CSM Replication sidecar and the complemental CSM Replication controller manager.\nPrerequisite To use Replication, you need at least two clusters:\n a source cluster which is the main cluster one or more target clusters which will serve as diaster recovery clusters for the main cluster  To configure all the clusters, follow the steps below:\n On your main cluster, follow the instructions available in CSM Replication for Installation using repctl. NOTE: On step 4 of the link above, you MUST use the command below to automatically package all clusters’ .kube config as a secret:  ./repctl cluster inject CSM Operator needs this admin configs instead of the service accounts’ configs to be able to properly manage the target clusters. The default service account that’ll be used is the CSM Operator service account.\nOn each of the target clusters, configure the prerequisites for deploying the driver via Dell CSM Operator. For example, PowerScale has the following prerequisites for deploying PowerScale via Dell CSM Operator  ","excerpt":"The CSM Replication module for supported Dell CSI Drivers can be …","ref":"/csm-docs/docs/deployment/csmoperator/modules/replication/","title":"Replication"},{"body":"The CSM Replication module for supported Dell CSI Drivers can be installed via the Dell CSM Operator. Dell CSM Operator will deploy CSM Replication sidecar and the complemental CSM Replication controller manager.\nPrerequisite To use Replication, you need at least two clusters:\n a source cluster which is the main cluster one or more target clusters which will serve as diaster recovery clusters for the main cluster  To configure all the clusters, follow the steps below:\n On your main cluster, follow the instructions available in CSM Replication for Installation using repctl. NOTE: On step 4 of the link above, you MUST use the command below to automatically package all clusters’ .kube config as a secret:  ./repctl cluster inject CSM Operator needs this admin configs instead of the service accounts’ configs to be able to properly manage the target clusters. The default service account that’ll be used is the CSM Operator service account.\nOn each of the target clusters, configure the prerequisites for deploying the driver via Dell CSM Operator. For example, PowerScale has the following prerequisites for deploying PowerScale via Dell CSM Operator  ","excerpt":"The CSM Replication module for supported Dell CSI Drivers can be …","ref":"/csm-docs/v1/deployment/csmoperator/modules/replication/","title":"Replication"},{"body":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE) v1.2.8.\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters. Installation on this cluster is done using helm and via Operator has not been qualified.\nRKE Examples ","excerpt":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE) v1.2.8. …","ref":"/csm-docs/docs/csidriver/partners/rancher/","title":"RKE"},{"body":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE) v1.2.8.\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters. Installation on this cluster is done using helm and via Operator has not been qualified.\nRKE Examples ","excerpt":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE) v1.2.8. …","ref":"/csm-docs/v1/csidriver/partners/rancher/","title":"RKE"},{"body":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE) v1.2.8.\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters. Installation on this cluster is done using helm and via Operator has not been qualified.\nRKE Examples ","excerpt":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE) v1.2.8. …","ref":"/csm-docs/v2/csidriver/partners/rancher/","title":"RKE"},{"body":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE) v1.2.8.\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters. Installation on this cluster is done using helm and via Operator has not been qualified.\nRKE Examples ","excerpt":"The Dell CSI Drivers support Rancher Kubernetes Engine (RKE) v1.2.8. …","ref":"/csm-docs/v3/csidriver/partners/rancher/","title":"RKE"},{"body":"","excerpt":"","ref":"/csm-docs/search/","title":"Search Results"},{"body":"","excerpt":"","ref":"/csm-docs/docs/csidriver/installation/test/","title":"Testing Drivers"},{"body":"","excerpt":"","ref":"/csm-docs/v1/csidriver/installation/test/","title":"Testing Drivers"},{"body":"","excerpt":"","ref":"/csm-docs/v2/csidriver/installation/test/","title":"Testing Drivers"},{"body":"","excerpt":"","ref":"/csm-docs/v3/csidriver/installation/test/","title":"Testing Drivers"},{"body":"The CSI Driver for Dell Unity can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for Unity Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume Kubernetes External Health Monitor, which provides volume health status  The node section of the Helm chart installs the following component in a DaemonSet:\n CSI Driver for Unity Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for Unity, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes or OpenShift (see supported versions) Install Helm v3 To use FC protocol, the host must be zoned with Unity array and Multipath needs to be configured To use iSCSI protocol, iSCSI initiator utils packages needs to be installed and Multipath needs to be configured To use NFS protocol, NFS utility packages needs to be installed Mount propagation is enabled on container runtime that is being used  Install CSI Driver Install CSI Driver for Unity using this procedure.\nBefore you begin\n You must have the downloaded files, including the Helm chart from the source git repository with the command git clone -b v2.2.0 https://github.com/dell/csi-unity.git, as a pre-requisite for running this procedure. In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. Ensure unity namespace exists in Kubernetes cluster. Use the kubectl create namespace unity command to create the namespace if the namespace is not present.  Procedure\n  Collect information from the Unity Systems like Unique ArrayId, IP address, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml and myvalues.yaml file.\nNote:\n ArrayId corresponds to the serial number of Unity array. Unity Array username must have role as Storage Administrator to be able to perform CRUD operations. If the user is using complex K8s version like “v1.21.3-mirantis-1”, use below kubeVersion check in helm/csi-unity/Chart.yaml file. kubeVersion: “\u003e= 1.21.0-0 \u003c 1.24.0-0”    Copy the helm/csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\n  Edit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     version helm version true -   logLevel LogLevel is used to set the logging level of the driver true info   allowRWOMultiPodAccess Flag to enable multiple pods to use the same PVC on the same node with RWO access mode. false false   kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet   syncNodeInfoInterval Time interval to add node info to the array. Default 15 minutes. The minimum value should be 1 minute. false 15   maxUnityVolumesPerNode Maximum number of volumes that controller can publish to the node. false 0   certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. (unity-cert-0..unity-cert-n). The minimum value should be 1. false 1   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent   podmon.enabled service to monitor failing jobs and notify false -   podmon.image pod man image name false -   tenantName Tenant name added while adding host entry to the array No    controller Allows configuration of the controller-specific parameters. - -   controllerCount Defines the number of csi-unity controller pods to deploy to the Kubernetes release Yes 2   volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s”   snapshot.enabled Enable/Disable volume snapshot feature Yes true   snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot”   resizer.enabled Enable/Disable volume expansion feature Yes true   nodeSelector Define node selection constraints for pods of controller deployment No    tolerations Define tolerations for the controller deployment, if required No    healthMonitor.enabled Enable/Disable deployment of external health monitor sidecar for controller side volume health monitoring. No false   healthMonitor.interval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s   node Allows configuration of the node-specific parameters. - -   dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet   healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false   nodeSelector Define node selection constraints for pods of node deployment No    tolerations Define tolerations for the node deployment, if required No     Note:\n  User should provide all boolean values with double-quotes. This applies only for myvalues.yaml. Example: “true”/“false”\n  controllerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails.\n  User can a create separate StorageClass (with topology-related keys) by referring to existing default storage classes.\n  Host IO Limit must have a minimum bandwidth of 1 MBPS to discover the volumes on node successfully.\n  User must not change the value of allowRWOMultiPodAccess to true unless intended to use the feature and is aware of the consequences. Enabling multiple pods to access the same PVC with RWO access mode on the same node might cause data to be overwritten and therefore leading to data loss in some cases.\n  Example myvalues.yaml\nlogLevel:\"info\"imagePullPolicy:AlwayscertSecretCount:1kubeletConfigDir:/var/lib/kubeletcontroller:controllerCount:2volumeNamePrefix :csivolsnapshot:enabled:truesnapNamePrefix:csi-snapresizer:enabled:falseallowRWOMultiPodAccess:falsesyncNodeInfoInterval:5maxUnityVolumesPerNode:0  For certificate validation of Unisphere REST API calls refer here. Otherwise, create an empty secret with file csi-unity/samples/secret/emptysecret.yaml file by running the kubectl create -f csi-unity/samples/secret/emptysecret.yaml command.\n  Prepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     storageArrayList.username Username for accessing Unity system true -   storageArrayList.password Password for accessing Unity system true -   storageArrayList.endpoint REST API gateway HTTPS endpoint Unity system true -   storageArrayList.arrayId ArrayID for Unity system true -   storageArrayList.skipCertificateValidation “skipCertificateValidation \" determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface. If it is set to false, then a secret unity-certs has to be created with an X.509 certificate of CA which signed the Unisphere certificate. true true   storageArrayList.isDefault An array having isDefault=true or isDefaultArray=true will be considered as the default array when arrayId is not specified in the storage class. This parameter should occur only once in the list. true -    Example: secret.yaml\nstorageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:trueUse the following command to create a new secret unity-creds from secret.yaml file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the yaml syntax and array-related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the yaml file.\nAlternatively, users can configure and use secret.yaml for driver configuration. The parameters remain the same as in the above table and below is a sample of secret.yaml. Samples of secret.yaml is available in the directory csi-unity/samples/secret/ .\nExample: secret.yaml\nstorageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:true```**Note:***Parameters\"allowRWOMultiPodAccess\"and\"syncNodeInfoInterval\"havebeenenabledforconfigurationinvalues.yamlandthishelpsuserstodynamicallychangethesevalueswithouttheneedfordriverre-installation.  Setup for snapshots.\nApplicable only if you decided to enable snapshot feature in values.yaml\ncontroller:snapshot:enabled:trueIn order to use the Kubernetes Volume Snapshot feature, you must ensure the following components have been deployed on your Kubernetes cluster\nVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Use v5.0.x for the installation.\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  Use v5.0.x for the installation.\nInstallation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - Note:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.    Run the ./csi-install.sh --namespace unity --values ./myvalues.yaml command to proceed with the installation.\nA successful installation must display messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.22 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.22 | |- Driver: csi-unity | |- Verifying Kubernetes version | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that optional secrets have been created Success | |- Verifying alpha snapshot resources | |--\u003e Verifying that alpha snapshot CRDs are not installed Success | |- Verifying sshpass installation.. | |- Verifying iSCSI installation Enter the root password of 10.**.**.**: Enter the root password of 10.**.**.**: Success | |- Verifying snapshot support | |--\u003e Verifying that snapshot CRDs are available Success | |--\u003e Verifying that the snapshot controller is available Success | |- Verifying helm version Success | |- Verifying helm values version Success ------------------------------------------------------ \u003e Verification Complete - Success ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for Deployment unity-controller to be ready Success | |--\u003e Waiting for DaemonSet unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results:\nAt the end of the script unity-controller Deployment and DaemonSet unity-node will be ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\n One or more Unity Controller (based on controllerCount) with 5/5 containers ready, and status displayed as Running. Agent pods with 2/2 containers and the status displayed as Running.    Certificate validation for Unisphere REST API calls This topic provides details about setting up the certificate validation for the CSI Driver for Dell Unity.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on the “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\n  To fetch the certificate, run the following command. openssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem\n  Run the following command to create the cert secret with index ‘0’: kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret: kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -\n  Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)\nNote:\n  “unity” is the namespace for helm-based installation but namespace can be user-defined in operator-based installation.\n  User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation.\n  Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\n    Volume Snapshot Class For CSI Driver for Unity version 1.6 and later, dell-csi-helm-installer does not create any Volume Snapshot classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI Unity v2.1 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI Unity to 1.6 or higher, before upgrading to 2.2.\nStorage Classes Storage Classes are an essential Kubernetes construct for Storage provisioning. To know more about Storage Classes, refer to https://kubernetes.io/docs/concepts/storage/storage-classes/\nA wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nFor CSI Driver for Unity, a wide set of annotated storage class manifests have been provided in the csi-unity/samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class: There are samples storage class yaml files available under csi-unity/samples/storageclass. These can be copied and modified as needed.\n Pick any of unity-fc.yaml, unity-iscsi.yaml or unity-nfs.yaml Copy the file as unity-\u003cARRAY_ID\u003e-fc.yaml, unity-\u003cARRAY_ID\u003e-iscsi.yaml or unity-\u003cARRAY_ID\u003e-nfs.yaml Replace \u003cARRAY_ID\u003e with the Array Id of the Unity Array to be used Replace \u003cSTORAGE_POOL\u003e with the storage pool you have Replace \u003cTIERING_POLICY\u003e with the Tiering policy that is to be used for provisioning Replace \u003cHOST_IO_LIMIT_NAME\u003e with the Host IO Limit Name that is to be used for provisioning Replace \u003cmountOption1\u003e with the necessary mount options. If not required, this can be removed from the storage class Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f unity-\u003cARRAY_ID\u003e-fc.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-iscsi.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-nfs.yaml  Note:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nDynamically update the unity-creds secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\n kubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note: Updating unity-certs-x secrets is a manual process, unlike unity-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nDynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\n","excerpt":"The CSI Driver for Dell Unity can be deployed by using the provided …","ref":"/csm-docs/v1/csidriver/installation/helm/unity/","title":"Unity"},{"body":"CSI Driver for Unity Pre-requisites Create secret to store Unity credentials Create a namespace called unity (it can be any user-defined name; But commands in this section assumes that the namespace is unity) Prepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing Unity system true -   password Password for accessing Unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for Unity system true -   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. true -    Ex: secret.yaml\nstorageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:truekubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml\nUse the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\nCreate secret for client side TLS verification Please refer detailed documentation on how to create this secret here\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:unity-certs-0namespace:unitytype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\nModify/Set the following optional environment variables Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT Specifies the HTTP endpoint for Unity. No /var/run/csi/csi.sock   X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS Flag to enable multiple pods use the same pvc on the same node with RWO access mode No false   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true   X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin No    Node parameters      X_CSI_MODE Driver starting mode No node   X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands. No /noderoot   X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Node plugin No     Example CR for Unity Refer samples from here. Below is an example CR:\napiVersion:storage.dell.com/v1kind:CSIUnitymetadata:name:test-unitynamespace:test-unityspec:driver:configVersion:v2.2.0replicas:2dnsPolicy:ClusterFirstWithHostNetforceUpdate:falsecommon:image:\"dellemc/csi-unity:v2.2.0\"imagePullPolicy:IfNotPresentsideCars:- name:provisionerargs:[\"--volume-name-prefix=csiunity\",\"--default-fstype=ext4\"]- name:snapshotterargs:[\"--snapshot-name-prefix=csiunitysnap\"]# Enable/Disable health monitor of CSI volumes from node plugin. Provides details of volume usage.# - name: external-health-monitor# args: [\"--monitor-interval=60s\"] controller:envs:# X_CSI_ENABLE_VOL_HEALTH_MONITOR: Enable/Disable health monitor of CSI volumes from Controller plugin. Provides details of volume status and volume condition.# As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"# nodeSelector: Define node selection constraints for controller pods.# For the pod to be eligible to run on a node, the node must have each# of the indicated key-value pairs as labels.# Leave as blank to consider all nodes# Allowed values: map of key-value pairs# Default value: None# Examples:# node-role.kubernetes.io/master: \"\"nodeSelector:# node-role.kubernetes.io/master: \"\"# tolerations: Define tolerations for the controllers, if required.# Leave as blank to install controller on worker nodes# Default value: Nonetolerations:# - key: \"node-role.kubernetes.io/master\"# operator: \"Exists\"# effect: \"NoSchedule\"node:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"# nodeSelector: Define node selection constraints for node pods.# For the pod to be eligible to run on a node, the node must have each# of the indicated key-value pairs as labels.# Leave as blank to consider all nodes# Allowed values: map of key-value pairs# Default value: None# Examples:# node-role.kubernetes.io/master: \"\"nodeSelector:# node-role.kubernetes.io/master: \"\"# tolerations: Define tolerations for the controllers, if required.# Leave as blank to install controller on worker nodes# Default value: Nonetolerations:# - key: \"node-role.kubernetes.io/master\"# operator: \"Exists\"# effect: \"NoSchedule\"---apiVersion:v1kind:ConfigMapmetadata:name:unity-config-paramsnamespace:test-unitydata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"info\"ALLOW_RWO_MULTIPOD_ACCESS:\"false\"MAX_UNITY_VOLUMES_PER_NODE:\"0\"SYNC_NODE_INFO_TIME_INTERVAL:\"15\"TENANT_NAME:\"\"Dynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params Note :\n Prior to CSI Driver for unity version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  Volume Health Monitoring This feature is introduced in CSI Driver for unity version 2.1.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n # Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_ENABLE_VOL_HEALTH_MONITOR to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin- volume status, volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" ","excerpt":"CSI Driver for Unity Pre-requisites Create secret to store Unity …","ref":"/csm-docs/v1/csidriver/installation/operator/unity/","title":"Unity"},{"body":"Test deploying a simple Pod and Pvc with Unity storage In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes and automatically mounts them to the pod.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./test/sample.yaml   You can find all the created resources in test-unity namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n test-unity If it is in CrashLoopback state then the driver installation was not successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./test/sample.yaml   Support for SLES 15 SP2 The CSI Driver for Dell Unity requires the following set of packages installed on all worker nodes that run on SLES 15 SP2.\n open-iscsi open-iscsi is required in order to make use of iSCSI protocol for provisioning nfs-utils nfs-utils is required in order to make use of NFS protocol for provisioning multipath-tools multipath-tools is required in order to make use of FC and iSCSI protocols for provisioning  After installing open-iscsi, ensure “iscsi” and “iscsid” services have been started and /etc/isci/initiatorname.iscsi is created and has the host initiator id. The pre-requisites are mandatory for provisioning with the iSCSI protocol to work.\n","excerpt":"Test deploying a simple Pod and Pvc with Unity storage In the …","ref":"/csm-docs/v1/csidriver/installation/test/unity/","title":"Test Unity CSI Driver"},{"body":"Release Notes - CSI Unity v2.2.0 New Features/Changes  Added support for Kubernetes 1.23. Added support for Standalone Helm Charts.  Fixed Issues Known Issues    Issue Workaround     Topology-related node labels are not removed automatically. Currently, when the driver is uninstalled, topology-related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released, remove the labels manually after the driver un-installation using command kubectl label node \u003cnode_name\u003e - - … Example: kubectl label node  csi-unity.dellemc.com/array123-iscsi- Note: there must be - at the end of each label to remove it.   NFS Clone - Resize of the snapshot is not supported by Unity Platform. Currently, when the driver takes a clone of NFS volume, it succeeds. But when the user tries to resize the NFS volumesnapshot, the driver will throw an error. The user should never try to resize the cloned NFS volume.   Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100    Note:  Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI Unity v2.2.0 New Features/Changes  Added support …","ref":"/csm-docs/v1/csidriver/release/unity/","title":"Unity"},{"body":"    Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs show that the driver can’t connect to Unity - Authentication failure. Check if you have created a secret with correct credentials   fsGroup specified in pod spec is not reflected in files or directories at mounted path of volume. fsType of PVC must be set for fsGroup to work. fsType can be specified while creating a storage class. For NFS protocol, fsType can be specified as nfs. fsGroup doesn’t work for ephemeral inline volumes.   Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver controller and node pod should be restarted with command kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}'| xargs kubectl delete -n unity pod when topology-based storage classes are used. For dynamic array addition without topology, the driver will detect the newly added or removed arrays automatically   If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in the cluster but on array, it will still be present and marked for deletion. All the cloned PVC should be deleted in order to delete the source PVC from the array.   PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass “unity-iscsi”: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}'| xargs kubectl delete -n unity pod   Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.21.0 \u003c= 1.23.0 which is incompatible with Kubernetes V1.21.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart at helm/csi-unity/Chart.yaml and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported.    ","excerpt":"    Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/v1/csidriver/troubleshooting/unity/","title":"Unity"},{"body":"The CSI Driver for Dell Unity can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for Unity Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume Kubernetes External Health Monitor, which provides volume health status  The node section of the Helm chart installs the following component in a DaemonSet:\n CSI Driver for Unity Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for Unity, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes or OpenShift (see supported versions) Install Helm v3 To use FC protocol, the host must be zoned with Unity array and Multipath needs to be configured To use iSCSI protocol, iSCSI initiator utils packages needs to be installed and Multipath needs to be configured To use NFS protocol, NFS utility packages needs to be installed Mount propagation is enabled on container runtime that is being used  Install CSI Driver Install CSI Driver for Unity using this procedure.\nBefore you begin\n You must have the downloaded files, including the Helm chart from the source git repository with the command git clone -b v2.2.0 https://github.com/dell/csi-unity.git, as a pre-requisite for running this procedure. In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. Ensure unity namespace exists in Kubernetes cluster. Use the kubectl create namespace unity command to create the namespace if the namespace is not present.  Procedure\n  Collect information from the Unity Systems like Unique ArrayId, IP address, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml and myvalues.yaml file.\nNote:\n ArrayId corresponds to the serial number of Unity array. Unity Array username must have role as Storage Administrator to be able to perform CRUD operations. If the user is using complex K8s version like “v1.21.3-mirantis-1”, use below kubeVersion check in helm/csi-unity/Chart.yaml file. kubeVersion: “\u003e= 1.21.0-0 \u003c 1.24.0-0”    Copy the helm/csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\n  Edit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     version helm version true -   logLevel LogLevel is used to set the logging level of the driver true info   allowRWOMultiPodAccess Flag to enable multiple pods to use the same PVC on the same node with RWO access mode. false false   kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet   syncNodeInfoInterval Time interval to add node info to the array. Default 15 minutes. The minimum value should be 1 minute. false 15   maxUnityVolumesPerNode Maximum number of volumes that controller can publish to the node. false 0   certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. (unity-cert-0..unity-cert-n). The minimum value should be 1. false 1   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent   podmon.enabled service to monitor failing jobs and notify false -   podmon.image pod man image name false -   tenantName Tenant name added while adding host entry to the array No    controller Allows configuration of the controller-specific parameters. - -   controllerCount Defines the number of csi-unity controller pods to deploy to the Kubernetes release Yes 2   volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s”   snapshot.enabled Enable/Disable volume snapshot feature Yes true   snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot”   resizer.enabled Enable/Disable volume expansion feature Yes true   nodeSelector Define node selection constraints for pods of controller deployment No    tolerations Define tolerations for the controller deployment, if required No    healthMonitor.enabled Enable/Disable deployment of external health monitor sidecar for controller side volume health monitoring. No false   healthMonitor.interval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s   node Allows configuration of the node-specific parameters. - -   dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet   healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false   nodeSelector Define node selection constraints for pods of node deployment No    tolerations Define tolerations for the node deployment, if required No     Note:\n  User should provide all boolean values with double-quotes. This applies only for myvalues.yaml. Example: “true”/“false”\n  controllerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails.\n  User can a create separate StorageClass (with topology-related keys) by referring to existing default storage classes.\n  Host IO Limit must have a minimum bandwidth of 1 MBPS to discover the volumes on node successfully.\n  User must not change the value of allowRWOMultiPodAccess to true unless intended to use the feature and is aware of the consequences. Enabling multiple pods to access the same PVC with RWO access mode on the same node might cause data to be overwritten and therefore leading to data loss in some cases.\n  Example myvalues.yaml\nlogLevel:\"info\"imagePullPolicy:AlwayscertSecretCount:1kubeletConfigDir:/var/lib/kubeletcontroller:controllerCount:2volumeNamePrefix :csivolsnapshot:enabled:truesnapNamePrefix:csi-snapresizer:enabled:falseallowRWOMultiPodAccess:falsesyncNodeInfoInterval:5maxUnityVolumesPerNode:0  For certificate validation of Unisphere REST API calls refer here. Otherwise, create an empty secret with file csi-unity/samples/secret/emptysecret.yaml file by running the kubectl create -f csi-unity/samples/secret/emptysecret.yaml command.\n  Prepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     storageArrayList.username Username for accessing Unity system true -   storageArrayList.password Password for accessing Unity system true -   storageArrayList.endpoint REST API gateway HTTPS endpoint Unity system true -   storageArrayList.arrayId ArrayID for Unity system true -   storageArrayList.skipCertificateValidation “skipCertificateValidation \" determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface. If it is set to false, then a secret unity-certs has to be created with an X.509 certificate of CA which signed the Unisphere certificate. true true   storageArrayList.isDefault An array having isDefault=true or isDefaultArray=true will be considered as the default array when arrayId is not specified in the storage class. This parameter should occur only once in the list. true -    Example: secret.yaml\nstorageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:trueUse the following command to create a new secret unity-creds from secret.yaml file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the yaml syntax and array-related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the yaml file.\nAlternatively, users can configure and use secret.yaml for driver configuration. The parameters remain the same as in the above table and below is a sample of secret.yaml. Samples of secret.yaml is available in the directory csi-unity/samples/secret/ .\nExample: secret.yaml\nstorageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:true```**Note:***Parameters\"allowRWOMultiPodAccess\"and\"syncNodeInfoInterval\"havebeenenabledforconfigurationinvalues.yamlandthishelpsuserstodynamicallychangethesevalueswithouttheneedfordriverre-installation.  Setup for snapshots.\nApplicable only if you decided to enable snapshot feature in values.yaml\ncontroller:snapshot:enabled:trueIn order to use the Kubernetes Volume Snapshot feature, you must ensure the following components have been deployed on your Kubernetes cluster\nVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Use v5.0.x for the installation.\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  Use v5.0.x for the installation.\nInstallation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - Note:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.    Run the ./csi-install.sh --namespace unity --values ./myvalues.yaml command to proceed with the installation.\nA successful installation must display messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.22 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.22 | |- Driver: csi-unity | |- Verifying Kubernetes version | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that optional secrets have been created Success | |- Verifying alpha snapshot resources | |--\u003e Verifying that alpha snapshot CRDs are not installed Success | |- Verifying sshpass installation.. | |- Verifying iSCSI installation Enter the root password of 10.**.**.**: Enter the root password of 10.**.**.**: Success | |- Verifying snapshot support | |--\u003e Verifying that snapshot CRDs are available Success | |--\u003e Verifying that the snapshot controller is available Success | |- Verifying helm version Success | |- Verifying helm values version Success ------------------------------------------------------ \u003e Verification Complete - Success ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for Deployment unity-controller to be ready Success | |--\u003e Waiting for DaemonSet unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results:\nAt the end of the script unity-controller Deployment and DaemonSet unity-node will be ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\n One or more Unity Controller (based on controllerCount) with 5/5 containers ready, and status displayed as Running. Agent pods with 2/2 containers and the status displayed as Running.    Certificate validation for Unisphere REST API calls This topic provides details about setting up the certificate validation for the CSI Driver for Dell Unity.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on the “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\n  To fetch the certificate, run the following command. openssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem\n  Run the following command to create the cert secret with index ‘0’: kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret: kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -\n  Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)\nNote:\n  “unity” is the namespace for helm-based installation but namespace can be user-defined in operator-based installation.\n  User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation.\n  Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\n    Volume Snapshot Class For CSI Driver for Unity version 1.6 and later, dell-csi-helm-installer does not create any Volume Snapshot classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI Unity v2.1 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI Unity to 1.6 or higher, before upgrading to 2.2.\nStorage Classes Storage Classes are an essential Kubernetes construct for Storage provisioning. To know more about Storage Classes, refer to https://kubernetes.io/docs/concepts/storage/storage-classes/\nA wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nFor CSI Driver for Unity, a wide set of annotated storage class manifests have been provided in the csi-unity/samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class: There are samples storage class yaml files available under csi-unity/samples/storageclass. These can be copied and modified as needed.\n Pick any of unity-fc.yaml, unity-iscsi.yaml or unity-nfs.yaml Copy the file as unity-\u003cARRAY_ID\u003e-fc.yaml, unity-\u003cARRAY_ID\u003e-iscsi.yaml or unity-\u003cARRAY_ID\u003e-nfs.yaml Replace \u003cARRAY_ID\u003e with the Array Id of the Unity Array to be used Replace \u003cSTORAGE_POOL\u003e with the storage pool you have Replace \u003cTIERING_POLICY\u003e with the Tiering policy that is to be used for provisioning Replace \u003cHOST_IO_LIMIT_NAME\u003e with the Host IO Limit Name that is to be used for provisioning Replace \u003cmountOption1\u003e with the necessary mount options. If not required, this can be removed from the storage class Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f unity-\u003cARRAY_ID\u003e-fc.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-iscsi.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-nfs.yaml  Note:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nDynamically update the unity-creds secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\n kubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note: Updating unity-certs-x secrets is a manual process, unlike unity-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nDynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\n","excerpt":"The CSI Driver for Dell Unity can be deployed by using the provided …","ref":"/csm-docs/v2/csidriver/installation/helm/unity/","title":"Unity"},{"body":"CSI Driver for Unity Pre-requisites Create secret to store Unity credentials Create a namespace called unity (it can be any user-defined name; But commands in this section assumes that the namespace is unity) Prepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing Unity system true -   password Password for accessing Unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for Unity system true -   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. true -    Ex: secret.yaml\nstorageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:truekubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml\nUse the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\nCreate secret for client side TLS verification Please refer detailed documentation on how to create this secret here\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:unity-certs-0namespace:unitytype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\nModify/Set the following optional environment variables Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT Specifies the HTTP endpoint for Unity. No /var/run/csi/csi.sock   X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS Flag to enable multiple pods use the same pvc on the same node with RWO access mode No false   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true   X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin No    Node parameters      X_CSI_MODE Driver starting mode No node   X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands. No /noderoot   X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Node plugin No     Example CR for Unity Refer samples from here. Below is an example CR:\napiVersion:storage.dell.com/v1kind:CSIUnitymetadata:name:test-unitynamespace:test-unityspec:driver:configVersion:v2.2.0replicas:2dnsPolicy:ClusterFirstWithHostNetforceUpdate:falsecommon:image:\"dellemc/csi-unity:v2.2.0\"imagePullPolicy:IfNotPresentsideCars:- name:provisionerargs:[\"--volume-name-prefix=csiunity\",\"--default-fstype=ext4\"]- name:snapshotterargs:[\"--snapshot-name-prefix=csiunitysnap\"]# Enable/Disable health monitor of CSI volumes from node plugin. Provides details of volume usage.# - name: external-health-monitor# args: [\"--monitor-interval=60s\"] controller:envs:# X_CSI_ENABLE_VOL_HEALTH_MONITOR: Enable/Disable health monitor of CSI volumes from Controller plugin. Provides details of volume status and volume condition.# As a prerequisite, external-health-monitor sidecar section should be uncommented in samples which would install the sidecar# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"# nodeSelector: Define node selection constraints for controller pods.# For the pod to be eligible to run on a node, the node must have each# of the indicated key-value pairs as labels.# Leave as blank to consider all nodes# Allowed values: map of key-value pairs# Default value: None# Examples:# node-role.kubernetes.io/master: \"\"nodeSelector:# node-role.kubernetes.io/master: \"\"# tolerations: Define tolerations for the controllers, if required.# Leave as blank to install controller on worker nodes# Default value: Nonetolerations:# - key: \"node-role.kubernetes.io/master\"# operator: \"Exists\"# effect: \"NoSchedule\"node:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"# nodeSelector: Define node selection constraints for node pods.# For the pod to be eligible to run on a node, the node must have each# of the indicated key-value pairs as labels.# Leave as blank to consider all nodes# Allowed values: map of key-value pairs# Default value: None# Examples:# node-role.kubernetes.io/master: \"\"nodeSelector:# node-role.kubernetes.io/master: \"\"# tolerations: Define tolerations for the controllers, if required.# Leave as blank to install controller on worker nodes# Default value: Nonetolerations:# - key: \"node-role.kubernetes.io/master\"# operator: \"Exists\"# effect: \"NoSchedule\"---apiVersion:v1kind:ConfigMapmetadata:name:unity-config-paramsnamespace:test-unitydata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"info\"ALLOW_RWO_MULTIPOD_ACCESS:\"false\"MAX_UNITY_VOLUMES_PER_NODE:\"0\"SYNC_NODE_INFO_TIME_INTERVAL:\"15\"TENANT_NAME:\"\"Dynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params Note :\n Prior to CSI Driver for unity version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  Volume Health Monitoring This feature is introduced in CSI Driver for unity version 2.1.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n # Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_ENABLE_VOL_HEALTH_MONITOR to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin- volume status, volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" ","excerpt":"CSI Driver for Unity Pre-requisites Create secret to store Unity …","ref":"/csm-docs/v2/csidriver/installation/operator/unity/","title":"Unity"},{"body":"Test deploying a simple Pod and Pvc with Unity storage In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes and automatically mounts them to the pod.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./test/sample.yaml   You can find all the created resources in test-unity namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n test-unity If it is in CrashLoopback state then the driver installation was not successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./test/sample.yaml   Support for SLES 15 SP2 The CSI Driver for Dell Unity requires the following set of packages installed on all worker nodes that run on SLES 15 SP2.\n open-iscsi open-iscsi is required in order to make use of iSCSI protocol for provisioning nfs-utils nfs-utils is required in order to make use of NFS protocol for provisioning multipath-tools multipath-tools is required in order to make use of FC and iSCSI protocols for provisioning  After installing open-iscsi, ensure “iscsi” and “iscsid” services have been started and /etc/isci/initiatorname.iscsi is created and has the host initiator id. The pre-requisites are mandatory for provisioning with the iSCSI protocol to work.\n","excerpt":"Test deploying a simple Pod and Pvc with Unity storage In the …","ref":"/csm-docs/v2/csidriver/installation/test/unity/","title":"Test Unity CSI Driver"},{"body":"Release Notes - CSI Unity v2.2.0 New Features/Changes  Added support for Kubernetes 1.23. Added support for Standalone Helm Charts.  Fixed Issues Known Issues    Issue Workaround     Topology-related node labels are not removed automatically. Currently, when the driver is uninstalled, topology-related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released, remove the labels manually after the driver un-installation using command kubectl label node \u003cnode_name\u003e - - … Example: kubectl label node  csi-unity.dellemc.com/array123-iscsi- Note: there must be - at the end of each label to remove it.   NFS Clone - Resize of the snapshot is not supported by Unity Platform. Currently, when the driver takes a clone of NFS volume, it succeeds. But when the user tries to resize the NFS volumesnapshot, the driver will throw an error. The user should never try to resize the cloned NFS volume.   Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100    Note:  Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI Unity v2.2.0 New Features/Changes  Added support …","ref":"/csm-docs/v2/csidriver/release/unity/","title":"Unity"},{"body":"    Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs show that the driver can’t connect to Unity - Authentication failure. Check if you have created a secret with correct credentials   fsGroup specified in pod spec is not reflected in files or directories at mounted path of volume. fsType of PVC must be set for fsGroup to work. fsType can be specified while creating a storage class. For NFS protocol, fsType can be specified as nfs. fsGroup doesn’t work for ephemeral inline volumes.   Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver controller and node pod should be restarted with command kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}'| xargs kubectl delete -n unity pod when topology-based storage classes are used. For dynamic array addition without topology, the driver will detect the newly added or removed arrays automatically   If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in the cluster but on array, it will still be present and marked for deletion. All the cloned PVC should be deleted in order to delete the source PVC from the array.   PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass “unity-iscsi”: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}'| xargs kubectl delete -n unity pod   Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.21.0 \u003c= 1.23.0 which is incompatible with Kubernetes V1.21.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart at helm/csi-unity/Chart.yaml and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported.    ","excerpt":"    Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/v2/csidriver/troubleshooting/unity/","title":"Unity"},{"body":"The CSI Driver for Dell EMC Unity can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for Unity Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet:\n CSI Driver for Unity Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for Unity, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes or OpenShift (see supported versions) Install Helm v3 To use FC protocol, the host must be zoned with Unity array and Multipath needs to be configured To use iSCSI protocol, iSCSI initiator utils packages needs to be installed and Multipath needs to be configured To use NFS protocol, NFS utility packages needs to be installed Mount propagation is enabled on container runtime that is being used  Install CSI Driver Install CSI Driver for Unity using this procedure.\nBefore you begin\n You must have the downloaded files, including the Helm chart from the source git repository with the command git clone -b v2.1.0 https://github.com/dell/csi-unity.git, as a pre-requisite for running this procedure. In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. Ensure unity namespace exists in Kubernetes cluster. Use the kubectl create namespace unity command to create the namespace if the namespace is not present.  Procedure\n  Collect information from the Unity Systems like Unique ArrayId, IP address, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml and myvalues.yaml file.\nNote:\n ArrayId corresponds to the serial number of Unity array. Unity Array username must have role as Storage Administrator to be able to perform CRUD operations.    Copy the helm/csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\n  Edit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     version helm version true -   logLevel LogLevel is used to set the logging level of the driver true info   allowRWOMultiPodAccess Flag to enable multiple pods to use the same PVC on the same node with RWO access mode. false false   kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet   syncNodeInfoInterval Time interval to add node info to the array. Default 15 minutes. The minimum value should be 1 minute. false 15   maxUnityVolumesPerNode Maximum number of volumes that controller can publish to the node. false 0   certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. (unity-cert-0..unity-cert-n). The minimum value should be 1. false 1   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent   podmon.enabled service to monitor failing jobs and notify false -   podmon.image pod man image name false -   controller Allows configuration of the controller-specific parameters. - -   controllerCount Defines the number of csi-unity controller pods to deploy to the Kubernetes release Yes 2   volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s”   snapshot.enabled Enable/Disable volume snapshot feature Yes true   snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot”   resizer.enabled Enable/Disable volume expansion feature Yes true   nodeSelector Define node selection constraints for pods of controller deployment No    tolerations Define tolerations for the controller deployment, if required No    volumeHealthMonitor.enabled Enable/Disable deployment of external health monitor sidecar for controller side volume health monitoring. No false   volumeHealthMonitor.interval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s   node Allows configuration of the node-specific parameters. - -   tolerations Define tolerations for the node daemonset, if required No    dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet   volumeHealthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false   tenantName Tenant name added while adding host entry to the array No     Note:\n  User should provide all boolean values with double-quotes. This applies only for myvalues.yaml. Example: “true”/“false”\n  controllerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails.\n  User can a create separate StorageClass (with topology-related keys) by referring to existing default storage classes.\n  Host IO Limit must have a minimum bandwidth of 1 MBPS to discover the volumes on node successfully.\n  User must not change the value of allowRWOMultiPodAccess to true unless intended to use the feature and is aware of the consequences. Enabling multiple pods to access the same PVC with RWO access mode on the same node might cause data to be overwritten and therefore leading to data loss in some cases.\n  Example myvalues.yaml\nlogLevel:\"info\"imagePullPolicy:AlwayscertSecretCount:1kubeletConfigDir:/var/lib/kubeletcontroller:controllerCount:2volumeNamePrefix :csivolsnapshot:enabled:truesnapNamePrefix:csi-snapresizer:enabled:falseallowRWOMultiPodAccess:falsesyncNodeInfoInterval:5maxUnityVolumesPerNode:0  For certificate validation of Unisphere REST API calls refer here. Otherwise, create an empty secret with file helm/emptysecret.yaml file by running the kubectl create -f helm/emptysecret.yaml command.\n  Prepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     storageArrayList.username Username for accessing Unity system true -   storageArrayList.password Password for accessing Unity system true -   storageArrayList.endpoint REST API gateway HTTPS endpoint Unity system true -   storageArrayList.arrayId ArrayID for Unity system true -   storageArrayList.skipCertificateValidation “skipCertificateValidation \" determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface. If it is set to false, then a secret unity-certs has to be created with an X.509 certificate of CA which signed the Unisphere certificate. true true   storageArrayList.isDefault An array having isDefault=true or isDefaultArray=true will be considered as the default array when arrayId is not specified in the storage class. This parameter should occur only once in the list. false false    Example: secret.yaml\nstorageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:trueUse the following command to create a new secret unity-creds from secret.yaml file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the yaml syntax and array-related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the yaml file.\nAlternatively, users can configure and use secret.yaml for driver configuration. The parameters remain the same as in the above table and below is a sample of secret.yaml. Samples of secret.yaml is available in the directory csi-unity/samples/secret/ .\nExample: secret.yaml\nstorageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:true```**Note:***Parameters\"allowRWOMultiPodAccess\"and\"syncNodeInfoTimeInterval\"havebeenenabledforconfigurationinvalues.yamlandthishelpsuserstodynamicallychangethesevalueswithouttheneedfordriverre-installation.  Setup for snapshots.\nApplicable only if you decided to enable snapshot feature in values.yaml\ncontroller:snapshot:enabled:trueIn order to use the Kubernetes Volume Snapshot feature, you must ensure the following components have been deployed on your Kubernetes cluster\nVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Use v4.2.x for the installation.\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  Use v4.2.x for the installation.\nNote:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v4.0.x   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller Note:\n It is recommended to use 4.2.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.    Run the ./csi-install.sh --namespace unity --values ./myvalues.yaml command to proceed with the installation.\nA successful installation must display messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.20 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.20 | |- Driver: csi-unity | |- Verifying Kubernetes versions | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that required secrets have been created Success | |- Verifying alpha snapshot resources | |--\u003e Verifying that alpha snapshot CRDs are not installed Success | |- Verifying sshpass installation.. | |- Verifying iSCSI installation Enter the root password of 10.**.**.**: Enter the root password of 10.**.**.**: Success | |- Verifying snapshot support | |--\u003e Verifying that snapshot CRDs are available Success | |--\u003e Verifying that the snapshot controller is available Success | |- Verifying helm version Success | |- Verifying helm values version Success ------------------------------------------------------ \u003e Verification Complete - Success ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for Deployment unity-controller to be ready Success | |--\u003e Waiting for DaemonSet unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results:\nAt the end of the script unity-controller Deployment and DaemonSet unity-node will be ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\n One or more Unity Controller (based on controllerCount) with 5/5 containers ready, and status displayed as Running. Agent pods with 2/2 containers and the status displayed as Running.    Certificate validation for Unisphere REST API calls This topic provides details about setting up the certificate validation for the CSI Driver for Dell EMC Unity.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on the “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\n  To fetch the certificate, run the following command. openssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem\n  Run the following command to create the cert secret with index ‘0’: kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret: kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -\n  Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)\nNote:\n  “unity” is the namespace for helm-based installation but namespace can be user-defined in operator-based installation.\n  User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation.\n  Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\n    Volume Snapshot Class For CSI Driver for Unity version 1.6 and later, dell-csi-helm-installer does not create any Volume Snapshot classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI Unity v2.0 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI Unity to 1.6 or higher, before upgrading to 2.1.\nStorage Classes Storage Classes are an essential Kubernetes construct for Storage provisioning. To know more about Storage Classes, refer to https://kubernetes.io/docs/concepts/storage/storage-classes/\nA wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nFor CSI Driver for Unity, a wide set of annotated storage class manifests have been provided in the csi-unity/samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class: There are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\n Pick any of unity-fc.yaml, unity-iscsi.yaml or unity-nfs.yaml Copy the file as unity-\u003cARRAY_ID\u003e-fc.yaml, unity-\u003cARRAY_ID\u003e-iscsi.yaml or unity-\u003cARRAY_ID\u003e-nfs.yaml Replace \u003cARRAY_ID\u003e with the Array Id of the Unity Array to be used Replace \u003cSTORAGE_POOL\u003e with the storage pool you have Replace \u003cTIERING_POLICY\u003e with the Tiering policy that is to be used for provisioning Replace \u003cHOST_IO_LIMIT_NAME\u003e with the Host IO Limit Name that is to be used for provisioning Replace \u003cmountOption1\u003e with the necessary mount options. If not required, this can be removed from the storage class Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f unity-\u003cARRAY_ID\u003e-fc.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-iscsi.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-nfs.yaml  Note:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nDynamically update the unity-creds secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\n kubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note: Updating unity-certs-x secrets is a manual process, unlike unity-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nDynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nHelm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\n","excerpt":"The CSI Driver for Dell EMC Unity can be deployed by using the …","ref":"/csm-docs/v3/csidriver/installation/helm/unity/","title":"Unity"},{"body":"CSI Driver for Unity Pre-requisites Create secret to store Unity credentials Create a namespace called unity (it can be any user-defined name; But commands in this section assumes that the namespace is unity) Prepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing Unity system true -   password Password for accessing Unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for Unity system true -   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. false false    Ex: secret.yaml\nstorageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:truekubectl create secret generic unity-creds -n unity --from-file=config=secret.secret\nUse the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\nCreate secret for client side TLS verification Please refer detailed documentation on how to create this secret here\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:unity-certs-0namespace:unitytype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\nModify/Set the following optional environment variables Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT Specifies the HTTP endpoint for Unity. No /var/run/csi/csi.sock   X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS Flag to enable multiple pods use the same pvc on the same node with RWO access mode No false   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true   Node parameters      X_CSI_MODE Driver starting mode No node   X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands. No /noderoot    Example CR for Unity Refer samples from here. Below is an example CR:\napiVersion:storage.dell.com/v1kind:CSIUnitymetadata:name:test-unitynamespace:test-unityspec:driver:configVersion:v2.0.0replicas:2dnsPolicy:ClusterFirstWithHostNetforceUpdate:falsecommon:image:\"dellemc/csi-unity:v2.0.0\"imagePullPolicy:IfNotPresentsideCars:- name:provisionerargs:[\"--volume-name-prefix=csiunity\",\"--default-fstype=ext4\"]- name:snapshotterargs:[\"--snapshot-name-prefix=csiunitysnap\"]---apiVersion:v1kind:ConfigMapmetadata:name:unity-config-paramsnamespace:test-unitydata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"info\"ALLOW_RWO_MULTIPOD_ACCESS:\"false\"MAX_UNITY_VOLUMES_PER_NODE:\"0\"SYNC_NODE_INFO_TIME_INTERVAL:\"0\"TENANT_NAME:\"\"Dynamic Logging Configuration This feature is introduced in CSI Driver for unity version 2.0.0.\nOperator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params Note :\n Prior to CSI Driver for unity version 2.0.0, the log level was allowed to be updated dynamically through logLevel attribute in the secret object. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  Volume Health Monitoring This feature is introduced in CSI Driver for unity version 2.1.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n # Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_ENABLE_VOL_HEALTH_MONITOR to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin- volume status, volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_ENABLE_VOL_HEALTH_MONITOR: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_ENABLE_VOL_HEALTH_MONITOR value: \"false\" ","excerpt":"CSI Driver for Unity Pre-requisites Create secret to store Unity …","ref":"/csm-docs/v3/csidriver/installation/operator/unity/","title":"Unity"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes and automatically mounts them to the pod.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./test/sample.yaml   You can find all the created resources in test-unity namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n test-unity If it is in CrashLoopback state then the driver installation was not successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./test/sample.yaml   ","excerpt":"In the repository, a simple test manifest exists that creates three …","ref":"/csm-docs/v3/csidriver/installation/test/unity/","title":"Test Unity CSI Driver"},{"body":"Release Notes - CSI Unity v2.1.0 New Features/Changes  Added support for OpenShift v4.9. Added support for CSI spec 1.5. Added support for new access modes in CSI Spec 1.5. Added ability to associate a tenant with storage volumes. - Added support for volume health monitoring.  Fixed Issues Known Issues    Issue Workaround     Topology-related node labels are not removed automatically. Currently, when the driver is uninstalled, topology-related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released, remove the labels manually after the driver un-installation using command kubectl label node \u003cnode_name\u003e - - … Example: kubectl label node  csi-unity.dellemc.com/array123-iscsi- Note: there must be - at the end of each label to remove it.   NFS Clone - Resize of the snapshot is not supported by Unity Platform. Currently, when the driver takes a clone of NFS volume, it succeeds. But when the user tries to resize the NFS volumesnapshot, the driver will throw an error. The user should never try to resize the cloned NFS volume.   Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100    Note:  Support for kurbernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode introduced in the release will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI Unity v2.1.0 New Features/Changes  Added support …","ref":"/csm-docs/v3/csidriver/release/unity/","title":"Unity"},{"body":"    Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs show that the driver can’t connect to Unity - Authentication failure. Check if you have created a secret with correct credentials   fsGroup specified in pod spec is not reflected in files or directories at mounted path of volume. fsType of PVC must be set for fsGroup to work. fsType can be specified while creating a storage class. For NFS protocol, fsType can be specified as nfs. fsGroup doesn’t work for ephemeral inline volumes.   Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver controller and node pod should be restarted with command kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}'| xargs kubectl delete -n unity pod when topology-based storage classes are used. For dynamic array addition without topology, the driver will detect the newly added or removed arrays automatically   If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in the cluster but on array, it will still be present and marked for deletion. All the cloned PVC should be deleted in order to delete the source PVC from the array.   PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass “unity-iscsi”: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}'| xargs kubectl delete -n unity pod    ","excerpt":"    Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/v3/csidriver/troubleshooting/unity/","title":"Unity"},{"body":"The CSI Driver for Dell Unity XT can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for Unity XT Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume Kubernetes External Health Monitor, which provides volume health status  The node section of the Helm chart installs the following component in a DaemonSet:\n CSI Driver for Unity XT Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for Unity XT, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes or OpenShift (see supported versions) Install Helm v3 To use FC protocol, the host must be zoned with Unity XT array and Multipath needs to be configured To use iSCSI protocol, iSCSI initiator utils packages needs to be installed and Multipath needs to be configured To use NFS protocol, NFS utility packages needs to be installed Mount propagation is enabled on container runtime that is being used  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell Unity XT.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell Unity XT supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell Unity XT:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port must be done.  Set up the iSCSI Initiator The CSI Driver for Dell Unity XT supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\n Ensure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI port on the Dell Unity XT array that has IP interfaces. Manually create IP routes for each node that connects to the Dell Unity XT. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi.  For more information about configuring iSCSI, see Dell Host Connectivity guide.\nLinux multipathing requirements Dell Unity XT supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell Unity XT.\nSet up Linux multipathing as follows:\n Ensure that all nodes have the Device Mapper Multipathing package installed.   You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\n  Enable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes.  As a best practice, use the following options to help the operating system and the mulitpathing software detect path changes efficiently:\npath_grouping_policy multibus path_checker tur features \"1 queue_if_no_path\" path_selector \"round-robin 0\" no_path_retry 10 Install CSI Driver Install CSI Driver for Unity XT using this procedure.\nBefore you begin\n You must have the downloaded files, including the Helm chart from the source git repository with the command git clone -b v2.3.0 https://github.com/dell/csi-unity.git, as a pre-requisite for running this procedure. In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. Ensure unity namespace exists in Kubernetes cluster. Use the kubectl create namespace unity command to create the namespace if the namespace is not present.  Procedure\n  Collect information from the Unity XT Systems like Unique ArrayId, IP address, username, and password. Make a note of the value for these parameters as they must be entered in the secret.yaml and myvalues.yaml file.\nNote:\n ArrayId corresponds to the serial number of Unity XT array. Unity XT Array username must have role as Storage Administrator to be able to perform CRUD operations. If the user is using complex K8s version like “v1.21.3-mirantis-1”, use below kubeVersion check in helm/csi-unity/Chart.yaml file. kubeVersion: “\u003e= 1.21.0-0 \u003c 1.25.0-0”    Copy the helm/csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\n  Edit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity XT driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     version helm version true -   logLevel LogLevel is used to set the logging level of the driver true info   allowRWOMultiPodAccess Flag to enable multiple pods to use the same PVC on the same node with RWO access mode. false false   kubeletConfigDir Specify kubelet config dir path Yes /var/lib/kubelet   syncNodeInfoInterval Time interval to add node info to the array. Default 15 minutes. The minimum value should be 1 minute. false 15   maxUnityVolumesPerNode Maximum number of volumes that controller can publish to the node. false 0   certSecretCount Represents the number of certificate secrets, which the user is going to create for SSL authentication. (unity-cert-0..unity-cert-n). The minimum value should be 1. false 1   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. Yes IfNotPresent   podmon.enabled service to monitor failing jobs and notify false -   podmon.image pod man image name false -   tenantName Tenant name added while adding host entry to the array No    controller Allows configuration of the controller-specific parameters. - -   controllerCount Defines the number of csi-unity controller pods to deploy to the Kubernetes release Yes 2   volumeNamePrefix Defines a string prefix for the names of PersistentVolumes created Yes “k8s”   snapshot.enabled Enable/Disable volume snapshot feature Yes true   snapshot.snapNamePrefix Defines a string prefix for the names of the Snapshots created Yes “snapshot”   resizer.enabled Enable/Disable volume expansion feature Yes true   nodeSelector Define node selection constraints for pods of controller deployment No    tolerations Define tolerations for the controller deployment, if required No    healthMonitor.enabled Enable/Disable deployment of external health monitor sidecar for controller side volume health monitoring. No false   healthMonitor.interval Interval of monitoring volume health condition. Allowed values: Number followed by unit (s,m,h) No 60s   node Allows configuration of the node-specific parameters. - -   dnsPolicy Define the DNS Policy of the Node service Yes ClusterFirstWithHostNet   healthMonitor.enabled Enable/Disable health monitor of CSI volumes- volume usage, volume condition No false   nodeSelector Define node selection constraints for pods of node deployment No    tolerations Define tolerations for the node deployment, if required No     Note:\n  User should provide all boolean values with double-quotes. This applies only for myvalues.yaml. Example: “true”/“false”\n  controllerCount parameter value should be \u003c= number of nodes in the kubernetes cluster else install script fails.\n  User can a create separate StorageClass (with topology-related keys) by referring to existing default storage classes.\n  Host IO Limit must have a minimum bandwidth of 1 MBPS to discover the volumes on node successfully.\n  User must not change the value of allowRWOMultiPodAccess to true unless intended to use the feature and is aware of the consequences. Enabling multiple pods to access the same PVC with RWO access mode on the same node might cause data to be overwritten and therefore leading to data loss in some cases.\n  Example myvalues.yaml\nlogLevel:\"info\"imagePullPolicy:AlwayscertSecretCount:1kubeletConfigDir:/var/lib/kubeletcontroller:controllerCount:2volumeNamePrefix :csivolsnapshot:enabled:truesnapNamePrefix:csi-snapresizer:enabled:falseallowRWOMultiPodAccess:falsesyncNodeInfoInterval:5maxUnityVolumesPerNode:0  For certificate validation of Unisphere REST API calls refer here. Otherwise, create an empty secret with file csi-unity/samples/secret/emptysecret.yaml file by running the kubectl create -f csi-unity/samples/secret/emptysecret.yaml command.\n  Prepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     storageArrayList.username Username for accessing Unity XT system true -   storageArrayList.password Password for accessing Unity XT system true -   storageArrayList.endpoint REST API gateway HTTPS endpoint Unity XT system true -   storageArrayList.arrayId ArrayID for Unity XT system true -   storageArrayList.skipCertificateValidation “skipCertificateValidation \" determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface. If it is set to false, then a secret unity-certs has to be created with an X.509 certificate of CA which signed the Unisphere certificate. true true   storageArrayList.isDefault An array having isDefault=true or isDefaultArray=true will be considered as the default array when arrayId is not specified in the storage class. This parameter should occur only once in the list. true -    Example: secret.yaml\nstorageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:trueUse the following command to create a new secret unity-creds from secret.yaml file.\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the yaml syntax and array-related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the yaml file.\nAlternatively, users can configure and use secret.yaml for driver configuration. The parameters remain the same as in the above table and below is a sample of secret.yaml. Samples of secret.yaml is available in the directory csi-unity/samples/secret/ .\nExample: secret.yaml\nstorageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:true```**Note:***Parameters\"allowRWOMultiPodAccess\"and\"syncNodeInfoInterval\"havebeenenabledforconfigurationinvalues.yamlandthishelpsuserstodynamicallychangethesevalueswithouttheneedfordriverre-installation.  Setup for snapshots.\nApplicable only if you decided to enable snapshot feature in values.yaml\ncontroller:snapshot:enabled:trueIn order to use the Kubernetes Volume Snapshot feature, you must ensure the following components have been deployed on your Kubernetes cluster\nVolume Snapshot CRD’s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github. Use v5.0.x for the installation.\nVolume Snapshot Controller The CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  Use v5.0.x for the installation.\nInstallation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u003cyour-version\u003e kubectl kustomize client/config/crd | kubectl create -f - kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f - Note:\n It is recommended to use 5.0.x version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.    Run the ./csi-install.sh --namespace unity --values ./myvalues.yaml command to proceed with the installation using bash script.\nA successful installation must display messages that look similar to the following samples:\n------------------------------------------------------ \u003e Installing CSI Driver: csi-unity on 1.22 ------------------------------------------------------ ------------------------------------------------------ \u003e Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u003e Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.22 | |- Driver: csi-unity | |- Verifying Kubernetes version | |--\u003e Verifying minimum Kubernetes version Success | |--\u003e Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that optional secrets have been created Success | |- Verifying alpha snapshot resources | |--\u003e Verifying that alpha snapshot CRDs are not installed Success | |- Verifying sshpass installation.. | |- Verifying iSCSI installation Enter the root password of 10.**.**.**: Enter the root password of 10.**.**.**: Success | |- Verifying snapshot support | |--\u003e Verifying that snapshot CRDs are available Success | |--\u003e Verifying that the snapshot controller is available Success | |- Verifying helm version Success | |- Verifying helm values version Success ------------------------------------------------------ \u003e Verification Complete - Success ------------------------------------------------------ | |- Installing Driver Success | |--\u003e Waiting for Deployment unity-controller to be ready Success | |--\u003e Waiting for DaemonSet unity-node to be ready Success ------------------------------------------------------ \u003e Operation complete ------------------------------------------------------ Results:\nAt the end of the script unity-controller Deployment and DaemonSet unity-node will be ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\n One or more Unity XT Controllers (based on controllerCount) with 5/5 containers ready, and status displayed as Running. Agent pods with 2/2 containers and the status displayed as Running.  Note: To install nightly or latest csi driver build using bash script use this command: /csi-install.sh --namespace unity --values ./myvalues.yaml --version nightly/latest\n  You can also install the driver using standalone helm chart by running helm install command, first using the –dry-run flag to confirm various parameters are as desired. Once the parameters are validated, run the command without the –dry-run flag. Note: This example assumes that the user is at repo root helm folder i.e csi-unity/helm.\nSyntax:helm install --dry-run --values \u003cmyvalues.yaml location\u003e --namespace \u003cnamespace\u003e \u003cname of secret\u003e \u003chelmPath\u003e  \u003cnamespace\u003e - namespace of the driver installation.  \u003cname of secret\u003e - unity in case of unity-creds and unity-certs-0 secrets.  \u003chelmPath\u003e - Path of the helm directory.  e.g: helm install –dry-run –values ./csi-unity/myvalues.yaml –namespace unity unity ./csi-unity\n  Certificate validation for Unisphere REST API calls This topic provides details about setting up the Dell Unity XT certificate validation for the CSI Driver.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on the “.Values.certSecretCount” parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nIf this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\n  To fetch the certificate, run the following command. openssl s_client -showcerts -connect \u003cUnisphere IP:Port\u003e \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u003c/dev/null 2\u003e/dev/null | openssl x509 -outform PEM \u003e ca_cert_0.pem\n  Run the following command to create the cert secret with index ‘0’: kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret: kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -\n  Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)\nNote:\n  “unity” is the namespace for helm-based installation but namespace can be user-defined in operator-based installation.\n  User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to Kubernetes secret size limitation.\n  Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\n    Volume Snapshot Class A wide set of annotated storage class manifests have been provided in the csi-unity/samples/volumesnapshotclass/ folder. Use these samples to create new Volume Snapshot to provision storage.\nWhat happens to my existing Volume Snapshot Classes? Upgrading from CSI Unity XT v2.1.0 driver: The existing volume snapshot class will be retained.\nUpgrading from an older version of the driver: It is strongly recommended to upgrade the earlier versions of CSI Unity XT to v1.6.0 or higher, before upgrading to v2.3.0.\nStorage Classes Storage Classes are an essential Kubernetes construct for Storage provisioning. To know more about Storage Classes, refer to https://kubernetes.io/docs/concepts/storage/storage-classes/\nA wide set of annotated storage class manifests have been provided in the samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nFor the Unity XT CSI Driver, a wide set of annotated storage class manifests have been provided in the csi-unity/samples/storageclass folder. Use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from an older version of the driver: The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation “helm.sh/resource-policy”: keep before performing an upgrade.\n Note: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n Steps to create storage class: There are samples storage class yaml files available under csi-unity/samples/storageclass. These can be copied and modified as needed.\n Pick any of unity-fc.yaml, unity-iscsi.yaml or unity-nfs.yaml Copy the file as unity-\u003cARRAY_ID\u003e-fc.yaml, unity-\u003cARRAY_ID\u003e-iscsi.yaml or unity-\u003cARRAY_ID\u003e-nfs.yaml Replace \u003cARRAY_ID\u003e with the Array Id of the Unity Array to be used Replace \u003cSTORAGE_POOL\u003e with the storage pool you have Replace \u003cTIERING_POLICY\u003e with the Tiering policy that is to be used for provisioning Replace \u003cHOST_IO_LIMIT_NAME\u003e with the Host IO Limit Name that is to be used for provisioning Replace \u003cmountOption1\u003e with the necessary mount options. If not required, this can be removed from the storage class Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f unity-\u003cARRAY_ID\u003e-fc.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-iscsi.yaml or kubectl create -f unity-\u003cARRAY_ID\u003e-nfs.yaml  Note:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \"\u003csc-name\u003e\" with kind StorageClass: StorageClass.storage.k8s.io \"\u003csc-name\u003e\" is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You cannot provision new PVCs until at least one storage class is newly created.\nDynamically update the unity-creds secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the “Host” information in an array. User can update secret using the following command:\n kubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run=client | kubectl replace -f - Note: Updating unity-certs-x secrets is a manual process, unlike unity-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\nDynamic Logging Configuration Helm based installation As part of driver installation, a ConfigMap with the name unity-config-params is created, which contains an attribute CSI_LOG_LEVEL which specifies the current log level of CSI driver.\nUsers can set the default log level by specifying log level to logLevel attribute in values.yaml during driver installation.\nTo change the log level dynamically to a different value user can edit the same values.yaml, and run the following command\ncd dell-csi-helm-installer ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade Note: myvalues.yaml is a values.yaml file which user has used for driver installation.\n","excerpt":"The CSI Driver for Dell Unity XT can be deployed by using the provided …","ref":"/csm-docs/docs/csidriver/installation/helm/unity/","title":"Unity XT"},{"body":"CSI Driver for Unity XT Pre-requisites Create secret to store Unity XT credentials Create a namespace called unity (it can be any user-defined name; But commands in this section assumes that the namespace is unity) Prepare the secret.yaml for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing Unity XT system true -   password Password for accessing Unity XT system true -   restGateway REST API gateway HTTPS endpoint Unity XT system true -   arrayId ArrayID for Unity XT system true -   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. true -    Ex: secret.yaml\nstorageArrayList:- arrayId:\"APM00******1\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.1/\"skipCertificateValidation:trueisDefault:true- arrayId:\"APM00******2\"username:\"user\"password:\"password\"endpoint:\"https://10.1.1.2/\"skipCertificateValidation:truekubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml\nUse the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.yaml -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the YAML syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the YAML file.\nCreate secret for client side TLS verification Please refer detailed documentation on how to create this secret here\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:unity-certs-0namespace:unitytype:Opaquedata:cert-0:\"\"Execute command: kubectl create -f empty-secret.yaml\nModify/Set the following optional environment variables Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT Specifies the HTTP endpoint for Unity XT. No /var/run/csi/csi.sock   X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS Flag to enable multiple pods use same pvc on same node with RWO access mode No false   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true   X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Controller plugin No    Node parameters      X_CSI_MODE Driver starting mode No node   X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands No /noderoot   X_CSI_HEALTH_MONITOR_ENABLED Enable/Disable health monitor of CSI volumes from Node plugin No     Example CR for Unity XT Refer samples from here. Below is an example CR:\napiVersion:storage.dell.com/v1kind:CSIUnitymetadata:name:test-unitynamespace:test-unityspec:driver:configVersion:v2.3.0replicas:2dnsPolicy:ClusterFirstWithHostNetforceUpdate:falsecommon:image:\"dellemc/csi-unity:v2.3.0\"imagePullPolicy:IfNotPresentsideCars:- name:provisionerargs:[\"--volume-name-prefix=csiunity\",\"--default-fstype=ext4\"]- name:snapshotterargs:[\"--snapshot-name-prefix=csiunitysnap\"]# Enable/Disable health monitor of CSI volumes from node plugin. Provides details of volume usage.# - name: external-health-monitor# args: [\"--monitor-interval=60s\"] controller:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin - volume condition.# Install the 'external-health-monitor' sidecar accordingly.# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"# nodeSelector: Define node selection constraints for controller pods.# For the pod to be eligible to run on a node, the node must have each# of the indicated key-value pairs as labels.# Leave as blank to consider all nodes# Allowed values: map of key-value pairs# Default value: NonenodeSelector:# Uncomment if nodes you wish to use have the node-role.kubernetes. io/control-plane taint# node-role.kubernetes.io/control-plane: \"\"# tolerations: Define tolerations for the controllers, if required.# Leave as blank to install controller on worker nodes# Default value: Nonetolerations:# Uncomment if nodes you wish to use have the node-role.kubernetes.io/control-plane taint# - key: \"node-role.kubernetes.io/control-plane\"# operator: \"Exists\"# effect: \"NoSchedule\"node:envs:# X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage# Allowed values:# true: enable checking of health condition of CSI volumes# false: disable checking of health condition of CSI volumes# Default value: false- name:X_CSI_HEALTH_MONITOR_ENABLEDvalue:\"false\"# nodeSelector: Define node selection constraints for node pods.# For the pod to be eligible to run on a node, the node must have each# of the indicated key-value pairs as labels.# Leave as blank to consider all nodes# Allowed values: map of key-value pairs# Default value: NonenodeSelector:# Uncomment if nodes you wish to use have the node-role.kubernetes.io/control-plane taint# node-role.kubernetes.io/control-plane: \"\"# tolerations: Define tolerations for the node daemonset, if required.# Default value: Nonetolerations:# Uncomment if nodes you wish to use have the node-role.kubernetes.io/control-plane taint# - key: \"node-role.kubernetes.io/control-plane\"# operator: \"Exists\"# effect: \"NoSchedule\"# - key: \"node.kubernetes.io/memory-pressure\"# operator: \"Exists\"# effect: \"NoExecute\"# - key: \"node.kubernetes.io/disk-pressure\"# operator: \"Exists\"# effect: \"NoExecute\"# - key: \"node.kubernetes.io/network-unavailable\"# operator: \"Exists\"# effect: \"NoExecute\"---apiVersion:v1kind:ConfigMapmetadata:name:unity-config-paramsnamespace:test-unitydata:driver-config-params.yaml:| CSI_LOG_LEVEL: \"info\"ALLOW_RWO_MULTIPOD_ACCESS:\"false\"MAX_UNITY_VOLUMES_PER_NODE:\"0\"SYNC_NODE_INFO_TIME_INTERVAL:\"15\"TENANT_NAME:\"\"Dynamic Logging Configuration Operator based installation As part of driver installation, a ConfigMap with the name unity-config-params is created using the manifest located in the sample file. This ConfigMap contains an attribute CSI_LOG_LEVEL which specifies the current log level of the CSI driver. To set the default/initial log level user can set this field during driver installation.\nTo update the log level dynamically user has to edit the ConfigMap unity-config-params and update CSI_LOG_LEVEL to the desired log level.\nkubectl edit configmap -n unity unity-config-params Note :\n The log level is not allowed to be updated dynamically through logLevel attribute in the secret object. “Kubelet config dir path” is not yet configurable in case of Operator based driver installation. Also, snapshotter and resizer sidecars are not optional to choose, it comes default with Driver installation.  Volume Health Monitoring This feature is introduced in CSI Driver for Unity XT version v2.1.0.\nOperator based installation Volume Health Monitoring feature is optional and by default this feature is disabled for drivers when installed via operator. To enable this feature, add the below block to the driver manifest before installing the driver. This ensures to install external health monitor sidecar. To get the volume health state value under controller should be set to true as seen below. To get the volume stats value under node should be set to true.\n # Uncomment the following to install 'external-health-monitor' sidecar to enable health monitor of CSI volumes from Controller plugin. # Also set the env variable controller.envs.X_CSI_ENABLE_VOL_HEALTH_MONITOR to \"true\". # - name: external-health-monitor # args: [\"--monitor-interval=60s\"] controller: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from Controller plugin- volume status, volume condition. # Install the 'external-health-monitor' sidecar accordingly. # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" node: envs: # X_CSI_HEALTH_MONITOR_ENABLED: Enable/Disable health monitor of CSI volumes from node plugin - volume usage # Allowed values: # true: enable checking of health condition of CSI volumes # false: disable checking of health condition of CSI volumes # Default value: false - name: X_CSI_HEALTH_MONITOR_ENABLED value: \"false\" ","excerpt":"CSI Driver for Unity XT Pre-requisites Create secret to store Unity XT …","ref":"/csm-docs/docs/csidriver/installation/operator/unity/","title":"Unity XT"},{"body":"Test deploying a simple Pod and Pvc with Unity XT storage In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes and automatically mounts them to the pod.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./test/sample.yaml   You can find all the created resources in test-unity namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n test-unity If it is in CrashLoopback state then the driver installation was not successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./test/sample.yaml   Support for SLES 15 SP2 The CSI Driver for Dell Unity XT requires the following set of packages installed on all worker nodes that run on SLES 15 SP2.\n open-iscsi open-iscsi is required in order to make use of iSCSI protocol for provisioning nfs-utils nfs-utils is required in order to make use of NFS protocol for provisioning multipath-tools multipath-tools is required in order to make use of FC and iSCSI protocols for provisioning  After installing open-iscsi, ensure “iscsi” and “iscsid” services have been started and /etc/isci/initiatorname.iscsi is created and has the host initiator id. The pre-requisites are mandatory for provisioning with the iSCSI protocol to work.\n","excerpt":"Test deploying a simple Pod and Pvc with Unity XT storage In the …","ref":"/csm-docs/docs/csidriver/installation/test/unity/","title":"Test Unity XT CSI Driver"},{"body":"Release Notes - CSI Unity XT v2.3.0 New Features/Changes  Removed beta volumesnapshotclass sample files. Added support for Kubernetes 1.24. Added support for OpenShift 4.10.  Fixed Issues CSM Resiliency: Occasional failure unmounting Unity volume for raw block devices via iSCSI.\nKnown Issues    Issue Workaround     Topology-related node labels are not removed automatically. Currently, when the driver is uninstalled, topology-related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released, remove the labels manually after the driver un-installation using command kubectl label node \u003cnode_name\u003e - - … Example: kubectl label node  csi-unity.dellemc.com/array123-iscsi- Note: there must be - at the end of each label to remove it.   NFS Clone - Resize of the snapshot is not supported by Unity XT Platform, however the user should never try to resize the cloned NFS volume. Currently, when the driver takes a clone of NFS volume, it succeeds but if the user tries to resize the NFS volumesnapshot, the driver will throw an error.   Delete namespace that has PVCs and pods created with the driver. The External health monitor sidecar crashes as a result of this operation. Deleting the namespace deletes the PVCs first and then removes the pods in the namespace. This brings a condition where pods exist without their PVCs and causes the external-health-monitor sidecar to crash. This is a known issue and has been reported at https://github.com/kubernetes-csi/external-health-monitor/issues/100   When a node goes down, the block volumes attached to the node cannot be attached to another node This is a known issue and has been reported at https://github.com/kubernetes-csi/external-attacher/issues/215. Workaround: 1. Force delete the pod running on the node that went down 2. Delete the VolumeAttachment to the node that went down. Now the volume can be attached to the new node.    Note:  Support for Kubernetes alpha features like Volume Health Monitoring and RWOP (ReadWriteOncePod) access mode will not be available in Openshift environment as Openshift doesn’t support enabling of alpha features for Production Grade clusters.  ","excerpt":"Release Notes - CSI Unity XT v2.3.0 New Features/Changes  Removed beta …","ref":"/csm-docs/docs/csidriver/release/unity/","title":"Unity XT"},{"body":"    Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods unity-controller-\u003csuffix\u003e –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   The kubectl logs -n unity unity-node-\u003csuffix\u003e driver logs show that the driver can’t connect to Unity XT - Authentication failure. Check if you have created a secret with correct credentials   fsGroup specified in pod spec is not reflected in files or directories at mounted path of volume. fsType of PVC must be set for fsGroup to work. fsType can be specified while creating a storage class. For NFS protocol, fsType can be specified as nfs. fsGroup doesn’t work for ephemeral inline volumes.   Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver controller and node pod should be restarted with command kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}'| xargs kubectl delete -n unity pod when topology-based storage classes are used. For dynamic array addition without topology, the driver will detect the newly added or removed arrays automatically   If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in the cluster but on array, it will still be present and marked for deletion. All the cloned PVC should be deleted in order to delete the source PVC from the array.   PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass “unity-iscsi”: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity –no-headers=true | awk ‘/unity-/{print $1}'| xargs kubectl delete -n unity pod   Driver install or upgrade fails because of an incompatible Kubernetes version, even though the version seems to be within the range of compatibility. For example: Error: UPGRADE FAILED: chart requires kubeVersion: \u003e= 1.21.0 \u003c 1.25.0 which is incompatible with Kubernetes V1.21.11-mirantis-1 If you are using an extended Kubernetes version, please see the helm Chart at helm/csi-unity/Chart.yaml and use the alternate kubeVersion check that is provided in the comments. Please note that this is not meant to be used to enable the use of pre-release alpha and beta versions, which is not supported.   When a node goes down, the block volumes attached to the node cannot be attached to another node 1. Force delete the pod running on the node that went down 2. Delete the VolumeAttachment to the node that went down. Now the volume can be attached to the new node.    ","excerpt":"    Symptoms Prevention, Resolution or Workaround     When you run the …","ref":"/csm-docs/docs/csidriver/troubleshooting/unity/","title":"Unity XT"},{"body":"The CSI Driver for Dell Unity XT and PowerScale supports VMware Tanzu. The deployment of these Tanzu clusters is done using the VMware Tanzu supervisor cluster and the supervisor namespace.\nCurrently, VMware Tanzu with normal configuration(without NAT) supports Kubernetes 1.20 and higher. The CSI driver can be installed on this cluster using Helm. Installation of CSI drivers in Tanzu via Operator has not been qualified.\nTo login to the Tanzu cluster, download kubectl and kubectl vsphere binaries to any of the system\nRefer: https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-0F6E45C4-3CB1-4562-9370-686668519FCA.html\nConnect to the VCenter using kubectl vSphere commands as shown below.\nkubectl vsphere login --insecure-skip-tls-verify --vsphere-username vSphere username --server=https://\u003ctanzu-server-ip\u003e/ -v 5  Once login is done to the Tanzu cluster, the installation of CSI driver is done using kubectl binary similar to how we do for other systems.\nTanzu example ","excerpt":"The CSI Driver for Dell Unity XT and PowerScale supports VMware Tanzu. …","ref":"/csm-docs/docs/csidriver/partners/tanzu/","title":"VMware Tanzu"},{"body":"The CSI Driver for Dell Unity and PowerScale supports VMware Tanzu and deployment of these Tanzu clusters is done using the VMware Tanzu supervisor cluster and supervisor namespace.\nCurrently, VMware Tanzu with normal configuration(without NAT) supports Kubernetes 1.20 and higher. The CSI driver can be installed on this cluster using Helm. Installation of CSI drivers in Tanzu via Operator has not been qualified.\nTo login to the Tanzu cluster, download kubectl and kubectl vsphere binaries to any of the system\nRefer: https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-0F6E45C4-3CB1-4562-9370-686668519FCA.html\nConnect to the VCenter using kubectl vSphere commands as shown below.\nkubectl vsphere login --insecure-skip-tls-verify --vsphere-username vSphere username --server=https://\u003ctanzu-server-ip\u003e/ -v 5  Once login is done to the Tanzu cluster, the installation of CSI driver is done using kubectl binary similar to how we do for other systems.\nTanzu example ","excerpt":"The CSI Driver for Dell Unity and PowerScale supports VMware Tanzu and …","ref":"/csm-docs/v1/csidriver/partners/tanzu/","title":"VMware Tanzu"},{"body":"The CSI Driver for Dell Unity and PowerScale supports VMware Tanzu and deployment of these Tanzu clusters is done using the VMware Tanzu supervisor cluster and supervisor namespace.\nCurrently, VMware Tanzu with normal configuration(without NAT) supports Kubernetes 1.20 and higher. The CSI driver can be installed on this cluster using Helm. Installation of CSI drivers in Tanzu via Operator has not been qualified.\nTo login to the Tanzu cluster, download kubectl and kubectl vsphere binaries to any of the system\nRefer: https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-0F6E45C4-3CB1-4562-9370-686668519FCA.html\nConnect to the VCenter using kubectl vSphere commands as shown below.\nkubectl vsphere login --insecure-skip-tls-verify --vsphere-username vSphere username --server=https://\u003ctanzu-server-ip\u003e/ -v 5  Once login is done to the Tanzu cluster, the installation of CSI driver is done using kubectl binary similar to how we do for other systems.\nTanzu example ","excerpt":"The CSI Driver for Dell Unity and PowerScale supports VMware Tanzu and …","ref":"/csm-docs/v2/csidriver/partners/tanzu/","title":"VMware Tanzu"},{"body":"The CSI Driver for Dell EMC Unity and PowerScale supports VMware Tanzu and deployment of these Tanzu clusters is done using the VMware Tanzu supervisor cluster and supervisor namespace.\nCurrently, VMware Tanzu with normal configuration(without NAT) supports Kubernetes 1.20 and higher. The CSI driver can be installed on this cluster using Helm. Installation of CSI drivers in Tanzu via Operator has not been qualified.\nTo login to the Tanzu cluster, download kubectl and kubectl vsphere binaries to any of the system\nRefer: https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-0F6E45C4-3CB1-4562-9370-686668519FCA.html\nConnect to the VCenter using kubectl vSphere commands as shown below.\nkubectl vsphere login --insecure-skip-tls-verify --vsphere-username vSphere username --server=https://\u003ctanzu-server-ip\u003e/ -v 5  Once login is done to the Tanzu cluster, the installation of CSI driver is done using kubectl binary similar to how we do for other systems.\nTanzu example ","excerpt":"The CSI Driver for Dell EMC Unity and PowerScale supports VMware Tanzu …","ref":"/csm-docs/v3/csidriver/partners/tanzu/","title":"VMware Tanzu"}]